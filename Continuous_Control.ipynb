{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: './python': Expected package name at the start of dependency specifier\n",
      "    ./python\n",
      "    ^\n",
      "Hint: It looks like a path. File './python' does not exist.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/Managed'\n",
      "Mono config path = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/MonoBleedingEdge/etc'\n",
      "Starting managed debugger on port 56506\n",
      "Using monoOptions --debugger-agent=transport=dt_socket,embedding=1,server=y,suspend=n,address=0.0.0.0:56506\n",
      "Preloaded 'lib_burst_generated.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Found 1 interfaces on host : 0) 172.18.0.2\n",
      "Player connection [140258690236288] Multi-casting \"[IP] 172.18.0.2 [Port] 55000 [Flags] 2 [Guid] 98628506 [EditorId] 2560969981 [Version] 1048832 [Id] LinuxServer(43,172.18.0.2) [Debug] 1 [PackageName] LinuxServer [ProjectName] robotics_reaching_environment\" to [225.0.0.222:54997]...\n",
      "\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "Initialize engine version: 2022.3.36f1 (95a4219250e5)\n",
      "[Subsystems] Discovering subsystems at path /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; threaded=0; jobified=0\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Loaded All Assemblies, in  0.124 seconds\n",
      "- Finished resetting the current domain, in  0.003 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "There is no texture data available to upload.\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "UnloadTime: 0.861143 ms\n"
     ]
    }
   ],
   "source": [
    "import mlagents\n",
    "import mlagents_envs\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux.x86_64', seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Behaviour name: HandAgent?team=0\n"
     ]
    }
   ],
   "source": [
    "# Start the environment\n",
    "env.reset()\n",
    "\n",
    "# Get behaviour names\n",
    "behaviour_names = env.behavior_specs.keys()\n",
    "behaviour_name = list(env.behavior_specs.keys())[0]\n",
    "print(f\"Behaviour name: {behaviour_name}\")\n",
    "\n",
    "# # get the default brain\n",
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Printing decisionSteps:  <mlagents_envs.base_env.DecisionSteps object at 0x7fa63808b970>\n",
      "Number of agents: 20\n",
      "Continuous action size: 4\n",
      "Discrete action size: 0\n",
      "There are 20 agents. Each observes a state with length: 52\n",
      "The state for the first agent looks like: [ 1.9981155e+00  1.0000000e+00  5.9970856e-02  0.0000000e+00\n",
      " -1.5707318e-02 -0.0000000e+00  9.9987668e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  4.3590826e-01  0.0000000e+00\n",
      " -1.0999048e-10 -4.4527781e-17  5.8420269e-15  1.0000000e+00\n",
      " -1.1516385e-06 -3.2951250e+00 -1.8607207e-09  1.1294162e-09\n",
      " -4.1197904e-15  2.2469334e-13  0.0000000e+00  3.5122869e+00\n",
      "  0.0000000e+00  1.2247365e-09  1.4922823e-16  4.2405148e-08\n",
      "  1.0000000e+00  2.8249647e-06 -1.4555795e+00 -1.2519078e-07\n",
      " -2.2184851e-07  1.3806863e-14 -2.6890327e-06  0.0000000e+00\n",
      "  1.5322870e+00  0.0000000e+00  1.3035524e-10 -2.4315641e-17\n",
      " -0.0000000e+00  1.0000000e+00  1.1615396e-06 -1.4555788e+00\n",
      "  0.0000000e+00  7.1078759e-08  1.6745450e-14  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Get the decsion and terminal steps\n",
    "decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "print(\"Printing decisionSteps: \", decisionSteps)\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(decisionSteps)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# size of each action\n",
    "behaviour_spec = env.behavior_specs[behaviour_name]\n",
    "action_spec = behaviour_spec.action_spec\n",
    "\n",
    "# continuious and discrete action size\n",
    "continuious_action_size = env.behavior_specs[behaviour_name].action_spec.continuous_size\n",
    "discrete_actions_size = env.behavior_specs[behaviour_name].action_spec.discrete_size\n",
    "print(f\"Continuous action size: {continuious_action_size}\")\n",
    "print(f\"Discrete action size: {discrete_actions_size}\")\n",
    "\n",
    "#examine the state space\n",
    "states = decisionSteps.obs[0]\n",
    "state_size = states.shape[1] if states.ndim > 1 else states.shape[0]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decisionSteps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlagents_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActionTuple\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()                                                                    \u001b[38;5;66;03m# reset the environment \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m stateVector \u001b[38;5;241m=\u001b[39m \u001b[43mdecisionSteps\u001b[49m\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;241m0\u001b[39m]                                             \u001b[38;5;66;03m# get the current state (for each agent)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_agents)                                                  \u001b[38;5;66;03m# initialize the score (for each agent)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decisionSteps' is not defined"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "env.reset()                                                                    # reset the environment \n",
    "stateVector = decisionSteps.obs[0]                                             # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                                                  # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, continuious_action_size)             # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1) \n",
    "    action_tuple = ActionTuple(continuous=actions)                                      # all actions between -1 and 1\n",
    "    env.set_actions(behavior_name=behaviour_name, action=actions)               # send all actions to tne environment\n",
    "    env.step()                                                                 # step the environment to get the next states\n",
    "    decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)  # get next state (for each agent)\n",
    "    next_states = decisionSteps.obs[0]                                         # get next state (for each agent)\n",
    "    rewards = decisionSteps.reward                                             # get reward (for each agent)\n",
    "    dones = [agent_id in terminalSteps for agent_id in decisionSteps.agent_id] # see if episode finished\n",
    "    scores += rewards                                                          # update the score (for each agent)\n",
    "    stateVector = next_states                                                  # roll over states to next time step\n",
    "    if np.any(dones):                                                          # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "PlayerConnection::CleanupMemory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 1 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AUDIO_FMOD mixer thread]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AUDIO_FMOD stream thread]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 199 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 134.9 KB\n",
      "      Overflow Count 4\n",
      "    [ALLOC_TEMP_Background Job.Worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Profiler.Dispatcher]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 11\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 6\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 128 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_MEMORYPROFILER]\n",
      "  Peak usage frame count: [64.0 KB-128.0 KB]: 2 frames\n",
      "  Requested Block Size 1.0 MB\n",
      "  Peak Block count 1\n",
      "  Peak Allocated memory 103.9 KB\n",
      "  Peak Large allocation bytes 0 B\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 27\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [8.0 MB-16.0 MB]: 2 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 13.1 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 2 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 1 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 66.8 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 2 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 64.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 2 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [256.0 KB-0.5 MB]: 1 frames, [1.0 MB-2.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 1.1 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 2 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 104 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [0-1.0 KB]: 2 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 0\n",
      "      Peak Allocated memory 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_PROFILER]\n",
      "  Peak usage frame count: [32.0 KB-64.0 KB]: 2 frames\n",
      "  Requested Block Size 16.0 MB\n",
      "  Peak Block count 1\n",
      "  Peak Allocated memory 38.9 KB\n",
      "  Peak Large allocation bytes 0 B\n",
      "    [ALLOC_PROFILER_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 0.6 KB\n",
      "##utp:{\"type\":\"MemoryLeaks\",\"version\":2,\"phase\":\"Immediate\",\"time\":1723639613227,\"processId\":699310,\"allocatedMemory\":2333795,\"memoryLabels\":[{\"Default\":18969},{\"Permanent\":1248},{\"NewDelete\":22787},{\"Thread\":34676},{\"Manager\":16427},{\"VertexData\":12},{\"Geometry\":280},{\"Texture\":16},{\"Shader\":68914},{\"Material\":24},{\"GfxDevice\":33536},{\"Animation\":344},{\"Audio\":3976},{\"FontEngine\":336},{\"Physics\":512},{\"Serialization\":276},{\"Input\":8960},{\"JobScheduler\":33008},{\"Mono\":40},{\"ScriptingNativeRuntime\":248},{\"BaseObject\":1617444},{\"Resource\":528},{\"Renderer\":2352},{\"Transform\":48},{\"File\":800},{\"WebCam\":40},{\"Culling\":40},{\"Terrain\":953},{\"Wind\":24},{\"String\":5534},{\"DynamicArray\":33008},{\"HashMap\":7680},{\"Utility\":285380},{\"PoolAlloc\":1128},{\"TypeTree\":4176},{\"ScriptManager\":80},{\"RuntimeInitializeOnLoadManager\":80},{\"SpriteAtlas\":128},{\"GI\":3584},{\"Director\":7872},{\"WebRequest\":720},{\"VR\":45529},{\"SceneManager\":424},{\"Video\":32},{\"LazyScriptCache\":40},{\"NativeArray\":12},{\"Camera\":25},{\"Secure\":1},{\"SerializationCache\":1576},{\"APIUpdating\":11856},{\"Subsystems\":392},{\"VirtualTexturing\":57552},{\"CoreBusinessMetrics\":128},{\"AssetReference\":40}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debugger-agent: Unable to listen on 6\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. DDPG Algorithm training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed'\n",
      "Mono config path = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'lib_burst_generated.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "Initialize engine version: 2022.3.36f1 (95a4219250e5)\n",
      "[Subsystems] Discovering subsystems at path /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; threaded=0; jobified=0\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Assembly-CSharp.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Assembly-CSharp.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Burst.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Burst.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.ONNX.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.ONNX.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.Core.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.Core.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.BurstBLAS.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Barracuda.BurstBLAS.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/UnityEngine.UI.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/UnityEngine.UI.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.Flow.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.Flow.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Timeline.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Timeline.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Animation.Rigging.DocCodeExamples.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Animation.Rigging.DocCodeExamples.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.ML-Agents.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.ML-Agents.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.ML-Agents.CommunicatorObjects.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.ML-Agents.CommunicatorObjects.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Mathematics.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Mathematics.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.TextMeshPro.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.TextMeshPro.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.State.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.VisualScripting.State.dll\n",
      "Symbol file /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Animation.Rigging.pdb doesn't match image /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux_Data/Managed/Unity.Animation.Rigging.dll\n",
      "- Loaded All Assemblies, in  0.117 seconds\n",
      "- Finished resetting the current domain, in  0.003 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "There is no texture data available to upload.\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "UnloadTime: 1.020102 ms\n",
      "Behaviour name: HandAgent?team=0\n",
      "Behaviour specifications: Continuous: 4, Discrete: ()\n",
      "Number of agents: 20\n",
      "Enter ddpg...\n",
      "\n",
      "Episode number: 1\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7f2d8359c880>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8641491   1.          0.7027321  ...  0.24725208  6.287971\n",
      "  -0.09081976]\n",
      " [ 1.8641491   1.          0.7027216  ...  0.08213184  6.569253\n",
      "  -0.03001319]\n",
      " [ 1.8641624   1.          0.7027312  ...  0.01196317  6.598877\n",
      "   0.0115566 ]\n",
      " ...\n",
      " [ 1.8641624   1.          0.7027216  ... -0.03386811  6.546505\n",
      "   0.05103837]\n",
      " [ 1.8641624   1.          0.7027321  ...  0.25085655  6.2647753\n",
      "  -0.0829085 ]\n",
      " [ 1.8641491   1.          0.7027321  ...  0.58832276  5.2406087\n",
      "  -0.17138998]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5615988    1.           1.21805    ...  -0.2558785  -11.98711\n",
      "   -1.261054  ]\n",
      " [  1.5615988    1.           1.2180662  ...  -0.37953115  -6.3564014\n",
      "   -0.41830662]\n",
      " [  1.5615711    1.           1.2180706  ...   0.03095953   3.4296768\n",
      "    0.07704765]\n",
      " ...\n",
      " [  1.5615711    1.           1.2180662  ...   0.28987813  -1.538743\n",
      "    0.5689096 ]\n",
      " [  1.5615711    1.           1.21805    ...  -0.12576294 -11.699964\n",
      "   -1.2579818 ]\n",
      " [  1.5615988    1.           1.21805    ...   0.2587697   -7.0016503\n",
      "   -1.0490928 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1162643   1.          1.6170254  ...  1.1548433  -3.1523588\n",
      "   0.33049166]\n",
      " [ 1.1162653   1.          1.6170006  ...  0.57365537 -1.8335223\n",
      "   0.14580806]\n",
      " [ 1.1162186   1.          1.6170149  ...  0.35605404  4.124557\n",
      "   0.39967403]\n",
      " ...\n",
      " [ 1.1162167   1.          1.6169977  ... -0.15944752 -2.867932\n",
      "   0.54039824]\n",
      " [ 1.1162186   1.          1.6170254  ...  0.55233955 -3.9648247\n",
      "   0.53790355]\n",
      " [ 1.1162643   1.          1.6170254  ...  0.5332942  -1.8393297\n",
      "   0.71764433]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.57091904  1.          1.861723   ...  1.283071   -2.7525325\n",
      "  -0.29465014]\n",
      " [ 0.57092     1.          1.8617     ...  0.4314264  -2.6151009\n",
      "   0.08708781]\n",
      " [ 0.57086945  1.          1.8617234  ... -0.7774817  13.190447\n",
      "   0.0656305 ]\n",
      " ...\n",
      " [ 0.57086754  1.          1.8616953  ... -1.7137634   0.8900628\n",
      "  -0.7129005 ]\n",
      " [ 0.57086945  1.          1.8617191  ...  0.4367326  -8.3107815\n",
      "  -0.7872778 ]\n",
      " [ 0.57091904  1.          1.861723   ...  0.55671    -1.6641579\n",
      "  -0.36179137]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.02248287  1.          1.9293995  ...  0.28368998 -3.212747\n",
      "   0.38542402]\n",
      " [-0.02248192  1.          1.9293823  ...  0.26420254  1.5656409\n",
      "   0.68985105]\n",
      " [-0.02248955  1.          1.9293981  ... -1.3973763  -9.536946\n",
      "  -0.10483754]\n",
      " ...\n",
      " [-0.02249146  1.          1.9293718  ...  1.1595918   0.17665267\n",
      "   0.9566879 ]\n",
      " [-0.02248955  1.          1.929388   ...  0.78611314 -2.8408048\n",
      "  -0.27161232]\n",
      " [-0.02248287  1.          1.9293995  ... -0.07581711 -0.91864777\n",
      "   0.14107633]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.6090536    1.           1.8136578  ...   0.06987095  -2.63125\n",
      "   -0.03708291]\n",
      " [ -0.60905266   1.           1.8136282  ...  -2.4035697    5.5466137\n",
      "    1.1025345 ]\n",
      " [ -0.60907364   1.           1.8136331  ...  -1.04012    -12.022743\n",
      "    1.2996881 ]\n",
      " ...\n",
      " [ -0.60907364   1.           1.8136082  ...   0.6198939    2.773078\n",
      "    0.8246888 ]\n",
      " [ -0.60907364   1.           1.8136387  ...   1.0707356  -14.868491\n",
      "    0.5985965 ]\n",
      " [ -0.6090536    1.           1.8136578  ...   1.1964438   -4.269419\n",
      "   -0.25770342]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.1324539    1.           1.5249825  ...   0.09905434   1.0859365\n",
      "    0.08424723]\n",
      " [ -1.132453     1.           1.5249405  ...   4.8831277  -13.357943\n",
      "    1.2982507 ]\n",
      " [ -1.1324196    1.           1.5249375  ...   0.33366346   7.6237817\n",
      "    0.8245584 ]\n",
      " ...\n",
      " [ -1.1324139    1.           1.5249138  ...   0.05055535   0.65749633\n",
      "    1.2929773 ]\n",
      " [ -1.1324196    1.           1.5249615  ...   2.314872   -10.840627\n",
      "   -0.79797596]\n",
      " [ -1.1324539    1.           1.5249825  ...   2.563995    -3.9536169\n",
      "   -1.4333262 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5424528   1.          1.0905495  ...  3.373534   -4.0001016\n",
      "  -0.24842377]\n",
      " [-1.5424538   1.          1.0905323  ...  1.2522852  -3.8169346\n",
      "  -0.23446073]\n",
      " [-1.5423832   1.          1.090514   ...  0.24846968  4.8671637\n",
      "  -0.31449962]\n",
      " ...\n",
      " [-1.5423775   1.          1.0904999  ...  2.1320982  -2.8678288\n",
      "   0.31114185]\n",
      " [-1.542387    1.          1.0905285  ...  0.01318255 -6.900633\n",
      "   1.5115377 ]\n",
      " [-1.5424528   1.          1.0905495  ... -1.1656773   4.774885\n",
      "  -1.0805529 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.4 0.  0.4 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7998285e+00  1.0000000e+00  5.5118370e-01 ...  3.5287118e-01\n",
      "  -1.1506920e+00 -2.9563272e-01]\n",
      " [-1.7998266e+00  1.0000000e+00  5.5115891e-01 ...  3.7556319e+00\n",
      "  -1.0881758e+01 -3.4598958e-01]\n",
      " [-1.7997646e+00  1.0000000e+00  5.5113077e-01 ... -9.7492456e-02\n",
      "   9.8103914e+00  1.0690145e+00]\n",
      " ...\n",
      " [-1.7997570e+00  1.0000000e+00  5.5112362e-01 ...  9.4431343e+00\n",
      "  -8.7465916e+00 -8.9346218e-01]\n",
      " [-1.7997684e+00  1.0000000e+00  5.5116272e-01 ... -2.4403772e+00\n",
      "   2.7613640e-03  7.5879931e-01]\n",
      " [-1.7998285e+00  1.0000000e+00  5.5118370e-01 ... -1.4642617e+00\n",
      "   6.7859583e+00  6.7186737e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.5       0.        0.        0.\n",
      " 0.        0.6       0.        0.        0.        1.0000001 0.\n",
      " 0.8000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.8788433    1.          -0.04117393 ...   4.7078366   -8.269477\n",
      "   -1.0463021 ]\n",
      " [ -1.8788395    1.          -0.04121113 ...   0.41689014   0.20221424\n",
      "   -0.02131903]\n",
      " [ -1.878809     1.          -0.04124693 ...  -0.5860056  -10.586531\n",
      "    0.5125435 ]\n",
      " ...\n",
      " [ -1.8788013    1.          -0.04124641 ...  -3.3882313    4.384348\n",
      "    0.7922218 ]\n",
      " [ -1.8788185    1.          -0.04119492 ...   0.53526545  -0.52281475\n",
      "    0.4229319 ]\n",
      " [ -1.8788433    1.          -0.04117393 ...  -1.7508361   11.008213\n",
      "   -0.5040754 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.0000001  0.         0.         0.70000005\n",
      " 0.5        0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.7711201    1.          -0.6291199  ...   0.19567353  -5.814984\n",
      "   -2.1074808 ]\n",
      " [ -1.7711096    1.          -0.6291628  ...   0.1187247   -0.7384124\n",
      "   -0.07239276]\n",
      " [ -1.7710629    1.          -0.62918377 ...   0.34950513 -11.5373745\n",
      "    0.13998812]\n",
      " ...\n",
      " [ -1.7710533    1.          -0.6291971  ...  -3.0989418    1.9674133\n",
      "    0.41787872]\n",
      " [ -1.7710781    1.          -0.62914085 ...  -0.0488534    1.7065568\n",
      "    0.17369568]\n",
      " [ -1.7711201    1.          -0.6291199  ...   1.830084    -1.9791117\n",
      "   -0.616927  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.9000001  0.         0.         0.5\n",
      " 0.         0.         0.         0.         0.70000005 0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4867163e+00  1.0000000e+00 -1.1543522e+00 ...  1.7223036e-01\n",
      "  -3.1709633e+00 -1.4824851e+00]\n",
      " [-1.4867029e+00  1.0000000e+00 -1.1544275e+00 ... -7.7524424e-02\n",
      "  -2.5499201e-01  3.9229095e-03]\n",
      " [-1.4866180e+00  1.0000000e+00 -1.1544230e+00 ...  7.3097557e-01\n",
      "   1.0835121e+01  4.9779201e-01]\n",
      " ...\n",
      " [-1.4866047e+00  1.0000000e+00 -1.1544571e+00 ... -1.4562982e+01\n",
      "   1.2957396e+01 -1.8921494e-03]\n",
      " [-1.4866524e+00  1.0000000e+00 -1.1543713e+00 ... -3.5476727e+00\n",
      "   7.4882293e+00 -2.8835845e-01]\n",
      " [-1.4867163e+00  1.0000000e+00 -1.1543522e+00 ... -6.6469479e-01\n",
      "  -1.7395697e+00 -3.1560767e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.053175     1.          -1.5658569  ...   1.8598509    3.9672093\n",
      "   -0.28531855]\n",
      " [ -1.0531559    1.          -1.5659094  ...   0.17545426  -1.5256462\n",
      "   -0.05159003]\n",
      " [ -1.0530128    1.          -1.5658957  ...  -0.18637037  10.758207\n",
      "    0.2904207 ]\n",
      " ...\n",
      " [ -1.0529995    1.          -1.5659304  ... -11.499482    10.533646\n",
      "   -0.16224656]\n",
      " [ -1.0530548    1.          -1.5658684  ...  -4.1940727    9.072968\n",
      "    0.08273131]\n",
      " [ -1.053175     1.          -1.5658569  ...  -0.83580923   0.79159164\n",
      "   -0.2909655 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.9000001 0.        0.        0.        0.        0.        0.5\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.513567    1.         -1.8220882  ...  3.677198    9.456563\n",
      "   0.06518015]\n",
      " [-0.51354504  1.         -1.822135   ... -0.60955507  2.952662\n",
      "   0.07406227]\n",
      " [-0.5133667   1.         -1.8220991  ...  0.4158956  10.608006\n",
      "   0.770894  ]\n",
      " ...\n",
      " [-0.51334953  1.         -1.8221397  ... -2.5439467   4.8852024\n",
      "  -1.4171697 ]\n",
      " [-0.51340866  1.         -1.8220825  ... -2.804267    6.2895207\n",
      "   0.09046565]\n",
      " [-0.513567    1.         -1.8220882  ...  1.1054521   6.4471073\n",
      "  -1.1765459 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.9124451e-02  1.0000000e+00 -1.8976250e+00 ... -3.8431840e+00\n",
      "  -3.3449082e+00 -8.4460020e-02]\n",
      " [ 7.9147339e-02  1.0000000e+00 -1.8976879e+00 ... -3.1982988e-01\n",
      "   5.4468174e+00 -1.1386069e+00]\n",
      " [ 7.9315186e-02  1.0000000e+00 -1.8976220e+00 ...  1.1398132e+00\n",
      "  -2.9061961e+00 -1.1618972e-02]\n",
      " ...\n",
      " [ 7.9332352e-02  1.0000000e+00 -1.8976574e+00 ... -4.5717707e+00\n",
      "   1.2365166e+01  9.0420872e-01]\n",
      " [ 7.9273224e-02  1.0000000e+00 -1.8975964e+00 ... -4.3463607e+00\n",
      "   1.7333662e+01 -1.1455312e+00]\n",
      " [ 7.9124451e-02  1.0000000e+00 -1.8976250e+00 ... -1.7620053e+00\n",
      "  -2.6163297e+00  5.2236550e-02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.6662626   1.         -1.7849579  ... -3.125689   -4.3588705\n",
      "   0.16597009]\n",
      " [ 0.66628456  1.         -1.7849817  ... -1.0212725  -2.3805957\n",
      "   0.14056236]\n",
      " [ 0.6664505   1.         -1.7848766  ...  0.85380316 -3.9814563\n",
      "   1.0866628 ]\n",
      " ...\n",
      " [ 0.66646767  1.         -1.7849207  ... -0.6807767  -3.7676857\n",
      "  -0.5386714 ]\n",
      " [ 0.66641426  1.         -1.784895   ... -1.4872539   7.0818777\n",
      "  -0.7340526 ]\n",
      " [ 0.6662626   1.         -1.7849579  ... -1.8093061  -4.8887906\n",
      "  -0.35206842]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.1886778    1.          -1.4956951  ...  -6.313218    -8.250554\n",
      "    1.5782654 ]\n",
      " [  1.188695     1.          -1.4956617  ...  -0.5308118   -1.4709816\n",
      "   -0.16179991]\n",
      " [  1.1888676    1.          -1.4955504  ...   6.0929604  -14.606407\n",
      "    0.54745615]\n",
      " ...\n",
      " [  1.1888828    1.          -1.4955759  ...  -1.2618667   -2.0661736\n",
      "   -0.9655311 ]\n",
      " [  1.1888428    1.          -1.4956112  ...   2.5264373   -5.6231365\n",
      "   -1.3008928 ]\n",
      " [  1.1886778    1.          -1.4956951  ...  -1.4966474   -8.281787\n",
      "    1.5194973 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5958958   1.         -1.0582066  ... -3.2582533  -4.056805\n",
      "   3.0381613 ]\n",
      " [ 1.5959072   1.         -1.058156   ...  0.68209666  2.9369638\n",
      "  -0.58395696]\n",
      " [ 1.5960293   1.         -1.0580224  ...  4.210167   -7.6014633\n",
      "   1.7359337 ]\n",
      " ...\n",
      " [ 1.5960445   1.         -1.0580416  ... -0.45934272 -1.1074657\n",
      "  -0.22276187]\n",
      " [ 1.5960102   1.         -1.0580997  ...  0.47925276  4.7795305\n",
      "   0.73472595]\n",
      " [ 1.5958958   1.         -1.0582066  ... -1.1112237  -6.68125\n",
      "   1.665406  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.1        0.         0.         0.\n",
      " 0.         0.6        0.         0.6        0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84722233e+00  1.00000000e+00 -5.16071320e-01 ... -5.53608179e-01\n",
      "   3.80003929e+00 -1.51824498e+00]\n",
      " [ 1.84722900e+00  1.00000000e+00 -5.16034126e-01 ...  2.23500085e+00\n",
      "   1.01191235e+01 -1.91456437e+00]\n",
      " [ 1.84731102e+00  1.00000000e+00 -5.15873373e-01 ...  7.38883257e+00\n",
      "  -1.39806385e+01  6.73439622e-01]\n",
      " ...\n",
      " [ 1.84732628e+00  1.00000000e+00 -5.15907288e-01 ... -4.06332970e-01\n",
      "  -6.95071697e-01 -3.30972672e-03]\n",
      " [ 1.84730721e+00  1.00000000e+00 -5.15947342e-01 ...  1.02739191e+00\n",
      "   3.27506685e+00 -4.44420218e-01]\n",
      " [ 1.84722233e+00  1.00000000e+00 -5.16071320e-01 ... -2.31173921e+00\n",
      "  -3.93666744e-01  5.57256699e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.8000001 0.        0.        0.\n",
      " 0.3       0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9182281e+00  1.0000000e+00  7.7007294e-02 ...  1.7316376e+00\n",
      "   8.0363913e+00 -1.2770543e+00]\n",
      " [ 1.9182234e+00  1.0000000e+00  7.7052116e-02 ... -9.5358539e-01\n",
      "  -3.4470968e+00 -1.9796625e-01]\n",
      " [ 1.9182720e+00  1.0000000e+00  7.7216648e-02 ...  4.0607581e+00\n",
      "  -1.0075174e+01  7.6575649e-01]\n",
      " ...\n",
      " [ 1.9182873e+00  1.0000000e+00  7.7186584e-02 ...  1.0341406e-01\n",
      "   1.4057112e-01  1.9199848e-03]\n",
      " [ 1.9182796e+00  1.0000000e+00  7.7136993e-02 ...  1.0950427e+00\n",
      "   3.4030929e+00  3.8725936e-01]\n",
      " [ 1.9182281e+00  1.0000000e+00  7.7007294e-02 ...  6.1716938e-01\n",
      "  -1.1203260e+00  1.1786089e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 1.0000001 0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8021431   1.          0.6633072  ...  0.7968669   4.61501\n",
      "   1.786527  ]\n",
      " [ 1.8021288   1.          0.66334915 ... -1.0492952  -3.8870835\n",
      "   0.1061427 ]\n",
      " [ 1.8021412   1.          0.6635021  ...  2.4733026  -6.5950494\n",
      "  -0.09571183]\n",
      " ...\n",
      " [ 1.8021564   1.          0.66347885 ...  0.57116795  0.702713\n",
      "   0.21821856]\n",
      " [ 1.8021622   1.          0.6634331  ...  1.5728741   5.696705\n",
      "   0.8489262 ]\n",
      " [ 1.8021431   1.          0.6633072  ...  0.44248605  1.8185701\n",
      "   0.4016869 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        1.0000001 1.0000001 0.        0.        0.\n",
      " 0.        0.        0.8000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.6      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5104523    1.           1.1850262  ...  -0.49703228   2.643243\n",
      "    1.0634875 ]\n",
      " [  1.5104294    1.           1.1850548  ...  -3.3487067   -9.719183\n",
      "    0.09387696]\n",
      " [  1.510395     1.           1.1851704  ...   3.8377757  -10.0776005\n",
      "   -0.11342323]\n",
      " ...\n",
      " [  1.5104141    1.           1.1851606  ...   1.6709023    1.8999848\n",
      "    0.18629241]\n",
      " [  1.5104256    1.           1.1851387  ...   4.6739173   10.284992\n",
      "    1.1024305 ]\n",
      " [  1.5104523    1.           1.1850262  ...  -5.446703    12.184559\n",
      "   -0.05326009]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 1.0000001 0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.        0.6\n",
      " 0.        0.        0.        0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0720129   1.          1.5910797  ... -5.4040422  10.069174\n",
      "   2.4278874 ]\n",
      " [ 1.0719795   1.          1.5911064  ... -0.15609884  0.95127964\n",
      "  -0.13223644]\n",
      " [ 1.0719509   1.          1.5911815  ...  0.51834595  2.7492833\n",
      "  -0.5835622 ]\n",
      " ...\n",
      " [ 1.0719719   1.          1.5911827  ...  5.241193   10.232576\n",
      "  -0.9901974 ]\n",
      " [ 1.0719872   1.          1.5911636  ... -0.73810744 -1.5888724\n",
      "   0.15516704]\n",
      " [ 1.0720129   1.          1.5910797  ... -1.9325923   1.348794\n",
      "  -1.3344047 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.4        0.         0.\n",
      " 0.9000001  0.         0.         0.         0.         0.5\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.1       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.5298567   1.          1.8419933  ...  5.419849   -5.5405464\n",
      "   0.16584691]\n",
      " [ 0.5298195   1.          1.8419781  ... -0.5111693  -0.24639416\n",
      "   0.14635459]\n",
      " [ 0.5297928   1.          1.8420148  ... -0.3654634   6.1724434\n",
      "  -0.6464511 ]\n",
      " ...\n",
      " [ 0.5298157   1.          1.8420238  ...  2.9185603  10.313917\n",
      "  -0.8170916 ]\n",
      " [ 0.52983284  1.          1.8420353  ... -0.96463764 -3.125339\n",
      "   0.04116419]\n",
      " [ 0.5298586   1.          1.8419933  ... -3.3743415   3.1598601\n",
      "  -1.3675294 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.06343174  1.          1.9134865  ...  8.820625   -8.709019\n",
      "  -1.5687798 ]\n",
      " [-0.06347084  1.          1.9134607  ... -0.34334147 -0.0964613\n",
      "   0.13410401]\n",
      " [-0.06351089  1.          1.9134685  ... -2.3037229  -6.6623354\n",
      "   0.22278637]\n",
      " ...\n",
      " [-0.06348801  1.          1.9134827  ...  2.338314    6.884301\n",
      "   1.1749946 ]\n",
      " [-0.06346893  1.          1.9134941  ...  1.1899376   3.6925669\n",
      "   0.35160357]\n",
      " [-0.06342983  1.          1.9134865  ... -2.7469363   1.9091783\n",
      "   0.38559914]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.64961815  1.          1.7985344  ...  1.8586047  -2.9876752\n",
      "  -0.32972574]\n",
      " [-0.64965534  1.          1.7985048  ...  0.2726426  -0.196486\n",
      "  -0.07501411]\n",
      " [-0.6497326   1.          1.7984807  ... -2.0353324  -8.67511\n",
      "  -0.16367114]\n",
      " ...\n",
      " [-0.6497116   1.          1.7985039  ...  0.32655632  1.1132503\n",
      "   0.09970087]\n",
      " [-0.64969444  1.          1.7985077  ...  0.3188983   9.673964\n",
      "   1.5603446 ]\n",
      " [-0.64961624  1.          1.7985363  ... -0.8014717   0.42236495\n",
      "  -1.2421541 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1721354   1.          1.5083332  ...  3.8181229  -6.4384904\n",
      "  -1.8021808 ]\n",
      " [-1.1721697   1.          1.5082779  ...  1.5647788   6.644946\n",
      "   0.8126517 ]\n",
      " [-1.1722012   1.          1.5082393  ...  1.5301094   2.7826707\n",
      "   0.28711984]\n",
      " ...\n",
      " [-1.1721859   1.          1.5082569  ... -2.3154922   5.100817\n",
      "   2.0990791 ]\n",
      " [-1.1721725   1.          1.5082874  ...  1.7682046  -6.669899\n",
      "   0.5641423 ]\n",
      " [-1.1721325   1.          1.5083351  ... -0.57597923  1.843648\n",
      "  -0.07976556]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5796013e+00  1.0000000e+00  1.0711441e+00 ...  2.1083934e+00\n",
      "  -4.2848349e+00 -6.0458994e-01]\n",
      " [-1.5796289e+00  1.0000000e+00  1.0710497e+00 ...  3.1889808e-01\n",
      "  -1.1304424e+01  2.1608019e-01]\n",
      " [-1.5796337e+00  1.0000000e+00  1.0710099e+00 ...  6.4632070e-01\n",
      "   7.0911360e-01 -8.4881574e-02]\n",
      " ...\n",
      " [-1.5796261e+00  1.0000000e+00  1.0710144e+00 ...  3.2739639e-03\n",
      "   1.7403221e+00 -7.2703528e-01]\n",
      " [-1.5796127e+00  1.0000000e+00  1.0710831e+00 ...  1.7475462e-01\n",
      "  -2.4506979e+00 -1.0089803e-01]\n",
      " [-1.5795984e+00  1.0000000e+00  1.0711441e+00 ... -5.4208899e-01\n",
      "   2.5158622e+00 -3.7630337e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.8322306    1.           0.52941513 ...   0.01766515  -3.8906763\n",
      "   -2.2664547 ]\n",
      " [ -1.832242     1.           0.52933407 ...  -0.8400004   -5.8400745\n",
      "   -0.4795787 ]\n",
      " [ -1.8322144    1.           0.5292853  ...   0.5186405   -0.01750755\n",
      "   -0.36198902]\n",
      " ...\n",
      " [ -1.8322105    1.           0.5292902  ...   4.0324774  -10.006602\n",
      "    0.3218968 ]\n",
      " [ -1.8322086    1.           0.52933884 ...   2.0695279   -9.248358\n",
      "   -1.0995524 ]\n",
      " [ -1.8322277    1.           0.52941513 ...   4.5407023   -3.6528497\n",
      "   -0.99407995]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.6       0.1       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9052773   1.         -0.0636425  ... -0.31541538 -3.261324\n",
      "  -1.391974  ]\n",
      " [-1.9052706   1.         -0.06375599 ...  1.1072139  -5.722757\n",
      "  -1.4731894 ]\n",
      " [-1.9052048   1.         -0.06379861 ...  1.4159031   3.09063\n",
      "   0.5853215 ]\n",
      " ...\n",
      " [-1.9052105   1.         -0.06380177 ... -6.194295   10.063957\n",
      "  -0.8135303 ]\n",
      " [-1.9052181   1.         -0.0637207  ...  0.3394214   7.818615\n",
      "   0.35795915]\n",
      " [-1.9052744   1.         -0.0636425  ...  1.5222731  -0.5454645\n",
      "   0.9543704 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.6       0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7914248   1.         -0.65029144 ...  0.6213422   0.10063648\n",
      "  -0.21699142]\n",
      " [-1.791399    1.         -0.6503811  ... -0.84167206 -2.162084\n",
      "  -0.67651594]\n",
      " [-1.7912636   1.         -0.6504174  ...  4.2881804  11.403657\n",
      "   1.6128675 ]\n",
      " ...\n",
      " [-1.7912769   1.         -0.6504221  ... -3.57006     7.1885004\n",
      "  -0.7813884 ]\n",
      " [-1.7912979   1.         -0.6503658  ... -0.7033498   8.891995\n",
      "   0.4572224 ]\n",
      " [-1.7914228   1.         -0.65029144 ... 10.058748   -2.3931181\n",
      "  -0.80906796]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 0.3       0.        0.        0.        0.6       0.\n",
      " 0.2       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5020657   1.         -1.1729984  ...  0.82127357  1.1928997\n",
      "   0.16357327]\n",
      " [-1.5020218   1.         -1.1730976  ...  0.10136867 -0.6544566\n",
      "   0.28120017]\n",
      " [-1.5018673   1.         -1.173111   ...  2.5416257   6.48902\n",
      "   0.32094288]\n",
      " ...\n",
      " [-1.5018883   1.         -1.1731272  ... -1.7195585   7.168067\n",
      "  -1.3109311 ]\n",
      " [-1.5019245   1.         -1.1730595  ...  0.02177218  0.4264202\n",
      "   0.184461  ]\n",
      " [-1.5020638   1.         -1.1729984  ...  9.488394   -1.8118227\n",
      "  -0.27519116]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.5       0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0654106   1.         -1.580553   ...  1.3537595   1.270174\n",
      "  -0.27489567]\n",
      " [-1.0653543   1.         -1.5806427  ... -1.2845039  -5.4016585\n",
      "   0.6581192 ]\n",
      " [-1.065176    1.         -1.5806289  ...  2.3541296   7.231341\n",
      "   1.1084274 ]\n",
      " ...\n",
      " [-1.065197    1.         -1.580657   ... -2.1954634   5.431277\n",
      "   0.9910763 ]\n",
      " [-1.0652485   1.         -1.5805988  ...  0.5202472  11.911005\n",
      "   1.0339344 ]\n",
      " [-1.0654106   1.         -1.580553   ... 10.800575   -3.2592041\n",
      "  -1.657468  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.6       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.6       0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.52373123  1.         -1.8333263  ...  0.67426705  1.0690298\n",
      "   1.2265197 ]\n",
      " [-0.5236616   1.         -1.8333702  ... -0.04559803 14.231522\n",
      "  -1.2460741 ]\n",
      " [-0.52345276  1.         -1.8333342  ...  1.7984979   4.567535\n",
      "   0.8239149 ]\n",
      " ...\n",
      " [-0.52347374  1.         -1.8333626  ... -0.16339844  0.04481244\n",
      "   0.33231938]\n",
      " [-0.5235348   1.         -1.8333359  ...  1.0028898  11.076044\n",
      "  -0.4931632 ]\n",
      " [-0.52373123  1.         -1.8333263  ... -2.5879197   0.14303493\n",
      "  -0.23798463]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.06914711  1.         -1.9061413  ...  0.59663403  0.39534998\n",
      "   1.2493526 ]\n",
      " [ 0.06921673  1.         -1.9061899  ...  1.3041961   7.6393595\n",
      "   0.11493087]\n",
      " [ 0.06945419  1.         -1.9061081  ... -1.4936665   4.2674828\n",
      "   1.3493872 ]\n",
      " ...\n",
      " [ 0.06943321  1.         -1.9061432  ...  0.5115514   1.4206252\n",
      "   0.01340963]\n",
      " [ 0.06937218  1.         -1.9061241  ...  1.3043847  11.780613\n",
      "  -0.1733256 ]\n",
      " [ 0.06914711  1.         -1.9061432  ... -1.7487435  -0.64191604\n",
      "  -0.8680738 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.5590858e-01  1.0000000e+00 -1.7918415e+00 ...  4.7686648e-01\n",
      "   6.6838694e-01  7.6424623e-01]\n",
      " [ 6.5597248e-01  1.0000000e+00 -1.7918711e+00 ...  6.8545562e-01\n",
      "   2.0824808e+01 -9.8696125e-01]\n",
      " [ 6.5620041e-01  1.0000000e+00 -1.7917398e+00 ... -2.4329851e+00\n",
      "   1.2145534e+01 -7.0082724e-01]\n",
      " ...\n",
      " [ 6.5617943e-01  1.0000000e+00 -1.7917795e+00 ... -1.0745376e-02\n",
      "   1.9233801e+00  4.0151066e-01]\n",
      " [ 6.5611839e-01  1.0000000e+00 -1.7917938e+00 ... -1.8464485e-01\n",
      "  -3.5693972e+00 -9.4466627e-02]\n",
      " [ 6.5590858e-01  1.0000000e+00 -1.7918415e+00 ... -1.2136831e+00\n",
      "   5.8475375e-01 -5.0440812e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1783609   1.         -1.5016422  ... -0.08505535  1.5894594\n",
      "   0.23125386]\n",
      " [ 1.1784143   1.         -1.5016642  ...  1.2978684  16.316408\n",
      "   0.7348639 ]\n",
      " [ 1.1786537   1.         -1.5014764  ... -1.6846783   7.4098644\n",
      "   0.41928428]\n",
      " ...\n",
      " [ 1.1786346   1.         -1.5015306  ... -0.1439995   1.3831432\n",
      "   0.62128294]\n",
      " [ 1.1785812   1.         -1.5015469  ... -0.88714147 -8.6935425\n",
      "  -0.32301223]\n",
      " [ 1.1783609   1.         -1.5016422  ... -0.77150345  1.7999804\n",
      "  -0.18997252]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5853109   1.         -1.0644703  ... -0.66994166  2.9764013\n",
      "   0.27936518]\n",
      " [ 1.58535     1.         -1.0644522  ... -0.8847239  16.267036\n",
      "   0.62378794]\n",
      " [ 1.5855465   1.         -1.0642185  ...  4.3208838  -7.868101\n",
      "  -1.5853268 ]\n",
      " ...\n",
      " [ 1.5855312   1.         -1.0642881  ...  0.11793119  0.6573486\n",
      "   0.66790885]\n",
      " [ 1.585495    1.         -1.0643291  ...  0.3471551  -5.7831397\n",
      "   0.09970072]\n",
      " [ 1.5853109   1.         -1.0644703  ... -0.78746414  1.4894507\n",
      "  -0.0755977 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.6       0.        0.        0.        0.6\n",
      " 0.        0.        0.        0.        0.9000001 1.0000001 0.4\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.8372307    1.          -0.52267265 ...   0.8771809    5.8961177\n",
      "   -0.16747314]\n",
      " [  1.8372488    1.          -0.5226345  ...   2.6303196  -12.022735\n",
      "    0.04917932]\n",
      " [  1.8374023    1.          -0.5223708  ...   0.9362488   -2.8949304\n",
      "   -1.323462  ]\n",
      " ...\n",
      " [  1.8373947    1.          -0.5224409  ...   0.74473953  -0.3840568\n",
      "    0.4555555 ]\n",
      " [  1.8373699    1.          -0.52251434 ...  -0.439851   -13.12739\n",
      "    0.25318718]\n",
      " [  1.8372307    1.          -0.52267265 ...   2.236518    -2.0535364\n",
      "   -0.24391246]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.        0.        0.2       1.0000001 1.0000001 1.0000001 1.0000001\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9092693   1.          0.07037163 ...  2.7743368  14.299421\n",
      "  -2.3486419 ]\n",
      " [ 1.9092712   1.          0.07043457 ...  0.36235893 -3.9648232\n",
      "  -0.20797509]\n",
      " [ 1.9093704   1.          0.07069284 ...  0.28113747 -2.6280484\n",
      "   0.05887294]\n",
      " ...\n",
      " [ 1.9093838   1.          0.07063198 ...  1.590323   -1.5287218\n",
      "   0.1425345 ]\n",
      " [ 1.90938     1.          0.07054329 ...  0.8016455  -7.0922246\n",
      "   0.12390178]\n",
      " [ 1.9092693   1.          0.07037163 ...  8.025837   -5.0973167\n",
      "  -0.27575892]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.6       0.        0.        1.0000001 1.0000001 1.0000001 1.0000001\n",
      " 0.        0.        0.        1.0000001 0.6       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.7942486    1.           0.6572113  ...   2.4617622    9.396804\n",
      "   -0.87935907]\n",
      " [  1.7942286    1.           0.6572609  ...   1.6277804   -9.471932\n",
      "   -1.4012643 ]\n",
      " [  1.7942562    1.           0.65750736 ...   0.15479016  -6.325604\n",
      "   -1.4622679 ]\n",
      " ...\n",
      " [  1.7942829    1.           0.65744495 ...   1.3682623   -3.257619\n",
      "   -0.13214889]\n",
      " [  1.7943001    1.           0.65737534 ...   2.7072086  -15.831718\n",
      "   -0.33517405]\n",
      " [  1.7942486    1.           0.6572113  ...   0.55436516  -0.51896644\n",
      "   -0.02952346]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.70000005 0.         0.         0.\n",
      " 1.0000001  1.0000001  0.         0.         1.0000001  1.0000001\n",
      " 1.0000001  0.70000005 0.         0.         0.         0.\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5037699    1.           1.179182   ...  -1.6274207  -10.953303\n",
      "    1.0866144 ]\n",
      " [  1.5037346    1.           1.1791906  ...   0.19159377   6.9982457\n",
      "   -0.45373917]\n",
      " [  1.5037441    1.           1.1793952  ...   0.6512118   -5.235149\n",
      "   -0.9299044 ]\n",
      " ...\n",
      " [  1.5037766    1.           1.179348   ...   3.7035518   -2.726038\n",
      "   -0.16419399]\n",
      " [  1.5038109    1.           1.1793213  ...   1.7503335  -10.69058\n",
      "    1.3658487 ]\n",
      " [  1.5037699    1.           1.179182   ...   0.29707956  -1.9936862\n",
      "   -0.6940968 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.9000001\n",
      " 1.0000001 0.        0.        0.5       0.9000001 0.5       0.\n",
      " 0.        0.        0.        0.        0.6       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0661898   1.          1.5858269  ... -0.7994691  -0.6519494\n",
      "  -0.63410264]\n",
      " [ 1.066144    1.          1.5858259  ...  0.35559404  4.6980658\n",
      "   0.24764377]\n",
      " [ 1.0661373   1.          1.585999   ... -0.6627211  -0.7521124\n",
      "  -0.7397723 ]\n",
      " ...\n",
      " [ 1.0661736   1.          1.5859528  ...  3.4164112  -2.048492\n",
      "  -0.26598608]\n",
      " [ 1.0662289   1.          1.585928   ... -2.8866808   8.354294\n",
      "  -0.7786337 ]\n",
      " [ 1.0661898   1.          1.5858269  ...  0.37345934 -1.6906877\n",
      "  -0.35056853]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.6]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.5238676   1.          1.8374081  ... -2.682757   -7.6689386\n",
      "  -0.16262889]\n",
      " [ 0.52381516  1.          1.8374138  ...  1.09321     5.973705\n",
      "   0.8355859 ]\n",
      " [ 0.5237961   1.          1.8375065  ...  0.31989294  3.2796574\n",
      "  -0.0795517 ]\n",
      " ...\n",
      " [ 0.52383995  1.          1.8374977  ...  8.368719   -6.786268\n",
      "   1.3621908 ]\n",
      " [ 0.5239029   1.          1.8374557  ... -0.33753818  5.191475\n",
      "  -0.63043034]\n",
      " [ 0.5238676   1.          1.8374081  ... -0.0316658  -0.49334002\n",
      "  -0.36662364]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.9438934e-02  1.0000000e+00  1.9091778e+00 ... -1.7195539e+00\n",
      "  -7.4799876e+00  3.7702298e-01]\n",
      " [-6.9492340e-02  1.0000000e+00  1.9091616e+00 ... -1.3872674e+00\n",
      "   3.7935741e+00  1.3487935e+00]\n",
      " [-6.9511414e-02  1.0000000e+00  1.9092007e+00 ...  1.5788901e-01\n",
      "   7.0307035e+00 -9.2548364e-01]\n",
      " ...\n",
      " [-6.9465637e-02  1.0000000e+00  1.9091978e+00 ... -2.5794966e+00\n",
      "   1.7688578e+00 -5.2839386e-01]\n",
      " [-6.9400787e-02  1.0000000e+00  1.9091988e+00 ... -1.8552074e+00\n",
      "   7.5759282e+00 -3.2953849e-01]\n",
      " [-6.9438934e-02  1.0000000e+00  1.9091778e+00 ... -4.9404621e-02\n",
      "   1.0470295e-01 -6.9494247e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.6556597   1.          1.7938843  ... -1.4000564   0.15102339\n",
      "   3.1440318 ]\n",
      " [-0.65570927  1.          1.7938709  ... -1.2408527   5.57537\n",
      "   0.4007538 ]\n",
      " [-0.6557236   1.          1.7938682  ... -2.6102235  -7.155026\n",
      "  -0.13660169]\n",
      " ...\n",
      " [-0.6556816   1.          1.7938747  ...  2.574155   -3.6693466\n",
      "  -2.222149  ]\n",
      " [-0.65561867  1.          1.7938805  ...  0.428264   -0.18349552\n",
      "  -0.32065475]\n",
      " [-0.6556597   1.          1.7938843  ... -0.19062948  0.5892625\n",
      "   0.09749389]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1778889   1.          1.5030785  ...  1.9434563  -3.049544\n",
      "   1.4715366 ]\n",
      " [-1.1779308   1.          1.5030499  ... -0.22760427 -0.66567373\n",
      "   0.09943715]\n",
      " [-1.1779919   1.          1.503017   ... -3.9607213  -9.07349\n",
      "   0.8716156 ]\n",
      " ...\n",
      " [-1.1779518   1.          1.5030375  ...  1.89851    -5.289231\n",
      "  -0.8963668 ]\n",
      " [-1.1778908   1.          1.5030594  ...  0.873793    9.610585\n",
      "  -0.1868518 ]\n",
      " [-1.1778889   1.          1.5030785  ...  0.01298332  0.5268965\n",
      "   0.21890187]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5843945   1.          1.0654335  ... -0.81311464 -1.3601747\n",
      "   0.28369486]\n",
      " [-1.5844259   1.          1.0654135  ... -0.5768769   1.1491642\n",
      "  -0.03322722]\n",
      " [-1.5844612   1.          1.0653607  ...  0.4321978   0.88450813\n",
      "  -0.07465753]\n",
      " ...\n",
      " [-1.5844421   1.          1.0653896  ... -0.98345083  6.233708\n",
      "  -2.567576  ]\n",
      " [-1.5843868   1.          1.0654011  ...  0.88706523  3.161475\n",
      "  -0.36180776]\n",
      " [-1.5843954   1.          1.0654335  ... -1.6040125   1.6349285\n",
      "  -0.13853157]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.8000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.1       0.5       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8359985   1.          0.5235863  ... -4.7627697   6.450315\n",
      "  -0.86415094]\n",
      " [-1.8360233   1.          0.5235796  ... -0.44199586  1.4465771\n",
      "  -0.04718538]\n",
      " [-1.8360023   1.          0.52350414 ... -0.7841172  -0.31459713\n",
      "   0.2860434 ]\n",
      " ...\n",
      " [-1.8359985   1.          0.52355194 ...  0.15616262 -1.6856904\n",
      "   0.5508621 ]\n",
      " [-1.8359604   1.          0.52355194 ...  3.0363636   9.5049925\n",
      "  -1.3622466 ]\n",
      " [-1.8359995   1.          0.5235863  ... -3.3191075   3.3295846\n",
      "  -0.786411  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.3       0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9077606   1.         -0.06961823 ... 10.439396   -7.555225\n",
      "   0.08553997]\n",
      " [-1.9077711   1.         -0.06964684 ... -2.8194396   7.914609\n",
      "   0.63843584]\n",
      " [-1.9077301   1.         -0.06971148 ... -0.26395226 -0.19421339\n",
      "   0.12419987]\n",
      " ...\n",
      " [-1.9077396   1.         -0.06967545 ...  0.10898209  1.0272031\n",
      "  -0.3599378 ]\n",
      " [-1.9077168   1.         -0.06965446 ...  2.7604258   5.2577353\n",
      "  -1.1842337 ]\n",
      " [-1.9077616   1.         -0.06961823 ...  4.972818   -4.083088\n",
      "  -1.4067235 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7927256e+00  1.0000000e+00 -6.5618324e-01 ...  2.2525516e+00\n",
      "  -3.1031728e+00 -3.7716398e-01]\n",
      " [-1.7927227e+00  1.0000000e+00 -6.5622234e-01 ... -2.5176423e+00\n",
      "   6.1057315e+00  6.5516204e-01]\n",
      " [-1.7926559e+00  1.0000000e+00 -6.5627784e-01 ... -1.6014576e-03\n",
      "  -1.1539354e+00  9.5396042e-03]\n",
      " ...\n",
      " [-1.7926807e+00  1.0000000e+00 -6.5625095e-01 ...  1.2363092e+00\n",
      "   8.9698553e-01  6.5843284e-02]\n",
      " [-1.7926750e+00  1.0000000e+00 -6.5621948e-01 ... -3.6239972e+00\n",
      "  -7.0726242e+00 -4.5278752e-01]\n",
      " [-1.7927265e+00  1.0000000e+00 -6.5618706e-01 ... -8.6286402e-01\n",
      "   4.2849207e-01  1.5589488e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5020933   1.         -1.1782627  ...  8.514492   -7.0071125\n",
      "  -0.8256776 ]\n",
      " [-1.5020828   1.         -1.1783333  ...  1.2010481  -4.3990803\n",
      "   0.26402813]\n",
      " [-1.501997    1.         -1.1783653  ...  1.709468    3.9143085\n",
      "  -0.18987028]\n",
      " ...\n",
      " [-1.502037    1.         -1.1783552  ...  2.3952293   8.260099\n",
      "   0.10979936]\n",
      " [-1.5020485   1.         -1.1782951  ...  2.9577293   2.7801328\n",
      "   0.21169108]\n",
      " [-1.5020943   1.         -1.1782665  ...  0.6261649  -0.92649126\n",
      "  -0.06029204]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.8000001\n",
      " 0.9000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.1       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0642605   1.         -1.5849304  ...  1.3115355   0.61946225\n",
      "  -1.4957902 ]\n",
      " [-1.0642376   1.         -1.5850315  ...  5.6429863  -9.837\n",
      "   1.2116944 ]\n",
      " [-1.0641441   1.         -1.5850415  ...  2.008307    5.7256117\n",
      "   1.3883531 ]\n",
      " ...\n",
      " [-1.0641975   1.         -1.585043   ...  0.92663825  3.623913\n",
      "   0.23915258]\n",
      " [-1.0642166   1.         -1.5849571  ...  5.880594    7.0301867\n",
      "   0.01053685]\n",
      " [-1.0642614   1.         -1.5849323  ... -0.18515539 -0.5827966\n",
      "  -0.04507083]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.1        0.         0.         0.\n",
      " 0.4        0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.5223665    1.          -1.8361797  ...   0.722053     1.530481\n",
      "   -1.1099683 ]\n",
      " [ -0.52233696   1.          -1.8362818  ...   5.2341914  -10.27356\n",
      "   -0.23101163]\n",
      " [ -0.52220917   1.          -1.8362722  ...   0.176485     4.374216\n",
      "    0.71628684]\n",
      " ...\n",
      " [ -0.5222626    1.          -1.8362789  ...   0.7327206    3.3455446\n",
      "    0.19461402]\n",
      " [ -0.52228546   1.          -1.8361912  ...  -4.5480065   -3.2588205\n",
      "    0.34453464]\n",
      " [ -0.5223665    1.          -1.8361816  ...   0.17969322  -0.34959984\n",
      "    0.0108676 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.07102776  1.         -1.9078178  ... -0.09048259  2.7959971\n",
      "  -0.8557246 ]\n",
      " [ 0.07106113  1.         -1.9079361  ...  1.9192129  -6.1344905\n",
      "   0.33003235]\n",
      " [ 0.0711956   1.         -1.9078864  ... -0.15868168  2.1749516\n",
      "   1.0659525 ]\n",
      " ...\n",
      " [ 0.0711422   1.         -1.9079161  ...  2.8156395  12.939862\n",
      "  -0.15799522]\n",
      " [ 0.07111549  1.         -1.9078064  ... -2.6287494  -2.5139337\n",
      "   0.31746578]\n",
      " [ 0.07102776  1.         -1.907814   ...  4.5382357  -6.3761435\n",
      "  -0.29746586]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.65753746   1.          -1.7924728  ...   2.0349002    8.255088\n",
      "    1.1353955 ]\n",
      " [  0.6575651    1.          -1.7925997  ...   1.3585833   -4.3860345\n",
      "    0.7339856 ]\n",
      " [  0.6577053    1.          -1.7925118  ...  -0.7301161    9.18815\n",
      "    1.5207231 ]\n",
      " ...\n",
      " [  0.6576538    1.          -1.7925663  ...  -3.1258159  -11.558445\n",
      "    0.70359915]\n",
      " [  0.6576271    1.          -1.7924366  ...   8.801712     8.05628\n",
      "   -0.48981398]\n",
      " [  0.65753746   1.          -1.792469   ...   3.8551228   -7.043512\n",
      "   -0.408646  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1795235   1.         -1.5015583  ...  5.85759     9.575926\n",
      "  -1.4669983 ]\n",
      " [ 1.1795464   1.         -1.5016804  ...  3.0371366  -7.644701\n",
      "  -1.3092765 ]\n",
      " [ 1.179718    1.         -1.5015564  ... -3.363236    3.7082546\n",
      "   1.7929313 ]\n",
      " ...\n",
      " [ 1.1796684   1.         -1.5016346  ... -0.03893065  3.763516\n",
      "   0.23293172]\n",
      " [ 1.1796494   1.         -1.5015049  ... -0.10238171  0.92649364\n",
      "   0.36029017]\n",
      " [ 1.1795235   1.         -1.5015545  ... -0.47075462  4.7778697\n",
      "  -1.7969949 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5857277   1.         -1.0638504  ...  3.1636424   4.1630845\n",
      "  -0.7847692 ]\n",
      " [ 1.5857468   1.         -1.063983   ... -0.26775086  1.6835327\n",
      "   0.26724362]\n",
      " [ 1.5858994   1.         -1.0638217  ... -4.300885    7.2150583\n",
      "   0.31205076]\n",
      " ...\n",
      " [ 1.5858574   1.         -1.0639229  ... -0.33561838  5.9083805\n",
      "   1.7398803 ]\n",
      " [ 1.5858402   1.         -1.0637779  ...  1.181145    3.4315205\n",
      "   1.1679487 ]\n",
      " [ 1.5857277   1.         -1.0638466  ... -1.897516    9.903722\n",
      "  -0.5290615 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.2 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8369169   1.         -0.52122116 ...  3.7841396   4.6488247\n",
      "  -0.99592763]\n",
      " [ 1.836936    1.         -0.5214062  ...  0.8235059   0.0510006\n",
      "  -0.67777455]\n",
      " [ 1.8370667   1.         -0.5212061  ...  4.9782724  -6.2841406\n",
      "  -1.5331919 ]\n",
      " ...\n",
      " [ 1.8370438   1.         -0.52133274 ...  1.0592104  -3.2109396\n",
      "  -0.25104418]\n",
      " [ 1.8370247   1.         -0.5211468  ... -0.3156469  -0.8304615\n",
      "   1.458499  ]\n",
      " [ 1.836916    1.         -0.52121735 ... -0.38969016 13.191488\n",
      "  -1.655471  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.6       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.6       0.        0.        0.\n",
      " 0.        0.        1.0000001 0.9000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9080458   1.          0.072155   ... -4.606466   -4.230807\n",
      "  -1.0874091 ]\n",
      " [ 1.9080715   1.          0.07195854 ...  0.18878032  2.3742208\n",
      "   0.26046014]\n",
      " [ 1.9081745   1.          0.0721684  ...  0.83013916 -0.17916799\n",
      "   0.55255175]\n",
      " ...\n",
      " [ 1.908165    1.          0.07203388 ... -0.90023977  1.2535462\n",
      "   0.0750668 ]\n",
      " [ 1.9081383   1.          0.07222939 ...  0.60090166 -3.000721\n",
      "   0.06070971]\n",
      " [ 1.908042    1.          0.07215881 ...  0.5559089   3.0640035\n",
      "  -0.22516799]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7924643   1.          0.6583195  ...  3.5735598   1.0429852\n",
      "   1.268685  ]\n",
      " [ 1.7925053   1.          0.6580982  ...  0.05151403  3.6243458\n",
      "   0.62120724]\n",
      " [ 1.7925262   1.          0.6583071  ...  0.47724962  0.517848\n",
      "  -1.4585351 ]\n",
      " ...\n",
      " [ 1.7925377   1.          0.6581669  ... -1.403295    0.81105566\n",
      "   0.3596573 ]\n",
      " [ 1.7924957   1.          0.65838814 ...  1.6152434  -6.32858\n",
      "   1.3575137 ]\n",
      " [ 1.7924595   1.          0.6583214  ...  0.5064304   2.156796\n",
      "  -0.01592021]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5012827    1.           1.1801968  ...   3.7079997    2.610849\n",
      "    1.1792023 ]\n",
      " [  1.5013523    1.           1.1800108  ...  -1.1079093    1.4565682\n",
      "    0.16809288]\n",
      " [  1.5012836    1.           1.1802024  ...  -1.6942518    3.04536\n",
      "   -0.50032425]\n",
      " ...\n",
      " [  1.5013046    1.           1.1800671  ...  -0.98311365   2.0476363\n",
      "   -0.9139554 ]\n",
      " [  1.5012512    1.           1.1802464  ...   1.6414301  -10.107596\n",
      "   -1.558759  ]\n",
      " [  1.501276     1.           1.1801987  ...   1.2535024    2.7448916\n",
      "   -0.49890184]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.6       0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.6       0.        0.3       1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0633373   1.          1.5863152  ...  2.29198     2.4165683\n",
      "   0.5875083 ]\n",
      " [ 1.0634212   1.          1.5861149  ... -0.20492643  0.9954796\n",
      "   0.0347321 ]\n",
      " [ 1.0632877   1.          1.5862947  ... -1.0100775  -2.8104267\n",
      "   1.3313019 ]\n",
      " ...\n",
      " [ 1.0633259   1.          1.5861588  ... -0.88627803  2.199883\n",
      "  -1.4171071 ]\n",
      " [ 1.0632515   1.          1.5863552  ... -0.33868977  3.577189\n",
      "  -0.16043746]\n",
      " [ 1.0633307   1.          1.5863171  ...  1.1656823   3.9548402\n",
      "   0.34915614]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.4       0.        0.        0.        0.        0.        0.\n",
      " 0.3       0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.5208988    1.           1.8373699  ...   1.5174742    3.434524\n",
      "    0.8200047 ]\n",
      " [  0.5210047    1.           1.8372087  ...   0.28893423   0.46351337\n",
      "   -0.26064456]\n",
      " [  0.52082825   1.           1.8373351  ...  -0.2103954   -3.1030183\n",
      "   -0.530291  ]\n",
      " ...\n",
      " [  0.52087975   1.           1.8372221  ...   0.10238221   2.3269432\n",
      "   -0.36199045]\n",
      " [  0.520792     1.           1.8373871  ...   2.56107    -14.494049\n",
      "   -0.48368946]\n",
      " [  0.5208931    1.           1.8373756  ...   0.6687224    5.0263085\n",
      "    0.5981891 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.2401047e-02  1.0000000e+00  1.9084263e+00 ...  1.9033102e+00\n",
      "   3.1497617e+00  1.2739027e+00]\n",
      " [-7.2289467e-02  1.0000000e+00  1.9083185e+00 ...  1.7759261e+00\n",
      "   7.1624432e+00  1.0678786e-01]\n",
      " [-7.2477341e-02  1.0000000e+00  1.9083894e+00 ... -5.3528619e-01\n",
      "  -8.8284769e+00  1.0545373e-02]\n",
      " ...\n",
      " [-7.2425842e-02  1.0000000e+00  1.9083118e+00 ...  9.0254915e-01\n",
      "   2.1150470e+00  7.0198208e-02]\n",
      " [-7.2513580e-02  1.0000000e+00  1.9084244e+00 ...  3.7875385e+00\n",
      "  -1.4929382e+01  2.9028445e-02]\n",
      " [-7.2406769e-02  1.0000000e+00  1.9084320e+00 ...  3.3256540e+00\n",
      "   9.1367788e+00  1.6004398e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.6583681   1.          1.7925472  ...  5.0150733  17.353386\n",
      "  -1.7394139 ]\n",
      " [-0.6582613   1.          1.7924871  ...  0.43361872 -6.68697\n",
      "  -0.24886233]\n",
      " [-0.65846825  1.          1.7924827  ... -0.0336206   0.6282077\n",
      "  -0.27557242]\n",
      " ...\n",
      " [-0.6584282   1.          1.7924471  ...  0.4090522   1.3964989\n",
      "   0.15197921]\n",
      " [-0.6585026   1.          1.7925186  ... -2.200381    5.197957\n",
      "  -0.97505367]\n",
      " [-0.65837383  1.          1.792553   ...  1.0134861   5.987387\n",
      "   1.2485938 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1800776e+00  1.0000000e+00  1.5012093e+00 ...  2.7567432e+00\n",
      "   9.1011858e+00 -6.2527470e-03]\n",
      " [-1.1799889e+00  1.0000000e+00  1.5012064e+00 ... -6.7940766e-01\n",
      "  -1.2831158e+01 -4.0470123e-01]\n",
      " [-1.1801357e+00  1.0000000e+00  1.5011462e+00 ... -2.3171413e+00\n",
      "  -9.2099323e+00 -6.2097329e-01]\n",
      " ...\n",
      " [-1.1801128e+00  1.0000000e+00  1.5011339e+00 ... -2.4807210e-01\n",
      "   7.5183314e-01  6.8886495e-01]\n",
      " [-1.1801662e+00  1.0000000e+00  1.5011597e+00 ... -1.5929846e+00\n",
      "   6.9033275e+00 -8.0106640e-01]\n",
      " [-1.1800833e+00  1.0000000e+00  1.5012150e+00 ...  2.6483381e-01\n",
      "  -2.9926805e+00  1.2905525e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5864563e+00  1.0000000e+00  1.0627766e+00 ... -3.3088446e-02\n",
      "   1.7214503e+00  1.5617537e-01]\n",
      " [-1.5864019e+00  1.0000000e+00  1.0627937e+00 ...  1.8138140e-03\n",
      "   4.2566652e+00  9.8951221e-02]\n",
      " [-1.5865364e+00  1.0000000e+00  1.0626957e+00 ... -2.6400821e+00\n",
      "  -5.1395764e+00 -9.3760419e-01]\n",
      " ...\n",
      " [-1.5865345e+00  1.0000000e+00  1.0626984e+00 ...  7.0120150e-01\n",
      "  -1.8689691e+00  4.8734680e-01]\n",
      " [-1.5865574e+00  1.0000000e+00  1.0627079e+00 ...  2.9371804e-01\n",
      "   7.6983752e+00 -1.0915558e+00]\n",
      " [-1.5864601e+00  1.0000000e+00  1.0627823e+00 ... -7.4674982e-01\n",
      "  -2.0715380e-01 -4.8102403e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.2 0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8371935   1.          0.520483   ... -1.1945047   3.2585022\n",
      "   1.0598307 ]\n",
      " [-1.837163    1.          0.52054024 ...  0.81389844 -8.581806\n",
      "   0.25532538]\n",
      " [-1.8372746   1.          0.5203988  ... -1.5669849  -3.5891604\n",
      "  -0.73475033]\n",
      " ...\n",
      " [-1.8372822   1.          0.5204296  ...  4.4915586  -8.486716\n",
      "  -1.122118  ]\n",
      " [-1.8372936   1.          0.5203953  ... -2.155503    6.0732894\n",
      "   1.3196577 ]\n",
      " [-1.8371954   1.          0.52048683 ... -1.3872126   3.9519136\n",
      "  -0.31851003]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9081402   1.         -0.07304573 ... -1.3075756   3.93826\n",
      "   0.57947373]\n",
      " [-1.9081335   1.         -0.07298946 ...  0.91919565 -4.668059\n",
      "   0.2839066 ]\n",
      " [-1.908186    1.         -0.07314512 ...  0.16739321  0.5425091\n",
      "   0.14559948]\n",
      " ...\n",
      " [-1.9082146   1.         -0.0731039  ... -3.7181094   7.7650356\n",
      "  -0.19077374]\n",
      " [-1.9082012   1.         -0.07313728 ...  0.27843165 -6.768659\n",
      "   1.0541159 ]\n",
      " [-1.9081383   1.         -0.07304192 ... -0.9026154   0.6947403\n",
      "   1.4141324 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         1.0000001  0.         0.4        0.\n",
      " 0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.792284    1.         -0.6589222  ... -0.34252334  2.2963793\n",
      "   0.764845  ]\n",
      " [-1.7923021   1.         -0.6588907  ...  1.8282502  -7.25636\n",
      "   1.3048859 ]\n",
      " [-1.7922535   1.         -0.65902925 ... -0.3198142  -0.6951952\n",
      "  -0.1991837 ]\n",
      " ...\n",
      " [-1.7923069   1.         -0.65899754 ... -3.0994902   3.740025\n",
      "   0.75930834]\n",
      " [-1.7922668   1.         -0.65901184 ... -0.31084085 -1.5461354\n",
      "   0.30032986]\n",
      " [-1.7922783   1.         -0.6589184  ... -0.8056202   1.3034458\n",
      "  -0.9770889 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         1.0000001  0.         0.         0.\n",
      " 0.70000005 0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.50074196e+00  1.00000000e+00 -1.18080521e+00 ...  2.21776962e-02\n",
      "   7.13847637e-01  3.56275856e-01]\n",
      " [-1.50079060e+00  1.00000000e+00 -1.18075275e+00 ...  3.10459375e-01\n",
      "   1.28473759e-01 -6.61444664e-03]\n",
      " [-1.50068665e+00  1.00000000e+00 -1.18086112e+00 ...  4.79818153e+00\n",
      "   8.27746582e+00  3.27139020e-01]\n",
      " ...\n",
      " [-1.50075912e+00  1.00000000e+00 -1.18084145e+00 ... -1.02630205e+01\n",
      "   1.45131721e+01 -1.77294350e+00]\n",
      " [-1.50069046e+00  1.00000000e+00 -1.18088150e+00 ... -2.97565579e-01\n",
      "  -8.12980652e-01  4.45581794e-01]\n",
      " [-1.50073433e+00  1.00000000e+00 -1.18080139e+00 ... -1.12398863e-01\n",
      "   3.50360870e-02 -2.43717909e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.9000001 0.        0.5       0.        1.0000001\n",
      " 0.        0.        0.8000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0625582   1.         -1.5866833  ...  0.46433258 -1.1385045\n",
      "  -0.03545588]\n",
      " [-1.0626259   1.         -1.5866451  ...  0.40405464 -1.8292503\n",
      "   0.3443308 ]\n",
      " [-1.0625172   1.         -1.5867126  ...  0.59474325  1.1182289\n",
      "   1.3057647 ]\n",
      " ...\n",
      " [-1.0625954   1.         -1.58671    ... -2.092733    4.7738504\n",
      "  -0.3258547 ]\n",
      " [-1.0625191   1.         -1.5867271  ...  0.27358484  0.2962675\n",
      "   0.58168626]\n",
      " [-1.0625477   1.         -1.5866795  ... -0.25131655  0.14833355\n",
      "  -0.44988322]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.3       0.        0.9000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.5196152   1.         -1.8374081  ... -2.6748257   2.675233\n",
      "  -0.07400577]\n",
      " [-0.5196924   1.         -1.8373718  ... -0.34837317  0.8267317\n",
      "  -0.12700963]\n",
      " [-0.51958466  1.         -1.8373873  ...  4.0111275  12.274624\n",
      "   0.85181963]\n",
      " ...\n",
      " [-0.5196705   1.         -1.8374062  ...  1.4244758   2.156112\n",
      "  -1.6617694 ]\n",
      " [-0.51958656  1.         -1.8374214  ... -1.0215187   2.9373288\n",
      "   0.03447831]\n",
      " [-0.5196047   1.         -1.8374043  ... -0.16843176  0.16037226\n",
      "  -0.242594  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.07333565  1.         -1.9078865  ...  0.96241283  1.3501506\n",
      "  -1.307629  ]\n",
      " [ 0.07325363  1.         -1.9079008  ... -5.287352   14.187636\n",
      "  -1.0887933 ]\n",
      " [ 0.073349    1.         -1.9078667  ...  3.4598393  11.290718\n",
      "   0.60304934]\n",
      " ...\n",
      " [ 0.07326317  1.         -1.9079084  ...  0.63712955  3.9370298\n",
      "  -0.4796182 ]\n",
      " [ 0.07334709  1.         -1.9078884  ... -5.995495   10.836414\n",
      "   0.04407084]\n",
      " [ 0.07334614  1.         -1.9078827  ... -0.16925025  0.44710398\n",
      "  -0.09565377]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.65966797   1.          -1.7917747  ...  -0.49704206   1.7451631\n",
      "    0.34244788]\n",
      " [  0.6595888    1.          -1.791832   ...   2.933858   -10.49375\n",
      "   -1.0859187 ]\n",
      " [  0.65964127   1.          -1.7917622  ...   0.97968817   6.6597385\n",
      "    1.7879729 ]\n",
      " ...\n",
      " [  0.65955925   1.          -1.7918053  ...   0.49951065   4.41026\n",
      "   -0.1414578 ]\n",
      " [  0.65963745   1.          -1.7917595  ...  -2.5078356    6.386577\n",
      "   -0.56080174]\n",
      " [  0.65967846   1.          -1.7917709  ...  -0.12623382   0.74587965\n",
      "    0.07754099]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1813717   1.         -1.5000725  ...  0.45931125  0.33577847\n",
      "  -0.9008877 ]\n",
      " [ 1.1813059   1.         -1.5000992  ...  1.8871188  -9.420076\n",
      "  -0.44763958]\n",
      " [ 1.1813335   1.         -1.5000135  ...  0.16465616  3.3757157\n",
      "   0.5440847 ]\n",
      " ...\n",
      " [ 1.1812611   1.         -1.5000572  ...  8.269369   14.845562\n",
      "   1.041119  ]\n",
      " [ 1.1813297   1.         -1.5000458  ... -3.6881084  13.882449\n",
      "   1.0503109 ]\n",
      " [ 1.1813812   1.         -1.5000648  ... -1.3330369   3.3291326\n",
      "  -0.1781013 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5871248   1.         -1.0617256  ... -0.0517315   1.1824788\n",
      "   0.05637264]\n",
      " [ 1.5870752   1.         -1.0617857  ... -0.39763278 -0.8562527\n",
      "  -0.18454587]\n",
      " [ 1.5870476   1.         -1.0616559  ... -1.0042179   8.645359\n",
      "   1.96939   ]\n",
      " ...\n",
      " [ 1.5869961   1.         -1.061738   ... -1.7195941  -5.2709284\n",
      "  -0.3490969 ]\n",
      " [ 1.5870457   1.         -1.0616913  ...  2.7645507  -7.658057\n",
      "   0.4990011 ]\n",
      " [ 1.5871305   1.         -1.0617161  ... -3.8607817   7.1267724\n",
      "  -1.5585186 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.3       0.        0.        0.        0.\n",
      " 0.        0.        0.3       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.5       0.4      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8376255   1.         -0.519331   ...  0.03968143  1.0146441\n",
      "  -0.42617723]\n",
      " [ 1.8375988   1.         -0.5194092  ...  0.5953648  -1.295361\n",
      "  -0.07430428]\n",
      " [ 1.8375416   1.         -0.51926154 ... -1.1632512   4.3180695\n",
      "   0.6649515 ]\n",
      " ...\n",
      " [ 1.8375072   1.         -0.5193567  ... -1.2702184  -2.487844\n",
      "  -0.05724442]\n",
      " [ 1.8375454   1.         -0.51929474 ...  1.7017748  -3.8712838\n",
      "  -0.3867681 ]\n",
      " [ 1.8376293   1.         -0.51932144 ...  2.1256738  -7.3316293\n",
      "  -0.42573056]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.1\n",
      " 0.        0.        0.        0.        1.0000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.9082336    1.           0.07413673 ...  -0.08247252   1.4695888\n",
      "   -0.29863524]\n",
      " [  1.9082394    1.           0.07403564 ...   0.20587164   0.39916182\n",
      "   -0.02409399]\n",
      " [  1.9081631    1.           0.07419682 ...  -3.6765532   10.412544\n",
      "    1.5867565 ]\n",
      " ...\n",
      " [  1.908144     1.           0.07408905 ...  -5.242275   -10.22268\n",
      "   -0.75029117]\n",
      " [  1.9081688    1.           0.07417297 ...  -0.7496928   -1.5350909\n",
      "   -0.52791077]\n",
      " [  1.9082327    1.           0.07414627 ...   2.3537047   -6.7046957\n",
      "    0.4694139 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.5       0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7919865   1.          0.660162   ... -0.6324687   2.677012\n",
      "  -0.18564796]\n",
      " [ 1.792017    1.          0.6600847  ... -0.6867037   7.5009923\n",
      "   0.31340584]\n",
      " [ 1.7919197   1.          0.66023433 ... -1.8009019   2.9549615\n",
      "   0.5717718 ]\n",
      " ...\n",
      " [ 1.7919197   1.          0.6601372  ... -0.04021835 -3.955357\n",
      "  -0.40091157]\n",
      " [ 1.7919273   1.          0.6601982  ... -0.16672698  0.59054613\n",
      "  -0.14276828]\n",
      " [ 1.7919846   1.          0.6601715  ... -3.3098755   3.8024573\n",
      "   0.10205579]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.3       1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4999971   1.          1.1816311  ...  0.8932226   5.1577287\n",
      "  -0.04829428]\n",
      " [ 1.5000582   1.          1.1815491  ... -1.3023536  -4.1620684\n",
      "   0.18109101]\n",
      " [ 1.499918    1.          1.181702   ... -0.38746166 -0.66598845\n",
      "   0.1872555 ]\n",
      " ...\n",
      " [ 1.499939    1.          1.1815948  ...  3.6425557   7.43292\n",
      "   2.06761   ]\n",
      " [ 1.4999256   1.          1.1816635  ...  1.1410724   1.572063\n",
      "   0.11925448]\n",
      " [ 1.4999895   1.          1.1816406  ... -2.0177736   3.8936152\n",
      "  -0.48499155]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.5       0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.2\n",
      " 0.        0.        0.        0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0614214   1.          1.5873451  ...  5.204504    9.4516735\n",
      "  -0.76731527]\n",
      " [ 1.0615015   1.          1.5873051  ...  0.45980424 -6.0657134\n",
      "  -0.16973764]\n",
      " [ 1.061346    1.          1.5874166  ...  1.734513   -0.9660411\n",
      "   0.3118959 ]\n",
      " ...\n",
      " [ 1.0613861   1.          1.5873384  ...  0.29226208  4.9213667\n",
      "   1.3680737 ]\n",
      " [ 1.0613575   1.          1.5873718  ... -0.12099051 -0.9978266\n",
      "  -0.11291477]\n",
      " [ 1.0614128   1.          1.5873508  ... -1.4299302   7.0099506\n",
      "  -1.8138187 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.5       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.8000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.51858044   1.           1.8377934  ...  -2.1347063   -3.8250608\n",
      "   -0.53341496]\n",
      " [  0.5186796    1.           1.8377676  ...   0.8929156   -7.1919665\n",
      "    0.6757423 ]\n",
      " [  0.5185394    1.           1.8378379  ...   8.771498    -9.9317\n",
      "    0.3165174 ]\n",
      " ...\n",
      " [  0.51859283   1.           1.83778    ...   0.8475329    3.7106318\n",
      "    0.9783986 ]\n",
      " [  0.5185566    1.           1.8378067  ...   1.8727446  -10.54752\n",
      "   -1.0920366 ]\n",
      " [  0.51857185   1.           1.8377991  ...  -1.784716     6.620554\n",
      "   -0.07219759]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.07466221   1.           1.9081059  ...   4.712637     6.687767\n",
      "    0.16562411]\n",
      " [ -0.07456112   1.           1.9081297  ...  -2.2189915   -0.32881498\n",
      "    0.783938  ]\n",
      " [ -0.07470512   1.           1.90816    ...   9.8792515  -10.708579\n",
      "   -1.4922341 ]\n",
      " ...\n",
      " [ -0.07464409   1.           1.9081297  ...  -1.5592203    5.472613\n",
      "    1.3226125 ]\n",
      " [ -0.07468033   1.           1.9081135  ...   0.71879697   3.9927638\n",
      "    0.21149611]\n",
      " [ -0.07467079   1.           1.9081097  ...   0.78296506 -13.00305\n",
      "    0.30067644]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.6609554   1.          1.791565   ...  0.49755645  2.6688468\n",
      "   0.61006296]\n",
      " [-0.66085625  1.          1.7915974  ... -1.902672    8.442808\n",
      "  -0.51489073]\n",
      " [-0.6610031   1.          1.7915944  ...  2.4304292  -6.121727\n",
      "  -1.3515934 ]\n",
      " ...\n",
      " [-0.6609459   1.          1.7915812  ... -2.332241   11.93709\n",
      "   1.2146604 ]\n",
      " [-0.6609802   1.          1.7915611  ... -0.5488853   6.6986756\n",
      "   0.11142933]\n",
      " [-0.660964    1.          1.7915688  ...  3.1396253  16.245762\n",
      "   0.7377324 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.1820126    1.           1.4995079  ...   3.92806      9.702412\n",
      "    0.7928258 ]\n",
      " [ -1.181922     1.           1.4995832  ...  -2.1995738    5.272983\n",
      "   -0.68936336]\n",
      " [ -1.182087     1.           1.4995387  ...   5.315609   -13.133735\n",
      "    0.14854968]\n",
      " ...\n",
      " [ -1.1820488    1.           1.4995518  ...  -0.9983672    1.5343653\n",
      "    1.3578378 ]\n",
      " [ -1.1820717    1.           1.4994965  ...  -1.2464273  -19.96035\n",
      "    2.3646617 ]\n",
      " [ -1.1820221    1.           1.4995117  ...   0.40203106   7.54081\n",
      "    0.05844325]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5878153   1.          1.060358   ...  0.42455477  5.703013\n",
      "   1.8040423 ]\n",
      " [-1.5877485   1.          1.0604677  ... -0.6923344   1.439713\n",
      "  -0.5640482 ]\n",
      " [-1.5878754   1.          1.0603912  ... -0.48760462 -1.9759135\n",
      "  -0.5467369 ]\n",
      " ...\n",
      " [-1.5878563   1.          1.0604162  ...  0.649099   -1.5764768\n",
      "   0.15645663]\n",
      " [-1.5878639   1.          1.0603294  ... -0.6743876  -3.4061956\n",
      "   0.5210041 ]\n",
      " [-1.5878239   1.          1.06036    ...  0.81580687 -0.6153307\n",
      "   1.9002533 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8378258   1.          0.51761246 ...  0.18715692  4.80494\n",
      "   1.177781  ]\n",
      " [-1.8377848   1.          0.5177717  ...  0.68105173  4.776165\n",
      "  -0.9872627 ]\n",
      " [-1.8379002   1.          0.5176594  ... -0.05447888 -0.75912\n",
      "   0.10723817]\n",
      " ...\n",
      " [-1.837904    1.          0.5177069  ... -0.88513476  1.8138361\n",
      "   0.11384736]\n",
      " [-1.8378887   1.          0.51758003 ...  0.16266364  1.8559122\n",
      "  -0.88929605]\n",
      " [-1.8378325   1.          0.51761436 ...  0.3738485  -2.6323576\n",
      "   0.04909064]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.3       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9079132   1.         -0.0754528  ... -3.0456326   9.68112\n",
      "   2.4272332 ]\n",
      " [-1.907898    1.         -0.07528687 ...  0.6303119  15.003809\n",
      "  -0.38740337]\n",
      " [-1.907959    1.         -0.0754014  ...  0.656528    0.82110405\n",
      "   0.02397931]\n",
      " ...\n",
      " [-1.9079876   1.         -0.07535744 ... -3.1490185   2.6157708\n",
      "   0.7032551 ]\n",
      " [-1.9079475   1.         -0.07548714 ...  1.13625     1.9924359\n",
      "   0.9745518 ]\n",
      " [-1.9079189   1.         -0.0754509  ... -0.77678084 -6.356704\n",
      "  -0.8071727 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.4       0.6       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.8000001 0.        1.0000001 0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.7911444    1.          -0.6618061  ...  -0.7999673    2.5058439\n",
      "    0.20038329]\n",
      " [ -1.7911644    1.          -0.6616211  ...  -1.4491789   -5.6462884\n",
      "    0.14854586]\n",
      " [ -1.7912064    1.          -0.6617418  ...   0.05083573   6.0370946\n",
      "   -0.3035848 ]\n",
      " ...\n",
      " [ -1.7912502    1.          -0.6616869  ... -10.910004    10.034821\n",
      "   -0.4680779 ]\n",
      " [ -1.791193     1.          -0.6618366  ...   0.42862183   7.7363405\n",
      "   -0.44583863]\n",
      " [ -1.7911482    1.          -0.6618042  ...  -1.8018854  -10.986864\n",
      "   -1.1091856 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.6       0.        0.\n",
      " 1.0000001 0.        1.0000001 0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.4990644    1.          -1.1827717  ...  -0.45608485  -1.2736087\n",
      "    0.12095174]\n",
      " [ -1.4991198    1.          -1.1826286  ...  -2.1420398  -11.214071\n",
      "    1.5001359 ]\n",
      " [ -1.4991436    1.          -1.1827291  ...  -1.5062335    2.8753114\n",
      "    0.3690572 ]\n",
      " ...\n",
      " [ -1.4992008    1.          -1.182684   ...  -5.410585     5.9970894\n",
      "   -0.98023856]\n",
      " [ -1.4991245    1.          -1.1827984  ...  -1.5814095    2.6137452\n",
      "    0.8309949 ]\n",
      " [ -1.4990683    1.          -1.1827698  ...   3.0982733    4.978893\n",
      "    1.2993357 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         1.0000001  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.70000005 1.0000001\n",
      " 0.         0.         1.0000001  0.         0.9000001  0.\n",
      " 0.         0.6       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0598679   1.         -1.5884609  ...  4.081097   -6.014238\n",
      "   1.3609529 ]\n",
      " [-1.0599566   1.         -1.5883102  ...  0.8318704   3.2532754\n",
      "   0.03451045]\n",
      " [-1.059927    1.         -1.5883918  ...  0.21679375 -6.9066205\n",
      "   0.22496867]\n",
      " ...\n",
      " [-1.0599957   1.         -1.5883493  ... -6.044676   10.8235035\n",
      "  -1.2735353 ]\n",
      " [-1.0598984   1.         -1.5884857  ... -1.004829   -3.3243632\n",
      "  -0.513999  ]\n",
      " [-1.0598717   1.         -1.588459   ...  0.36033046  5.012233\n",
      "   1.8905659 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.9000001 0.        0.\n",
      " 0.3       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.51716805   1.          -1.8382816  ...  -7.4076524    9.044615\n",
      "    0.61519283]\n",
      " [ -0.5172806    1.          -1.8381882  ...  -1.0238457   -4.0577927\n",
      "   -0.11698268]\n",
      " [ -0.517231     1.          -1.8382355  ...   1.1521709   -6.6695824\n",
      "    1.193146  ]\n",
      " ...\n",
      " [ -0.5173111    1.          -1.8382092  ...  -2.6276207    7.614419\n",
      "   -0.15176862]\n",
      " [ -0.51719475   1.          -1.8383007  ...   0.72860885  -5.7999096\n",
      "    0.48218584]\n",
      " [ -0.5171728    1.          -1.8382778  ...  -0.25593275 -10.266029\n",
      "    1.0351555 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.07608795  1.         -1.90802    ...  0.57120275  0.39914465\n",
      "   0.23061213]\n",
      " [ 0.07597065  1.         -1.9079561  ... -1.0941187  -6.318372\n",
      "  -0.86117935]\n",
      " [ 0.0760231   1.         -1.9079705  ... -3.3218236   9.8394985\n",
      "   0.7659475 ]\n",
      " ...\n",
      " [ 0.07594299  1.         -1.9079685  ...  0.80888164  4.2795134\n",
      "  -0.99036324]\n",
      " [ 0.07606125  1.         -1.9080353  ...  0.97865736 -7.0891247\n",
      "   1.6491172 ]\n",
      " [ 0.07608318  1.         -1.9080162  ... -0.0402478  -6.496538\n",
      "  -0.5820959 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.66224194  1.         -1.7910652  ... -0.26532698  0.3753481\n",
      "   0.04424286]\n",
      " [ 0.66213036  1.         -1.7910633  ... -0.62331104 -2.0421696\n",
      "  -0.24001735]\n",
      " [ 0.66218376  1.         -1.7910402  ...  0.08278012  1.2525339\n",
      "   0.05084842]\n",
      " ...\n",
      " [ 0.6621132   1.         -1.7910738  ... -0.77799726  7.145072\n",
      "   0.26259434]\n",
      " [ 0.6622143   1.         -1.7910728  ...  3.0521529  -2.1974313\n",
      "  -0.9421444 ]\n",
      " [ 0.66223717  1.         -1.7910633  ...  2.4893486  10.704004\n",
      "   0.2669148 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1834927   1.         -1.4987049  ...  0.00656891 -0.32849884\n",
      "   0.01583493]\n",
      " [ 1.1833944   1.         -1.498703   ...  0.01167059  0.36795568\n",
      "   0.02713252]\n",
      " [ 1.1834526   1.         -1.49866    ... -0.8563068   1.187077\n",
      "  -0.04415639]\n",
      " ...\n",
      " [ 1.1833973   1.         -1.4987106  ... -0.08116478 -3.244471\n",
      "   2.2727365 ]\n",
      " [ 1.1834736   1.         -1.4987125  ... -0.894501   -1.5035524\n",
      "   0.30334687]\n",
      " [ 1.1834898   1.         -1.4987087  ...  0.8654567   5.9409885\n",
      "  -0.08624583]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.1       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.9000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5888233   1.         -1.0595474  ...  1.652111   -2.2822049\n",
      "  -0.10004404]\n",
      " [ 1.588747    1.         -1.0595531  ...  2.6352618   5.047432\n",
      "  -0.16309929]\n",
      " [ 1.5887833   1.         -1.0594988  ...  0.2548666   1.7916245\n",
      "  -0.39008862]\n",
      " ...\n",
      " [ 1.5887508   1.         -1.0595598  ...  0.11799366 -1.4628782\n",
      "   0.25369537]\n",
      " [ 1.5887966   1.         -1.059555   ...  2.2277544  -7.635364\n",
      "  -0.3817569 ]\n",
      " [ 1.5888214   1.         -1.0595512  ...  1.6411806   7.837986\n",
      "   1.1648469 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.1       0.        0.        0.\n",
      " 0.5       0.        0.5       0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8384209e+00  1.0000000e+00 -5.1702309e-01 ...  3.9561093e+00\n",
      "  -4.8912039e+00 -1.0710752e+00]\n",
      " [ 1.8383818e+00  1.0000000e+00 -5.1705647e-01 ... -1.2749815e-01\n",
      "   2.7759473e+00  5.0449514e-01]\n",
      " [ 1.8383961e+00  1.0000000e+00 -5.1699060e-01 ... -8.2753849e-01\n",
      "   4.8970380e+00  1.3067365e-02]\n",
      " ...\n",
      " [ 1.8383884e+00  1.0000000e+00 -5.1706409e-01 ... -6.7574596e-01\n",
      "  -2.6495762e+00 -1.4644398e-01]\n",
      " [ 1.8384037e+00  1.0000000e+00 -5.1703453e-01 ...  8.3806877e+00\n",
      "  -1.7860153e+01  1.9461895e+00]\n",
      " [ 1.8384247e+00  1.0000000e+00 -5.1703072e-01 ... -1.2493788e+00\n",
      "  -1.3815309e+01 -2.6229593e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.9000001  0.         0.         0.\n",
      " 0.         1.0000001  1.0000001  0.         1.0000001  0.4\n",
      " 0.         0.70000005 1.0000001  0.         1.0000001  0.\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:1, Score:8.86, Best Score:8.86, Average Score:8.86, Best Avg Score:8.86\n",
      "Episode number: 2\n",
      "Hand Exit\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7f2d81ed9640>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7908936   1.          0.6626568  ... -0.35272884 -0.6545396\n",
      "  -0.01796898]\n",
      " [ 1.7908964   1.          0.66270065 ... -2.084419   -6.133582\n",
      "   2.1332753 ]\n",
      " [ 1.7908573   1.          0.66273975 ... -0.07182992 -3.3984709\n",
      "   0.14273238]\n",
      " ...\n",
      " [ 1.7908936   1.          0.662693   ... -0.4433468  -3.9166555\n",
      "   1.4882361 ]\n",
      " [ 1.790863    1.          0.66264534 ... -3.281359    6.038479\n",
      "   0.79435116]\n",
      " [ 1.790904    1.          0.66264915 ... -0.13173184  0.5930958\n",
      "   0.03239012]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         0.70000005 1.0000001  0.\n",
      " 0.         0.9000001  1.0000001  0.         1.0000001  0.6\n",
      " 0.         0.8000001  1.0000001  0.         0.4        0.\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4985981   1.          1.1834717  ...  0.0763278  -0.4761753\n",
      "   0.00647552]\n",
      " [ 1.4986248   1.          1.183507   ... -0.7514076  -0.19912624\n",
      "   0.31045985]\n",
      " [ 1.4985542   1.          1.1835382  ... -0.6922788  -0.32478905\n",
      "  -0.9119921 ]\n",
      " ...\n",
      " [ 1.4986191   1.          1.1835003  ...  0.537262   -1.3058877\n",
      "   0.18346189]\n",
      " [ 1.4985695   1.          1.1834602  ... -2.7558088   3.4425368\n",
      "   0.28434238]\n",
      " [ 1.4986134   1.          1.183464   ...  0.7912743  -3.5984592\n",
      "   1.2468892 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 1.0000001 0.        0.\n",
      " 0.        0.1       0.        1.0000001 0.        0.        0.\n",
      " 0.9000001 0.        0.        0.        0.6       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0591965e+00  1.0000000e+00  1.5889130e+00 ...  1.2326598e-01\n",
      "  -1.7619133e-03  9.2849135e-03]\n",
      " [ 1.0592299e+00  1.0000000e+00  1.5889177e+00 ... -2.6970160e-01\n",
      "  -7.5814819e-01  2.7193582e-01]\n",
      " [ 1.0591583e+00  1.0000000e+00  1.5889382e+00 ...  1.6949172e-01\n",
      "   5.0980711e-01 -8.2465410e-03]\n",
      " ...\n",
      " [ 1.0592365e+00  1.0000000e+00  1.5889120e+00 ...  1.3648992e+00\n",
      "   8.6084604e+00 -1.1345992e+00]\n",
      " [ 1.0591774e+00  1.0000000e+00  1.5889015e+00 ... -6.5683341e+00\n",
      "   9.4993496e+00  8.0195355e-01]\n",
      " [ 1.0592127e+00  1.0000000e+00  1.5889053e+00 ...  3.8961806e+00\n",
      "  -1.0640285e+01 -1.0936340e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4        0.         0.         1.0000001  0.70000005 0.\n",
      " 0.         0.         0.         0.         0.6        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.1630878e-01  1.0000000e+00  1.8384571e+00 ...  8.3495331e-01\n",
      "  -1.3660679e+00  2.0785861e-02]\n",
      " [ 5.1634598e-01  1.0000000e+00  1.8384552e+00 ... -4.0062547e-02\n",
      "  -6.7074871e-01  1.2259698e-01]\n",
      " [ 5.1626205e-01  1.0000000e+00  1.8384596e+00 ...  1.1710873e+00\n",
      "   6.6269283e+00 -7.6827651e-01]\n",
      " ...\n",
      " [ 5.1635361e-01  1.0000000e+00  1.8384504e+00 ...  8.6755097e-01\n",
      "   1.0809902e+01  1.0022680e+00]\n",
      " [ 5.1628685e-01  1.0000000e+00  1.8384457e+00 ... -4.6443148e+00\n",
      "   3.8826120e+00 -4.1833878e-02]\n",
      " [ 5.1632595e-01  1.0000000e+00  1.8384514e+00 ... -5.3130260e+00\n",
      "   1.1423604e+01 -2.0197928e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.07714558   1.           1.907999   ...   3.0735419   -6.1857715\n",
      "   -1.2244447 ]\n",
      " [ -0.07710743   1.           1.9080067  ...   0.23722231  -0.90297794\n",
      "   -0.04037201]\n",
      " [ -0.07720757   1.           1.9079826  ...  -2.9362233  -16.274729\n",
      "   -1.0326767 ]\n",
      " ...\n",
      " [ -0.07711411   1.           1.908001   ...   0.6097361  -11.320663\n",
      "    0.92983586]\n",
      " [ -0.07718086   1.           1.9079876  ...  -0.09476924   1.7750959\n",
      "   -1.5130649 ]\n",
      " [ -0.07712841   1.           1.9079933  ...  -3.210497    10.409912\n",
      "   -0.93203676]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.6632328    1.           1.790678   ...   0.67410827  -3.629025\n",
      "   -0.58569247]\n",
      " [ -0.66319466   1.           1.7907076  ...   1.5807735    9.794935\n",
      "   -0.42689592]\n",
      " [ -0.6633072    1.           1.7906619  ...  -0.38016772  -4.8434854\n",
      "    0.07196406]\n",
      " ...\n",
      " [ -0.66321564   1.           1.7907038  ...   0.5852965  -13.11281\n",
      "    0.1707899 ]\n",
      " [ -0.6632824    1.           1.7906628  ...   1.1924111    0.7600622\n",
      "   -0.8444723 ]\n",
      " [ -0.66321564   1.           1.7906761  ...   0.72838545  -4.902012\n",
      "   -0.6772803 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1839199   1.          1.4980679  ...  1.536694   -7.330382\n",
      "  -1.5205007 ]\n",
      " [-1.1838875   1.          1.4981203  ...  0.93453234  8.370008\n",
      "   0.07328737]\n",
      " [-1.1840153   1.          1.498045   ... -2.022429   -9.7857\n",
      "   0.5843897 ]\n",
      " ...\n",
      " [-1.1839333   1.          1.4981165  ...  0.16579846  2.2561107\n",
      "   0.4435687 ]\n",
      " [-1.1839924   1.          1.4980392  ...  0.2477066   2.7064822\n",
      "  -0.86539865]\n",
      " [-1.1839027   1.          1.498066   ...  0.3830942  -2.755055\n",
      "  -0.4070627 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.5890207    1.           1.058773   ...   0.24482538   0.46302986\n",
      "    0.03928745]\n",
      " [ -1.5890017    1.           1.0588531  ...  -0.03617316  13.557302\n",
      "    0.3234822 ]\n",
      " [ -1.5890923    1.           1.0587711  ...  -0.8111834    5.0689425\n",
      "    0.30119407]\n",
      " ...\n",
      " [ -1.5890236    1.           1.0588446  ...  -0.02079609   4.7583933\n",
      "   -0.13209769]\n",
      " [ -1.5890713    1.           1.0587387  ...   0.23643446   4.2246675\n",
      "   -0.8058516 ]\n",
      " [ -1.5890055    1.           1.0587711  ...   4.1246448  -15.63747\n",
      "   -1.155078  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.5        0.         0.         0.70000005 0.9000001\n",
      " 0.         0.         0.         1.0000001  0.70000005 0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8383646   1.          0.51555824 ...  0.7536222  -1.7666845\n",
      "  -0.93158674]\n",
      " [-1.8383646   1.          0.51566505 ...  0.1440821   8.127999\n",
      "  -1.2176461 ]\n",
      " [-1.83844     1.          0.5155706  ... -1.5122747  14.93556\n",
      "  -1.8989639 ]\n",
      " ...\n",
      " [-1.8383961   1.          0.51565456 ... -0.01496747  9.724901\n",
      "  -0.1737785 ]\n",
      " [-1.838419    1.          0.5155182  ...  3.7826133   8.709889\n",
      "   1.6943235 ]\n",
      " [-1.8383512   1.          0.51555634 ...  0.04668415 -3.057783\n",
      "   0.2752203 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.9000001 0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.3       0.2       1.0000001 1.0000001 0.        0.\n",
      " 0.        1.0000001 0.1       0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9078026   1.         -0.07779503 ... -0.4439432  -0.5858965\n",
      "   0.0571605 ]\n",
      " [-1.907814    1.         -0.07770729 ... -1.1215159  -1.9547482\n",
      "   0.04467499]\n",
      " [-1.9078617   1.         -0.07780927 ... -1.0224322   5.995981\n",
      "   0.24793974]\n",
      " ...\n",
      " [-1.9078407   1.         -0.07771778 ...  0.44938287  2.2354183\n",
      "  -0.27210423]\n",
      " [-1.9078407   1.         -0.07783508 ... -1.5784146  -4.963784\n",
      "   0.28019276]\n",
      " [-1.9077892   1.         -0.07779694 ... -1.6275007   3.4963498\n",
      "   0.33397597]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 1.0000001 0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 1.0000001 1.0000001 1.0000001 0.        0.\n",
      " 0.        0.9000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.7904482    1.          -0.6636028  ...  -0.8167775   -4.084271\n",
      "   -0.23448414]\n",
      " [ -1.7904854    1.          -0.6635227  ...  -0.97725654  -2.8721166\n",
      "   -0.41572607]\n",
      " [ -1.7904682    1.          -0.66363084 ...  -1.7511822    6.0175085\n",
      "    0.6039685 ]\n",
      " ...\n",
      " [ -1.7904682    1.          -0.6635332  ...   1.2238284    7.744355\n",
      "   -0.5397673 ]\n",
      " [ -1.7904453    1.          -0.66363907 ...  -2.5913367  -10.358113\n",
      "   -0.18166506]\n",
      " [ -1.7904358    1.          -0.66360474 ...   0.99167585   2.7968338\n",
      "   -0.11898619]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001  1.0000001  0.         0.         0.         0.\n",
      " 0.         1.0000001  1.0000001  1.0000001  1.0000001  1.0000001\n",
      " 0.         0.         0.70000005 0.         0.         0.8000001\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.497549     1.          -1.1845131  ...  -0.0258413    9.305468\n",
      "   -0.8242637 ]\n",
      " [ -1.4976082    1.          -1.1844606  ...  -3.6123939  -12.02108\n",
      "    1.3227823 ]\n",
      " [ -1.4975929    1.          -1.1845495  ...  -2.8820782    6.414466\n",
      "    0.53531605]\n",
      " ...\n",
      " [ -1.4976158    1.          -1.1844692  ...   1.5184557    4.53592\n",
      "   -0.6772583 ]\n",
      " [ -1.4975624    1.          -1.1845455  ...  -3.3640862  -11.782026\n",
      "    1.153629  ]\n",
      " [ -1.4975357    1.          -1.184515   ...  -0.3716822    2.1435013\n",
      "    1.1078115 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 1.0000001 0.5       0.5       1.0000001 0.3       0.        0.\n",
      " 0.4       0.        0.        1.0000001 0.9000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0581865   1.         -1.5893993  ... -1.2035512  -7.826872\n",
      "  -0.5733851 ]\n",
      " [-1.0582561   1.         -1.5893888  ...  4.816212   10.217198\n",
      "   0.18878475]\n",
      " [-1.0581989   1.         -1.589457   ... -3.322315    6.0235996\n",
      "   1.2586777 ]\n",
      " ...\n",
      " [-1.058239    1.         -1.5893946  ...  1.1656173   4.4840584\n",
      "  -1.0571253 ]\n",
      " [-1.0581627   1.         -1.589407   ...  0.8556347  10.175085\n",
      "   0.25817835]\n",
      " [-1.0581732   1.         -1.5893955  ...  0.37844503  0.11865306\n",
      "   0.6963978 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.3       0.        0.        0.        0.        0.\n",
      " 0.2       0.        0.        0.9000001 0.        0.        0.\n",
      " 0.        0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.5148821    1.          -1.8387356  ...  -0.12162256  -4.4458365\n",
      "    0.4569869 ]\n",
      " [ -0.5149584    1.          -1.838747   ...   2.6340473    8.67608\n",
      "    0.10964862]\n",
      " [ -0.51490974   1.          -1.8387822  ...   7.998111   -10.122648\n",
      "    1.0202006 ]\n",
      " ...\n",
      " [ -0.5149574    1.          -1.8387508  ...   0.07649994   0.30808306\n",
      "   -0.12119031]\n",
      " [ -0.5148697    1.          -1.838728   ...   0.03458339  11.924826\n",
      "    0.6837316 ]\n",
      " [ -0.5148678    1.          -1.8387299  ...   0.44204903  -1.4102397\n",
      "    0.32594037]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.07845974  1.         -1.907917   ... -2.0574327  -7.1127887\n",
      "   0.941621  ]\n",
      " [ 0.07838154  1.         -1.9079666  ...  2.45888     5.9336286\n",
      "   1.4351438 ]\n",
      " [ 0.07840919  1.         -1.9079589  ... -5.0209885   3.4875517\n",
      "  -0.02422345]\n",
      " ...\n",
      " [ 0.0783577   1.         -1.9079676  ...  2.6979918   3.6182253\n",
      "  -0.5972014 ]\n",
      " [ 0.07844925  1.         -1.9078922  ...  1.8708603  -9.7367735\n",
      "   0.8312023 ]\n",
      " [ 0.07847404  1.         -1.9079094  ...  3.9701247  -5.543866\n",
      "  -0.74679565]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.66427517   1.          -1.7902889  ...   0.14464104   4.994597\n",
      "    0.275751  ]\n",
      " [  0.66419697   1.          -1.7903929  ...  -0.5565675  -13.600906\n",
      "   -0.15991926]\n",
      " [  0.6641941    1.          -1.7903699  ...   0.8228283    0.6243546\n",
      "    0.35568702]\n",
      " ...\n",
      " [  0.6641464    1.          -1.7903938  ...   5.210061     8.766039\n",
      "   -0.6283647 ]\n",
      " [  0.66423416   1.          -1.7902546  ...   4.0319476  -10.879949\n",
      "   -0.41417933]\n",
      " [  0.6642885    1.          -1.7902794  ...  -8.7073      11.285931\n",
      "   -0.25613838]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.1852646    1.          -1.4972744  ...   1.1066896   -6.6896787\n",
      "    0.23248142]\n",
      " [  1.1851988    1.          -1.4973621  ...   0.5245881   -7.20133\n",
      "   -0.77847815]\n",
      " [  1.1852016    1.          -1.4973313  ...  -0.49831486  -0.45982313\n",
      "   -0.11321841]\n",
      " ...\n",
      " [  1.1851597    1.          -1.497364   ...  -7.8071485  -11.870198\n",
      "    0.19905639]\n",
      " [  1.1852398    1.          -1.4972363  ...  -1.8916831    9.499168\n",
      "   -0.21667808]\n",
      " [  1.1852732    1.          -1.497263   ...  -3.2653008    3.9725742\n",
      "    0.38351074]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.1       0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5898819    1.          -1.0578995  ...  -0.16051656  -4.705424\n",
      "   -1.2680343 ]\n",
      " [  1.5898333    1.          -1.0579929  ...  -0.30805328   5.750758\n",
      "    0.3053235 ]\n",
      " [  1.5898209    1.          -1.0579599  ...   1.7754674   -2.1234684\n",
      "   -0.2661087 ]\n",
      " ...\n",
      " [  1.5897884    1.          -1.0579967  ...   0.3027954    0.3232789\n",
      "    1.1299322 ]\n",
      " [  1.5898552    1.          -1.0578575  ...  -2.5841823   14.000235\n",
      "    1.0277119 ]\n",
      " [  1.5898857    1.          -1.0578842  ... -12.258956    12.169238\n",
      "    0.9443339 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.1       0.        0.        0.2       0.6       0.        0.\n",
      " 0.3       0.        0.4       0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8389273   1.         -0.5144329  ...  1.9953458   4.196126\n",
      "   0.15658912]\n",
      " [ 1.8389006   1.         -0.5145035  ... -1.4438252  14.231499\n",
      "   1.3986098 ]\n",
      " [ 1.8388653   1.         -0.5144736  ...  2.1918805  -2.3726773\n",
      "  -0.99355316]\n",
      " ...\n",
      " [ 1.8388443   1.         -0.51450825 ...  4.6878157  10.432786\n",
      "   0.04798776]\n",
      " [ 1.8388844   1.         -0.51439095 ...  2.4183242  -6.6011333\n",
      "   0.23436141]\n",
      " [ 1.8389273   1.         -0.51441765 ... -1.4551344   1.2701678\n",
      "  -1.5489787 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.1        0.         0.         0.70000005 0.1\n",
      " 0.         1.0000001  0.         0.9000001  1.0000001  0.2\n",
      " 0.         0.         1.0000001  0.         0.         0.\n",
      " 0.9000001  0.3       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9079361   1.          0.07889938 ...  1.5777333   5.5511675\n",
      "   0.11620176]\n",
      " [ 1.9079351   1.          0.07885075 ... -3.4372618   9.936924\n",
      "   0.52798826]\n",
      " [ 1.9078903   1.          0.07888014 ... -0.20201492  0.41406918\n",
      "  -0.54417133]\n",
      " ...\n",
      " [ 1.9078884   1.          0.07884502 ...  1.2550828   5.7785263\n",
      "   1.2353659 ]\n",
      " [ 1.907898    1.          0.07894135 ...  1.8885725  -5.590463\n",
      "  -0.45990527]\n",
      " [ 1.9079323   1.          0.07891464 ...  1.2939155  -0.6145587\n",
      "  -0.67338705]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.8000001 0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        1.0000001 1.0000001 0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        1.0000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.7901754    1.           0.6647053  ...  -0.5648308    2.0365152\n",
      "    0.57728493]\n",
      " [  1.7901993    1.           0.66462994 ...   4.5173407   -5.532171\n",
      "    1.077143  ]\n",
      " [  1.7901592    1.           0.66467625 ...   0.31232715  -0.46180964\n",
      "   -0.02351403]\n",
      " ...\n",
      " [  1.7901802    1.           0.66462517 ...   1.5210586    8.778427\n",
      "    0.6901685 ]\n",
      " [  1.790142     1.           0.6647415  ...   4.1799574  -18.241962\n",
      "   -1.315275  ]\n",
      " [  1.790164     1.           0.6647167  ...  -1.4630024    1.4262775\n",
      "    0.34579575]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 1.0000001 0.        0.        0.5       0.\n",
      " 0.6       0.        1.0000001 0.9000001 0.        0.        0.\n",
      " 0.3       0.        0.        0.        0.9000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.4971495    1.           1.1854019  ...  -0.9833834   -1.5659771\n",
      "    0.03010994]\n",
      " [  1.4971914    1.           1.1853313  ...   1.017941    -1.3613772\n",
      "   -0.05543685]\n",
      " [  1.4971409    1.           1.1853682  ...   5.492316   -10.818869\n",
      "    0.6418688 ]\n",
      " ...\n",
      " [  1.4971676    1.           1.1853285  ...  -0.77755105   1.4890203\n",
      "    0.30595663]\n",
      " [  1.4971027    1.           1.1854343  ...  -0.0723058   -7.8442664\n",
      "   -0.33008975]\n",
      " [  1.4971361    1.           1.1854115  ...   0.82705903   0.6620095\n",
      "   -0.6830493 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.6      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0572329   1.          1.590292   ... -2.327376   -4.9402075\n",
      "  -0.48051584]\n",
      " [ 1.0572968   1.          1.5902061  ...  2.1147943  -5.0612655\n",
      "  -0.7846919 ]\n",
      " [ 1.0572052   1.          1.5902405  ... -0.44009686  1.1552687\n",
      "  -0.00678427]\n",
      " ...\n",
      " [ 1.0572453   1.          1.5902052  ... -1.3332018   3.0276527\n",
      "   1.1119349 ]\n",
      " [ 1.0571518   1.          1.590313   ...  1.8977405  -4.8615174\n",
      "  -0.43447563]\n",
      " [ 1.0572157   1.          1.5902996  ... -0.25155818  1.543638\n",
      "   0.01316115]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.2 0.1 0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.5138912   1.          1.8391762  ...  5.044621    6.0732365\n",
      "   0.58871955]\n",
      " [ 0.5139704   1.          1.8391314  ...  2.854992   -5.140817\n",
      "  -1.6321682 ]\n",
      " [ 0.5138855   1.          1.83914    ...  0.16087925 -0.34224463\n",
      "  -0.01304525]\n",
      " ...\n",
      " [ 0.5139332   1.          1.8391304  ... -3.1058054  10.548164\n",
      "  -0.4598384 ]\n",
      " [ 0.51382065  1.          1.8391876  ...  0.27696228  5.4885817\n",
      "  -0.04300076]\n",
      " [ 0.51387215  1.          1.8391819  ... -0.3373226   1.35513\n",
      "   0.15752518]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.9477310e-02  1.0000000e+00  1.9079819e+00 ...  3.3192050e+00\n",
      "   5.3337450e+00  8.7958038e-02]\n",
      " [-7.9394341e-02  1.0000000e+00  1.9079580e+00 ...  1.6519504e+00\n",
      "  -7.9092636e+00 -1.2396168e+00]\n",
      " [-7.9492569e-02  1.0000000e+00  1.9079436e+00 ...  3.4070134e-02\n",
      "  -2.3451471e-01 -3.1930134e-03]\n",
      " ...\n",
      " [-7.9439163e-02  1.0000000e+00  1.9079571e+00 ...  1.1234069e-01\n",
      "  -8.2768059e-01  2.1271327e-01]\n",
      " [-7.9561234e-02  1.0000000e+00  1.9079781e+00 ...  1.4817927e+00\n",
      "   9.8874273e+00  7.9562086e-01]\n",
      " [-7.9496384e-02  1.0000000e+00  1.9079838e+00 ... -6.3968682e-01\n",
      "   2.1103740e+00  5.2618158e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.6527176e-01  1.0000000e+00  1.7900238e+00 ... -5.1801424e+00\n",
      "  -8.7451248e+00  1.8758047e-01]\n",
      " [-6.6519165e-01  1.0000000e+00  1.7900190e+00 ... -5.7124719e-02\n",
      "   1.3215422e+01 -1.2783377e+00]\n",
      " [-6.6529274e-01  1.0000000e+00  1.7899729e+00 ...  1.2952816e-01\n",
      "  -3.1311989e-01 -1.0039717e-02]\n",
      " ...\n",
      " [-6.6524506e-01  1.0000000e+00  1.7900171e+00 ... -5.0948210e+00\n",
      "   1.0743456e+01  2.2585496e-01]\n",
      " [-6.6535187e-01  1.0000000e+00  1.7900009e+00 ...  1.2524468e-01\n",
      "   4.5156612e+00  2.6152149e-01]\n",
      " [-6.6529083e-01  1.0000000e+00  1.7900181e+00 ... -2.0597768e+00\n",
      "   5.1799407e+00  7.0265937e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.1859646    1.           1.4967175  ...   0.4749608    0.6027427\n",
      "    0.04889563]\n",
      " [ -1.185895     1.           1.4967375  ...  -3.2899613  -12.373846\n",
      "    0.9698132 ]\n",
      " [ -1.186018     1.           1.4966747  ...  -0.57830405   3.184743\n",
      "    0.08156565]\n",
      " ...\n",
      " [ -1.1859837    1.           1.4967327  ...  -3.6835485    5.7261715\n",
      "    1.0283616 ]\n",
      " [ -1.1860657    1.           1.4966755  ...  -1.1237758    1.2779188\n",
      "    1.0142455 ]\n",
      " [ -1.1859779    1.           1.496706   ...  -1.6668098    4.5481825\n",
      "    1.4225117 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5904589   1.          1.0568829  ...  0.18897009  1.4480224\n",
      "   0.14671546]\n",
      " [-1.5904016   1.          1.0569286  ... -0.96334875 -3.656044\n",
      "   0.7535306 ]\n",
      " [-1.5904865   1.          1.0568385  ... -1.7373184   6.49767\n",
      "  -1.1241263 ]\n",
      " ...\n",
      " [-1.5904713   1.          1.0569134  ...  4.127717   -3.2348318\n",
      "   0.2578746 ]\n",
      " [-1.5905228   1.          1.0568295  ...  0.20322818 -0.74214697\n",
      "   0.13636446]\n",
      " [-1.5904694   1.          1.0568695  ...  2.62696    -5.10455\n",
      "   0.5877607 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.9000001  0.         0.70000005 0.         0.\n",
      " 0.         0.         1.0000001  0.2        0.         0.\n",
      " 0.5        0.         0.4        0.9000001  0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8390465   1.          0.51332283 ... -0.43900108  2.5563025\n",
      "   0.80510676]\n",
      " [-1.8390083   1.          0.5134058  ... -1.3955467  -4.153665\n",
      "   0.17571747]\n",
      " [-1.8390713   1.          0.513317   ...  0.21337587  4.8088484\n",
      "  -1.2463152 ]\n",
      " ...\n",
      " [-1.8390732   1.          0.5133867  ...  7.5391636  -7.311159\n",
      "  -0.91232514]\n",
      " [-1.8390903   1.          0.51325226 ...  2.3759282  -6.905833\n",
      "   1.0210199 ]\n",
      " [-1.839056    1.          0.51330566 ... -1.7522871   0.93149436\n",
      "  -1.026112  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        1.0000001 0.2       0.5       0.5\n",
      " 0.9000001 0.        1.0000001 0.        0.4       1.0000001 0.1\n",
      " 0.9000001 0.5       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.9077225    1.          -0.08016586 ...   0.4151528    2.7941325\n",
      "    0.5481435 ]\n",
      " [ -1.9077063    1.          -0.08011436 ...  -5.424838   -14.315958\n",
      "    0.356543  ]\n",
      " [ -1.9077244    1.          -0.08020329 ...  -0.19938517   3.3295207\n",
      "   -0.8310604 ]\n",
      " ...\n",
      " [ -1.9077511    1.          -0.08013344 ...   8.945865    -6.6801047\n",
      "   -1.3485196 ]\n",
      " [ -1.9077263    1.          -0.08023834 ...  -4.408167    13.482726\n",
      "    0.58608043]\n",
      " [ -1.9077253    1.          -0.08018494 ...   0.6704161    1.332627\n",
      "    0.3971936 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         1.0000001  0.70000005 1.0000001  1.0000001  0.1\n",
      " 1.0000001  1.0000001  0.         1.0000001  0.         1.0000001\n",
      " 0.1        1.0000001  0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7894087   1.         -0.6659031  ...  0.4712119   4.170882\n",
      "  -0.13665062]\n",
      " [-1.7894125   1.         -0.6658268  ... -3.2436495  -6.810645\n",
      "  -1.6039809 ]\n",
      " [-1.789402    1.         -0.66592246 ...  1.6144582   3.1107655\n",
      "  -0.5938413 ]\n",
      " ...\n",
      " [-1.789444    1.         -0.66584396 ...  4.7421265  -4.923286\n",
      "  -1.3950421 ]\n",
      " [-1.7893887   1.         -0.66596985 ... -2.271907    7.8536205\n",
      "  -0.19819756]\n",
      " [-1.7894077   1.         -0.66592216 ...  0.6150894   3.1075673\n",
      "   0.01580387]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 1.0000001 1.0000001 1.0000001 0.        1.0000001\n",
      " 1.0000001 0.        1.0000001 0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4961281   1.         -1.186266   ... -0.7350385   9.667513\n",
      "  -1.372156  ]\n",
      " [-1.4961519   1.         -1.1862268  ...  2.3693113   4.076752\n",
      "  -0.20545338]\n",
      " [-1.4961262   1.         -1.1863031  ...  3.4189034   9.276201\n",
      "  -0.63214624]\n",
      " ...\n",
      " [-1.4961891   1.         -1.1862411  ...  1.2052531  -0.8062453\n",
      "  -0.30171877]\n",
      " [-1.4961014   1.         -1.1863232  ... -1.5848339   2.8694239\n",
      "  -0.31883103]\n",
      " [-1.4961205   1.         -1.1862831  ...  0.49551404  1.0998278\n",
      "   0.76736504]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.8000001 1.0000001 0.9000001 1.0000001 0.        0.\n",
      " 1.0000001 0.        1.0000001 0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0562696   1.         -1.5906582  ... -0.8652941  -8.939134\n",
      "  -0.6835022 ]\n",
      " [-1.056304    1.         -1.5906487  ...  3.892984    4.975997\n",
      "   0.8127627 ]\n",
      " [-1.0562363   1.         -1.5906936  ... -4.305525   -7.48606\n",
      "   0.45744455]\n",
      " ...\n",
      " [-1.0563126   1.         -1.5906553  ...  1.2531369  -0.47822762\n",
      "  -0.4457609 ]\n",
      " [-1.0562      1.         -1.5906944  ...  0.7047579   2.3067446\n",
      "  -0.56182474]\n",
      " [-1.0562563   1.         -1.5906715  ...  0.82645273 -1.2514448\n",
      "  -0.19508791]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.4       0.        0.5       0.        0.\n",
      " 0.8000001 0.        0.1       0.        0.4       0.        0.5\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.5127001    1.          -1.8393745  ...  -2.6604314  -12.526262\n",
      "    0.4092567 ]\n",
      " [ -0.5127449    1.          -1.8393517  ...   2.7816408    4.8508935\n",
      "    0.5433347 ]\n",
      " [ -0.5126343    1.          -1.8393891  ...  -3.5351183   -8.221962\n",
      "    0.13450801]\n",
      " ...\n",
      " [ -0.5127182    1.          -1.8393507  ...  -0.05708408   0.04719877\n",
      "    0.02192199]\n",
      " [ -0.5125904    1.          -1.839386   ...   0.8628654    3.8511004\n",
      "   -0.7317306 ]\n",
      " [ -0.51268387   1.          -1.839386   ...   0.9443178   -3.1632462\n",
      "   -0.13045108]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.0806551   1.         -1.9078674  ...  0.3645286   6.6676836\n",
      "   0.03906548]\n",
      " [ 0.08060741  1.         -1.9078674  ...  4.167811    7.77195\n",
      "   2.1845093 ]\n",
      " [ 0.0807476   1.         -1.9078717  ... -4.0337048  -7.6355596\n",
      "   0.64802945]\n",
      " ...\n",
      " [ 0.08065987  1.         -1.9078579  ...  0.12933898 -0.44743776\n",
      "  -0.01763785]\n",
      " [ 0.08079338  1.         -1.9078598  ...  0.29392684  1.27455\n",
      "  -0.1219002 ]\n",
      " [ 0.08067131  1.         -1.9078751  ...  5.8409085  -6.3208523\n",
      "  -0.5195382 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.6666746   1.         -1.7892704  ... -0.15132704  3.2776186\n",
      "  -0.00690261]\n",
      " [ 0.66662884  1.         -1.7892895  ... -1.4146724   2.612247\n",
      "   0.9874181 ]\n",
      " [ 0.6667576   1.         -1.789266   ...  1.6623828   1.1725416\n",
      "   0.24300468]\n",
      " ...\n",
      " [ 0.6666775   1.         -1.7892733  ...  0.27663136 -0.95817757\n",
      "   0.02961051]\n",
      " [ 0.66680336  1.         -1.7892418  ...  0.54065585  2.5402179\n",
      "   0.8961656 ]\n",
      " [ 0.6666899   1.         -1.7892761  ...  5.122985   -5.9091616\n",
      "   1.1621454 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1870356   1.         -1.4955959  ... -0.50557166  3.6448598\n",
      "   0.01728481]\n",
      " [ 1.1870012   1.         -1.4956598  ... -0.6643993   3.8094602\n",
      "   0.6863258 ]\n",
      " [ 1.1871395   1.         -1.4956027  ... -0.52537346  1.5839224\n",
      "   0.5959778 ]\n",
      " ...\n",
      " [ 1.1870804   1.         -1.4956312  ... -4.161605    4.898162\n",
      "  -0.89048135]\n",
      " [ 1.1871834   1.         -1.4955349  ... -0.0180732   7.529588\n",
      "   1.4816518 ]\n",
      " [ 1.1870489   1.         -1.4955997  ... -1.037734    1.1961546\n",
      "   0.5285107 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5911207   1.         -1.0556297  ... -1.1502659  10.608614\n",
      "  -0.22589535]\n",
      " [ 1.5911131   1.         -1.0557184  ... -1.8925025  11.821975\n",
      "  -1.7420456 ]\n",
      " [ 1.5912285   1.         -1.0556281  ...  1.1768507   7.448081\n",
      "   0.5774183 ]\n",
      " ...\n",
      " [ 1.5911903   1.         -1.0556784  ...  2.4754472  -6.8651896\n",
      "  -0.36853993]\n",
      " [ 1.5912666   1.         -1.0555382  ...  0.39937425  2.2441854\n",
      "   0.57097894]\n",
      " [ 1.5911283   1.         -1.0556297  ...  4.95755    -3.1174846\n",
      "  -0.01823756]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2       0.        0.        0.        0.3       0.6       0.2\n",
      " 0.        0.        0.        0.        0.        0.8000001 0.\n",
      " 0.        0.        1.0000001 0.4       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8394527   1.         -0.5118885  ... -0.947598    1.9603658\n",
      "   0.28102356]\n",
      " [ 1.839469    1.         -0.51200485 ... -1.2262044   9.128222\n",
      "  -0.711576  ]\n",
      " [ 1.8395405   1.         -0.51190746 ... -0.12936455 -7.3553286\n",
      "  -1.104516  ]\n",
      " ...\n",
      " [ 1.8395233   1.         -0.51196384 ...  3.0620613  -8.157189\n",
      "   0.9252552 ]\n",
      " [ 1.839571    1.         -0.5117874  ...  0.07273158 -6.3667083\n",
      "   0.31672335]\n",
      " [ 1.8394566   1.         -0.5118885  ...  0.2659645  -1.9919827\n",
      "  -1.392178  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.9000001 0.        0.9000001 1.0000001 1.0000001 1.0000001\n",
      " 0.        0.        0.        0.        0.        0.1       0.\n",
      " 0.        0.        1.0000001 1.0000001 0.        0.8000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9076061e+00  1.0000000e+00  8.1821442e-02 ... -1.2963066e+00\n",
      "   7.5476646e-01  9.4765431e-01]\n",
      " [ 1.9076500e+00  1.0000000e+00  8.1698418e-02 ...  3.9090455e-01\n",
      "  -4.1604609e+00 -6.2936616e-01]\n",
      " [ 1.9077034e+00  1.0000000e+00  8.1790358e-02 ... -3.4992027e-01\n",
      "  -6.2944865e+00  1.4370537e-01]\n",
      " ...\n",
      " [ 1.9077091e+00  1.0000000e+00  8.1740379e-02 ...  9.3459129e-01\n",
      "  -4.4208574e+00 -1.3658488e-01]\n",
      " [ 1.9076977e+00  1.0000000e+00  8.1924438e-02 ... -4.0677052e+00\n",
      "   1.0643396e+01 -4.2857653e-01]\n",
      " [ 1.9076033e+00  1.0000000e+00  8.1821442e-02 ... -7.1875781e-01\n",
      "  -1.7483830e-03 -1.0794840e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.9000001 0.        1.0000001 1.0000001 0.4       1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.1\n",
      " 0.        0.        1.0000001 1.0000001 0.9000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.788991     1.           0.6675148  ...   1.1013472   -1.1127114\n",
      "    0.1700716 ]\n",
      " [  1.7890663    1.           0.66736984 ...  -0.07995239   0.3897028\n",
      "   -0.02998212]\n",
      " [  1.7890491    1.           0.6674598  ...  -0.6057871  -10.280672\n",
      "   -0.27565897]\n",
      " ...\n",
      " [  1.7890759    1.           0.66740894 ...   2.0823596   -4.3415\n",
      "   -0.7881852 ]\n",
      " [  1.7890072    1.           0.66760635 ...  -3.119481    12.377887\n",
      "   -0.9182393 ]\n",
      " [  1.7889824    1.           0.6675129  ...   0.67245525   1.3578278\n",
      "   -0.01736569]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 1.0000001 0.        0.2\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        1.0000001 0.3       1.0000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.4953423    1.           1.1876202  ...   4.831775    -6.2917876\n",
      "    1.6193031 ]\n",
      " [  1.4954481    1.           1.1874619  ...  -0.7405554   -7.0381846\n",
      "   -0.6699318 ]\n",
      " [  1.4953976    1.           1.1875441  ...  -0.50346833 -12.713221\n",
      "    0.33371568]\n",
      " ...\n",
      " [  1.4954414    1.           1.1874971  ...  -1.187009    -2.9070096\n",
      "   -1.644376  ]\n",
      " [  1.4953289    1.           1.1876945  ...   3.6528358   -7.5709763\n",
      "   -0.43441802]\n",
      " [  1.4953299    1.           1.1876163  ...  -0.09684289   2.6823452\n",
      "   -0.28427276]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         0.8000001  0.5        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.0000001  0.         0.         0.         0.\n",
      " 1.0000001  0.70000005]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.0550575    1.           1.5915871  ...  -2.7303123    4.7821903\n",
      "   -0.02264982]\n",
      " [  1.055192     1.           1.5914354  ...  -0.92333114  -3.8487463\n",
      "   -0.21514863]\n",
      " [  1.055088     1.           1.5914923  ...   0.80717015  -2.0753121\n",
      "   -0.5042554 ]\n",
      " ...\n",
      " [  1.0551395    1.           1.5914602  ...  -2.4343057  -10.549605\n",
      "   -0.4455453 ]\n",
      " [  1.0550022    1.           1.5916348  ...   3.3074245   -9.263056\n",
      "    0.03455627]\n",
      " [  1.0550442    1.           1.5915833  ...   0.282389     2.319148\n",
      "   -0.5115731 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.51123047  1.          1.8399467  ... -0.98850465  2.5909154\n",
      "  -1.4783775 ]\n",
      " [ 0.51138496  1.          1.8397961  ... -2.6654449  -6.2532353\n",
      "  -1.4807476 ]\n",
      " [ 0.51124     1.          1.8398311  ...  0.646588   -5.812419\n",
      "   0.84119385]\n",
      " ...\n",
      " [ 0.5112972   1.          1.8398056  ... -1.611232   -2.53721\n",
      "  -0.41738275]\n",
      " [ 0.51114845  1.          1.8399601  ...  3.6137676  -9.209459\n",
      "  -0.07166147]\n",
      " [ 0.51121616  1.          1.8399372  ... -0.02350163  1.4818325\n",
      "  -0.43544352]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.08216381   1.           1.9079285  ...  -2.8542666   10.818956\n",
      "   -0.14483964]\n",
      " [ -0.08200169   1.           1.9078083  ...  -2.7647192   -5.3338003\n",
      "   -0.67946124]\n",
      " [ -0.08213234   1.           1.9078145  ...   0.11643505 -17.729641\n",
      "    0.18634292]\n",
      " ...\n",
      " [ -0.08207321   1.           1.9078045  ...  -2.5218253   -3.9684634\n",
      "   -0.5949655 ]\n",
      " [ -0.08222771   1.           1.9079132  ...   0.56963587  -1.6893249\n",
      "    0.19963038]\n",
      " [ -0.08217812   1.           1.9079189  ...  -0.20421746   2.888173\n",
      "   -0.06675124]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.6680145   1.          1.7890892  ...  0.60112643  3.3008196\n",
      "  -0.5805492 ]\n",
      " [-0.6678591   1.          1.7889996  ...  2.356194    5.5430865\n",
      "  -0.04763663]\n",
      " [-0.66799736  1.          1.7889832  ...  0.05968088 -5.6730924\n",
      "   1.6397445 ]\n",
      " ...\n",
      " [-0.66794014  1.          1.7889843  ... -2.8933501  -3.93929\n",
      "   0.06342901]\n",
      " [-0.668087    1.          1.7890625  ...  4.2460933  -9.923059\n",
      "   1.0114812 ]\n",
      " [-0.66802883  1.          1.7890778  ...  0.24167788 11.100315\n",
      "   2.045224  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1882563   1.          1.4950447  ...  0.49509835 -1.3693843\n",
      "  -1.6713586 ]\n",
      " [-1.1881351   1.          1.4949913  ...  2.9586847   2.4751363\n",
      "  -0.61911076]\n",
      " [-1.1882458   1.          1.4949431  ...  0.23068663 -2.5252514\n",
      "   0.3665284 ]\n",
      " ...\n",
      " [-1.1881981   1.          1.4949598  ... -6.8198295  -8.110878\n",
      "  -0.6047516 ]\n",
      " [-1.1883259   1.          1.4949989  ... -1.124743    2.311873\n",
      "  -0.293976  ]\n",
      " [-1.1882706   1.          1.4950333  ... -0.313861   -6.2474465\n",
      "   1.2360392 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.5919666    1.           1.0548992  ...  -1.2324984   -1.1623006\n",
      "    0.0140636 ]\n",
      " [ -1.5918713    1.           1.0548716  ...  -5.607314    -3.1966286\n",
      "   -1.2139115 ]\n",
      " [ -1.5919781    1.           1.0547928  ...   0.3699687  -11.99942\n",
      "    1.0520022 ]\n",
      " ...\n",
      " [ -1.5919399    1.           1.0548334  ...   8.726039     7.702325\n",
      "    0.27515364]\n",
      " [ -1.5920391    1.           1.0548325  ...  -0.20823681   3.193534\n",
      "   -0.4151187 ]\n",
      " [ -1.5919809    1.           1.0548878  ...  -1.4913152    0.86783075\n",
      "    0.43532136]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8401518e+00  1.0000000e+00  5.1094818e-01 ... -4.0586624e+00\n",
      "  -8.7333632e+00 -1.7899637e+00]\n",
      " [-1.8400812e+00  1.0000000e+00  5.1093960e-01 ... -1.4860964e-01\n",
      "   2.2561741e-01  6.7630947e-02]\n",
      " [-1.8401566e+00  1.0000000e+00  5.1084262e-01 ...  1.4593698e+00\n",
      "  -8.9123631e+00  9.6864498e-01]\n",
      " ...\n",
      " [-1.8401318e+00  1.0000000e+00  5.1089573e-01 ...  3.5637474e-01\n",
      "   7.1797252e-01 -8.6076856e-03]\n",
      " [-1.8401890e+00  1.0000000e+00  5.1086807e-01 ...  6.7005944e-01\n",
      "   2.6505208e+00 -7.3559207e-01]\n",
      " [-1.8401651e+00  1.0000000e+00  5.1093674e-01 ...  9.5895171e-01\n",
      "  -3.9190245e-01  1.6467469e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.3       0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.9000001 0.1       0.6\n",
      " 0.        0.        0.        0.4       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9079714   1.         -0.08270454 ...  3.1013112   6.250717\n",
      "  -0.60247475]\n",
      " [-1.9079247   1.         -0.0827055  ...  1.5997138   2.5913033\n",
      "   0.90613717]\n",
      " [-1.9079609   1.         -0.08280981 ... -1.7988162   5.534511\n",
      "   0.7136122 ]\n",
      " ...\n",
      " [-1.9079475   1.         -0.08274937 ...  0.21119547  1.1008182\n",
      "   0.08185922]\n",
      " [-1.9079819   1.         -0.08278465 ...  0.31302443  9.612654\n",
      "   1.1599169 ]\n",
      " [-1.9079838   1.         -0.08271599 ...  0.34939003 -0.26742887\n",
      "   0.14857048]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 1.0000001 0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        1.0000001 1.0000001 1.0000001\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7890816   1.         -0.6680393  ...  2.6311345   5.1393824\n",
      "   0.730907  ]\n",
      " [-1.7890539   1.         -0.66801834 ...  4.138737    5.8427553\n",
      "   0.40018082]\n",
      " [-1.7890682   1.         -0.6681057  ... -0.01695192  1.9765391\n",
      "  -0.1149289 ]\n",
      " ...\n",
      " [-1.7890663   1.         -0.6680603  ...  0.7277212   1.9635234\n",
      "   0.18112415]\n",
      " [-1.789072    1.         -0.6681175  ...  0.25843117  3.921216\n",
      "   0.10841501]\n",
      " [-1.789094    1.         -0.66805077 ...  4.3228517  -4.5730486\n",
      "   0.54012394]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  1.0000001  0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         1.0000001\n",
      " 1.0000001  1.0000001  0.         0.         0.         1.0000001\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4949913   1.         -1.1882324  ... -0.5777645  -6.02719\n",
      "  -0.14477557]\n",
      " [-1.4949732   1.         -1.1882696  ... -0.25514174 -0.31288242\n",
      "   0.37823528]\n",
      " [-1.4949455   1.         -1.1883276  ... -1.6522632   8.759138\n",
      "  -0.6625085 ]\n",
      " ...\n",
      " [-1.4949627   1.         -1.1883068  ...  0.5221906   1.7211523\n",
      "   0.44329178]\n",
      " [-1.4949474   1.         -1.1883049  ...  0.71612644 12.734819\n",
      "   0.29274365]\n",
      " [-1.4950037   1.         -1.1882439  ...  6.710101   -8.592342\n",
      "   0.09356579]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 1.0000001 1.0000001\n",
      " 0.        0.        0.        0.8000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0544195   1.         -1.5919476  ... -0.15698743 -2.2098184\n",
      "   0.6539103 ]\n",
      " [-1.0544071   1.         -1.5919924  ...  3.3527942   7.470133\n",
      "   0.6881075 ]\n",
      " [-1.0543423   1.         -1.5920377  ...  0.16635111  2.634749\n",
      "  -0.5348178 ]\n",
      " ...\n",
      " [-1.0543747   1.         -1.5920172  ...  0.97285247  1.7540178\n",
      "  -0.07045126]\n",
      " [-1.0543346   1.         -1.5920067  ... -1.2426127  -9.54641\n",
      "   1.6186075 ]\n",
      " [-1.0544319   1.         -1.591959   ...  5.0028806  -5.2107897\n",
      "  -1.2723191 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001 0.2       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.3\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.5107937   1.         -1.8398705  ... -1.6945424  -8.709587\n",
      "   0.012411  ]\n",
      " [-0.5107832   1.         -1.8399286  ...  0.06555206  4.2582493\n",
      "   2.015513  ]\n",
      " [-0.5106697   1.         -1.8399572  ...  0.67007047 -1.7551289\n",
      "  -0.41795573]\n",
      " ...\n",
      " [-0.51070976  1.         -1.8399401  ...  2.25338     3.7522902\n",
      "  -0.32357168]\n",
      " [-0.51065254  1.         -1.8399048  ... -0.01503903 -3.8863697\n",
      "  -0.08992362]\n",
      " [-0.5108061   1.         -1.8398819  ... -1.9907378   4.718034\n",
      "  -0.04254317]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.08302689  1.         -1.9076385  ... -1.008464    4.4572763\n",
      "  -0.20520896]\n",
      " [ 0.08303738  1.         -1.907692   ... -0.16224554  3.8065846\n",
      "   0.5017989 ]\n",
      " [ 0.08310318  1.         -1.9076933  ... -0.5375716   0.81043196\n",
      "   0.0939523 ]\n",
      " ...\n",
      " [ 0.0830574   1.         -1.9076853  ...  1.9910231   3.287909\n",
      "  -1.9323701 ]\n",
      " [ 0.08312035  1.         -1.9076328  ...  0.66385543 -6.8289585\n",
      "   1.3153476 ]\n",
      " [ 0.08301449  1.         -1.90765    ... -3.386706    8.651255\n",
      "   1.363716  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.6860867e-01  1.0000000e+00 -1.7886257e+00 ... -5.5457199e-01\n",
      "   7.2678556e+00  7.3864609e-01]\n",
      " [ 6.6861916e-01  1.0000000e+00 -1.7886381e+00 ... -2.3822737e+00\n",
      "   6.4625831e+00  1.7452650e+00]\n",
      " [ 6.6870689e-01  1.0000000e+00 -1.7886051e+00 ... -1.8927598e-01\n",
      "  -6.6792154e-01  9.0120640e-03]\n",
      " ...\n",
      " [ 6.6866493e-01  1.0000000e+00 -1.7886209e+00 ... -6.6689301e-01\n",
      "  -3.0078855e+00  2.2428048e-01]\n",
      " [ 6.6872406e-01  1.0000000e+00 -1.7885914e+00 ... -2.3021250e+00\n",
      "   9.8047457e+00  5.6769538e-01]\n",
      " [ 6.6859627e-01  1.0000000e+00 -1.7886372e+00 ...  1.8319486e+00\n",
      "  -1.7447803e+00 -5.0955340e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1885719   1.         -1.4944744  ...  2.041678   -3.4500422\n",
      "  -0.18658447]\n",
      " [ 1.1885843   1.         -1.4945059  ... -1.4500313   2.579649\n",
      "   0.28751707]\n",
      " [ 1.1886406   1.         -1.4944445  ... -1.3628745  -5.6602116\n",
      "  -0.18286349]\n",
      " ...\n",
      " [ 1.1886044   1.         -1.4944735  ... -1.9012225  -6.0052543\n",
      "   0.22712958]\n",
      " [ 1.188652    1.         -1.4944115  ... -0.40384698  4.673884\n",
      "  -0.2684662 ]\n",
      " [ 1.1885614   1.         -1.4944859  ... -0.37363076  1.7688992\n",
      "   0.05422002]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5924797   1.         -1.053751   ...  1.9630866  -4.889924\n",
      "  -0.12269795]\n",
      " [ 1.5924988   1.         -1.0537233  ... -3.2969716   5.1315703\n",
      "   1.0408431 ]\n",
      " [ 1.5925674   1.         -1.0536637  ... -0.12891436 -4.125753\n",
      "  -0.7083738 ]\n",
      " ...\n",
      " [ 1.5925331   1.         -1.0536814  ... -0.05075121  1.2101007\n",
      "  -0.21379654]\n",
      " [ 1.5925751   1.         -1.0536613  ... -1.4453325   5.0671167\n",
      "  -1.5028234 ]\n",
      " [ 1.5924759   1.         -1.0537663  ...  0.02106309  0.92943275\n",
      "  -0.1428645 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.3       0.        0.5\n",
      " 0.        0.        0.        0.        0.8000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8401814   1.         -0.51019096 ... -0.5412488  -2.4060988\n",
      "  -0.7191614 ]\n",
      " [ 1.8402061   1.         -0.51014996 ... -2.2606034   3.183934\n",
      "   0.2584961 ]\n",
      " [ 1.8401947   1.         -0.51008433 ... -1.245568   -4.5348163\n",
      "  -0.6497015 ]\n",
      " ...\n",
      " [ 1.8401699   1.         -0.51010036 ...  0.43109202  0.32046843\n",
      "   0.09672105]\n",
      " [ 1.8401985   1.         -0.5100937  ...  0.6930262  -9.438292\n",
      "  -0.15601552]\n",
      " [ 1.8401852   1.         -0.51021004 ...  0.8226558  -3.2656481\n",
      "  -0.49229977]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9077902e+00  1.0000000e+00  8.3375931e-02 ... -3.0450490e+00\n",
      "   5.5550265e+00  1.9404417e-01]\n",
      " [ 1.9078150e+00  1.0000000e+00  8.3450317e-02 ... -6.1806498e+00\n",
      "   6.0066605e+00  1.3716607e+00]\n",
      " [ 1.9078083e+00  1.0000000e+00  8.3519906e-02 ...  2.1770635e+00\n",
      "   8.5707197e+00  1.1487556e+00]\n",
      " ...\n",
      " [ 1.9077892e+00  1.0000000e+00  8.3499908e-02 ... -3.2323563e-01\n",
      "   2.8716040e-01  6.0012043e-03]\n",
      " [ 1.9078083e+00  1.0000000e+00  8.3477020e-02 ... -9.9163765e-01\n",
      "  -1.4718624e+01 -1.6861558e-01]\n",
      " [ 1.9077969e+00  1.0000000e+00  8.3351135e-02 ...  2.1377993e+00\n",
      "  -6.9168348e+00 -1.3969495e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         0.         0.         0.\n",
      " 1.0000001  0.         0.         0.         0.         1.0000001\n",
      " 0.         0.70000005 0.         0.         0.8000001  0.\n",
      " 0.         1.0000001 ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7886      1.          0.66900444 ... -1.6086543   4.943112\n",
      "  -0.50030357]\n",
      " [ 1.7886238   1.          0.6690693  ...  3.157572   -1.0695567\n",
      "  -0.12031043]\n",
      " [ 1.7886333   1.          0.66913366 ...  1.8368007   3.92398\n",
      "  -0.815938  ]\n",
      " ...\n",
      " [ 1.7886238   1.          0.66911316 ... -0.18492222  1.2109227\n",
      "   0.02423817]\n",
      " [ 1.7886333   1.          0.6690903  ... -0.89160347  7.9089966\n",
      "  -0.6633281 ]\n",
      " [ 1.788619    1.          0.66898155 ... -2.6756828  12.739593\n",
      "   0.11450818]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        1.0000001 0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4939556   1.          1.1890831  ... -4.278411    7.6064415\n",
      "  -0.960336  ]\n",
      " [ 1.4939737   1.          1.1891813  ...  2.5339093  -0.739943\n",
      "  -0.43959752]\n",
      " [ 1.4939594   1.          1.1892242  ...  7.017      15.502016\n",
      "   0.7234399 ]\n",
      " ...\n",
      " [ 1.4939594   1.          1.1892204  ...  1.3074445   7.9671354\n",
      "   0.393376  ]\n",
      " [ 1.4939613   1.          1.1891594  ...  0.68122214  6.922223\n",
      "  -1.1115843 ]\n",
      " [ 1.4939795   1.          1.1890659  ... -0.27397835  3.1524673\n",
      "   1.8327882 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.3       0.        0.        1.0000001\n",
      " 0.        0.        0.6       0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        1.0000001 0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.0532656    1.           1.5926323  ...  -0.9769065    5.796167\n",
      "   -2.041463  ]\n",
      " [  1.0532808    1.           1.5927219  ...   1.470371    -0.01286602\n",
      "    0.08064723]\n",
      " [  1.0532665    1.           1.5927457  ...   0.48921418   0.8067651\n",
      "    0.20900744]\n",
      " ...\n",
      " [  1.0532703    1.           1.5927505  ...   0.45885414 -11.087223\n",
      "   -0.22259986]\n",
      " [  1.0532703    1.           1.5926819  ...  -2.5157979  -12.297577\n",
      "   -1.4385998 ]\n",
      " [  1.053299     1.           1.5926208  ...   2.1919973   -4.4420185\n",
      "    0.25001335]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.3        0.         0.         0.70000005 0.         0.\n",
      " 0.5        0.         0.         0.70000005 0.         0.3\n",
      " 0.         0.5        0.         0.         1.0000001  0.\n",
      " 0.         0.5       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.5095959   1.          1.8401489  ... -0.49810898  4.63552\n",
      "  -1.1428175 ]\n",
      " [ 0.50960827  1.          1.8402443  ...  1.1865177  -1.7664442\n",
      "  -0.11955106]\n",
      " [ 0.50959015  1.          1.8402714  ... -0.759145   -0.97975683\n",
      "  -0.14396334]\n",
      " ...\n",
      " [ 0.5095959   1.          1.8402615  ... -0.67248887 -9.168329\n",
      "  -0.5167776 ]\n",
      " [ 0.5095959   1.          1.840189   ... -0.32249665 -2.477003\n",
      "   0.03795332]\n",
      " [ 0.50963116  1.          1.8401508  ...  1.8124063  -5.6695614\n",
      "  -1.3209299 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.0841608   1.          1.9075947  ...  1.6916573   3.4314134\n",
      "  -0.5553781 ]\n",
      " [-0.08414841  1.          1.9076881  ...  1.7900314  -2.6916094\n",
      "  -0.4309091 ]\n",
      " [-0.08416176  1.          1.9076856  ...  0.01205659  4.709517\n",
      "   0.9426165 ]\n",
      " ...\n",
      " [-0.08415604  1.          1.90769    ...  0.37104285 -7.524372\n",
      "  -0.2594018 ]\n",
      " [-0.08415604  1.          1.9076176  ... -0.9009526  -5.624125\n",
      "   1.2665642 ]\n",
      " [-0.08412457  1.          1.9076042  ...  3.0722017  -9.394285\n",
      "  -0.8989098 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.6965580e-01  1.0000000e+00  1.7882442e+00 ... -4.3430924e-03\n",
      "  -5.5245590e-01 -1.3387966e-01]\n",
      " [-6.6964340e-01  1.0000000e+00  1.7882948e+00 ...  8.0046821e-01\n",
      "  -1.3722756e+00 -2.6839930e-01]\n",
      " [-6.6959953e-01  1.0000000e+00  1.7882886e+00 ... -4.5763904e-01\n",
      "   1.7193796e+01 -4.0180206e-01]\n",
      " ...\n",
      " [-6.6959763e-01  1.0000000e+00  1.7882910e+00 ... -5.7604659e-01\n",
      "   9.8304892e-01 -6.4009190e-02]\n",
      " [-6.6959572e-01  1.0000000e+00  1.7882423e+00 ... -7.1429312e-01\n",
      "   1.0770487e+01  4.2837417e-01]\n",
      " [-6.6961956e-01  1.0000000e+00  1.7882671e+00 ... -7.3617774e-01\n",
      "  -6.3935761e+00 -1.4377230e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.1897612    1.           1.4936237  ...  -2.4612155   -7.2586765\n",
      "   -1.7353579 ]\n",
      " [ -1.1897488    1.           1.493701   ...  -0.05820572  -0.47194862\n",
      "    0.13882375]\n",
      " [ -1.1897202    1.           1.4936682  ...   0.20457205  -2.1966348\n",
      "   -0.10467732]\n",
      " ...\n",
      " [ -1.189724     1.           1.4936924  ...   0.75695825   1.0320778\n",
      "   -0.32606155]\n",
      " [ -1.1897221    1.           1.4936142  ...  -0.23477846   4.9660125\n",
      "   -0.39443716]\n",
      " [ -1.1897297    1.           1.49366    ...  -0.50939065 -11.987934\n",
      "   -0.5327976 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.5931177    1.           1.0528183  ...   1.9641908    8.345811\n",
      "   -0.42065805]\n",
      " [ -1.5931091    1.           1.0529146  ...  -0.25480068  -0.46396446\n",
      "   -0.3833843 ]\n",
      " [ -1.5930977    1.           1.0528821  ...  -0.37498116   1.501195\n",
      "    0.05557793]\n",
      " ...\n",
      " [ -1.5931072    1.           1.052906   ...   0.8316366    9.501084\n",
      "   -0.1767329 ]\n",
      " [ -1.5931034    1.           1.0528069  ...  -0.83866614   6.025804\n",
      "   -0.93962646]\n",
      " [ -1.5930996    1.           1.0528679  ...  -0.7142869  -10.733084\n",
      "   -0.06252849]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.5       0.        0.        0.5       1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.8405008    1.           0.5091572  ...   2.3035686    7.9027305\n",
      "    0.2881744 ]\n",
      " [ -1.8404961    1.           0.5092583  ...  -0.11477017  -0.6074841\n",
      "   -0.5480207 ]\n",
      " [ -1.8405151    1.           0.5092169  ...  -1.2521188    2.4327497\n",
      "    0.57005715]\n",
      " ...\n",
      " [ -1.8405285    1.           0.5092478  ...  -3.4223776  -13.218164\n",
      "    0.5300129 ]\n",
      " [ -1.840519     1.           0.5091438  ...   0.86311066   2.281577\n",
      "   -0.6069257 ]\n",
      " [ -1.8404884    1.           0.5092144  ...  -0.05857384  -1.5212274\n",
      "   -1.0254891 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.9000001 0.3      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9077911   1.         -0.08452034 ... -0.037763   -3.1353683\n",
      "  -0.41200903]\n",
      " [-1.9078007   1.         -0.08440399 ... -0.13350725 -0.60225827\n",
      "  -0.60719025]\n",
      " [-1.9078312   1.         -0.08443999 ... -2.4019072   5.0030217\n",
      "   0.7299942 ]\n",
      " ...\n",
      " [-1.9078522   1.         -0.08441353 ... -0.41995907 -6.0343904\n",
      "   0.09270334]\n",
      " [-1.9078293   1.         -0.0845356  ...  1.0724316   2.2405581\n",
      "  -0.6845596 ]\n",
      " [-1.907793    1.         -0.08446312 ... -1.0441029  -7.760128\n",
      "   0.5262281 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4       0.        0.        0.8000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 1.0000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7881765   1.         -0.67027473 ...  0.5953673   2.5039167\n",
      "   0.09477143]\n",
      " [-1.7881994   1.         -0.6701937  ... -0.2522292  -0.489893\n",
      "  -0.70116925]\n",
      " [-1.7881832   1.         -0.67022306 ... -3.8944788   6.260056\n",
      "  -0.63172907]\n",
      " ...\n",
      " [-1.7882156   1.         -0.67020226 ... -1.4441899  -9.097399\n",
      "   1.1135104 ]\n",
      " [-1.7881718   1.         -0.67029    ...  1.5863688   5.763833\n",
      "   0.30262342]\n",
      " [-1.7881908   1.         -0.67022514 ... -1.5266843  -2.2258062\n",
      "   1.1610826 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         1.0000001  0.\n",
      " 0.4        0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.4\n",
      " 1.0000001  0.70000005]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4935131e+00  1.0000000e+00 -1.1900024e+00 ...  5.2638054e-03\n",
      "   6.2659216e+00  1.1485890e+00]\n",
      " [-1.4935570e+00  1.0000000e+00 -1.1899710e+00 ... -2.8556758e-01\n",
      "  -3.8193911e-01 -5.4531866e-01]\n",
      " [-1.4935398e+00  1.0000000e+00 -1.1899991e+00 ... -6.4144363e+00\n",
      "   1.2331636e+01 -1.1215260e+00]\n",
      " ...\n",
      " [-1.4935856e+00  1.0000000e+00 -1.1899796e+00 ... -1.2170748e+00\n",
      "   1.4635687e+01  5.8663505e-01]\n",
      " [-1.4935188e+00  1.0000000e+00 -1.1900158e+00 ...  7.0020342e-01\n",
      "   9.8887062e-01 -1.6735458e-01]\n",
      " [-1.4935427e+00  1.0000000e+00 -1.1899605e+00 ...  5.1606816e-01\n",
      "   1.6698730e+01 -6.7345178e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.0522928    1.          -1.5932426  ...   1.2325237   -6.1091704\n",
      "   -0.4350021 ]\n",
      " [ -1.0523472    1.          -1.5932217  ...  -0.21233469  -0.25283164\n",
      "   -0.15932679]\n",
      " [ -1.052351     1.          -1.5932516  ...   2.4757502   -9.995843\n",
      "   -1.1056416 ]\n",
      " ...\n",
      " [ -1.0524025    1.          -1.5932293  ...   1.9887924  -10.169881\n",
      "   -1.0365465 ]\n",
      " [ -1.0523186    1.          -1.593256   ...   0.34981227  -0.61021566\n",
      "   -0.43054187]\n",
      " [ -1.052331     1.          -1.5932026  ...  -0.23056915   7.954234\n",
      "    1.0501243 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.2       0.\n",
      " 0.        0.        0.        0.        0.1       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0834084e-01  1.0000000e+00 -1.8405094e+00 ...  7.1672839e-01\n",
      "  -6.4892726e+00 -1.3744545e-01]\n",
      " [-5.0840092e-01  1.0000000e+00 -1.8404636e+00 ... -1.1893320e-01\n",
      "   1.3160884e-01  2.4734068e-01]\n",
      " [-5.0838661e-01  1.0000000e+00 -1.8404939e+00 ... -1.7444465e+00\n",
      "   1.7772305e+01  8.6788163e-03]\n",
      " ...\n",
      " [-5.0844193e-01  1.0000000e+00 -1.8404722e+00 ...  8.2039928e-01\n",
      "  -7.9276981e+00  1.9480598e-01]\n",
      " [-5.0835037e-01  1.0000000e+00 -1.8405170e+00 ... -2.9813042e+00\n",
      "  -2.8005838e+00 -7.4580264e-01]\n",
      " [-5.0838280e-01  1.0000000e+00 -1.8404713e+00 ...  1.3529963e+00\n",
      "  -4.7177138e+00  9.0476096e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.08536148   1.          -1.9075432  ...  -1.4554322    3.1190672\n",
      "   -0.57254016]\n",
      " [  0.08530045   1.          -1.9075146  ...  -1.2698631    1.5232961\n",
      "    0.23162127]\n",
      " [  0.08535004   1.          -1.9075233  ...   1.4347533   -7.469925\n",
      "    0.85625315]\n",
      " ...\n",
      " [  0.08529282   1.          -1.9075232  ...   1.9719816  -11.002943\n",
      "   -1.7965918 ]\n",
      " [  0.08538628   1.          -1.907547   ...  10.466855    13.804638\n",
      "   -1.2896955 ]\n",
      " [  0.08531857   1.          -1.9075184  ...  -0.18882203   1.0126609\n",
      "    0.9464381 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.67055225  1.         -1.7878761  ... -1.4738543   6.38236\n",
      "   0.16463852]\n",
      " [ 0.6704931   1.         -1.7878647  ... -0.14473677  1.5080416\n",
      "   0.07343285]\n",
      " [ 0.67056656  1.         -1.7878737  ...  0.52998924 -3.2434826\n",
      "  -0.04938447]\n",
      " ...\n",
      " [ 0.67050934  1.         -1.7878752  ... -0.07875878  6.0794153\n",
      "  -0.3129686 ]\n",
      " [ 0.6706028   1.         -1.7878761  ...  3.1762996   4.451857\n",
      "   1.0177563 ]\n",
      " [ 0.6705103   1.         -1.7878685  ...  0.23589087  0.30739927\n",
      "   0.20695949]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.190547    1.         -1.493      ...  1.5717628  -1.205214\n",
      "  -0.38557577]\n",
      " [ 1.1904945   1.         -1.4929667  ...  8.483883    2.550841\n",
      "  -0.857113  ]\n",
      " [ 1.1905746   1.         -1.4929768  ...  1.3430232  -8.560461\n",
      "  -0.85470796]\n",
      " ...\n",
      " [ 1.190525    1.         -1.492979   ...  0.40247887  8.965069\n",
      "   0.46419477]\n",
      " [ 1.190609    1.         -1.4929924  ...  1.6402198   3.61977\n",
      "   0.35616028]\n",
      " [ 1.1905117   1.         -1.4930134  ...  0.778033   -1.8019793\n",
      "  -0.03731275]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5935812    1.          -1.0520668  ...   0.1705445    0.12784481\n",
      "    0.11646205]\n",
      " [  1.5935383    1.          -1.052042   ...  11.67012      3.5637288\n",
      "   -1.4973129 ]\n",
      " [  1.5935879    1.          -1.0520394  ...   0.69057816   5.7748156\n",
      "    0.40701288]\n",
      " ...\n",
      " [  1.5935497    1.          -1.0520573  ...   1.1093163  -13.851074\n",
      "   -0.9065221 ]\n",
      " [  1.5936127    1.          -1.0520496  ...   2.432567     7.2999887\n",
      "    2.0219414 ]\n",
      " [  1.5935555    1.          -1.052084   ...   0.0722003   -0.3277588\n",
      "   -0.09961572]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001 0.        0.        0.        1.0000001 0.        0.1\n",
      " 0.        0.        0.        0.6       0.        0.        0.\n",
      " 0.        0.1       0.        0.        0.        0.2      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8407631   1.         -0.5080986  ...  0.30649745  0.46644735\n",
      "   0.58681285]\n",
      " [ 1.8407297   1.         -0.50807476 ...  5.029958    0.78302157\n",
      "  -0.6296975 ]\n",
      " [ 1.8407688   1.         -0.5080634  ...  0.5625359   6.644493\n",
      "   0.16172814]\n",
      " ...\n",
      " [ 1.8407383   1.         -0.5080919  ... -0.28215927 -7.1198397\n",
      "  -0.16924089]\n",
      " [ 1.8407803   1.         -0.5080795  ... -0.11888814  2.2144423\n",
      "   0.41932726]\n",
      " [ 1.8407497   1.         -0.5081215  ... -0.18646145  0.45651293\n",
      "  -0.01597806]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.8000001 1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9076958   1.          0.08581924 ... -2.6722848   3.7789655\n",
      "   0.80417305]\n",
      " [ 1.90767     1.          0.08584785 ...  2.8269148  -1.1802381\n",
      "  -1.4159977 ]\n",
      " [ 1.9076862   1.          0.08585642 ... -1.4880261   1.1893821\n",
      "  -0.09676225]\n",
      " ...\n",
      " [ 1.9076653   1.          0.08583069 ...  0.8588289  -7.9338546\n",
      "  -0.38470376]\n",
      " [ 1.907692    1.          0.08583832 ... -1.5986413   0.1731224\n",
      "   1.0193225 ]\n",
      " [ 1.9076967   1.          0.08579063 ...  0.20548964  0.62358665\n",
      "  -0.05792654]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 1.0000001 0.3       1.0000001\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.8000001 1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7877941e+00  1.0000000e+00  6.7120171e-01 ...  4.0208645e+00\n",
      "  -9.3127518e+00  1.3501995e+00]\n",
      " [ 1.7877731e+00  1.0000000e+00  6.7121315e-01 ...  2.0136600e+00\n",
      "  -8.4026897e-01 -1.8767680e+00]\n",
      " [ 1.7877827e+00  1.0000000e+00  6.7121953e-01 ... -1.2134920e+00\n",
      "  -2.5920539e+00 -1.1313462e-01]\n",
      " ...\n",
      " [ 1.7877712e+00  1.0000000e+00  6.7119598e-01 ... -4.3603948e-01\n",
      "  -2.3152637e-01 -9.4638288e-02]\n",
      " [ 1.7877846e+00  1.0000000e+00  6.7122078e-01 ... -3.0192733e-01\n",
      "   2.0956564e-01  2.8541481e-01]\n",
      " [ 1.7878094e+00  1.0000000e+00  6.7117500e-01 ... -2.6090145e-02\n",
      "   2.4215746e-01 -2.0581484e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         1.0000001  0.70000005 1.0000001\n",
      " 1.0000001  0.         0.         0.         1.0000001  0.\n",
      " 0.         0.         0.         1.0000001  0.         0.\n",
      " 1.0000001  1.0000001 ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4928474   1.          1.1908779  ...  1.1885087  -3.2925406\n",
      "  -0.39688963]\n",
      " [ 1.4928312   1.          1.1909189  ...  1.6033509  -0.89919376\n",
      "  -2.0080512 ]\n",
      " [ 1.492815    1.          1.1909183  ... -2.4658232  -7.527347\n",
      "   0.28687274]\n",
      " ...\n",
      " [ 1.4928226   1.          1.1909046  ...  0.41900095  2.1086316\n",
      "  -0.26928857]\n",
      " [ 1.4928131   1.          1.1908855  ...  0.43176007 -1.3610969\n",
      "   0.15090775]\n",
      " [ 1.4928694   1.          1.190855   ... -0.01199722 -0.16115713\n",
      "   0.02450091]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         1.0000001  0.         0.1\n",
      " 1.0000001  0.         0.         0.         0.5        0.\n",
      " 0.         0.70000005 0.70000005 1.0000001  0.         0.\n",
      " 1.0000001  1.0000001 ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0515871   1.          1.5938511  ... -0.32920885  0.10828876\n",
      "  -1.2494125 ]\n",
      " [ 1.0515699   1.          1.5938759  ... -3.560797    8.569168\n",
      "  -1.386809  ]\n",
      " [ 1.0515652   1.          1.5938741  ...  4.7458143   9.172396\n",
      "   0.68649626]\n",
      " ...\n",
      " [ 1.0515785   1.          1.5938616  ... -0.20926493  8.038557\n",
      "  -0.59177554]\n",
      " [ 1.0515633   1.          1.593853   ...  2.7872288  -9.453716\n",
      "   1.5041518 ]\n",
      " [ 1.0516186   1.          1.593832   ...  3.2344823  -5.385587\n",
      "  -0.68478405]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.5       0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.        0.8000001\n",
      " 0.6       0.6       0.        0.        0.8000001 0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.5074644    1.           1.8406391  ...  -0.17722368   0.17833328\n",
      "   -0.23154962]\n",
      " [  0.50744724   1.           1.840661   ...  -2.7573504    8.447053\n",
      "   -1.7895362 ]\n",
      " [  0.50738144   1.           1.8406708  ...   0.8055248    1.5999103\n",
      "   -0.03096408]\n",
      " ...\n",
      " [  0.5073967    1.           1.8406525  ...  -1.8863866   -8.793005\n",
      "   -0.10068524]\n",
      " [  0.50737953   1.           1.8406353  ...   0.890128    -4.51371\n",
      "    0.32200336]\n",
      " [  0.5075016    1.           1.8406334  ...   7.01883    -10.071272\n",
      "   -1.4045775 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.08650303  1.          1.9073963  ... -0.1503098   2.4707751\n",
      "  -0.3546555 ]\n",
      " [-0.0865202   1.          1.9074144  ...  1.445792    1.8448558\n",
      "  -0.8685264 ]\n",
      " [-0.08659744  1.          1.9074155  ...  8.034339   18.503052\n",
      "   1.4519899 ]\n",
      " ...\n",
      " [-0.08658028  1.          1.9074068  ... -1.977672   -9.016514\n",
      "   0.7720695 ]\n",
      " [-0.08659935  1.          1.907383   ...  0.13367438 -6.5532703\n",
      "  -0.55512667]\n",
      " [-0.08646584  1.          1.9074059  ...  1.808853   -4.679441\n",
      "  -1.7838676 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.6717663   1.          1.787262   ...  0.29296374  5.6659307\n",
      "  -0.7569389 ]\n",
      " [-0.67178345  1.          1.7872858  ...  1.5201191   4.1050916\n",
      "  -0.45828056]\n",
      " [-0.67183495  1.          1.7872692  ...  3.1667857   9.587837\n",
      "   0.69997704]\n",
      " ...\n",
      " [-0.6718216   1.          1.7872753  ...  2.0815506   6.8585105\n",
      "   1.079745  ]\n",
      " [-0.67183495  1.          1.7872314  ...  0.76682544 -3.5863671\n",
      "  -1.3278176 ]\n",
      " [-0.67173195  1.          1.7872753  ...  1.136598   -3.4875546\n",
      "  -1.5614806 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1912384   1.          1.4922619  ... -1.7239475   1.3287249\n",
      "  -0.7764545 ]\n",
      " [-1.1912537   1.          1.4922953  ...  1.4622855   5.0049205\n",
      "  -0.01220739]\n",
      " [-1.1913052   1.          1.4922627  ...  0.0240674  -8.158575\n",
      "   1.4074858 ]\n",
      " ...\n",
      " [-1.1912994   1.          1.4922771  ... -0.52584416  4.075981\n",
      "   1.497804  ]\n",
      " [-1.1913052   1.          1.4922104  ... -0.6735958  -2.0428343\n",
      "  -0.5215057 ]\n",
      " [-1.1912165   1.          1.4922848  ... -0.9886441  -0.23651028\n",
      "   0.07522085]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5942574   1.          1.0509281  ... -0.1608398  -6.3051305\n",
      "   0.03821403]\n",
      " [-1.5942726   1.          1.0509787  ...  3.3426783   9.17638\n",
      "   1.7157288 ]\n",
      " [-1.5943222   1.          1.0509194  ... -1.0763688  -9.65309\n",
      "   0.18688056]\n",
      " ...\n",
      " [-1.5943184   1.          1.0509529  ... -0.4067367   4.1354084\n",
      "   0.537304  ]\n",
      " [-1.5943184   1.          1.0508652  ...  1.0018048   2.990218\n",
      "  -0.21515732]\n",
      " [-1.5942488   1.          1.0509624  ...  0.2963963  -0.4629364\n",
      "  -0.01490211]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.3        0.         0.70000005 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5        0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.8411589    1.           0.5064945  ...   0.8817756    5.5290895\n",
      "   -0.07590647]\n",
      " [ -1.8411741    1.           0.5065603  ...  -2.0871174   -9.29586\n",
      "    0.20001692]\n",
      " [ -1.841217     1.           0.5064887  ...   0.8945866  -17.20126\n",
      "    0.78266954]\n",
      " ...\n",
      " [ -1.841217     1.           0.5065279  ...  -0.49111462  -1.5748291\n",
      "    0.1674428 ]\n",
      " [ -1.8412094    1.           0.50642586 ...   2.078545     7.3113627\n",
      "   -0.574411  ]\n",
      " [ -1.841156     1.           0.5065346  ...   0.17480391  -0.10907793\n",
      "   -0.01828629]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        1.0000001 0.        0.        0.4\n",
      " 0.        0.        0.        0.        0.        0.        0.5\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90760612e+00  1.00000000e+00 -8.72459412e-02 ...  4.47323740e-01\n",
      "   7.28130150e+00  1.44694912e+00]\n",
      " [-1.90762615e+00  1.00000000e+00 -8.71772766e-02 ...  2.48489213e+00\n",
      "   1.08969145e+01 -1.86117351e-01]\n",
      " [-1.90767097e+00  1.00000000e+00 -8.72542113e-02 ...  1.70821548e-01\n",
      "  -1.01034908e+01 -3.54567736e-01]\n",
      " ...\n",
      " [-1.90768242e+00  1.00000000e+00 -8.72087479e-02 ... -3.62457573e-01\n",
      "   3.34850812e+00 -9.44411010e-03]\n",
      " [-1.90765572e+00  1.00000000e+00 -8.73146057e-02 ... -4.62685299e+00\n",
      "  -8.11819839e+00 -7.82579184e-02]\n",
      " [-1.90761662e+00  1.00000000e+00 -8.72020721e-02 ... -3.23368520e-01\n",
      "   7.84857321e+00  1.78852051e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        1.0000001 0.        0.        0.2\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.787385     1.          -0.6723385  ...   0.6155398   -1.08109\n",
      "   -0.0451799 ]\n",
      " [ -1.7874126    1.          -0.6722603  ...   4.4797583   18.34785\n",
      "   -0.14895838]\n",
      " [ -1.7874565    1.          -0.6723369  ...   0.08150315 -11.019922\n",
      "    1.2447828 ]\n",
      " ...\n",
      " [ -1.7874737    1.          -0.67228985 ...  -2.417676     8.333751\n",
      "    0.9968585 ]\n",
      " [ -1.7874298    1.          -0.6723957  ...   1.1743643    1.0343866\n",
      "    0.34211853]\n",
      " [ -1.787405     1.          -0.6722965  ...  -0.9763761    8.868435\n",
      "   -0.45511997]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        1.0000001 0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.5\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.491971     1.          -1.1920319  ...   0.84231925  -2.7719073\n",
      "    0.2845602 ]\n",
      " [ -1.492013     1.          -1.1919651  ...  -0.3530295    4.0878134\n",
      "    1.0138378 ]\n",
      " [ -1.4920177    1.          -1.1920193  ...  -1.6274885    5.4757047\n",
      "    0.35550317]\n",
      " ...\n",
      " [ -1.4920444    1.          -1.1919842  ...   5.0358458  -10.737013\n",
      "    0.0691452 ]\n",
      " [ -1.4919853    1.          -1.1920815  ...   0.3262024    1.9571941\n",
      "    0.40073022]\n",
      " [ -1.4920073    1.          -1.1919937  ...   0.02644612  -4.2304196\n",
      "    0.64779985]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.3       0.        1.0000001 0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0508108e+00  1.0000000e+00 -1.5946159e+00 ...  2.5182495e+00\n",
      "  -7.6176162e+00  1.0248673e-01]\n",
      " [-1.0508642e+00  1.0000000e+00 -1.5945549e+00 ... -4.5597854e-01\n",
      "   9.4351215e+00 -3.5648804e-02]\n",
      " [-1.0508442e+00  1.0000000e+00 -1.5946176e+00 ... -1.6054752e+00\n",
      "   6.4249258e+00 -9.5076406e-01]\n",
      " ...\n",
      " [-1.0508766e+00  1.0000000e+00 -1.5945683e+00 ...  1.0202458e+00\n",
      "  -4.1573458e+00 -5.6036133e-01]\n",
      " [-1.0508022e+00  1.0000000e+00 -1.5946484e+00 ...  1.2078384e+00\n",
      "   6.4965715e+00  1.6776242e+00]\n",
      " [-1.0508604e+00  1.0000000e+00 -1.5945892e+00 ... -1.2869650e-01\n",
      "  -1.9499736e+00 -3.3819526e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-0.5062847   1.         -1.8413048  ...  1.0966911  -2.220683\n",
      "   0.02221638]\n",
      " [-0.5063429   1.         -1.8412199  ... -1.4423088   1.1592169\n",
      "   1.1134293 ]\n",
      " [-0.5063362   1.         -1.8412688  ... -0.99988824 -2.5647707\n",
      "  -0.5451139 ]\n",
      " ...\n",
      " [-0.50637627  1.         -1.8412323  ...  2.7463894  -9.195969\n",
      "  -1.4340284 ]\n",
      " [-0.5062866   1.         -1.84132    ...  0.4597626  -5.423363\n",
      "  -0.7483537 ]\n",
      " [-0.5063391   1.         -1.8412933  ... -1.0649512  -6.2977843\n",
      "  -0.9946294 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.08744335  1.         -1.9076405  ...  2.8694     -7.237714\n",
      "   0.38057664]\n",
      " [ 0.08738518  1.         -1.9075785  ... -3.2131784  10.9373455\n",
      "   0.39069197]\n",
      " [ 0.08737755  1.         -1.9076164  ...  1.0535821  -9.344956\n",
      "  -0.16864562]\n",
      " ...\n",
      " [ 0.08733749  1.         -1.907589   ...  0.41245258 -0.27862692\n",
      "  -0.15820134]\n",
      " [ 0.08742714  1.         -1.9076385  ... -0.04121798 -5.919614\n",
      "   0.16122144]\n",
      " [ 0.08738899  1.         -1.90765    ... -1.3469753  -6.246585\n",
      "  -0.53437364]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 0.6727028   1.         -1.7870522  ... -0.24174476 -1.080903\n",
      "  -1.1898468 ]\n",
      " [ 0.67264557  1.         -1.7869864  ...  3.567691   -3.2159514\n",
      "   0.75397563]\n",
      " [ 0.67264366  1.         -1.7870201  ...  1.4375364   4.7248282\n",
      "   0.01332092]\n",
      " ...\n",
      " [ 0.6726074   1.         -1.7870007  ...  0.39136803  1.2281594\n",
      "  -0.2692771 ]\n",
      " [ 0.67269135  1.         -1.7870369  ... -0.05576539 -6.4804564\n",
      "  -0.03319718]\n",
      " [ 0.6726494   1.         -1.7870693  ...  0.7664583   4.708511\n",
      "   0.43632922]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1920452   1.         -1.4917221  ...  3.3018794  -7.866148\n",
      "  -0.2602669 ]\n",
      " [ 1.1919937   1.         -1.4916296  ...  2.578987   -2.1278172\n",
      "   0.3519674 ]\n",
      " [ 1.1919861   1.         -1.4916574  ...  1.2099452   6.058248\n",
      "  -0.62480515]\n",
      " ...\n",
      " [ 1.1919518   1.         -1.4916506  ...  0.5155732   1.8845005\n",
      "  -0.47831082]\n",
      " [ 1.192028    1.         -1.4916973  ...  0.65813756 -6.8744154\n",
      "   1.4352868 ]\n",
      " [ 1.1919975   1.         -1.4917488  ...  2.059559    3.9846056\n",
      "  -0.79757154]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5946665e+00  1.0000000e+00 -1.0501175e+00 ...  1.1939502e-01\n",
      "   6.0376554e+00 -8.3871806e-01]\n",
      " [ 1.5946188e+00  1.0000000e+00 -1.0500479e+00 ...  1.9086750e+00\n",
      "  -4.0851030e+00 -3.5439730e-03]\n",
      " [ 1.5946007e+00  1.0000000e+00 -1.0500693e+00 ... -1.1960924e+00\n",
      "  -2.0586181e+00  3.8556421e-01]\n",
      " ...\n",
      " [ 1.5945740e+00  1.0000000e+00 -1.0500717e+00 ...  3.0896533e-01\n",
      "   2.1889272e+00  2.6394725e-02]\n",
      " [ 1.5946331e+00  1.0000000e+00 -1.0500870e+00 ...  8.9299655e-01\n",
      "  -1.5135331e+00  3.1905663e-01]\n",
      " [ 1.5946274e+00  1.0000000e+00 -1.0501499e+00 ... -5.0218639e+00\n",
      "  -5.2747388e+00  1.3204369e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.5 0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.5 0.  0.\n",
      " 0.3 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8412800e+00  1.0000000e+00 -5.0559616e-01 ... -2.9347854e+00\n",
      "  -1.0085281e+01  5.2601802e-01]\n",
      " [ 1.8412361e+00  1.0000000e+00 -5.0553513e-01 ...  9.9946618e-02\n",
      "  -2.4028139e+00  7.3641062e-02]\n",
      " [ 1.8412132e+00  1.0000000e+00 -5.0554597e-01 ... -5.7696927e-01\n",
      "  -9.8688126e-01 -2.7520359e-03]\n",
      " ...\n",
      " [ 1.8411961e+00  1.0000000e+00 -5.0556278e-01 ... -2.2168517e-01\n",
      "   3.8830276e+00  2.6475692e-01]\n",
      " [ 1.8412342e+00  1.0000000e+00 -5.0556564e-01 ...  2.5896859e-01\n",
      "   5.7078028e-01  1.5717089e-02]\n",
      " [ 1.8412533e+00  1.0000000e+00 -5.0563431e-01 ... -2.3069792e+00\n",
      "  -3.7413945e+00  4.4280875e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        1.0000001 0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:2, Score:15.21, Best Score:15.21, Average Score:12.04, Best Avg Score:12.04\n",
      "Episode number: 3\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7f2d81ed9820>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7869663   1.          0.67323685 ...  8.94308    17.948235\n",
      "   1.8305347 ]\n",
      " [ 1.7869158   1.          0.6733532  ... -0.84893155 -1.2433355\n",
      "   0.5168028 ]\n",
      " [ 1.7868786   1.          0.67331845 ... -1.8516109  -3.9801369\n",
      "   0.525723  ]\n",
      " ...\n",
      " [ 1.7868786   1.          0.6733265  ...  0.2888644  -3.565507\n",
      "   0.37552834]\n",
      " [ 1.7868919   1.          0.67326736 ...  4.172811   -6.5421095\n",
      "  -0.29281586]\n",
      " [ 1.7869635   1.          0.6731949  ... -0.39855385  2.300848\n",
      "   1.4651254 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        1.0000001 0.6\n",
      " 0.4       0.        0.        1.0000001 0.        0.4       0.\n",
      " 0.        1.0000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4912567e+00  1.0000000e+00  1.1927891e+00 ...  4.0457821e+00\n",
      "   7.5162463e+00  7.0882404e-01]\n",
      " [ 1.4912024e+00  1.0000000e+00  1.1929073e+00 ...  2.1999497e+00\n",
      "   5.5335379e-01  1.1100424e+01]\n",
      " [ 1.4911423e+00  1.0000000e+00  1.1928689e+00 ... -4.8982706e+00\n",
      "  -1.2718243e+01  2.0108466e+00]\n",
      " ...\n",
      " [ 1.4911537e+00  1.0000000e+00  1.1928825e+00 ... -5.9112680e-01\n",
      "   7.7947378e+00  5.4167467e-01]\n",
      " [ 1.4911575e+00  1.0000000e+00  1.1928158e+00 ... -3.3649206e-03\n",
      "   1.1330940e+00 -1.5636964e+00]\n",
      " [ 1.4912643e+00  1.0000000e+00  1.1927547e+00 ...  2.2784832e-01\n",
      "   2.3927059e+00  9.8600709e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        1.0000001 1.0000001\n",
      " 0.9000001 0.        0.        0.9000001 0.        1.0000001 0.\n",
      " 0.        1.0000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0495796e+00  1.0000000e+00  1.5953007e+00 ... -1.2983332e+00\n",
      "  -3.9217868e+00  2.1883821e+00]\n",
      " [ 1.0495186e+00  1.0000000e+00  1.5954275e+00 ...  3.9881000e+02\n",
      "  -9.8395706e+01 -1.7843246e+02]\n",
      " [ 1.0494995e+00  1.0000000e+00  1.5953841e+00 ... -2.2172747e+00\n",
      "  -6.2336545e+00  4.6674913e-01]\n",
      " ...\n",
      " [ 1.0495148e+00  1.0000000e+00  1.5954065e+00 ...  1.2415917e-01\n",
      "   1.0532831e+01 -8.4383857e-01]\n",
      " [ 1.0495186e+00  1.0000000e+00  1.5953236e+00 ... -1.7514223e+00\n",
      "   6.0033994e+00  2.9454386e-01]\n",
      " [ 1.0496016e+00  1.0000000e+00  1.5952797e+00 ...  7.2166568e-01\n",
      "  -7.8192825e+00 -3.7893975e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.2        0.         0.2\n",
      " 0.9000001  0.70000005 0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         1.0000001  0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.50520325    1.            1.8415661  ...   -1.1154693\n",
      "    -2.4852338     0.4932282 ]\n",
      " [   0.50513744    1.            1.8416662  ...  134.22923\n",
      "  -176.1222     -102.26695   ]\n",
      " [   0.5051422     1.            1.8416226  ...   -1.1014066\n",
      "    -6.0524125     0.86650276]\n",
      " ...\n",
      " [   0.5051575     1.            1.8416471  ...   -1.8863511\n",
      "    -6.041419     -0.2804426 ]\n",
      " [   0.5051632     1.            1.8415833  ...    0.57413065\n",
      "     1.5888147    -0.22650045]\n",
      " [   0.5052347     1.            1.8415508  ...   -1.0511936\n",
      "    -5.659579     -0.60892385]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-8.8817596e-02  1.0000000e+00  1.9075394e+00 ... -8.6183989e-01\n",
      "  -6.2170792e+00  9.8275495e-01]\n",
      " [-8.8887215e-02  1.0000000e+00  1.9076233e+00 ...  2.1569241e+02\n",
      "   2.6770999e+02 -3.6139636e+02]\n",
      " [-8.8890076e-02  1.0000000e+00  1.9075793e+00 ... -1.6647483e+00\n",
      "  -5.4825892e+00  1.0104870e+00]\n",
      " ...\n",
      " [-8.8874817e-02  1.0000000e+00  1.9076052e+00 ... -8.5038328e-01\n",
      "  -6.7328453e+00  4.7981668e-01]\n",
      " [-8.8869095e-02  1.0000000e+00  1.9075413e+00 ...  1.4549688e+00\n",
      "   2.5094528e+00 -5.2170610e-01]\n",
      " [-8.8785172e-02  1.0000000e+00  1.9075356e+00 ... -2.8525424e-01\n",
      "  -5.8787584e+00 -9.6454477e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7405891e-01  1.0000000e+00  1.7867222e+00 ... -6.4364934e-01\n",
      "  -9.8151798e+00  9.5420039e-01]\n",
      " [-6.7412758e-01  1.0000000e+00  1.7867756e+00 ...  2.6221707e+03\n",
      "   1.9016605e+03 -4.9339160e+03]\n",
      " [-6.7412186e-01  1.0000000e+00  1.7867291e+00 ...  2.2710204e-01\n",
      "   1.6139665e+00 -2.8052503e-01]\n",
      " ...\n",
      " [-6.7410851e-01  1.0000000e+00  1.7867622e+00 ... -1.8541659e+00\n",
      "  -5.3001914e+00  1.0976155e+00]\n",
      " [-6.7410469e-01  1.0000000e+00  1.7866955e+00 ...  8.0561030e-01\n",
      "   2.9561024e+00  2.6010907e-01]\n",
      " [-6.7402935e-01  1.0000000e+00  1.7867336e+00 ...  9.6966624e-01\n",
      "   7.8442192e-01  8.1132030e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1932735e+00  1.0000000e+00  1.4909306e+00 ...  7.0402682e-01\n",
      "  -8.4592838e+00  7.3526442e-01]\n",
      " [-1.1933317e+00  1.0000000e+00  1.4909906e+00 ... -3.3476938e+02\n",
      "  -1.6334859e+02 -5.8425378e+02]\n",
      " [-1.1933537e+00  1.0000000e+00  1.4909256e+00 ... -9.0922844e-01\n",
      "   5.8054924e-03  7.4125147e-01]\n",
      " ...\n",
      " [-1.1933422e+00  1.0000000e+00  1.4909840e+00 ...  3.6443192e-01\n",
      "   8.7245846e+00  3.4640765e-01]\n",
      " [-1.1933384e+00  1.0000000e+00  1.4908886e+00 ...  3.8185406e-01\n",
      "   1.9098930e+00  1.6362154e-01]\n",
      " [-1.1932468e+00  1.0000000e+00  1.4909458e+00 ... -1.0403845e+00\n",
      "   3.4931681e+00  2.0611227e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5956278e+00  1.0000000e+00  1.0491543e+00 ...  7.9248577e-01\n",
      "  -6.9906235e+00  8.9543790e-01]\n",
      " [-1.5956688e+00  1.0000000e+00  1.0491943e+00 ... -5.1008325e+02\n",
      "  -7.1426376e+01  3.8977713e+02]\n",
      " [-1.5957336e+00  1.0000000e+00  1.0491298e+00 ... -3.3640820e-01\n",
      "   1.5346403e+00 -1.6666317e-01]\n",
      " ...\n",
      " [-1.5957260e+00  1.0000000e+00  1.0491915e+00 ...  1.1023030e+00\n",
      "  -3.4618106e+00 -3.4687716e-01]\n",
      " [-1.5957222e+00  1.0000000e+00  1.0491047e+00 ...  1.6467237e-01\n",
      "   1.9498534e+00  1.8317342e-02]\n",
      " [-1.5956059e+00  1.0000000e+00  1.0491714e+00 ...  3.6085470e+00\n",
      "   1.3022337e+01  4.7935694e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.9000001 0.5       0.        0.\n",
      " 0.        0.        0.        0.        0.1       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8416023e+00  1.0000000e+00  5.0466728e-01 ...  3.3251917e+00\n",
      "  -9.9789639e+00 -6.1482406e-01]\n",
      " [-1.8416290e+00  1.0000000e+00  5.0471973e-01 ...  7.3752884e+01\n",
      "   4.2666266e+02 -1.3218796e+02]\n",
      " [-1.8417263e+00  1.0000000e+00  5.0464731e-01 ... -3.0748224e-01\n",
      "   1.5356755e+00 -6.6939688e-01]\n",
      " ...\n",
      " [-1.8417282e+00  1.0000000e+00  5.0471592e-01 ...  4.5274258e-02\n",
      "   2.8111372e+00  3.8450491e-01]\n",
      " [-1.8417149e+00  1.0000000e+00  5.0461006e-01 ...  6.0580754e-01\n",
      "   2.7529852e+00  4.7797740e-02]\n",
      " [-1.8415918e+00  1.0000000e+00  5.0468826e-01 ...  1.1676441e-01\n",
      "  -5.5512094e+00  3.0619121e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.70000005 0.         0.\n",
      " 0.9000001  0.         0.         0.4        1.0000001  1.0000001\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9074354e+00  1.0000000e+00 -8.9155197e-02 ...  1.3331380e-01\n",
      "   6.9744763e+00  5.1271868e-01]\n",
      " [-1.9074507e+00  1.0000000e+00 -8.9129448e-02 ... -9.4756519e+02\n",
      "  -1.4009414e+03  1.9982957e+02]\n",
      " [-1.9075565e+00  1.0000000e+00 -8.9199528e-02 ... -2.2281647e-02\n",
      "   1.7986135e+00 -2.6185560e-01]\n",
      " ...\n",
      " [-1.9075603e+00  1.0000000e+00 -8.9134216e-02 ... -1.9760190e+00\n",
      "   7.3273306e+00  1.2660928e+00]\n",
      " [-1.9075432e+00  1.0000000e+00 -8.9216232e-02 ...  2.7600863e+00\n",
      "   6.4884267e+00  4.3665290e-01]\n",
      " [-1.9074383e+00  1.0000000e+00 -8.9132309e-02 ... -1.0290921e+00\n",
      "  -4.5355272e-01  7.2844332e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.5       0.        0.        1.0000001\n",
      " 0.        0.        1.0000001 1.0000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7864819e+00  1.0000000e+00 -6.7425156e-01 ... -6.2870491e-01\n",
      "   9.1121826e+00  1.5329827e+00]\n",
      " [-1.7864962e+00  1.0000000e+00 -6.7421627e-01 ... -3.3293845e+02\n",
      "   5.4128548e+01 -9.0391136e+01]\n",
      " [-1.7865562e+00  1.0000000e+00 -6.7429394e-01 ...  5.5816686e-01\n",
      "   7.2325325e-01  1.1279023e-01]\n",
      " ...\n",
      " [-1.7865696e+00  1.0000000e+00 -6.7422009e-01 ...  5.7066846e+00\n",
      "  -1.3788774e+01 -4.0177631e-01]\n",
      " [-1.7865429e+00  1.0000000e+00 -6.7430687e-01 ... -3.9453149e-01\n",
      "  -5.8038592e+00 -4.7378081e-01]\n",
      " [-1.7864933e+00  1.0000000e+00 -6.7422867e-01 ... -4.9862301e-01\n",
      "   1.9860234e+00 -1.3553044e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.6       1.0000001\n",
      " 0.3       0.        1.0000001 1.0000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4904804e+00  1.0000000e+00 -1.1935806e+00 ... -4.7595668e-01\n",
      "   8.8516865e+00  1.2413739e+00]\n",
      " [-1.4904947e+00  1.0000000e+00 -1.1935492e+00 ...  3.4033026e+02\n",
      "  -3.1241052e+00  1.7478470e+02]\n",
      " [-1.4905376e+00  1.0000000e+00 -1.1936212e+00 ... -6.7623138e-02\n",
      "   1.2083416e+00  2.5664791e-03]\n",
      " ...\n",
      " [-1.4905701e+00  1.0000000e+00 -1.1935530e+00 ...  2.6465564e+00\n",
      "  -7.6592870e+00 -1.0007863e+00]\n",
      " [-1.4905243e+00  1.0000000e+00 -1.1936321e+00 ...  5.3444195e-01\n",
      "   9.3916893e-02  3.8176012e-01]\n",
      " [-1.4905014e+00  1.0000000e+00 -1.1935577e+00 ... -1.5514574e+00\n",
      "   7.6185246e+00 -4.0913752e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.2       1.0000001\n",
      " 1.0000001 0.        1.0000001 0.1       1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0484848e+00  1.0000000e+00 -1.5957317e+00 ... -8.5660881e-01\n",
      "   1.5283567e+01  2.3209400e+00]\n",
      " [-1.0485001e+00  1.0000000e+00 -1.5956936e+00 ... -2.1293848e+02\n",
      "   5.6775250e+02 -8.8386932e+01]\n",
      " [-1.0485020e+00  1.0000000e+00 -1.5957507e+00 ... -1.3184651e+00\n",
      "   6.3824492e+00 -5.0820136e-01]\n",
      " ...\n",
      " [-1.0485420e+00  1.0000000e+00 -1.5956993e+00 ...  1.9807808e+00\n",
      "  -8.0720692e+00 -1.8535779e+00]\n",
      " [-1.0484886e+00  1.0000000e+00 -1.5957718e+00 ... -1.6289508e-01\n",
      "   2.4324746e+00  2.4793732e-01]\n",
      " [-1.0485115e+00  1.0000000e+00 -1.5957203e+00 ... -2.9295921e-01\n",
      "  -6.3290730e+00 -3.3483016e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.5        0.70000005 0.         0.1        0.         0.3\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4        0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.5042076    1.          -1.8416309  ...  -1.4855499   10.399664\n",
      "    1.7658628 ]\n",
      " [ -0.50422287   1.          -1.8415918  ...  77.98621    100.701675\n",
      "  -91.900055  ]\n",
      " [ -0.50424385   1.          -1.8416405  ...  -1.4992228    8.651707\n",
      "   -0.60568315]\n",
      " ...\n",
      " [ -0.50429153   1.          -1.8416033  ...   0.6190996    0.13772488\n",
      "   -0.37327886]\n",
      " [ -0.50422287   1.          -1.8416576  ...   0.48073858   7.36699\n",
      "    0.98010963]\n",
      " [ -0.5042372    1.          -1.8416233  ...  -0.53251386 -10.709923\n",
      "   -0.8710462 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.9934349e-02  1.0000000e+00 -1.9073486e+00 ...  2.0898412e-01\n",
      "  -4.7686691e+00  1.0109221e+00]\n",
      " [ 8.9919090e-02  1.0000000e+00 -1.9072886e+00 ... -1.2409547e+03\n",
      "  -3.2863904e+02  9.1081555e+02]\n",
      " [ 8.9908600e-02  1.0000000e+00 -1.9073296e+00 ... -4.7453424e-01\n",
      "   1.9737630e+00 -8.9730078e-01]\n",
      " ...\n",
      " [ 8.9857101e-02  1.0000000e+00 -1.9073029e+00 ...  3.6542505e-01\n",
      "   2.8282266e+00 -3.8620913e-01]\n",
      " [ 8.9931488e-02  1.0000000e+00 -1.9073658e+00 ...  1.3053095e+00\n",
      "  -5.8372645e+00 -3.1899363e-01]\n",
      " [ 8.9904785e-02  1.0000000e+00 -1.9073486e+00 ...  8.2404327e-01\n",
      "   1.3453054e-01  1.8379537e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7491913e-01  1.0000000e+00 -1.7862854e+00 ... -2.2622323e-01\n",
      "  -9.6563530e-01  8.6650348e-01]\n",
      " [ 6.7490673e-01  1.0000000e+00 -1.7862015e+00 ... -7.7982758e+02\n",
      "  -3.2312241e+02  4.6256061e+02]\n",
      " [ 6.7489052e-01  1.0000000e+00 -1.7862333e+00 ...  4.7518659e-01\n",
      "  -7.2887611e+00 -6.2678528e-01]\n",
      " ...\n",
      " [ 6.7484093e-01  1.0000000e+00 -1.7862196e+00 ...  1.3229536e+00\n",
      "   9.0889530e+00  9.3134224e-02]\n",
      " [ 6.7491341e-01  1.0000000e+00 -1.7862930e+00 ...  7.9747784e-01\n",
      "  -3.6793971e+00  3.2412797e-01]\n",
      " [ 6.7489338e-01  1.0000000e+00 -1.7862911e+00 ... -8.9633644e-02\n",
      "  -1.0494566e+00 -1.1812553e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1940479e+00  1.0000000e+00 -1.4902115e+00 ...  5.6909800e-02\n",
      "  -5.2709389e-01  8.9047635e-01]\n",
      " [ 1.1940355e+00  1.0000000e+00 -1.4901257e+00 ... -1.6747148e+02\n",
      "   5.9122997e+01 -2.2593412e+01]\n",
      " [ 1.1939831e+00  1.0000000e+00 -1.4901663e+00 ...  6.9210839e-01\n",
      "   1.6677704e+00  1.5721118e-01]\n",
      " ...\n",
      " [ 1.1939373e+00  1.0000000e+00 -1.4901505e+00 ... -5.4507003e+00\n",
      "   8.1363516e+00  1.3217008e+01]\n",
      " [ 1.1940022e+00  1.0000000e+00 -1.4902096e+00 ... -4.8710668e-01\n",
      "  -6.2967277e-01  1.3491008e+00]\n",
      " [ 1.1940241e+00  1.0000000e+00 -1.4902191e+00 ...  6.1156547e-01\n",
      "   1.9562883e+00  4.1344190e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.59621906e+00  1.00000000e+00 -1.04824257e+00 ...  4.01645660e-01\n",
      "   9.32970047e-02  2.15082765e-01]\n",
      " [ 1.59620667e+00  1.00000000e+00 -1.04815388e+00 ... -2.43340302e+01\n",
      "  -6.56002808e+01 -6.78255796e+00]\n",
      " [ 1.59614182e+00  1.00000000e+00 -1.04819548e+00 ...  1.31531692e+00\n",
      "   2.67025805e+00 -5.56218863e-01]\n",
      " ...\n",
      " [ 1.59610367e+00  1.00000000e+00 -1.04818439e+00 ...  1.24732262e+02\n",
      "   4.72933884e+01 -1.52368011e+02]\n",
      " [ 1.59615707e+00  1.00000000e+00 -1.04823494e+00 ...  1.04457736e+00\n",
      "  -1.97842360e+00  7.24333167e-01]\n",
      " [ 1.59620667e+00  1.00000000e+00 -1.04825211e+00 ...  2.46650934e-01\n",
      "   2.03639555e+00  1.04166746e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.3        0.         0.         0.1        0.3        0.\n",
      " 0.         0.         0.         0.         0.4        0.\n",
      " 0.         0.         0.         0.6        0.         0.\n",
      " 0.70000005 0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8419390e+00  1.0000000e+00 -5.0377655e-01 ...  1.9978762e-01\n",
      "   9.6585512e-01  4.9700215e-04]\n",
      " [ 1.8419199e+00  1.0000000e+00 -5.0368309e-01 ... -6.0096118e+02\n",
      "  -4.1611682e+02 -2.1032361e+02]\n",
      " [ 1.8419151e+00  1.0000000e+00 -5.0372511e-01 ...  4.9249172e-01\n",
      "   8.4099579e-01 -2.8307748e-01]\n",
      " ...\n",
      " [ 1.8418884e+00  1.0000000e+00 -5.0371933e-01 ... -7.6675499e+01\n",
      "   2.1414360e+01  4.6674393e+01]\n",
      " [ 1.8419304e+00  1.0000000e+00 -5.0376892e-01 ... -8.3061218e-02\n",
      "  -1.1106820e+00  4.3786901e-01]\n",
      " [ 1.8419380e+00  1.0000000e+00 -5.0378990e-01 ... -3.3901453e-01\n",
      "   2.0488858e+00 -1.5265797e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 1.0000001 0.2       0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90749645e+00  1.00000000e+00  9.03396606e-02 ...  3.65869761e+00\n",
      "  -4.89278078e+00 -1.25627840e+00]\n",
      " [ 1.90747070e+00  1.00000000e+00  9.04455185e-02 ...  4.48646049e+01\n",
      "  -8.76999817e+01  7.19436188e+01]\n",
      " [ 1.90743446e+00  1.00000000e+00  9.03993025e-02 ... -3.56972218e-01\n",
      "  -6.80758476e-01  4.95001435e-01]\n",
      " ...\n",
      " [ 1.90741539e+00  1.00000000e+00  9.04073715e-02 ... -2.51871037e+00\n",
      "  -9.98530045e+01 -2.40125320e+02]\n",
      " [ 1.90744972e+00  1.00000000e+00  9.03491974e-02 ... -7.30615139e-01\n",
      "   1.44410181e+00  2.76265323e-01]\n",
      " [ 1.90750217e+00  1.00000000e+00  9.03263092e-02 ...  1.02820635e-01\n",
      "   8.01993847e-01 -2.16864944e-02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 0.        0.6       0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7862101e+00  1.0000000e+00  6.7540169e-01 ...  1.7100831e+00\n",
      "  -4.5861945e+00 -8.6312902e-01]\n",
      " [ 1.7861700e+00  1.0000000e+00  6.7550564e-01 ...  1.1601604e+02\n",
      "  -2.3595096e+02  3.1472641e+01]\n",
      " [ 1.7861576e+00  1.0000000e+00  6.7546129e-01 ... -5.1894855e+00\n",
      "  -9.9839029e+00 -1.6205686e+00]\n",
      " ...\n",
      " [ 1.7861423e+00  1.0000000e+00  6.7547035e-01 ...  7.0795273e+01\n",
      "  -3.7360077e+01 -4.5609367e+01]\n",
      " [ 1.7861767e+00  1.0000000e+00  6.7541122e-01 ...  3.1716533e+00\n",
      "  -3.5444996e+00 -4.9941278e-01]\n",
      " [ 1.7862225e+00  1.0000000e+00  6.7538834e-01 ...  2.2238257e+00\n",
      "   4.4010620e+00 -7.7967525e-02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        1.0000001 0.        0.        0.\n",
      " 0.3       0.1       0.        1.0000001 0.        0.        0.\n",
      " 0.        0.9000001 0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4897881e+00  1.0000000e+00  1.1944523e+00 ...  2.1190593e+00\n",
      "  -5.1497817e+00 -2.2955709e+00]\n",
      " [ 1.4897327e+00  1.0000000e+00  1.1945324e+00 ... -6.8514488e+01\n",
      "   6.8913353e+01 -9.2534790e+01]\n",
      " [ 1.4897137e+00  1.0000000e+00  1.1944989e+00 ...  2.4605322e-01\n",
      "  -8.8741541e-01  6.2302566e-01]\n",
      " ...\n",
      " [ 1.4897022e+00  1.0000000e+00  1.1945009e+00 ... -1.8166277e+02\n",
      "   3.6186731e+02  4.8732269e+02]\n",
      " [ 1.4897385e+00  1.0000000e+00  1.1944599e+00 ...  8.4204550e+00\n",
      "  -9.4540997e+00 -9.3019271e-01]\n",
      " [ 1.4898071e+00  1.0000000e+00  1.1944408e+00 ...  3.6525617e+00\n",
      "   5.0564251e+00 -5.2223271e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.70000005 0.         0.         0.8000001  0.         0.\n",
      " 0.         1.0000001  0.         0.         1.0000001  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.0000001  0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0476141e+00  1.0000000e+00  1.5964947e+00 ... -9.0110570e-01\n",
      "  -4.3812776e+00 -2.5661578e+00]\n",
      " [ 1.0475483e+00  1.0000000e+00  1.5965767e+00 ... -9.5455498e+01\n",
      "   9.3323723e+01 -1.2044663e+01]\n",
      " [ 1.0475101e+00  1.0000000e+00  1.5965297e+00 ...  3.5527501e-01\n",
      "  -9.6777010e-01  1.4708077e+00]\n",
      " ...\n",
      " [ 1.0475006e+00  1.0000000e+00  1.5965471e+00 ...  2.5848621e+01\n",
      "   3.1178696e+02  1.6237714e+02]\n",
      " [ 1.0475388e+00  1.0000000e+00  1.5964909e+00 ...  2.6754680e+00\n",
      "  -5.1932750e+00 -2.2625964e+00]\n",
      " [ 1.0476379e+00  1.0000000e+00  1.5964909e+00 ...  5.8133698e-01\n",
      "  -1.3940601e+00  1.7954731e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.1        0.         0.         0.         0.\n",
      " 0.         0.6        0.         0.         0.70000005 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.1        0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0310135e-01  1.0000000e+00  1.8419724e+00 ...  1.6177034e-01\n",
      "  -1.9507852e+00 -1.6035213e+00]\n",
      " [ 5.0302982e-01  1.0000000e+00  1.8420334e+00 ...  3.2497977e+02\n",
      "   1.5983513e+01 -3.0361963e+02]\n",
      " [ 5.0299454e-01  1.0000000e+00  1.8420025e+00 ...  1.8173873e-02\n",
      "   1.0290613e+00  5.5080783e-01]\n",
      " ...\n",
      " [ 5.0298500e-01  1.0000000e+00  1.8420134e+00 ...  1.6068108e+02\n",
      "  -8.3140099e+01 -4.9111584e+02]\n",
      " [ 5.0302887e-01  1.0000000e+00  1.8419590e+00 ...  1.0092454e+00\n",
      "  -2.1007757e+00 -1.0671737e+00]\n",
      " [ 5.0312710e-01  1.0000000e+00  1.8419704e+00 ...  6.2377644e-01\n",
      "   1.7736616e+00  6.4511037e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -0.09109116   1.           1.9072952  ...   0.21624327  -0.31199646\n",
      "   -1.3571113 ]\n",
      " [ -0.09116459   1.           1.9073439  ...  72.22251    -16.043032\n",
      "    6.4696374 ]\n",
      " [ -0.09120941   1.           1.9073018  ...  -1.2485659    3.0329702\n",
      "    0.28364038]\n",
      " ...\n",
      " [ -0.09121895   1.           1.9073267  ... -47.457146    38.56929\n",
      "  -13.739359  ]\n",
      " [ -0.09117317   1.           1.9072742  ...  -0.5051962    0.64835167\n",
      "   -0.6321031 ]\n",
      " [ -0.09106541   1.           1.9073048  ...  -0.3667382    4.1303535\n",
      "    0.503298  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.75907135e-01  1.00000000e+00  1.78587341e+00 ...  1.96116209e+00\n",
      "   2.22659421e+00 -1.04944944e-01]\n",
      " [-6.75974846e-01  1.00000000e+00  1.78590584e+00 ...  7.97998810e+01\n",
      "   6.14475555e+01  1.24739666e+01]\n",
      " [-6.76044464e-01  1.00000000e+00  1.78586507e+00 ... -1.85707462e+00\n",
      "   9.82728767e+00  1.58851707e+00]\n",
      " ...\n",
      " [-6.76054001e-01  1.00000000e+00  1.78589058e+00 ...  7.33944321e+01\n",
      "   2.54330368e+02  1.18155327e+02]\n",
      " [-6.76008224e-01  1.00000000e+00  1.78583336e+00 ...  3.99957597e-01\n",
      "  -2.58708143e+00 -1.68499351e+00]\n",
      " [-6.75882339e-01  1.00000000e+00  1.78589630e+00 ...  2.46662331e+00\n",
      "   9.67319298e+00  5.89075923e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1950092e+00  1.0000000e+00  1.4894161e+00 ...  5.4185119e+00\n",
      "   5.8522854e+00  1.7560561e+00]\n",
      " [-1.1950693e+00  1.0000000e+00  1.4894142e+00 ... -7.2681709e+01\n",
      "  -9.0489601e+01 -1.2943581e+02]\n",
      " [-1.1951370e+00  1.0000000e+00  1.4893900e+00 ...  2.7867410e+00\n",
      "  -6.0699415e+00 -1.2360141e-01]\n",
      " ...\n",
      " [-1.1951427e+00  1.0000000e+00  1.4893990e+00 ...  3.2221106e+02\n",
      "  -2.6629770e+02  6.0553607e+02]\n",
      " [-1.1951027e+00  1.0000000e+00  1.4893684e+00 ...  9.8269540e-01\n",
      "   5.4027352e+00  8.8916910e-01]\n",
      " [-1.1949863e+00  1.0000000e+00  1.4894409e+00 ...  3.5666656e-01\n",
      "  -8.2672720e+00 -5.0422692e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5966749e+00  1.0000000e+00  1.0474949e+00 ... -6.4289708e+00\n",
      "  -7.2645054e+00  1.0701320e+00]\n",
      " [-1.5967169e+00  1.0000000e+00  1.0474911e+00 ...  1.4633369e+02\n",
      "   4.3247517e+01  7.7920398e+02]\n",
      " [-1.5967751e+00  1.0000000e+00  1.0474757e+00 ...  1.1764359e+00\n",
      "  -4.1239414e+00 -4.5051914e-01]\n",
      " ...\n",
      " [-1.5967789e+00  1.0000000e+00  1.0474739e+00 ... -1.5078854e+03\n",
      "  -1.0682642e+03 -3.7670176e+03]\n",
      " [-1.5967484e+00  1.0000000e+00  1.0474434e+00 ...  1.0348610e+00\n",
      "   4.4296122e+00 -8.6773425e-01]\n",
      " [-1.5966578e+00  1.0000000e+00  1.0475273e+00 ...  6.3243556e-01\n",
      "  -3.0242796e+00  3.9125657e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8423738e+00  1.0000000e+00  5.0242043e-01 ... -4.1923819e+00\n",
      "  -4.8032265e+00  1.0667163e-01]\n",
      " [-1.8423939e+00  1.0000000e+00  5.0241470e-01 ...  5.3575089e+01\n",
      "  -3.8172476e+02  1.2039714e+02]\n",
      " [-1.8424530e+00  1.0000000e+00  5.0240600e-01 ...  3.1325278e+00\n",
      "  -8.8664875e+00 -9.6146500e-01]\n",
      " ...\n",
      " [-1.8424568e+00  1.0000000e+00  5.0239468e-01 ...  4.9199194e+02\n",
      "  -6.4645142e+02  1.1328268e+03]\n",
      " [-1.8424301e+00  1.0000000e+00  5.0236702e-01 ... -7.0075088e+00\n",
      "  -1.1716766e+01  1.9814161e+00]\n",
      " [-1.8423615e+00  1.0000000e+00  5.0246048e-01 ...  1.0344553e-01\n",
      "  -6.0308361e-01  4.8957896e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.6       0.        0.        0.        0.        0.        0.5\n",
      " 0.        0.        0.5       0.3       0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9075069e+00  1.0000000e+00 -9.1321945e-02 ... -1.0657307e+01\n",
      "  -1.4110344e+01  1.0786715e+00]\n",
      " [-1.9075165e+00  1.0000000e+00 -9.1324806e-02 ... -4.4960663e+02\n",
      "  -1.8468912e+02  2.2200766e+02]\n",
      " [-1.9075546e+00  1.0000000e+00 -9.1332115e-02 ...  3.7021470e-01\n",
      "  -4.8355818e+00 -1.2236656e+00]\n",
      " ...\n",
      " [-1.9075546e+00  1.0000000e+00 -9.1345787e-02 ... -6.5719250e+04\n",
      "   4.4703582e+04  8.2419531e+03]\n",
      " [-1.9075336e+00  1.0000000e+00 -9.1379166e-02 ... -1.4636245e+00\n",
      "  -1.9099159e+00  5.3895593e-01]\n",
      " [-1.9075089e+00  1.0000000e+00 -9.1279984e-02 ...  3.1144524e-01\n",
      "  -1.0379982e-01  1.5842903e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7857924e+00  1.0000000e+00 -6.7666817e-01 ... -1.1953440e+00\n",
      "  -2.2501602e+00  1.8233493e-02]\n",
      " [-1.7857885e+00  1.0000000e+00 -6.7671108e-01 ...  1.5762427e+02\n",
      "  -1.0051610e+01 -5.0867968e+00]\n",
      " [-1.7858181e+00  1.0000000e+00 -6.7671090e-01 ...  3.7926114e-01\n",
      "  -5.4685588e+00 -1.4855187e+00]\n",
      " ...\n",
      " [-1.7858124e+00  1.0000000e+00 -6.7673111e-01 ...  9.8925576e+03\n",
      "   3.2058041e+04  4.8331250e+02]\n",
      " [-1.7857971e+00  1.0000000e+00 -6.7672348e-01 ...  9.6984625e-01\n",
      "   2.0910497e+00  5.4660320e-02]\n",
      " [-1.7858114e+00  1.0000000e+00 -6.7663002e-01 ...  2.4347956e+00\n",
      "  -7.7035575e+00 -5.2331543e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.3       0.        0.        0.        0.\n",
      " 0.9000001 0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4893703e+00  1.0000000e+00 -1.1952839e+00 ...  1.4020348e-01\n",
      "  -1.3260894e+00  5.1498771e-01]\n",
      " [-1.4893560e+00  1.0000000e+00 -1.1953249e+00 ... -4.0827884e+02\n",
      "   2.4043808e+01  7.6203079e+01]\n",
      " [-1.4893970e+00  1.0000000e+00 -1.1953247e+00 ...  1.7045641e-01\n",
      "   1.6419759e+00  3.3804715e-01]\n",
      " ...\n",
      " [-1.4893894e+00  1.0000000e+00 -1.1953421e+00 ... -2.4155355e+04\n",
      "  -9.6076416e+03  1.5770156e+04]\n",
      " [-1.4893761e+00  1.0000000e+00 -1.1953373e+00 ...  6.9316339e-01\n",
      "   1.1183834e+00  1.3840052e+00]\n",
      " [-1.4894028e+00  1.0000000e+00 -1.1952496e+00 ...  3.5401120e+00\n",
      "  -9.2004194e+00  6.8259239e-03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         1.0000001  0.70000005 0.         0.\n",
      " 0.         1.0000001  0.         0.         1.0000001  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5        0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0468140e+00  1.0000000e+00 -1.5970078e+00 ... -2.8692651e-01\n",
      "  -1.8441343e-01 -1.1452174e-01]\n",
      " [-1.0467882e+00  1.0000000e+00 -1.5970240e+00 ... -3.0170471e+01\n",
      "   3.5721701e+02  4.8725677e+02]\n",
      " [-1.0468750e+00  1.0000000e+00 -1.5970294e+00 ...  7.6451445e-01\n",
      "   6.7748356e-01 -7.0297134e-01]\n",
      " ...\n",
      " [-1.0468636e+00  1.0000000e+00 -1.5970354e+00 ...  9.7937871e+03\n",
      "   1.6174943e+04  1.0455832e+04]\n",
      " [-1.0468521e+00  1.0000000e+00 -1.5970478e+00 ...  6.7560142e-01\n",
      "  -8.0960722e+00  1.6499834e+00]\n",
      " [-1.0468502e+00  1.0000000e+00 -1.5969810e+00 ...  4.2132864e+00\n",
      "  -9.5651674e+00  2.8997824e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 0.4       0.        0.        0.\n",
      " 0.3       0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0198746e-01  1.0000000e+00 -1.8423634e+00 ... -2.9799032e-01\n",
      "   1.6327858e+00  1.2351343e-01]\n",
      " [-5.0195694e-01  1.0000000e+00 -1.8423653e+00 ...  9.8586006e+01\n",
      "   5.0416115e+01 -1.4684833e+02]\n",
      " [-5.0205803e-01  1.0000000e+00 -1.8423663e+00 ...  5.1329911e-01\n",
      "   1.5567083e+00  9.0913534e-02]\n",
      " ...\n",
      " [-5.0204659e-01  1.0000000e+00 -1.8423758e+00 ...  4.5101851e+03\n",
      "  -2.1148984e+04 -1.4203398e+04]\n",
      " [-5.0203514e-01  1.0000000e+00 -1.8423843e+00 ...  5.7186365e-01\n",
      "  -1.0237688e+01 -4.4947910e-01]\n",
      " [-5.0202656e-01  1.0000000e+00 -1.8423424e+00 ...  3.0380316e+00\n",
      "  -5.6923771e+00  3.1650865e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2 0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.1940880e-02  1.0000000e+00 -1.9073124e+00 ...  2.5082445e-01\n",
      "   2.0988307e+00  4.8053622e-01]\n",
      " [ 9.1971397e-02  1.0000000e+00 -1.9073296e+00 ...  2.3545476e+02\n",
      "  -2.2092267e+02 -1.8303033e+02]\n",
      " [ 9.1890335e-02  1.0000000e+00 -1.9073200e+00 ...  8.1503892e-01\n",
      "   2.0594902e+00  7.2798419e-01]\n",
      " ...\n",
      " [ 9.1901779e-02  1.0000000e+00 -1.9073420e+00 ...  4.2825992e+04\n",
      "   2.6626971e+04 -1.3723540e+04]\n",
      " [ 9.1913223e-02  1.0000000e+00 -1.9073200e+00 ... -2.2118180e-01\n",
      "  -1.3688134e+01 -1.4279783e+00]\n",
      " [ 9.1900826e-02  1.0000000e+00 -1.9073086e+00 ...  9.9352944e-01\n",
      "  -1.4194093e+00  5.8488441e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7671967e-01  1.0000000e+00 -1.7853394e+00 ...  2.7516000e+00\n",
      "   5.2919040e+00 -1.8305683e-01]\n",
      " [ 6.7674732e-01  1.0000000e+00 -1.7853556e+00 ... -2.2565321e+02\n",
      "   2.1223991e+02 -2.5547343e+02]\n",
      " [ 6.7668724e-01  1.0000000e+00 -1.7853644e+00 ... -1.6763210e-01\n",
      "   1.4017463e+00 -5.4977894e-02]\n",
      " ...\n",
      " [ 6.7669678e-01  1.0000000e+00 -1.7853746e+00 ...  7.4201123e+02\n",
      "   2.7642180e+03  7.6072534e+02]\n",
      " [ 6.7670822e-01  1.0000000e+00 -1.7853451e+00 ...  8.1190407e-02\n",
      "  -9.4787178e+00  4.2848697e-01]\n",
      " [ 6.7668056e-01  1.0000000e+00 -1.7853451e+00 ...  5.3187549e-01\n",
      "  -1.6051257e+00  5.7362038e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1957111e+00  1.0000000e+00 -1.4885921e+00 ...  9.3736649e-01\n",
      "   2.0479145e+00 -1.1277392e+00]\n",
      " [ 1.1957359e+00  1.0000000e+00 -1.4885664e+00 ... -4.0435260e+02\n",
      "  -2.9080237e+02  6.4229095e+01]\n",
      " [ 1.1956844e+00  1.0000000e+00 -1.4885913e+00 ... -1.7495084e-01\n",
      "   1.1998415e+00  4.0551573e-02]\n",
      " ...\n",
      " [ 1.1956902e+00  1.0000000e+00 -1.4885979e+00 ...  1.1185880e+02\n",
      "  -3.0603042e+03  2.1465139e+03]\n",
      " [ 1.1956940e+00  1.0000000e+00 -1.4885979e+00 ...  1.2716087e+00\n",
      "  -9.9053173e+00 -2.0027161e-01]\n",
      " [ 1.1956768e+00  1.0000000e+00 -1.4886055e+00 ...  1.4274230e+00\n",
      "  -1.9007819e+00  7.8714675e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5973473e+00  1.0000000e+00 -1.0461006e+00 ...  2.3558929e+00\n",
      "   3.5456824e+00 -8.7780081e-02]\n",
      " [ 1.5973692e+00  1.0000000e+00 -1.0460978e+00 ... -1.5815396e+02\n",
      "  -7.3974327e+01  6.6667877e+02]\n",
      " [ 1.5973206e+00  1.0000000e+00 -1.0461326e+00 ...  8.6480808e-01\n",
      "   2.6894679e+00 -9.4359696e-02]\n",
      " ...\n",
      " [ 1.5973244e+00  1.0000000e+00 -1.0461397e+00 ... -6.9200043e+02\n",
      "  -6.0213678e+02  1.4644525e+03]\n",
      " [ 1.5973225e+00  1.0000000e+00 -1.0461063e+00 ...  2.7024379e+00\n",
      "  -3.8177676e+00  3.8260704e-01]\n",
      " [ 1.5973215e+00  1.0000000e+00 -1.0461178e+00 ...  2.2287159e+00\n",
      "  -2.3003604e+00  9.8203874e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8424788e+00  1.0000000e+00 -5.0106239e-01 ...  3.4037864e+00\n",
      "   6.7674537e+00  1.5632381e+00]\n",
      " [ 1.8424921e+00  1.0000000e+00 -5.0105286e-01 ... -2.0376861e+00\n",
      "   2.2026584e+02  6.7812787e+02]\n",
      " [ 1.8424721e+00  1.0000000e+00 -5.0109619e-01 ...  3.3401556e+00\n",
      "   6.9501057e+00  4.3125820e-01]\n",
      " ...\n",
      " [ 1.8424721e+00  1.0000000e+00 -5.0109863e-01 ... -1.0742931e+03\n",
      "   2.1054285e+02 -1.2736863e+02]\n",
      " [ 1.8424664e+00  1.0000000e+00 -5.0106812e-01 ...  1.8130835e+00\n",
      "   1.0169241e+00  2.6390786e+00]\n",
      " [ 1.8424606e+00  1.0000000e+00 -5.0108147e-01 ... -1.8773890e-01\n",
      "  -8.4051919e-01  4.7068453e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 1.0000001 0.        0.9000001 0.        0.        0.1       0.\n",
      " 0.2       1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9072170e+00  1.0000000e+00  9.2903137e-02 ...  4.7669506e-01\n",
      "   4.3474169e+00  8.1412154e-01]\n",
      " [ 1.9072237e+00  1.0000000e+00  9.2883110e-02 ... -1.3489246e+02\n",
      "   9.7984550e+01 -2.0588164e+02]\n",
      " [ 1.9071770e+00  1.0000000e+00  9.2850409e-02 ... -1.6364503e-01\n",
      "  -4.2029228e+00 -1.6914010e-03]\n",
      " ...\n",
      " [ 1.9071770e+00  1.0000000e+00  9.2833519e-02 ...  2.3005364e+03\n",
      "  -1.7022349e+03  7.1184137e+02]\n",
      " [ 1.9071674e+00  1.0000000e+00  9.2897415e-02 ...  1.8650632e+00\n",
      "   1.4015669e+00  1.3297651e+00]\n",
      " [ 1.9072104e+00  1.0000000e+00  9.2882156e-02 ... -5.1535773e-01\n",
      "  -6.6255301e-01  3.2536826e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 1.0000001 0.        1.0000001 0.        0.        1.0000001 0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7851009e+00  1.0000000e+00  6.7788887e-01 ...  2.2134817e-01\n",
      "   1.1459133e+01  1.4764516e+00]\n",
      " [ 1.7850990e+00  1.0000000e+00  6.7787170e-01 ... -2.3409549e+02\n",
      "   2.6688524e+01  1.9459375e+02]\n",
      " [ 1.7850819e+00  1.0000000e+00  6.7783833e-01 ... -1.4953662e+00\n",
      "  -6.7320600e+00  4.0132618e-01]\n",
      " ...\n",
      " [ 1.7850819e+00  1.0000000e+00  6.7782497e-01 ...  8.8888525e+02\n",
      "  -3.3584232e+02 -5.2603247e+02]\n",
      " [ 1.7850647e+00  1.0000000e+00  6.7788315e-01 ...  4.8183942e-01\n",
      "   2.5753078e+00  3.9715307e+00]\n",
      " [ 1.7851019e+00  1.0000000e+00  6.7787170e-01 ... -6.3836664e-02\n",
      "  -8.4085852e-01 -3.1236798e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 1.0000001 0.1       0.5       0.        0.        1.0000001 0.\n",
      " 1.0000001 1.0000001 0.1       0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4881687e+00  1.0000000e+00  1.1964302e+00 ... -1.2234069e+00\n",
      "   5.1324368e+00  1.5988600e+00]\n",
      " [ 1.4881630e+00  1.0000000e+00  1.1964045e+00 ...  1.6106213e+02\n",
      "  -1.0299005e+02  1.6001312e+02]\n",
      " [ 1.4881344e+00  1.0000000e+00  1.1963845e+00 ... -4.9487305e-01\n",
      "   2.8935981e+00 -3.5338950e-01]\n",
      " ...\n",
      " [ 1.4881344e+00  1.0000000e+00  1.1963654e+00 ... -4.9514496e+02\n",
      "   7.0272461e+01 -2.1464775e+02]\n",
      " [ 1.4881077e+00  1.0000000e+00  1.1964245e+00 ... -2.3759949e+02\n",
      "   1.3916945e+02 -2.3751546e+02]\n",
      " [ 1.4881811e+00  1.0000000e+00  1.1964188e+00 ...  9.1510832e-02\n",
      "  -8.7601233e-01 -1.0238119e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        1.0000001 0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.2       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0456257e+00  1.0000000e+00  1.5978413e+00 ... -5.6451988e-01\n",
      "   3.3301463e+00  1.0307496e+00]\n",
      " [ 1.0456181e+00  1.0000000e+00  1.5978212e+00 ...  2.2225229e+02\n",
      "   1.3122148e+02 -3.1964612e+02]\n",
      " [ 1.0456219e+00  1.0000000e+00  1.5977991e+00 ... -2.2072399e-01\n",
      "   1.5759897e-01  5.5636287e-02]\n",
      " ...\n",
      " [ 1.0456219e+00  1.0000000e+00  1.5977907e+00 ... -1.1877159e+03\n",
      "   1.4692891e+03  3.2176350e+03]\n",
      " [ 1.0455837e+00  1.0000000e+00  1.5978355e+00 ...  1.4165779e+01\n",
      "  -3.5160889e+01 -3.6218393e+02]\n",
      " [ 1.0456467e+00  1.0000000e+00  1.5978394e+00 ...  1.5671802e-01\n",
      "  -8.3298445e-01 -1.2661042e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2       0.        0.        0.        1.0000001 0.        0.\n",
      " 0.3       0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0085163e-01  1.0000000e+00  1.8427715e+00 ... -1.4119637e+00\n",
      "   3.6323011e-01  2.5854179e-01]\n",
      " [ 5.0084400e-01  1.0000000e+00  1.8427486e+00 ...  7.0122223e+02\n",
      "   1.6881675e+03  8.4857642e+02]\n",
      " [ 5.0082970e-01  1.0000000e+00  1.8427451e+00 ... -8.5860968e-02\n",
      "   2.2069669e+00 -1.6525060e-01]\n",
      " ...\n",
      " [ 5.0082970e-01  1.0000000e+00  1.8427277e+00 ...  8.9046332e+02\n",
      "  -2.1966536e+03 -4.0632718e+02]\n",
      " [ 5.0078773e-01  1.0000000e+00  1.8427639e+00 ...  3.3194304e-02\n",
      "   6.3638268e+01 -8.8042679e+01]\n",
      " [ 5.0087261e-01  1.0000000e+00  1.8427773e+00 ...  8.3046174e-01\n",
      "  -1.6474698e+00 -1.1584582e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.4       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.        0.        1.0000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.3503952e-02  1.0000000e+00  1.9073601e+00 ...  1.9453498e+00\n",
      "  -3.0239937e+00  8.4786916e-01]\n",
      " [-9.3511581e-02  1.0000000e+00  1.9073601e+00 ... -9.5334932e+03\n",
      "  -1.7197254e+04 -6.0651758e+03]\n",
      " [-9.3521118e-02  1.0000000e+00  1.9073527e+00 ... -1.0290614e-01\n",
      "   6.2798023e+00 -4.6526259e-01]\n",
      " ...\n",
      " [-9.3521118e-02  1.0000000e+00  1.9073439e+00 ...  4.3926208e+02\n",
      "  -1.6998172e+02 -1.1460111e+03]\n",
      " [-9.3564987e-02  1.0000000e+00  1.9073505e+00 ...  2.1586901e+01\n",
      "  -7.4067068e+00 -1.2928390e+01]\n",
      " [-9.3482018e-02  1.0000000e+00  1.9073925e+00 ...  9.6100330e-01\n",
      "  -4.6313119e+00 -3.9773154e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.2]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7840958e-01  1.0000000e+00  1.7850800e+00 ...  7.2569394e-01\n",
      "  -2.6132545e+00  7.7765167e-01]\n",
      " [-6.7841911e-01  1.0000000e+00  1.7850971e+00 ... -3.9273938e+03\n",
      "   3.5877527e+03 -1.3758550e+03]\n",
      " [-6.7842865e-01  1.0000000e+00  1.7851008e+00 ... -1.6295628e-01\n",
      "   8.1325903e+00 -1.0267010e+00]\n",
      " ...\n",
      " [-6.7842865e-01  1.0000000e+00  1.7850819e+00 ... -4.2592529e+02\n",
      "  -4.5095571e+03 -4.1887178e+03]\n",
      " [-6.7846489e-01  1.0000000e+00  1.7850666e+00 ... -8.8660145e-01\n",
      "  -1.9049184e+00 -8.5937548e+00]\n",
      " [-6.7839050e-01  1.0000000e+00  1.7851238e+00 ...  3.0307140e+00\n",
      "   3.9460015e+00  1.6747063e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1966877e+00  1.0000000e+00  1.4882278e+00 ... -5.5781260e+00\n",
      "   6.0912189e+00  1.1647173e+00]\n",
      " [-1.1966972e+00  1.0000000e+00  1.4882326e+00 ... -1.9228583e+03\n",
      "  -3.9050259e+03  3.3733201e+03]\n",
      " [-1.1967163e+00  1.0000000e+00  1.4882491e+00 ...  1.6822812e-01\n",
      "   9.7181807e+00 -3.7858051e-01]\n",
      " ...\n",
      " [-1.1967163e+00  1.0000000e+00  1.4882174e+00 ...  1.1516327e+02\n",
      "  -2.8579939e+01  2.5792136e+02]\n",
      " [-1.1967487e+00  1.0000000e+00  1.4882069e+00 ...  2.4715388e+00\n",
      "  -7.2489443e+00  7.2569895e+00]\n",
      " [-1.1966734e+00  1.0000000e+00  1.4882832e+00 ...  1.6747274e+00\n",
      "  -1.2072945e+00 -2.9616518e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5980921e+00  1.0000000e+00  1.0454922e+00 ... -5.0616369e+00\n",
      "   5.1277075e+00 -1.8304622e+00]\n",
      " [-1.5981016e+00  1.0000000e+00  1.0455294e+00 ...  1.1041821e+03\n",
      "  -1.6356370e+03 -4.8968750e+02]\n",
      " [-1.5981064e+00  1.0000000e+00  1.0455300e+00 ... -5.7616413e-02\n",
      "  -6.8838663e+00  1.1258063e+00]\n",
      " ...\n",
      " [-1.5981045e+00  1.0000000e+00  1.0455141e+00 ... -1.5107599e+02\n",
      "   3.6222237e+01  4.4566394e+02]\n",
      " [-1.5981312e+00  1.0000000e+00  1.0454712e+00 ... -1.1056886e+00\n",
      "   1.8564323e+01 -1.1944952e+01]\n",
      " [-1.5980854e+00  1.0000000e+00  1.0455647e+00 ...  3.9889309e+00\n",
      "   3.6095530e-02  5.5365229e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8427954e+00  1.0000000e+00  5.0033379e-01 ...  2.2829823e+00\n",
      "  -5.6946797e+00 -1.7735392e-02]\n",
      " [-1.8428059e+00  1.0000000e+00  5.0035954e-01 ... -4.9617702e+01\n",
      "   2.8979398e+02  2.2126082e+02]\n",
      " [-1.8427963e+00  1.0000000e+00  5.0036877e-01 ... -2.9772678e-01\n",
      "  -1.9860263e+00 -8.4370959e-01]\n",
      " ...\n",
      " [-1.8427925e+00  1.0000000e+00  5.0034428e-01 ...  2.4898024e+01\n",
      "   1.1066713e+01 -7.7520691e+01]\n",
      " [-1.8428116e+00  1.0000000e+00  5.0030899e-01 ... -8.9481209e+01\n",
      "  -1.5578814e+01 -1.2535606e+02]\n",
      " [-1.8428001e+00  1.0000000e+00  5.0041199e-01 ...  1.8115323e+00\n",
      "   6.8083274e-01 -7.7919132e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.1       0.        0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9072151e+00  1.0000000e+00 -9.3570709e-02 ...  1.4911361e+00\n",
      "  -4.8403540e+00 -2.0498288e+00]\n",
      " [-1.9072332e+00  1.0000000e+00 -9.3549728e-02 ...  1.9807761e+02\n",
      "   2.7980017e+02 -5.9385876e+02]\n",
      " [-1.9072552e+00  1.0000000e+00 -9.3532085e-02 ... -4.3307638e-01\n",
      "  -1.1564224e+01  8.3475578e-01]\n",
      " ...\n",
      " [-1.9072475e+00  1.0000000e+00 -9.3564987e-02 ... -1.0453646e+01\n",
      "   4.1277813e+01  3.3038429e+01]\n",
      " [-1.9072552e+00  1.0000000e+00 -9.3597412e-02 ...  1.5434795e+02\n",
      "  -3.1353088e+02 -1.3671411e+02]\n",
      " [-1.9072409e+00  1.0000000e+00 -9.3490601e-02 ...  4.6147472e-01\n",
      "  -5.8816910e-02 -4.2839625e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.5       0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.78478909e+00  1.00000000e+00 -6.78831100e-01 ...  2.01601219e+00\n",
      "   7.44916248e+00 -9.37392831e-01]\n",
      " [-1.78481579e+00  1.00000000e+00 -6.78822517e-01 ...  4.88572083e+01\n",
      "   5.33161377e+02 -2.86938568e+02]\n",
      " [-1.78483391e+00  1.00000000e+00 -6.78801358e-01 ...  1.55983007e+00\n",
      "   7.26631165e+00 -1.32290840e-01]\n",
      " ...\n",
      " [-1.78482628e+00  1.00000000e+00 -6.78837776e-01 ... -1.61093216e+02\n",
      "   1.72440536e+02  7.41478760e+02]\n",
      " [-1.78482628e+00  1.00000000e+00 -6.78852081e-01 ... -2.09512539e+01\n",
      "   1.20807686e+02  7.43238831e+01]\n",
      " [-1.78483582e+00  1.00000000e+00 -6.78750992e-01 ... -5.63013554e-01\n",
      "  -1.23247862e-01 -3.47109437e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        1.0000001 0.5       0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4874678e+00  1.0000000e+00 -1.1971283e+00 ...  5.3662455e-01\n",
      "  -4.7842093e+00  1.2364849e+00]\n",
      " [-1.4874973e+00  1.0000000e+00 -1.1971254e+00 ... -3.9938931e+01\n",
      "  -1.6020542e+01  8.5995979e+01]\n",
      " [-1.4875298e+00  1.0000000e+00 -1.1970968e+00 ...  1.1229985e+00\n",
      "   8.7484016e+00  1.1775410e+00]\n",
      " ...\n",
      " [-1.4875164e+00  1.0000000e+00 -1.1971397e+00 ...  9.3229698e+01\n",
      "  -5.4750805e+00  2.0688208e+02]\n",
      " [-1.4875011e+00  1.0000000e+00 -1.1971493e+00 ...  7.8488144e+01\n",
      "  -5.6775177e+01  5.3945541e-02]\n",
      " [-1.4875269e+00  1.0000000e+00 -1.1970615e+00 ... -5.4178309e-01\n",
      "   2.9982567e-02 -3.7712273e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4       0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0449572e+00  1.0000000e+00 -1.5980949e+00 ...  1.5618267e+00\n",
      "  -1.1407418e+00 -9.4661891e-01]\n",
      " [-1.0449905e+00  1.0000000e+00 -1.5981054e+00 ...  3.0551964e+01\n",
      "   9.3509171e+01  1.7258940e+02]\n",
      " [-1.0450115e+00  1.0000000e+00 -1.5980914e+00 ...  9.0643829e-01\n",
      "   7.9411163e+00  6.0715891e-02]\n",
      " ...\n",
      " [-1.0449944e+00  1.0000000e+00 -1.5981169e+00 ...  1.1578708e+02\n",
      "   1.9434602e+02 -1.1122537e+02]\n",
      " [-1.0449715e+00  1.0000000e+00 -1.5981159e+00 ... -3.7621853e+01\n",
      "   3.6818943e+01  5.9178917e+01]\n",
      " [-1.0450258e+00  1.0000000e+00 -1.5980339e+00 ... -3.7381625e-01\n",
      "   2.1390033e-01 -4.4658059e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.9935055e-01  1.0000000e+00 -1.8429756e+00 ... -6.2282151e-01\n",
      "  -6.5832090e-01  2.7888918e-01]\n",
      " [-4.9938965e-01  1.0000000e+00 -1.8429480e+00 ... -2.3111969e+02\n",
      "  -5.2219402e+01 -6.6478119e+01]\n",
      " [-4.9941254e-01  1.0000000e+00 -1.8429414e+00 ... -1.3971822e+00\n",
      "  -3.6410232e+00 -1.2286663e+00]\n",
      " ...\n",
      " [-4.9939346e-01  1.0000000e+00 -1.8429565e+00 ...  9.7256561e+01\n",
      "   6.0737617e+01  1.6964447e+01]\n",
      " [-4.9937057e-01  1.0000000e+00 -1.8429966e+00 ...  1.4320612e+02\n",
      "   1.2395309e+02  3.3363998e+02]\n",
      " [-4.9943161e-01  1.0000000e+00 -1.8429298e+00 ... -3.4689355e-01\n",
      "   1.2974548e-01 -4.1451693e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.4434738e-02  1.0000000e+00 -1.9071560e+00 ... -7.0622563e-01\n",
      "  -5.3460298e+00 -1.3380629e-01]\n",
      " [ 9.4393730e-02  1.0000000e+00 -1.9071369e+00 ... -2.6949478e+03\n",
      "  -1.5300463e+03 -5.8261334e+02]\n",
      " [ 9.4394684e-02  1.0000000e+00 -1.9071307e+00 ... -6.9046426e-01\n",
      "  -5.2129564e+00  1.1363995e-01]\n",
      " ...\n",
      " [ 9.4415665e-02  1.0000000e+00 -1.9071426e+00 ... -1.5410762e+01\n",
      "  -8.9960304e+01  7.8317970e+01]\n",
      " [ 9.4440460e-02  1.0000000e+00 -1.9071751e+00 ... -3.0100232e+02\n",
      "   4.4590146e+02 -3.9845416e+02]\n",
      " [ 9.4350815e-02  1.0000000e+00 -1.9071274e+00 ... -4.1051698e-01\n",
      "   1.5694380e-01 -3.9985693e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.1       0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7947292e-01  1.0000000e+00 -1.7845688e+00 ...  1.8164900e-01\n",
      "  -1.0625494e+01  5.1619142e-01]\n",
      " [ 6.7943382e-01  1.0000000e+00 -1.7845688e+00 ... -2.0058078e+02\n",
      "  -4.2190381e+02  2.8392581e+02]\n",
      " [ 6.7943954e-01  1.0000000e+00 -1.7845604e+00 ... -2.4561002e+00\n",
      "  -1.0243240e+01  7.1216154e-01]\n",
      " ...\n",
      " [ 6.7945671e-01  1.0000000e+00 -1.7845745e+00 ... -3.9190628e+01\n",
      "  -1.2865109e+02  4.3005856e+01]\n",
      " [ 6.7948151e-01  1.0000000e+00 -1.7845821e+00 ...  2.1250571e+02\n",
      "  -2.0138434e+02  2.3421034e+01]\n",
      " [ 6.7939377e-01  1.0000000e+00 -1.7845573e+00 ... -4.2581224e-01\n",
      "   1.4140129e-01 -3.7933314e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.19776344e+00  1.00000000e+00 -1.48723793e+00 ...  3.32851171e-01\n",
      "  -8.83554268e+00  2.38773704e-01]\n",
      " [ 1.19772816e+00  1.00000000e+00 -1.48723412e+00 ...  1.06530312e+02\n",
      "  -5.22263367e+02  2.51279175e+02]\n",
      " [ 1.19771385e+00  1.00000000e+00 -1.48723626e+00 ... -6.73727870e-01\n",
      "   6.25628996e+00  1.21862888e-01]\n",
      " ...\n",
      " [ 1.19773102e+00  1.00000000e+00 -1.48723602e+00 ...  4.36549568e+01\n",
      "  -1.08428299e+02  4.65969124e+01]\n",
      " [ 1.19775009e+00  1.00000000e+00 -1.48725128e+00 ... -1.02751862e+02\n",
      "   7.05894714e+02 -3.25048828e+01]\n",
      " [ 1.19769382e+00  1.00000000e+00 -1.48725319e+00 ... -4.39177036e-01\n",
      "   1.16984844e-01 -3.51130247e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5986433e+00  1.0000000e+00 -1.0445309e+00 ...  8.8272858e-01\n",
      "  -7.9933791e+00  3.0934787e-01]\n",
      " [ 1.5986109e+00  1.0000000e+00 -1.0445518e+00 ... -1.4023659e+02\n",
      "  -6.3164726e+01 -7.5097748e+01]\n",
      " [ 1.5986099e+00  1.0000000e+00 -1.0445417e+00 ... -3.7061650e-01\n",
      "   8.2654018e+00 -1.2542003e+00]\n",
      " ...\n",
      " [ 1.5986252e+00  1.0000000e+00 -1.0445499e+00 ...  2.4698668e+02\n",
      "   6.9655609e+02  4.8268774e+02]\n",
      " [ 1.5986404e+00  1.0000000e+00 -1.0445423e+00 ...  3.9660049e+02\n",
      "  -1.3919496e+02  3.1772766e+02]\n",
      " [ 1.5985870e+00  1.0000000e+00 -1.0445652e+00 ... -3.7059617e-01\n",
      "   1.6208935e-01 -2.4524498e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8433046e+00  1.0000000e+00 -4.9904633e-01 ...  7.5779557e-01\n",
      "  -4.8196993e+00  3.4994984e-01]\n",
      " [ 1.8432856e+00  1.0000000e+00 -4.9904728e-01 ... -1.2183871e+01\n",
      "  -1.4706389e+02 -6.5378159e+01]\n",
      " [ 1.8433018e+00  1.0000000e+00 -4.9903464e-01 ...  4.2590758e-01\n",
      "  -4.4076824e-01 -7.3464639e-02]\n",
      " ...\n",
      " [ 1.8433170e+00  1.0000000e+00 -4.9904346e-01 ...  4.2747787e+01\n",
      "  -1.0222269e+01  9.7874088e+00]\n",
      " [ 1.8433247e+00  1.0000000e+00 -4.9905777e-01 ...  3.9191647e+01\n",
      "  -3.0822113e+01 -1.1331791e+02]\n",
      " [ 1.8432713e+00  1.0000000e+00 -4.9909210e-01 ... -3.0995679e-01\n",
      "   4.0917397e-01 -1.5168273e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.3       0.        0.        0.        0.3\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.2       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9073210e+00  1.0000000e+00  9.5079422e-02 ... -3.3402908e-01\n",
      "  -2.9175770e+00  6.9073838e-01]\n",
      " [ 1.9073076e+00  1.0000000e+00  9.5053673e-02 ... -2.7607529e+02\n",
      "  -3.8450589e+02  6.0841301e+01]\n",
      " [ 1.9073353e+00  1.0000000e+00  9.5077462e-02 ... -3.6232871e-01\n",
      "   1.8362265e+00  4.2162329e-02]\n",
      " ...\n",
      " [ 1.9073448e+00  1.0000000e+00  9.5058441e-02 ... -4.2242603e+01\n",
      "   1.2069405e+02 -1.6598828e+02]\n",
      " [ 1.9073486e+00  1.0000000e+00  9.5067978e-02 ... -1.6467443e+01\n",
      "   6.7195328e+01 -1.2172215e+01]\n",
      " [ 1.9073057e+00  1.0000000e+00  9.5029831e-02 ... -3.2213736e-01\n",
      "   6.6777849e-01 -9.8507881e-02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001 0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7845068e+00  1.0000000e+00  6.7988205e-01 ...  6.8929458e-01\n",
      "  -1.5614843e+00  8.1139767e-01]\n",
      " [ 1.7845011e+00  1.0000000e+00  6.7985725e-01 ...  2.6268488e+02\n",
      "   7.9400220e+02 -1.6825343e+03]\n",
      " [ 1.7845116e+00  1.0000000e+00  6.7987031e-01 ... -1.3127726e+00\n",
      "   3.5865526e+00  7.1845669e-01]\n",
      " ...\n",
      " [ 1.7845211e+00  1.0000000e+00  6.7986202e-01 ...  2.1349081e+01\n",
      "  -1.1838080e+02 -2.9566547e+02]\n",
      " [ 1.7845249e+00  1.0000000e+00  6.7987061e-01 ... -5.4824612e+01\n",
      "   3.0988916e+02  2.3943333e+02]\n",
      " [ 1.7845116e+00  1.0000000e+00  6.7984390e-01 ...  2.4541092e-01\n",
      "  -7.6861191e-01  1.4596057e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.48701763e+00  1.00000000e+00  1.19817162e+00 ... -2.28170872e-01\n",
      "   9.39086914e-01  3.97000551e-01]\n",
      " [ 1.48702145e+00  1.00000000e+00  1.19812679e+00 ... -1.19892075e+02\n",
      "  -9.20840225e+01 -2.10975052e+02]\n",
      " [ 1.48705101e+00  1.00000000e+00  1.19813931e+00 ... -4.05789280e+00\n",
      "   1.64586773e+01 -1.99807572e+00]\n",
      " ...\n",
      " [ 1.48706055e+00  1.00000000e+00  1.19812965e+00 ...  9.05547028e+01\n",
      "  -1.06718559e+01 -6.93993454e+01]\n",
      " [ 1.48706436e+00  1.00000000e+00  1.19816017e+00 ...  1.05972595e+02\n",
      "  -5.27051849e+01  8.52527313e+01]\n",
      " [ 1.48704052e+00  1.00000000e+00  1.19814110e+00 ...  1.26332557e+00\n",
      "  -3.00458431e+00 -1.26451969e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        1.0000001 0.        0.        0.        1.0000001\n",
      " 0.8000001 0.        0.        0.        0.3       0.        0.\n",
      " 0.        0.        0.        0.1       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0440350e+00  1.0000000e+00  1.5990200e+00 ... -5.8050184e+00\n",
      "   9.9829350e+00 -4.0289229e-01]\n",
      " [ 1.0440416e+00  1.0000000e+00  1.5989628e+00 ... -1.6165840e+02\n",
      "  -8.4106819e+01  6.0216107e+02]\n",
      " [ 1.0440617e+00  1.0000000e+00  1.5989891e+00 ... -1.7385414e+00\n",
      "   8.9910440e+00 -2.4285385e-01]\n",
      " ...\n",
      " [ 1.0440731e+00  1.0000000e+00  1.5989656e+00 ...  1.5799039e+02\n",
      "  -3.6632895e+02 -7.1185699e+01]\n",
      " [ 1.0440731e+00  1.0000000e+00  1.5990124e+00 ... -3.9669525e+01\n",
      "   9.4956331e+00 -1.3933521e+02]\n",
      " [ 1.0440683e+00  1.0000000e+00  1.5990028e+00 ...  2.9611149e+00\n",
      "  -6.2019076e+00  5.1534367e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4       0.        0.5       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.9895859e-01  1.0000000e+00  1.8433418e+00 ... -1.0542238e+00\n",
      "   2.6596310e+00 -1.9361771e+00]\n",
      " [ 4.9896812e-01  1.0000000e+00  1.8432493e+00 ...  5.7814789e+02\n",
      "   2.9210386e+03  1.1665684e+02]\n",
      " [ 4.9897575e-01  1.0000000e+00  1.8432990e+00 ... -1.0740414e+00\n",
      "   6.4384708e+00 -6.3611490e-01]\n",
      " ...\n",
      " [ 4.9898911e-01  1.0000000e+00  1.8432531e+00 ...  8.0225647e+02\n",
      "   8.0619128e+02 -4.3158438e+02]\n",
      " [ 4.9898720e-01  1.0000000e+00  1.8433342e+00 ...  3.7085850e+01\n",
      "   2.7195662e+01  1.1795427e+02]\n",
      " [ 4.9899578e-01  1.0000000e+00  1.8433361e+00 ...  1.9050075e+00\n",
      "  -4.7285523e+00  1.8374801e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.5330238e-02  1.0000000e+00  1.9073162e+00 ... -2.4460354e+00\n",
      "   3.4068565e+00 -2.3541898e-01]\n",
      " [-9.5320702e-02  1.0000000e+00  1.9071951e+00 ...  1.4415698e+03\n",
      "   1.1022477e+03 -5.8386060e+02]\n",
      " [-9.5296860e-02  1.0000000e+00  1.9072490e+00 ... -6.1399591e-01\n",
      "   4.8771768e+00 -3.9937270e-01]\n",
      " ...\n",
      " [-9.5283508e-02  1.0000000e+00  1.9072027e+00 ...  3.6073462e+02\n",
      "  -4.2246979e+02  2.9209067e+02]\n",
      " [-9.5287323e-02  1.0000000e+00  1.9073124e+00 ... -3.8237541e+01\n",
      "  -5.7587113e+00 -6.1344639e+01]\n",
      " [-9.5293045e-02  1.0000000e+00  1.9073277e+00 ... -1.7799819e-01\n",
      "  -2.7017162e+00  2.6873314e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.8012524e-01  1.0000000e+00  1.7842903e+00 ... -1.4725728e+00\n",
      "   5.3493228e+00 -2.6173776e-01]\n",
      " [-6.8011761e-01  1.0000000e+00  1.7841835e+00 ...  1.3301496e+01\n",
      "   3.8877084e+02 -2.6096338e+02]\n",
      " [-6.8008804e-01  1.0000000e+00  1.7842324e+00 ...  1.7138503e+00\n",
      "   3.5072298e+00 -1.3129572e+00]\n",
      " ...\n",
      " [-6.8007469e-01  1.0000000e+00  1.7842026e+00 ...  1.0537397e+02\n",
      "   3.2502551e+02  8.0027661e+02]\n",
      " [-6.8007660e-01  1.0000000e+00  1.7842960e+00 ...  1.7852262e+01\n",
      "  -5.7487667e+01 -2.6341257e+02]\n",
      " [-6.8009090e-01  1.0000000e+00  1.7843189e+00 ...  1.5112429e+00\n",
      "  -1.1456363e+00  3.4617729e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1981888e+00  1.0000000e+00  1.4868164e+00 ... -5.5858546e-01\n",
      "   1.2040982e+00 -2.1301210e-02]\n",
      " [-1.1981850e+00  1.0000000e+00  1.4866943e+00 ... -1.0595407e+02\n",
      "  -9.2188652e+01 -2.1961772e+02]\n",
      " [-1.1981430e+00  1.0000000e+00  1.4867566e+00 ...  1.3410839e+00\n",
      "   3.4487662e+00 -3.2851732e-01]\n",
      " ...\n",
      " [-1.1981316e+00  1.0000000e+00  1.4867191e+00 ... -1.8359642e+00\n",
      "   4.6659580e+01 -4.2161186e+01]\n",
      " [-1.1981316e+00  1.0000000e+00  1.4868374e+00 ...  1.3079491e+01\n",
      "  -4.4419943e+02  2.1020212e+02]\n",
      " [-1.1981602e+00  1.0000000e+00  1.4868603e+00 ... -5.1969528e-01\n",
      "  -1.0080469e-01  9.3899226e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.2 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.59917355e+00  1.00000000e+00  1.04351425e+00 ...  5.76636791e-02\n",
      "   7.17587471e-01  3.04983735e-01]\n",
      " [-1.59916973e+00  1.00000000e+00  1.04334259e+00 ...  3.44706964e+00\n",
      "   9.62937546e+01 -1.39378071e+01]\n",
      " [-1.59914589e+00  1.00000000e+00  1.04342067e+00 ...  6.28856373e+00\n",
      "   1.28668985e+01  2.37935662e+00]\n",
      " ...\n",
      " [-1.59913826e+00  1.00000000e+00  1.04337597e+00 ... -9.26168518e+02\n",
      "   2.65223364e+03  2.25405591e+03]\n",
      " [-1.59913445e+00  1.00000000e+00  1.04354286e+00 ... -3.20101715e+02\n",
      "   1.64517731e+02 -1.90484482e+02]\n",
      " [-1.59915447e+00  1.00000000e+00  1.04356956e+00 ... -3.34269118e+00\n",
      "  -1.06523156e-01  7.03676164e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.3       0.        0.        0.        0.5\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.84330750e+00  1.00000000e+00  4.98373032e-01 ...  4.23879147e-01\n",
      "   4.60988045e-01  3.06714177e-01]\n",
      " [-1.84329510e+00  1.00000000e+00  4.98216629e-01 ... -1.27424500e+03\n",
      "   1.75440344e+03  1.17713770e+03]\n",
      " [-1.84328651e+00  1.00000000e+00  4.98307019e-01 ...  3.41976929e+00\n",
      "   1.01783905e+01  1.40965772e+00]\n",
      " ...\n",
      " [-1.84328079e+00  1.00000000e+00  4.98259544e-01 ...  2.33497299e+02\n",
      "   2.50714401e+02 -2.15147995e+02]\n",
      " [-1.84328651e+00  1.00000000e+00  4.98405457e-01 ... -1.83159317e+02\n",
      "   2.08512440e+01  4.98874016e+01]\n",
      " [-1.84330273e+00  1.00000000e+00  4.98432159e-01 ...  1.19809604e+00\n",
      "   1.47070861e+00  3.40142757e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.5       0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.8000001]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90711975e+00  1.00000000e+00 -9.57336426e-02 ...  1.03798699e+00\n",
      "  -1.47442818e+00 -2.55310237e-02]\n",
      " [-1.90709305e+00  1.00000000e+00 -9.58786011e-02 ... -1.92349866e+03\n",
      "  -1.28637341e+03 -1.94463516e+02]\n",
      " [-1.90708923e+00  1.00000000e+00 -9.57944766e-02 ... -3.25425959e+00\n",
      "  -1.39743023e+01  1.65698612e+00]\n",
      " ...\n",
      " [-1.90708351e+00  1.00000000e+00 -9.58347321e-02 ... -5.98266785e+02\n",
      "   1.14839539e+03 -1.78438403e+03]\n",
      " [-1.90710258e+00  1.00000000e+00 -9.56993103e-02 ... -1.45584824e+02\n",
      "   1.72207123e+02  5.44131851e+01]\n",
      " [-1.90712643e+00  1.00000000e+00 -9.56726074e-02 ...  1.50557923e+00\n",
      "   1.57176352e+00  1.13346815e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 1.0000001 0.        0.        1.0000001\n",
      " 0.2       0.        0.        0.        0.9000001 0.9000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.78413963e+00  1.00000000e+00 -6.80557251e-01 ...  5.02498674e+00\n",
      "  -7.44711876e+00  3.21548909e-01]\n",
      " [-1.78410149e+00  1.00000000e+00 -6.80651665e-01 ... -1.01832623e+03\n",
      "  -9.83619812e+02 -3.52859741e+02]\n",
      " [-1.78415298e+00  1.00000000e+00 -6.80582523e-01 ... -1.05633545e+00\n",
      "  -5.61941004e+00  2.15653658e-01]\n",
      " ...\n",
      " [-1.78414345e+00  1.00000000e+00 -6.80609703e-01 ...  4.23596777e+03\n",
      "  -4.41629199e+03  3.57837500e+03]\n",
      " [-1.78417397e+00  1.00000000e+00 -6.80524826e-01 ...  1.52593353e+02\n",
      "   7.92581024e+01 -1.18323259e+01]\n",
      " [-1.78416157e+00  1.00000000e+00 -6.80498123e-01 ... -2.97359657e+00\n",
      "  -1.74822378e+00  2.03097534e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.8000001 1.0000001 0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        1.0000001 0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4865036e+00  1.0000000e+00 -1.1986408e+00 ...  4.0890570e+00\n",
      "  -6.3411989e+00 -5.7274288e-01]\n",
      " [-1.4864397e+00  1.0000000e+00 -1.1987524e+00 ...  5.1277428e+01\n",
      "   1.0741489e+02  1.9171532e+02]\n",
      " [-1.4865055e+00  1.0000000e+00 -1.1986789e+00 ... -3.3894312e-01\n",
      "  -8.4447689e+00  1.1363459e+00]\n",
      " ...\n",
      " [-1.4864902e+00  1.0000000e+00 -1.1987162e+00 ...  2.9474149e+02\n",
      "  -7.9582574e+02  3.0720608e+01]\n",
      " [-1.4865341e+00  1.0000000e+00 -1.1986179e+00 ... -2.7103529e+01\n",
      "   3.3095825e+02  2.8623129e+02]\n",
      " [-1.4865379e+00  1.0000000e+00 -1.1985950e+00 ...  1.0829931e+00\n",
      "   1.4212041e+00 -1.1407857e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0431499e+00  1.0000000e+00 -1.5994835e+00 ...  3.8196216e+00\n",
      "  -8.6552677e+00 -1.2440027e+00]\n",
      " [-1.0430651e+00  1.0000000e+00 -1.5995922e+00 ...  1.3992819e+02\n",
      "  -8.9609940e+01 -1.3574683e+02]\n",
      " [-1.0431557e+00  1.0000000e+00 -1.5995280e+00 ... -6.2453085e-01\n",
      "  -4.8199658e+00  6.5457219e-01]\n",
      " ...\n",
      " [-1.0431366e+00  1.0000000e+00 -1.5995617e+00 ... -1.9736268e+03\n",
      "  -3.7846658e+02  3.4772070e+02]\n",
      " [-1.0431919e+00  1.0000000e+00 -1.5994701e+00 ...  3.9927372e+01\n",
      "  -9.1335876e+01  9.1043823e+01]\n",
      " [-1.0431948e+00  1.0000000e+00 -1.5994473e+00 ... -5.6257162e+00\n",
      "  -1.0909626e+01  5.8887327e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.5       0.        0.        0.3\n",
      " 0.8000001 0.        0.        0.        0.4       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.97546196e-01  1.00000000e+00 -1.84363747e+00 ... -2.29811215e+00\n",
      "   4.51472950e+00 -2.10070539e+00]\n",
      " [-4.97441292e-01  1.00000000e+00 -1.84372044e+00 ...  1.97649811e+02\n",
      "   1.13326935e+02 -1.82890228e+02]\n",
      " [-4.97552872e-01  1.00000000e+00 -1.84366691e+00 ...  1.02842772e+00\n",
      "   1.07969904e+00  5.64895868e-02]\n",
      " ...\n",
      " [-4.97531891e-01  1.00000000e+00 -1.84370232e+00 ... -4.90510864e+02\n",
      "   2.25192276e+02 -8.51683197e+01]\n",
      " [-4.97600555e-01  1.00000000e+00 -1.84363174e+00 ... -1.02431076e+02\n",
      "  -1.22423416e+02  5.71576233e+01]\n",
      " [-4.97598648e-01  1.00000000e+00 -1.84361267e+00 ...  2.86764002e+00\n",
      "   7.59577870e-01 -2.90254974e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.6622467e-02  1.0000000e+00 -1.9071064e+00 ...  1.4726095e+00\n",
      "   9.0211391e-02 -2.1416843e-01]\n",
      " [ 9.6729279e-02  1.0000000e+00 -1.9072180e+00 ...  1.7201927e+02\n",
      "  -4.0262277e+02 -2.2188721e+02]\n",
      " [ 9.6622467e-02  1.0000000e+00 -1.9071695e+00 ... -7.7513623e-01\n",
      "  -8.8686562e-01  3.1323621e-01]\n",
      " ...\n",
      " [ 9.6645355e-02  1.0000000e+00 -1.9072266e+00 ...  2.6406559e+01\n",
      "   6.1866676e+01  2.5368261e-01]\n",
      " [ 9.6572876e-02  1.0000000e+00 -1.9071255e+00 ...  5.9009937e+01\n",
      "  -6.7943504e+01  3.1373205e+00]\n",
      " [ 9.6570015e-02  1.0000000e+00 -1.9071159e+00 ...  5.4010835e+00\n",
      "   3.8893385e+00 -1.8819943e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.8118668e-01  1.0000000e+00 -1.7839603e+00 ...  1.0000548e+00\n",
      "   2.1879621e+00  1.5489179e-01]\n",
      " [ 6.8128681e-01  1.0000000e+00 -1.7840261e+00 ...  5.1835825e+03\n",
      "   2.5214045e+03 -5.0080620e+03]\n",
      " [ 6.8112183e-01  1.0000000e+00 -1.7839980e+00 ... -7.3572755e-02\n",
      "   7.7517509e-02  7.9344511e-03]\n",
      " ...\n",
      " [ 6.8114471e-01  1.0000000e+00 -1.7840528e+00 ... -5.0218567e+01\n",
      "  -1.2323956e+01 -5.5095525e+00]\n",
      " [ 6.8107414e-01  1.0000000e+00 -1.7839947e+00 ...  9.7719788e+01\n",
      "   1.2012137e+02 -2.3627483e+01]\n",
      " [ 6.8113708e-01  1.0000000e+00 -1.7839851e+00 ...  1.0305806e+01\n",
      "   5.4411459e+00 -7.7907491e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1992292e+00  1.0000000e+00 -1.4860077e+00 ...  3.4417051e-01\n",
      "   1.7600894e+00 -5.2407622e-02]\n",
      " [ 1.1993122e+00  1.0000000e+00 -1.4860754e+00 ...  3.9893035e+03\n",
      "   6.9036670e+02 -1.2200508e+03]\n",
      " [ 1.1992092e+00  1.0000000e+00 -1.4860694e+00 ... -2.1982622e-01\n",
      "   6.3870635e+00  4.4085008e-01]\n",
      " ...\n",
      " [ 1.1992302e+00  1.0000000e+00 -1.4861336e+00 ... -6.6060467e+00\n",
      "   1.3821996e+02  8.4862566e+00]\n",
      " [ 1.1991711e+00  1.0000000e+00 -1.4860592e+00 ... -6.9889359e+00\n",
      "   9.1059509e+02  4.1162476e+01]\n",
      " [ 1.1991844e+00  1.0000000e+00 -1.4860497e+00 ... -2.2862804e+00\n",
      "  -1.7806059e+00 -4.8045614e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5997324e+00  1.0000000e+00 -1.0427227e+00 ... -3.2567942e-01\n",
      "  -1.6445508e+00 -7.3430061e-02]\n",
      " [ 1.5998001e+00  1.0000000e+00 -1.0427761e+00 ...  4.6632362e+01\n",
      "  -4.0790891e+02 -1.6680150e+03]\n",
      " [ 1.5997105e+00  1.0000000e+00 -1.0428065e+00 ...  3.3572242e+00\n",
      "  -1.7777592e+01 -1.2667613e+00]\n",
      " ...\n",
      " [ 1.5997295e+00  1.0000000e+00 -1.0428534e+00 ... -1.8885963e+02\n",
      "   4.2200607e+02  4.9325211e+02]\n",
      " [ 1.5996857e+00  1.0000000e+00 -1.0427876e+00 ... -2.8057892e+01\n",
      "  -4.6398132e+02 -4.4053842e+02]\n",
      " [ 1.5997009e+00  1.0000000e+00 -1.0427780e+00 ...  1.4744782e+00\n",
      "   2.3416996e+00  7.1680319e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.2        0.         0.\n",
      " 0.5        0.         0.         0.         0.         0.70000005\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8437157e+00  1.0000000e+00 -4.9713898e-01 ... -1.1254833e+00\n",
      "  -1.5805069e+01 -1.2892721e+00]\n",
      " [ 1.8437653e+00  1.0000000e+00 -4.9716377e-01 ...  6.7842674e+01\n",
      "  -2.4054355e+03  1.5767705e+03]\n",
      " [ 1.8437176e+00  1.0000000e+00 -4.9721098e-01 ...  8.1492460e-01\n",
      "  -7.0219293e+00 -4.5060325e-01]\n",
      " ...\n",
      " [ 1.8437386e+00  1.0000000e+00 -4.9725056e-01 ... -7.1160408e+01\n",
      "  -3.1293066e+02  1.0541301e+02]\n",
      " [ 1.8437042e+00  1.0000000e+00 -4.9720955e-01 ...  5.3746773e+01\n",
      "   1.3576034e+01  1.8370455e+01]\n",
      " [ 1.8437052e+00  1.0000000e+00 -4.9720001e-01 ...  1.9059775e+00\n",
      "   3.5116487e+00  1.3185111e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         1.0000001  0.70000005 0.\n",
      " 1.0000001  0.         0.         0.         0.         1.0000001\n",
      " 0.         0.         1.0000001  0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9071312e+00  1.0000000e+00  9.7066879e-02 ... -1.2355089e-01\n",
      "  -1.3441576e+01  6.8949473e-01]\n",
      " [ 1.9071531e+00  1.0000000e+00  9.7031593e-02 ...  6.2119350e+01\n",
      "  -4.7445325e+02  4.1151016e+02]\n",
      " [ 1.9071121e+00  1.0000000e+00  9.6994154e-02 ...  2.4224520e-02\n",
      "  -6.8200874e+00 -1.4025517e+00]\n",
      " ...\n",
      " [ 1.9071350e+00  1.0000000e+00  9.6941948e-02 ...  1.5012900e+02\n",
      "  -1.7123688e+02 -3.2655487e+00]\n",
      " [ 1.9071102e+00  1.0000000e+00  9.6992493e-02 ... -2.9027783e+02\n",
      "  -6.0113403e+01  1.9555060e+02]\n",
      " [ 1.9071360e+00  1.0000000e+00  9.7002029e-02 ...  4.6506262e+00\n",
      "   9.1391315e+00 -5.7421041e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.7837687     1.            0.68182755 ...    1.0353746\n",
      "     0.5465169    -0.54199016]\n",
      " [   1.7837772     1.            0.68179893 ... -184.96504\n",
      "   104.598206   -353.31796   ]\n",
      " [   1.783762      1.            0.68176526 ...    0.8386184\n",
      "     0.9711628    -0.43206882]\n",
      " ...\n",
      " [   1.7837906     1.            0.68171597 ...  330.91754\n",
      "   299.66147     107.61975   ]\n",
      " [   1.7837677     1.            0.68175507 ... -310.02423\n",
      "    42.911304     22.175667  ]\n",
      " [   1.7837906     1.            0.6817646  ...   -0.88055444\n",
      "    -5.2032022    -0.842315  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.48562145e+00  1.00000000e+00  1.19957924e+00 ...  9.55074906e-01\n",
      "  -1.52390146e+00  2.44858086e-01]\n",
      " [ 1.48561954e+00  1.00000000e+00  1.19953537e+00 ... -1.72405243e+02\n",
      "  -1.58420830e+01  7.43145447e+01]\n",
      " [ 1.48559761e+00  1.00000000e+00  1.19951665e+00 ... -7.07816243e-01\n",
      "  -1.63236094e+00 -1.21743679e-02]\n",
      " ...\n",
      " [ 1.48563576e+00  1.00000000e+00  1.19946194e+00 ... -9.37465057e+01\n",
      "   3.33467255e+01 -2.42401810e+02]\n",
      " [ 1.48561096e+00  1.00000000e+00  1.19951820e+00 ...  4.37932953e+02\n",
      "  -1.02777565e+02  8.90183029e+01]\n",
      " [ 1.48566723e+00  1.00000000e+00  1.19952583e+00 ...  6.99460149e-01\n",
      "   3.04500484e+00  6.08446062e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 1.0000001 0.        0.1\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0422115e+00  1.0000000e+00  1.5999870e+00 ...  8.3375031e-01\n",
      "  -4.5785356e+00  7.9568696e-01]\n",
      " [ 1.0422077e+00  1.0000000e+00  1.5999393e+00 ... -8.6706253e+01\n",
      "  -3.9957008e+01  5.1441364e+01]\n",
      " [ 1.0422058e+00  1.0000000e+00  1.5999372e+00 ... -1.9661436e+00\n",
      "  -8.2062263e+00 -1.3625994e+00]\n",
      " ...\n",
      " [ 1.0422459e+00  1.0000000e+00  1.5998898e+00 ...  3.6027420e+01\n",
      "   4.4901264e+01  4.9644791e+01]\n",
      " [ 1.0422192e+00  1.0000000e+00  1.5999393e+00 ...  1.0518870e+03\n",
      "   3.1318232e+03 -3.9245139e+03]\n",
      " [ 1.0422678e+00  1.0000000e+00  1.5999508e+00 ...  1.9410741e-01\n",
      "   8.2608681e+00  1.3304243e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.70000005 0.2        0.\n",
      " 0.         0.         0.         0.         0.         0.3\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.9658585e-01  1.0000000e+00  1.8437309e+00 ... -4.3735546e-01\n",
      "   5.4602325e-01  2.1991041e+00]\n",
      " [ 4.9658012e-01  1.0000000e+00  1.8437014e+00 ...  1.0296537e+02\n",
      "   1.3014354e+02  9.7390419e+01]\n",
      " [ 4.9658012e-01  1.0000000e+00  1.8437093e+00 ... -2.8346410e+00\n",
      "  -5.4097166e+00 -7.3472136e-01]\n",
      " ...\n",
      " [ 4.9662590e-01  1.0000000e+00  1.8436737e+00 ...  1.9241435e+02\n",
      "  -1.4049413e+02  4.4460258e+01]\n",
      " [ 4.9659729e-01  1.0000000e+00  1.8437004e+00 ...  7.3937463e+02\n",
      "   1.7032236e+02 -2.5935175e+01]\n",
      " [ 4.9664879e-01  1.0000000e+00  1.8437176e+00 ...  9.2065459e-01\n",
      "  -8.0321693e+00 -7.4566191e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.7634315e-02  1.0000000e+00  1.9069767e+00 ...  3.2375193e-01\n",
      "  -3.0990553e-01  7.8700542e-02]\n",
      " [-9.7640991e-02  1.0000000e+00  1.9069443e+00 ... -6.3805725e+02\n",
      "   7.7687024e+02 -2.3992749e+02]\n",
      " [-9.7629547e-02  1.0000000e+00  1.9069605e+00 ...  1.3675933e+00\n",
      "   8.7776117e+00  2.6859599e-01]\n",
      " ...\n",
      " [-9.7583771e-02  1.0000000e+00  1.9069452e+00 ...  5.1111029e+02\n",
      "   5.1705963e+02  5.4898537e+01]\n",
      " [-9.7612381e-02  1.0000000e+00  1.9069653e+00 ...  6.3048050e+01\n",
      "  -3.9076105e+02  8.0133736e+01]\n",
      " [-9.7567558e-02  1.0000000e+00  1.9069862e+00 ...  8.7108070e-01\n",
      "  -4.2576332e+00  2.7686846e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.8208981e-01  1.0000000e+00  1.7835522e+00 ... -4.2862988e-01\n",
      "  -3.4923506e-01  1.7424232e-01]\n",
      " [-6.8209648e-01  1.0000000e+00  1.7835178e+00 ...  1.6463707e+01\n",
      "  -4.2568218e+01  2.3703138e+01]\n",
      " [-6.8207550e-01  1.0000000e+00  1.7835323e+00 ...  7.5395936e-01\n",
      "   3.2369781e+00 -1.2649018e+00]\n",
      " ...\n",
      " [-6.8203354e-01  1.0000000e+00  1.7835398e+00 ...  5.4912086e+01\n",
      "   1.0803581e+02 -1.5205464e+02]\n",
      " [-6.8206215e-01  1.0000000e+00  1.7835522e+00 ... -4.6604379e+02\n",
      "  -2.8694574e+02  1.8178018e+02]\n",
      " [-6.8202877e-01  1.0000000e+00  1.7835789e+00 ... -8.4459972e-01\n",
      "   5.7565212e-02  1.5688381e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.20008278e+00  1.00000000e+00  1.48534584e+00 ...  5.84471083e+00\n",
      "  -7.91999340e+00 -5.31983137e-01]\n",
      " [-1.20008564e+00  1.00000000e+00  1.48529148e+00 ...  1.21224236e+02\n",
      "  -4.92019157e+01  1.29268463e+02]\n",
      " [-1.20008278e+00  1.00000000e+00  1.48532271e+00 ... -5.97358704e-01\n",
      "  -4.54946470e+00 -2.94109881e-01]\n",
      " ...\n",
      " [-1.20004463e+00  1.00000000e+00  1.48532581e+00 ... -1.74189804e+02\n",
      "  -1.79674664e+01 -3.05546478e+02]\n",
      " [-1.20007133e+00  1.00000000e+00  1.48536110e+00 ...  3.91563263e+01\n",
      "  -2.13332245e+02 -5.63392296e+01]\n",
      " [-1.20003033e+00  1.00000000e+00  1.48539925e+00 ...  1.27611518e+00\n",
      "  -1.15105367e+00  2.58106828e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.60046864e+00  1.00000000e+00  1.04171181e+00 ... -2.11647224e+00\n",
      "   4.45559359e+00  1.08967006e-01]\n",
      " [-1.60046768e+00  1.00000000e+00  1.04164696e+00 ... -2.22065109e+02\n",
      "   1.61537056e+01  1.16738029e+02]\n",
      " [-1.60045433e+00  1.00000000e+00  1.04167771e+00 ... -7.87944913e-01\n",
      "  -5.96548891e+00  5.83514094e-01]\n",
      " ...\n",
      " [-1.60042763e+00  1.00000000e+00  1.04170036e+00 ... -2.35591370e+02\n",
      "   3.38817108e+02 -2.73900085e+02]\n",
      " [-1.60044861e+00  1.00000000e+00  1.04173470e+00 ... -6.87747742e+02\n",
      "   7.17806396e+02  1.49815079e+02]\n",
      " [-1.60043240e+00  1.00000000e+00  1.04178047e+00 ... -3.64779625e+01\n",
      "   7.67110748e+01  7.42406158e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8440704e+00  1.0000000e+00  4.9621964e-01 ... -1.8597033e+00\n",
      "   4.6993084e+00 -5.2156484e-01]\n",
      " [-1.8440638e+00  1.0000000e+00  4.9613762e-01 ...  1.5938309e+02\n",
      "  -1.4993134e+02 -3.5705250e+01]\n",
      " [-1.8440552e+00  1.0000000e+00  4.9616700e-01 ... -6.5334213e-01\n",
      "   7.9428148e+00 -6.2488747e-01]\n",
      " ...\n",
      " [-1.8440342e+00  1.0000000e+00  4.9619579e-01 ... -4.1677966e+02\n",
      "  -2.8294293e+02  1.4976118e+02]\n",
      " [-1.8440552e+00  1.0000000e+00  4.9624825e-01 ...  4.8836142e+02\n",
      "   6.6719348e+02  2.3372102e+02]\n",
      " [-1.8440542e+00  1.0000000e+00  4.9630165e-01 ... -5.7542682e+02\n",
      "  -4.5603287e+02 -1.2910678e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9071541e+00  1.0000000e+00 -9.8245621e-02 ... -3.8446779e+00\n",
      "   1.4348345e+01  2.1423452e+00]\n",
      " [-1.9071417e+00  1.0000000e+00 -9.8304749e-02 ... -1.2507089e+01\n",
      "   3.4894199e+01 -1.4954932e+00]\n",
      " [-1.9071159e+00  1.0000000e+00 -9.8268375e-02 ... -4.2690977e-01\n",
      "   6.0822997e+00  2.0887697e-01]\n",
      " ...\n",
      " [-1.9071121e+00  1.0000000e+00 -9.8241806e-02 ...  3.7905859e+02\n",
      "   1.6515515e+02 -1.8262576e+02]\n",
      " [-1.9071312e+00  1.0000000e+00 -9.8217010e-02 ...  7.0192368e+01\n",
      "   6.5498726e+01  4.5136608e+01]\n",
      " [-1.9071636e+00  1.0000000e+00 -9.8159790e-02 ...  4.3563150e+02\n",
      "  -5.2047003e+02  9.3139343e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7835054e+00  1.0000000e+00 -6.8267059e-01 ...  4.4801551e-01\n",
      "  -8.8697749e-01  1.7521241e-01]\n",
      " [-1.7834854e+00  1.0000000e+00 -6.8270969e-01 ...  3.7515530e+01\n",
      "   2.1642818e+02  1.9236453e+02]\n",
      " [-1.7834454e+00  1.0000000e+00 -6.8268001e-01 ... -8.5119963e-01\n",
      "   7.6785221e+00  1.5408952e-02]\n",
      " ...\n",
      " [-1.7834454e+00  1.0000000e+00 -6.8265247e-01 ... -2.1503366e+02\n",
      "   2.6490887e+01  3.7166058e+02]\n",
      " [-1.7834663e+00  1.0000000e+00 -6.8264580e-01 ...  1.4124473e+00\n",
      "   3.0110205e+02 -2.5063715e+02]\n",
      " [-1.7835321e+00  1.0000000e+00 -6.8259621e-01 ... -2.3610402e+02\n",
      "   4.0993649e+02  4.8244072e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.6       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4852333e+00  1.0000000e+00 -1.2002850e+00 ...  2.0437127e-01\n",
      "  -9.6571505e-01  3.4688354e-02]\n",
      " [-1.4852066e+00  1.0000000e+00 -1.2003250e+00 ... -4.6446060e+02\n",
      "   4.7491068e+02 -1.5767021e+02]\n",
      " [-1.4852104e+00  1.0000000e+00 -1.2002939e+00 ... -1.1682346e+00\n",
      "   6.1719494e+00  1.3091890e+00]\n",
      " ...\n",
      " [-1.4852161e+00  1.0000000e+00 -1.2002726e+00 ...  2.9018253e+02\n",
      "  -3.5915695e+02  2.4741650e+02]\n",
      " [-1.4852428e+00  1.0000000e+00 -1.2002678e+00 ... -3.0300144e+03\n",
      "  -3.5306610e+02  2.4732004e+03]\n",
      " [-1.4852753e+00  1.0000000e+00 -1.2002258e+00 ... -6.0480865e+01\n",
      "   1.7655500e+01  2.6316010e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70000005 0.         0.\n",
      " 1.0000001  0.         1.0000001  0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.04144764e+00  1.00000000e+00 -1.60065079e+00 ... -6.92561388e-01\n",
      "  -1.16357541e+00  1.81882903e-02]\n",
      " [-1.04141235e+00  1.00000000e+00 -1.60067081e+00 ...  5.32398834e+01\n",
      "   1.54840256e+02  4.66976318e+01]\n",
      " [-1.04141045e+00  1.00000000e+00 -1.60065508e+00 ... -1.09349942e+00\n",
      "   3.26933885e+00  6.25731885e-01]\n",
      " ...\n",
      " [-1.04142952e+00  1.00000000e+00 -1.60062981e+00 ... -3.29597504e+02\n",
      "   3.60787323e+02  3.21552429e+01]\n",
      " [-1.04145622e+00  1.00000000e+00 -1.60063934e+00 ... -2.44867859e+01\n",
      "  -9.62932129e+02 -5.63083130e+02]\n",
      " [-1.04150963e+00  1.00000000e+00 -1.60060501e+00 ... -3.25633202e+01\n",
      "  -6.12454796e+01 -1.09259056e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.8000001 0.        0.        0.3       0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.9557590e-01  1.0000000e+00 -1.8441982e+00 ...  2.3038363e-01\n",
      "   2.6957029e-01  2.3445657e-01]\n",
      " [-4.9553776e-01  1.0000000e+00 -1.8442059e+00 ...  2.5333237e+01\n",
      "  -3.9147671e+01  5.4637440e+01]\n",
      " [-4.9552536e-01  1.0000000e+00 -1.8442034e+00 ... -5.0064497e+00\n",
      "   7.3126359e+00  1.9465390e+00]\n",
      " ...\n",
      " [-4.9554825e-01  1.0000000e+00 -1.8441734e+00 ...  3.2930084e+01\n",
      "   5.6833740e+01 -8.0668777e+01]\n",
      " [-4.9557495e-01  1.0000000e+00 -1.8441925e+00 ... -1.0220643e+02\n",
      "   1.7350733e+01 -2.7924933e+02]\n",
      " [-4.9564171e-01  1.0000000e+00 -1.8441772e+00 ...  6.3153740e+01\n",
      "  -6.7116211e+01 -2.6610098e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.8485947e-02  1.0000000e+00 -1.9070129e+00 ...  2.8726718e+00\n",
      "  -7.4448738e+00 -4.5046255e-01]\n",
      " [ 9.8526001e-02  1.0000000e+00 -1.9069920e+00 ... -2.5799280e+02\n",
      "   8.2889229e+01  1.9015720e+02]\n",
      " [ 9.8548889e-02  1.0000000e+00 -1.9069908e+00 ... -2.6901937e+00\n",
      "   4.6182413e+00  5.6450599e-01]\n",
      " ...\n",
      " [ 9.8526001e-02  1.0000000e+00 -1.9069805e+00 ...  1.3400792e+02\n",
      "   2.5758578e+02 -3.9219687e+02]\n",
      " [ 9.8499298e-02  1.0000000e+00 -1.9070110e+00 ... -1.5666566e+02\n",
      "  -2.7577639e+01 -1.6642360e+02]\n",
      " [ 9.8416328e-02  1.0000000e+00 -1.9070072e+00 ... -2.6162844e+00\n",
      "  -1.3617056e+02  2.2816282e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.6829605     1.           -1.7832413  ...    1.8290168\n",
      "    -8.095422     -0.4584146 ]\n",
      " [   0.68300056    1.           -1.7831869  ...   36.38203\n",
      "    12.818203    -75.67765   ]\n",
      " [   0.6830063     1.           -1.7832001  ...    4.813224\n",
      "    -8.245977     -1.6831396 ]\n",
      " ...\n",
      " [   0.6829853     1.           -1.7831831  ...  432.47263\n",
      "  -212.6498       98.49046   ]\n",
      " [   0.6829586     1.           -1.7832527  ...   -4.296647\n",
      "   -11.836271    392.2073    ]\n",
      " [   0.68289566    1.           -1.7832623  ...   -7.551817\n",
      "   -47.02177     -38.50207   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.20092392e+00  1.00000000e+00 -1.48465157e+00 ...  2.57555413e+00\n",
      "  -8.92050076e+00 -1.40314698e+00]\n",
      " [ 1.20095634e+00  1.00000000e+00 -1.48457241e+00 ...  1.02642143e+02\n",
      "   9.18404007e+01  1.03133476e+02]\n",
      " [ 1.20096397e+00  1.00000000e+00 -1.48459303e+00 ...  2.51821065e+00\n",
      "  -4.19602871e+00  1.64486885e-01]\n",
      " ...\n",
      " [ 1.20095253e+00  1.00000000e+00 -1.48457623e+00 ...  3.73626892e+02\n",
      "   3.80676270e+00  4.28944275e+02]\n",
      " [ 1.20092773e+00  1.00000000e+00 -1.48466873e+00 ...  7.33823792e+02\n",
      "   5.31689392e+02  4.91287292e+02]\n",
      " [ 1.20087337e+00  1.00000000e+00 -1.48469543e+00 ... -4.24827919e+01\n",
      "   1.65984940e+02  1.19982056e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6008654e+00  1.0000000e+00 -1.0410213e+00 ...  1.8482358e+00\n",
      "   7.6879930e-01  9.2633200e-01]\n",
      " [ 1.6008854e+00  1.0000000e+00 -1.0409069e+00 ... -8.8754285e+02\n",
      "  -1.5599847e+03  5.3014441e+02]\n",
      " [ 1.6009045e+00  1.0000000e+00 -1.0409398e+00 ...  4.2286963e+00\n",
      "  -9.0176277e+00 -1.6057432e+00]\n",
      " ...\n",
      " [ 1.6009007e+00  1.0000000e+00 -1.0409145e+00 ... -3.8449286e+02\n",
      "   3.8567639e+02  2.8009216e+02]\n",
      " [ 1.6008854e+00  1.0000000e+00 -1.0410461e+00 ... -3.6375839e+01\n",
      "   4.5553880e+00  1.2832994e+01]\n",
      " [ 1.6008310e+00  1.0000000e+00 -1.0410843e+00 ...  1.3532323e+02\n",
      "  -5.1878501e+03  7.3424774e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.9000001  0.         0.70000005 0.         1.0000001  0.\n",
      " 0.         0.         0.         0.         0.         0.5\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84415627e+00  1.00000000e+00 -4.95260239e-01 ...  5.45712650e-01\n",
      "   7.09680080e-01 -1.00497293e+00]\n",
      " [ 1.84416676e+00  1.00000000e+00 -4.95137215e-01 ... -2.07825089e+02\n",
      "   8.33113785e+01  2.51642120e+02]\n",
      " [ 1.84417725e+00  1.00000000e+00 -4.95172203e-01 ...  2.10540724e+00\n",
      "  -2.88230658e+00 -7.53389776e-01]\n",
      " ...\n",
      " [ 1.84418106e+00  1.00000000e+00 -4.95145798e-01 ...  2.08972443e+02\n",
      "   1.42939463e+01  1.18298893e+01]\n",
      " [ 1.84417915e+00  1.00000000e+00 -4.95285034e-01 ...  1.17757744e+02\n",
      "   1.56084778e+02 -7.18243164e+02]\n",
      " [ 1.84414387e+00  1.00000000e+00 -4.95332718e-01 ...  7.84408035e+01\n",
      "  -1.28079346e+02  8.73016052e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         1.0000001  0.         1.0000001  0.70000005\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9000001  0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:3, Score:10.32, Best Score:15.21, Average Score:11.46, Best Avg Score:12.04\n",
      "Episode number: 4\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7f2d81ed96a0>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7830267e+00  1.0000000e+00  6.8345070e-01 ...  1.5716436e+00\n",
      "   6.1246729e+00 -4.5047295e-01]\n",
      " [ 1.7829866e+00  1.0000000e+00  6.8359280e-01 ... -5.6355240e+01\n",
      "  -1.2255022e+02 -6.1842126e+02]\n",
      " [ 1.7829647e+00  1.0000000e+00  6.8352979e-01 ...  8.7762648e-01\n",
      "  -2.8150737e+00 -3.8983345e-01]\n",
      " ...\n",
      " [ 1.7829742e+00  1.0000000e+00  6.8358421e-01 ...  1.4162189e+02\n",
      "   1.7993023e+01 -3.3445209e+02]\n",
      " [ 1.7830124e+00  1.0000000e+00  6.8342590e-01 ...  8.9759850e+01\n",
      "   5.3629128e+01  1.9372078e+01]\n",
      " [ 1.7830667e+00  1.0000000e+00  6.8337631e-01 ... -1.9684914e+01\n",
      "  -1.8391159e+02  2.6335187e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 1.0000001 0.\n",
      " 0.        0.        1.0000001 0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.48441887e+00  1.00000000e+00  1.20121574e+00 ...  3.71639609e+00\n",
      "   6.16876078e+00 -9.42211211e-01]\n",
      " [ 1.48435211e+00  1.00000000e+00  1.20134735e+00 ...  6.62666245e+01\n",
      "  -2.84598236e+02  3.57237053e+01]\n",
      " [ 1.48432732e+00  1.00000000e+00  1.20129097e+00 ... -7.51241386e-01\n",
      "   1.12399065e+00 -4.64681566e-01]\n",
      " ...\n",
      " [ 1.48433495e+00  1.00000000e+00  1.20134068e+00 ...  1.84658279e+02\n",
      "   4.07159515e+02  9.61727966e+02]\n",
      " [ 1.48440361e+00  1.00000000e+00  1.20119095e+00 ... -1.10479256e+02\n",
      "  -1.59112885e+02  1.05012650e+02]\n",
      " [ 1.48447704e+00  1.00000000e+00  1.20115471e+00 ... -5.99154968e+01\n",
      "   7.96252289e+01 -1.94568558e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.4       0.\n",
      " 0.        0.        0.1       0.        0.        0.5       0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0403900e+00  1.0000000e+00  1.6011353e+00 ...  3.9885044e-01\n",
      "   1.9532795e+00 -1.6017385e+00]\n",
      " [ 1.0403070e+00  1.0000000e+00  1.6012392e+00 ...  3.2901694e+02\n",
      "   3.7437192e+02 -2.5568750e+02]\n",
      " [ 1.0402737e+00  1.0000000e+00  1.6011989e+00 ... -7.8451556e-01\n",
      "   8.6978359e+00 -1.0779114e+00]\n",
      " ...\n",
      " [ 1.0402813e+00  1.0000000e+00  1.6012354e+00 ... -1.9545148e+02\n",
      "   1.3480186e+02  6.0329781e+01]\n",
      " [ 1.0403652e+00  1.0000000e+00  1.6011124e+00 ...  1.3587608e+01\n",
      "  -1.0918137e+01  3.0353668e+01]\n",
      " [ 1.0404663e+00  1.0000000e+00  1.6010914e+00 ... -5.2906195e+02\n",
      "   6.5717224e+02  6.0107458e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.49455643    1.            1.844284   ...   -5.79658\n",
      "    -8.268676      0.4761843 ]\n",
      " [   0.49445438    1.            1.8443737  ...   -5.720873\n",
      "    11.666675      4.1233473 ]\n",
      " [   0.4944191     1.            1.8443335  ...   -0.33134305\n",
      "   -13.348446     -1.3783326 ]\n",
      " ...\n",
      " [   0.49442673    1.            1.8443737  ... -110.4214\n",
      "     1.8968284   -44.344948  ]\n",
      " [   0.494524      1.            1.8442631  ...  -65.52685\n",
      "    77.43236      39.332947  ]\n",
      " [   0.49464035    1.            1.8442612  ...  134.66255\n",
      "    34.077934   -114.32805   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.9518776e-02  1.0000000e+00  1.9068851e+00 ... -4.1260629e+00\n",
      "  -4.4056191e+00  4.4385999e-01]\n",
      " [-9.9621773e-02  1.0000000e+00  1.9069309e+00 ...  3.3353409e+02\n",
      "  -2.1802628e+02 -4.0457043e+02]\n",
      " [-9.9641800e-02  1.0000000e+00  1.9068948e+00 ...  1.2138038e+00\n",
      "  -2.8624597e+00  7.3206520e-01]\n",
      " ...\n",
      " [-9.9634171e-02  1.0000000e+00  1.9069309e+00 ...  8.4658951e+01\n",
      "  -3.7423210e+01 -4.8914734e+01]\n",
      " [-9.9536896e-02  1.0000000e+00  1.9068661e+00 ... -1.5070692e+02\n",
      "  -3.5584207e+00 -1.5103734e+02]\n",
      " [-9.9431992e-02  1.0000000e+00  1.9068756e+00 ...  3.3029645e+02\n",
      "  -3.4997656e+02 -1.0689797e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.84284210e-01  1.00000000e+00  1.78268623e+00 ...  1.28738880e+00\n",
      "   7.83988476e-01 -6.86345100e-02]\n",
      " [-6.84380531e-01  1.00000000e+00  1.78271770e+00 ... -1.62662582e+02\n",
      "   4.63393097e+01 -2.14395477e+02]\n",
      " [-6.84373856e-01  1.00000000e+00  1.78266251e+00 ...  1.79224873e+00\n",
      "  -1.34506035e+00 -4.02062833e-01]\n",
      " ...\n",
      " [-6.84366226e-01  1.00000000e+00  1.78271675e+00 ...  7.66012421e+01\n",
      "  -2.05639786e+02  2.34613525e+02]\n",
      " [-6.84272766e-01  1.00000000e+00  1.78266525e+00 ...  2.14949646e+02\n",
      "  -3.82927251e+00 -8.22764778e+00]\n",
      " [-6.84206963e-01  1.00000000e+00  1.78269386e+00 ...  1.11479355e+02\n",
      "  -1.42308151e+02 -2.10020142e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.2017212e+00  1.0000000e+00  1.4839687e+00 ... -7.3514509e-01\n",
      "  -1.2958479e-01  1.3346121e-01]\n",
      " [-1.2017984e+00  1.0000000e+00  1.4839783e+00 ...  5.1152530e+02\n",
      "   2.3674348e+02 -2.3511837e+01]\n",
      " [-1.2017574e+00  1.0000000e+00  1.4839339e+00 ...  9.7174597e-01\n",
      "  -2.7771330e-01  1.1228299e+00]\n",
      " ...\n",
      " [-1.2017498e+00  1.0000000e+00  1.4839745e+00 ...  2.6733658e+01\n",
      "   3.5667908e+01  8.1918579e+01]\n",
      " [-1.2016678e+00  1.0000000e+00  1.4839478e+00 ...  2.7687430e+02\n",
      "  -5.2308338e+01 -5.0843925e+01]\n",
      " [-1.2016506e+00  1.0000000e+00  1.4839878e+00 ...  2.4138256e+01\n",
      "  -1.3862256e+02  2.9564359e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6014404e+00  1.0000000e+00  1.0401649e+00 ... -1.9143486e-01\n",
      "  -4.5928001e-02  4.8029482e-02]\n",
      " [-1.6014986e+00  1.0000000e+00  1.0401201e+00 ... -1.3939429e+01\n",
      "   7.9331711e+01  5.0949173e+01]\n",
      " [-1.6014614e+00  1.0000000e+00  1.0400782e+00 ...  1.1491795e+00\n",
      "  -1.5296681e+00  2.8775430e-01]\n",
      " ...\n",
      " [-1.6014538e+00  1.0000000e+00  1.0401163e+00 ...  2.8512811e+02\n",
      "  -6.2719984e+00 -3.3943198e+02]\n",
      " [-1.6013908e+00  1.0000000e+00  1.0401459e+00 ...  1.7371352e+02\n",
      "  -1.0081682e+02 -5.4231091e+01]\n",
      " [-1.6013842e+00  1.0000000e+00  1.0402012e+00 ...  2.8065811e+02\n",
      "   4.2425052e+02  4.6062729e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.9000001 0.        0.        0.        0.        0.6       0.\n",
      " 0.        0.        0.8000001 0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8446159e+00  1.0000000e+00  4.9404144e-01 ... -3.5281229e-01\n",
      "  -9.4795227e-01  2.4417639e-02]\n",
      " [-1.8446503e+00  1.0000000e+00  4.9399185e-01 ...  7.2748604e+00\n",
      "  -3.9820528e+00  1.0452974e+01]\n",
      " [-1.8446140e+00  1.0000000e+00  4.9394560e-01 ...  1.0685539e+00\n",
      "  -3.1592746e+00  2.4355274e-01]\n",
      " ...\n",
      " [-1.8446121e+00  1.0000000e+00  4.9398804e-01 ...  9.9804550e+01\n",
      "   1.7293492e+02  4.4056950e+00]\n",
      " [-1.8445721e+00  1.0000000e+00  4.9402237e-01 ... -1.8242662e+01\n",
      "  -8.4799301e+01  3.6776581e+01]\n",
      " [-1.8445778e+00  1.0000000e+00  4.9408531e-01 ...  1.6783690e+02\n",
      "   7.2765483e+03  5.1681816e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90705490e+00  1.00000000e+00 -1.00004196e-01 ...  2.50240612e+00\n",
      "   3.14715505e+00 -1.03617556e-01]\n",
      " [-1.90704346e+00  1.00000000e+00 -9.99965668e-02 ... -3.68006587e+00\n",
      "   6.52868271e-01 -3.71043658e+00]\n",
      " [-1.90699577e+00  1.00000000e+00 -1.00062251e-01 ...  5.50962973e+00\n",
      "  -1.81999779e+00  1.00904799e+00]\n",
      " ...\n",
      " [-1.90700531e+00  1.00000000e+00 -1.00000381e-01 ...  7.48908691e+01\n",
      "   1.05095673e+02  3.31843300e+01]\n",
      " [-1.90698433e+00  1.00000000e+00 -1.00021362e-01 ... -2.48578156e+02\n",
      "   1.36805511e+02  3.59174377e+02]\n",
      " [-1.90703487e+00  1.00000000e+00 -9.99565125e-02 ...  1.91515976e+02\n",
      "   5.35026611e+02 -2.60074768e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.6       0.        0.        0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7827072e+00  1.0000000e+00 -6.8477249e-01 ...  3.6293790e+00\n",
      "   5.5735121e+00  1.3354769e+00]\n",
      " [-1.7826757e+00  1.0000000e+00 -6.8473244e-01 ...  2.1028225e+00\n",
      "  -3.6824741e+00  6.2809353e+00]\n",
      " [-1.7826405e+00  1.0000000e+00 -6.8480670e-01 ...  2.2130327e+00\n",
      "  -6.2091780e-01  7.1548200e-01]\n",
      " ...\n",
      " [-1.7826614e+00  1.0000000e+00 -6.8473625e-01 ... -1.6235550e+02\n",
      "  -1.2603799e+02  6.4016159e+01]\n",
      " [-1.7826557e+00  1.0000000e+00 -6.8478966e-01 ...  5.6685219e+01\n",
      "  -1.0216026e+01 -6.0307434e+01]\n",
      " [-1.7827044e+00  1.0000000e+00 -6.8473053e-01 ... -2.3240254e+02\n",
      "  -1.9051807e+02  6.8271973e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001  0.         0.         0.         0.         0.70000005\n",
      " 0.         0.         0.         0.         0.         0.6\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4839973e+00  1.0000000e+00 -1.2019253e+00 ...  1.9765573e+00\n",
      "   4.8856478e+00  9.3526441e-01]\n",
      " [-1.4839449e+00  1.0000000e+00 -1.2019148e+00 ...  4.0112257e-02\n",
      "   1.6702557e+00  9.4196022e-01]\n",
      " [-1.4839535e+00  1.0000000e+00 -1.2019817e+00 ...  2.6409078e+00\n",
      "   4.6052647e-01  1.7936420e-01]\n",
      " ...\n",
      " [-1.4839840e+00  1.0000000e+00 -1.2019186e+00 ... -1.8782055e+01\n",
      "  -2.0266507e+02 -3.5044350e+01]\n",
      " [-1.4839878e+00  1.0000000e+00 -1.2019424e+00 ...  5.0094555e+01\n",
      "   2.1131639e+01 -5.4167507e+01]\n",
      " [-1.4840069e+00  1.0000000e+00 -1.2018929e+00 ...  2.1272243e+01\n",
      "   6.9170929e+02 -5.5788666e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0399151e+00  1.0000000e+00 -1.6017132e+00 ...  1.5534728e+01\n",
      "  -1.4209915e+02 -7.6398439e+00]\n",
      " [-1.0398502e+00  1.0000000e+00 -1.6016607e+00 ...  2.4110637e+00\n",
      "   1.3857669e+00  1.6701719e+00]\n",
      " [-1.0398769e+00  1.0000000e+00 -1.6017293e+00 ...  7.4382648e+00\n",
      "  -9.3909222e-01  2.1487299e-01]\n",
      " ...\n",
      " [-1.0399132e+00  1.0000000e+00 -1.6016674e+00 ... -6.0067169e+02\n",
      "  -4.0537000e+02 -9.6258781e+01]\n",
      " [-1.0399284e+00  1.0000000e+00 -1.6017284e+00 ... -2.4790413e+02\n",
      "  -1.0200865e+02 -6.5010193e+01]\n",
      " [-1.0399361e+00  1.0000000e+00 -1.6016865e+00 ... -6.1768227e+01\n",
      "   5.9803192e+01  6.0294724e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4       0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.8000001 0.        0.\n",
      " 0.        0.1       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.9390697e-01  1.0000000e+00 -1.8446846e+00 ... -1.9580482e+02\n",
      "  -1.0314009e+01  1.5983551e+02]\n",
      " [-4.9384022e-01  1.0000000e+00 -1.8446388e+00 ...  2.8264174e+00\n",
      "   1.3341351e+00 -4.8349535e-01]\n",
      " [-4.9385262e-01  1.0000000e+00 -1.8446920e+00 ...  1.6192787e+01\n",
      "  -2.1277740e+00 -1.0232455e+00]\n",
      " ...\n",
      " [-4.9389076e-01  1.0000000e+00 -1.8446474e+00 ...  4.3385826e+01\n",
      "   1.4328684e+01  2.1528615e+02]\n",
      " [-4.9390793e-01  1.0000000e+00 -1.8446941e+00 ...  8.2260658e+01\n",
      "   3.9084633e+01 -4.9037521e+02]\n",
      " [-4.9393654e-01  1.0000000e+00 -1.8446674e+00 ...  2.4960890e+01\n",
      "  -1.6402008e+02  1.6180016e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.00684166e-01  1.00000000e+00 -1.90699959e+00 ...  8.11605377e+01\n",
      "   5.21941109e+01  1.70921188e+02]\n",
      " [ 1.00751877e-01  1.00000000e+00 -1.90689182e+00 ...  3.10012817e-01\n",
      "   1.10442889e+00  8.06522846e-01]\n",
      " [ 1.00734711e-01  1.00000000e+00 -1.90696406e+00 ...  4.77016687e+00\n",
      "  -1.80868411e+00 -5.67967892e-01]\n",
      " ...\n",
      " [ 1.00696564e-01  1.00000000e+00 -1.90690804e+00 ... -2.32468353e+02\n",
      "  -2.84801712e+01 -1.31315826e+02]\n",
      " [ 1.00677490e-01  1.00000000e+00 -1.90699959e+00 ... -1.69072617e+02\n",
      "  -1.10574509e+02  4.39380859e+02]\n",
      " [ 1.00651741e-01  1.00000000e+00 -1.90699005e+00 ... -1.53821655e+02\n",
      "   7.65461349e+01  1.47307953e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.8474102e-01  1.0000000e+00 -1.7826424e+00 ... -2.4213781e+01\n",
      "  -1.2099957e+02  2.2870739e+01]\n",
      " [ 6.8480206e-01  1.0000000e+00 -1.7825232e+00 ...  1.1510267e+00\n",
      "   2.1377969e-01 -5.1771879e-01]\n",
      " [ 6.8480682e-01  1.0000000e+00 -1.7825990e+00 ...  2.8406534e+00\n",
      "  -2.6896987e+00 -2.4194057e+00]\n",
      " ...\n",
      " [ 6.8477249e-01  1.0000000e+00 -1.7825480e+00 ... -1.1035157e+01\n",
      "  -9.2213249e+01  1.9779853e+00]\n",
      " [ 6.8475342e-01  1.0000000e+00 -1.7826405e+00 ... -7.4462837e+01\n",
      "  -1.7727228e+02 -4.3437726e+02]\n",
      " [ 6.8471146e-01  1.0000000e+00 -1.7826500e+00 ...  5.4120113e+01\n",
      "  -1.2073891e+02 -2.8295305e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.20218468e+00  1.00000000e+00 -1.48369026e+00 ...  1.22654221e+02\n",
      "  -9.08673096e+01  3.25156250e+01]\n",
      " [ 1.20223713e+00  1.00000000e+00 -1.48355675e+00 ...  1.75055504e-01\n",
      "   1.13292634e-01  4.08787727e-02]\n",
      " [ 1.20227432e+00  1.00000000e+00 -1.48362517e+00 ... -1.03506804e+00\n",
      "   2.61375046e+00 -2.77990818e+00]\n",
      " ...\n",
      " [ 1.20224953e+00  1.00000000e+00 -1.48358822e+00 ...  3.38664436e+01\n",
      "   1.31302689e+02  8.86481171e+01]\n",
      " [ 1.20223618e+00  1.00000000e+00 -1.48368835e+00 ... -9.06572571e+01\n",
      "  -2.37507343e+01 -2.06727600e+01]\n",
      " [ 1.20215988e+00  1.00000000e+00 -1.48371315e+00 ... -4.66074142e+01\n",
      "  -6.14654808e+01  6.90509567e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6020727e+00  1.0000000e+00 -1.0392246e+00 ...  1.1032609e+03\n",
      "  -6.0002838e+02  2.7583600e+02]\n",
      " [ 1.6021070e+00  1.0000000e+00 -1.0390520e+00 ...  5.0440359e-01\n",
      "   1.5324247e-01 -4.2968750e-02]\n",
      " [ 1.6021423e+00  1.0000000e+00 -1.0391450e+00 ...  8.5273123e-01\n",
      "   1.0344810e+01 -2.3561273e+00]\n",
      " ...\n",
      " [ 1.6021233e+00  1.0000000e+00 -1.0390902e+00 ...  1.7774986e+01\n",
      "  -7.1534591e+00  5.8033652e+00]\n",
      " [ 1.6021252e+00  1.0000000e+00 -1.0392227e+00 ...  3.4074426e+02\n",
      "   7.9710114e+02 -5.7048914e+02]\n",
      " [ 1.6020565e+00  1.0000000e+00 -1.0392532e+00 ... -7.6636818e+01\n",
      "   1.2677764e+02  6.1334759e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8447657e+00  1.0000000e+00 -4.9341965e-01 ...  2.5333304e+02\n",
      "  -1.9310593e+02 -2.0276949e+02]\n",
      " [ 1.8447733e+00  1.0000000e+00 -4.9324703e-01 ...  7.5692034e-01\n",
      "   3.4741187e-01  2.1141338e-01]\n",
      " [ 1.8448257e+00  1.0000000e+00 -4.9332312e-01 ...  3.8499302e-01\n",
      "   4.3527746e+00 -6.1945266e-01]\n",
      " ...\n",
      " [ 1.8448124e+00  1.0000000e+00 -4.9328613e-01 ... -1.4619696e+01\n",
      "  -2.4748549e+01 -5.6648636e+01]\n",
      " [ 1.8448334e+00  1.0000000e+00 -4.9341774e-01 ... -2.6097855e+02\n",
      "  -2.1058961e+02  4.7901730e+01]\n",
      " [ 1.8447628e+00  1.0000000e+00 -4.9345970e-01 ...  3.6960144e+01\n",
      "  -1.4078113e+00  4.5903721e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90670967e+00  1.00000000e+00  1.00843430e-01 ... -2.49094963e+00\n",
      "  -6.03370171e+01 -1.15546455e+02]\n",
      " [ 1.90667248e+00  1.00000000e+00  1.01023674e-01 ...  1.30160427e+00\n",
      "   4.79154050e-01  3.94912243e-01]\n",
      " [ 1.90675354e+00  1.00000000e+00  1.00938104e-01 ...  2.93222928e+00\n",
      "   1.42936087e+00 -1.57930195e+00]\n",
      " ...\n",
      " [ 1.90674019e+00  1.00000000e+00  1.00984573e-01 ...  3.40874977e+01\n",
      "  -1.61485710e+01  2.08166580e+02]\n",
      " [ 1.90678978e+00  1.00000000e+00  1.00845337e-01 ...  1.25642197e+02\n",
      "  -6.93970261e+01 -9.72761154e+01]\n",
      " [ 1.90671825e+00  1.00000000e+00  1.00803375e-01 ...  7.97106171e+00\n",
      "   8.85146179e+01  4.17011309e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7820826e+00  1.0000000e+00  6.8551445e-01 ... -4.6602806e+01\n",
      "   3.1502489e+01  2.1681278e+01]\n",
      " [ 1.7820187e+00  1.0000000e+00  6.8569088e-01 ...  2.5845423e+00\n",
      "   7.2666597e-01  4.1305447e-01]\n",
      " [ 1.7820911e+00  1.0000000e+00  6.8561107e-01 ...  3.4303527e+00\n",
      "   4.4305825e+00  1.2064624e-01]\n",
      " ...\n",
      " [ 1.7820740e+00  1.0000000e+00  6.8565273e-01 ... -1.0433598e+01\n",
      "  -3.0761290e+01  3.6538968e+00]\n",
      " [ 1.7821598e+00  1.0000000e+00  6.8551636e-01 ... -2.5786783e+02\n",
      "  -1.7265738e+02 -1.1883202e+02]\n",
      " [ 1.7821054e+00  1.0000000e+00  6.8547630e-01 ... -8.0553741e+01\n",
      "  -7.3672395e+00 -8.9251312e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         1.0000001\n",
      " 0.         0.         0.         0.1        0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.4830914    1.           1.2026577 ...  -60.27727    225.67386\n",
      "   489.09445  ]\n",
      " [   1.4829884    1.           1.2027941 ...    9.602861     5.3865933\n",
      "    -0.7675537]\n",
      " [   1.4830303    1.           1.2027217 ...    5.145253     7.189254\n",
      "     1.2065117]\n",
      " ...\n",
      " [   1.4830132    1.           1.2027578 ...   19.910166    88.93114\n",
      "    54.06737  ]\n",
      " [   1.4831142    1.           1.2026596 ...  231.87212    311.3192\n",
      "  -348.17816  ]\n",
      " [   1.4831314    1.           1.2026272 ...  -75.42991     36.43415\n",
      "   -19.871029 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.6       0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.4       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0387354e+00  1.0000000e+00  1.6021996e+00 ... -4.4722836e+01\n",
      "   1.0712866e+02 -2.9974613e+01]\n",
      " [ 1.0385990e+00  1.0000000e+00  1.6023474e+00 ... -5.0560760e-01\n",
      "  -9.9992037e-01 -1.3315800e-01]\n",
      " [ 1.0386276e+00  1.0000000e+00  1.6022809e+00 ... -6.0123110e-01\n",
      "  -2.4938293e+00  6.7768490e-01]\n",
      " ...\n",
      " [ 1.0386086e+00  1.0000000e+00  1.6023169e+00 ... -6.9308698e+02\n",
      "  -9.8113747e+01  2.5533825e+01]\n",
      " [ 1.0387325e+00  1.0000000e+00  1.6022015e+00 ...  4.7325562e+02\n",
      "   1.5264032e+02  5.2053272e+01]\n",
      " [ 1.0387812e+00  1.0000000e+00  1.6021862e+00 ...  6.0395912e+01\n",
      "   1.1050674e+02 -2.4773456e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.9247646e-01  1.0000000e+00  1.8448410e+00 ...  4.1999306e+01\n",
      "  -4.1919266e+01  1.1010363e+02]\n",
      " [ 4.9232769e-01  1.0000000e+00  1.8449984e+00 ...  2.5592184e-01\n",
      "   2.3707991e+00  8.5600722e-01]\n",
      " [ 4.9231720e-01  1.0000000e+00  1.8449367e+00 ...  3.9586930e+00\n",
      "   8.4504681e+00  1.8014538e+00]\n",
      " ...\n",
      " [ 4.9229813e-01  1.0000000e+00  1.8449774e+00 ... -1.2897824e+02\n",
      "  -5.8070465e+01 -7.3215981e+00]\n",
      " [ 4.9244690e-01  1.0000000e+00  1.8448448e+00 ... -2.6434454e+02\n",
      "   1.0078487e+02 -1.6830301e+01]\n",
      " [ 4.9252605e-01  1.0000000e+00  1.8448429e+00 ... -1.2763333e+02\n",
      "  -6.6176956e+01  1.5903514e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.01592064e-01  1.00000000e+00  1.90678596e+00 ... -5.60027428e+01\n",
      "   1.03725605e+01 -1.96247208e+02]\n",
      " [-1.01747513e-01  1.00000000e+00  1.90688705e+00 ...  6.65184259e-01\n",
      "   2.91562700e+00  7.97507048e-01]\n",
      " [-1.01778030e-01  1.00000000e+00  1.90684009e+00 ...  2.14680600e+00\n",
      "   4.81404972e+00  5.59115171e-01]\n",
      " ...\n",
      " [-1.01799011e-01  1.00000000e+00  1.90687180e+00 ...  2.19049683e+01\n",
      "  -1.55744873e+02 -1.94354446e+02]\n",
      " [-1.01646423e-01  1.00000000e+00  1.90678978e+00 ...  5.78855324e+01\n",
      "  -1.02620834e+02 -5.86575073e+02]\n",
      " [-1.01540565e-01  1.00000000e+00  1.90680504e+00 ...  2.62066895e+02\n",
      "   6.07179031e+01 -1.12709358e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.68595886    1.            1.7818508  ...   88.92841\n",
      "    51.200016     61.00293   ]\n",
      " [  -0.6861057     1.            1.7819176  ...    1.2553625\n",
      "     3.076962      0.32110214]\n",
      " [  -0.68611336    1.            1.7818755  ...    1.9931171\n",
      "     8.182964      1.1393042 ]\n",
      " ...\n",
      " [  -0.68613434    1.            1.781909   ...  112.746704\n",
      "    47.70215    -117.33997   ]\n",
      " [  -0.6859913     1.            1.7818489  ...  101.855095\n",
      "  -147.15536      86.44449   ]\n",
      " [  -0.6859112     1.            1.7818794  ...   80.24053\n",
      "   190.20387     -82.16021   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.1       0.        0.        0.        0.        0.\n",
      " 0.8000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.20333385e+00  1.00000000e+00  1.48247528e+00 ...  2.94379211e+02\n",
      "   1.97874130e+02 -2.63048004e+02]\n",
      " [-1.20346260e+00  1.00000000e+00  1.48250675e+00 ...  2.24783969e+00\n",
      "   1.03324003e+01 -1.55071962e+00]\n",
      " [-1.20349693e+00  1.00000000e+00  1.48246717e+00 ...  1.95806479e+00\n",
      "   8.90579987e+00  5.99198878e-01]\n",
      " ...\n",
      " [-1.20351410e+00  1.00000000e+00  1.48249531e+00 ... -5.87993202e+01\n",
      "   1.19189621e+02  1.06176636e+02]\n",
      " [-1.20339775e+00  1.00000000e+00  1.48247147e+00 ... -4.00306213e+02\n",
      "  -1.50176880e+02 -1.08848450e+03]\n",
      " [-1.20329666e+00  1.00000000e+00  1.48252106e+00 ... -4.52403183e+01\n",
      "  -6.16290951e+00 -1.08513741e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6026993e+00  1.0000000e+00  1.0379753e+00 ...  7.4977493e+01\n",
      "   1.7105045e+02 -4.0774564e+02]\n",
      " [-1.6027975e+00  1.0000000e+00  1.0379696e+00 ... -1.9725329e+00\n",
      "   3.7806306e+00  3.1234772e+00]\n",
      " [-1.6028347e+00  1.0000000e+00  1.0379307e+00 ... -3.5999700e-01\n",
      "  -1.2133526e+00 -1.9474971e-01]\n",
      " ...\n",
      " [-1.6028519e+00  1.0000000e+00  1.0379543e+00 ...  2.5046951e+03\n",
      "  -2.7173606e+03 -2.2734409e+02]\n",
      " [-1.6027546e+00  1.0000000e+00  1.0379696e+00 ...  7.3848953e+01\n",
      "  -2.9236269e+01  7.1673523e+01]\n",
      " [-1.6026812e+00  1.0000000e+00  1.0380306e+00 ... -1.1592579e+02\n",
      "  -9.5227768e+01 -4.7558483e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.3       0.        0.        0.        0.4       0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.84502792e+00  1.00000000e+00  4.92156982e-01 ...  1.89061401e+02\n",
      "   1.60111130e+02  2.05176025e+02]\n",
      " [-1.84508610e+00  1.00000000e+00  4.92085457e-01 ... -7.98819423e-01\n",
      "  -7.11538029e+00 -3.84753752e+00]\n",
      " [-1.84511185e+00  1.00000000e+00  4.92065728e-01 ...  2.38499910e-01\n",
      "   2.94872522e-01  6.29885197e-02]\n",
      " ...\n",
      " [-1.84512901e+00  1.00000000e+00  4.92069244e-01 ... -1.42056427e+02\n",
      "   5.11682678e+02 -1.20527405e+02]\n",
      " [-1.84506035e+00  1.00000000e+00  4.92151260e-01 ...  3.49954071e+01\n",
      "   6.48292923e+00  4.44729996e+01]\n",
      " [-1.84502697e+00  1.00000000e+00  4.92216110e-01 ...  4.52451897e+00\n",
      "   5.22666130e+01  6.94064903e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90682507e+00  1.00000000e+00 -1.02600098e-01 ...  1.27447968e+01\n",
      "   1.06326313e+01  2.02651367e+01]\n",
      " [-1.90685177e+00  1.00000000e+00 -1.02666855e-01 ... -5.31538606e-01\n",
      "   2.22882080e+00  7.15874076e-01]\n",
      " [-1.90690994e+00  1.00000000e+00 -1.02706820e-01 ... -3.39296609e-01\n",
      "  -3.04012299e-02 -3.90530229e-02]\n",
      " ...\n",
      " [-1.90692329e+00  1.00000000e+00 -1.02683067e-01 ...  3.28622093e+01\n",
      "  -8.23441620e+01 -2.42410984e+01]\n",
      " [-1.90690231e+00  1.00000000e+00 -1.02605820e-01 ... -7.38524961e+00\n",
      "   1.29603539e+01  1.27186356e+01]\n",
      " [-1.90684414e+00  1.00000000e+00 -1.02540970e-01 ...  8.98101013e+02\n",
      "   1.09789780e+02  4.69520874e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7818546e+00  1.0000000e+00 -6.8686104e-01 ... -7.9364300e+00\n",
      "   2.6396042e+02 -6.3213760e+01]\n",
      " [-1.7818394e+00  1.0000000e+00 -6.8689156e-01 ... -1.4312465e+00\n",
      "   1.7544208e+00 -6.3342601e-02]\n",
      " [-1.7818832e+00  1.0000000e+00 -6.8692231e-01 ... -2.5808880e-01\n",
      "   2.8746886e+00 -8.6771131e-02]\n",
      " ...\n",
      " [-1.7818928e+00  1.0000000e+00 -6.8690777e-01 ... -7.8783836e+01\n",
      "  -5.6005817e+01 -2.2578766e+02]\n",
      " [-1.7819042e+00  1.0000000e+00 -6.8686676e-01 ...  1.3521030e+02\n",
      "   1.3437332e+02  3.4223763e+01]\n",
      " [-1.7818880e+00  1.0000000e+00 -6.8680382e-01 ... -8.5309485e+02\n",
      "  -1.4448447e+02 -1.0717231e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4824419e+00  1.0000000e+00 -1.2038765e+00 ...  8.8293829e+00\n",
      "  -2.0708572e+02 -1.4894296e+02]\n",
      " [-1.4824057e+00  1.0000000e+00 -1.2038822e+00 ...  3.3952144e-01\n",
      "   8.2252827e+00 -7.6277316e-01]\n",
      " [-1.4824181e+00  1.0000000e+00 -1.2039075e+00 ...  9.4110727e-01\n",
      "   1.1017929e+01 -4.6750739e-02]\n",
      " ...\n",
      " [-1.4824276e+00  1.0000000e+00 -1.2038956e+00 ...  2.7754547e+02\n",
      "  -1.4400156e+02  1.8679959e+02]\n",
      " [-1.4824581e+00  1.0000000e+00 -1.2038803e+00 ... -2.9418006e+00\n",
      "  -1.0037032e+02 -4.8698788e+01]\n",
      " [-1.4824877e+00  1.0000000e+00 -1.2038231e+00 ...  1.5110719e+00\n",
      "  -3.9590065e+01 -8.0268082e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.03788       1.           -1.6031075  ...   49.590034\n",
      "   -20.012829   -376.7988    ]\n",
      " [  -1.0378246     1.           -1.6030712  ...    0.8587737\n",
      "     8.887302     -2.0668125 ]\n",
      " [  -1.0378323     1.           -1.603095   ...    1.0782357\n",
      "   -12.7300825    -0.63482964]\n",
      " ...\n",
      " [  -1.0378437     1.           -1.6030836  ...   41.24476\n",
      "   -58.14047      76.33745   ]\n",
      " [  -1.0378952     1.           -1.6031094  ...   43.379353\n",
      "    -4.353634    102.71771   ]\n",
      " [  -1.0379438     1.           -1.6030579  ... -292.4798\n",
      "  -238.09827     -32.7159    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.91621017e-01  1.00000000e+00 -1.84521484e+00 ...  3.54483276e+02\n",
      "   7.09390625e+02 -1.26240616e+02]\n",
      " [-4.91558075e-01  1.00000000e+00 -1.84516907e+00 ...  1.73541009e+00\n",
      "   2.54972911e+00 -1.25866890e+00]\n",
      " [-4.91558075e-01  1.00000000e+00 -1.84518516e+00 ...  8.27129006e-01\n",
      "  -8.74064445e+00 -8.71502519e-01]\n",
      " ...\n",
      " [-4.91571426e-01  1.00000000e+00 -1.84518147e+00 ...  1.41971420e+02\n",
      "   1.58441452e+02 -2.35672623e+02]\n",
      " [-4.91626740e-01  1.00000000e+00 -1.84521294e+00 ...  4.96677246e+01\n",
      "  -2.21540260e+01 -3.80605988e+02]\n",
      " [-4.91694450e-01  1.00000000e+00 -1.84518814e+00 ...  6.69281387e+01\n",
      "   5.49148560e+01 -9.01257172e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.02620125e-01  1.00000000e+00 -1.90680504e+00 ... -1.55880569e+02\n",
      "  -1.83910656e+01 -1.93687363e+01]\n",
      " [ 1.02683067e-01  1.00000000e+00 -1.90675259e+00 ...  4.22785997e+00\n",
      "   5.92526579e+00 -1.90149322e-01]\n",
      " [ 1.02708817e-01  1.00000000e+00 -1.90676045e+00 ...  5.17042994e-01\n",
      "  -1.03295965e+01 -1.56064653e+00]\n",
      " ...\n",
      " [ 1.02695465e-01  1.00000000e+00 -1.90676403e+00 ...  8.50006771e+00\n",
      "  -1.74269485e+02  7.30011978e+01]\n",
      " [ 1.02634430e-01  1.00000000e+00 -1.90680504e+00 ... -5.35416842e+00\n",
      "   3.35730782e+01  1.17406502e+02]\n",
      " [ 1.02544785e-01  1.00000000e+00 -1.90679550e+00 ...  7.89939499e+01\n",
      "  -2.86946930e+02 -1.30828979e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.8691730e-01  1.0000000e+00 -1.7817001e+00 ... -6.9038715e+00\n",
      "   2.6887989e+01 -1.5205701e+01]\n",
      " [ 6.8697453e-01  1.0000000e+00 -1.7816629e+00 ...  2.6505537e+00\n",
      "   5.5619135e+00  1.3404263e+00]\n",
      " [ 6.8703461e-01  1.0000000e+00 -1.7816590e+00 ...  1.3699859e-01\n",
      "  -3.8146558e+00 -2.7566528e-01]\n",
      " ...\n",
      " [ 6.8702316e-01  1.0000000e+00 -1.7816753e+00 ... -3.2897583e+02\n",
      "   3.5142966e+02  6.7590778e+02]\n",
      " [ 6.8696594e-01  1.0000000e+00 -1.7817078e+00 ... -4.2251217e+01\n",
      "  -5.7109726e+01  1.1084364e+01]\n",
      " [ 6.8684673e-01  1.0000000e+00 -1.7817307e+00 ...  1.3185313e+02\n",
      "  -3.8198456e+01 -3.0487213e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2040701e+00  1.0000000e+00 -1.4818764e+00 ...  1.6158009e+01\n",
      "   5.8381893e+01 -5.2876560e+01]\n",
      " [ 1.2041149e+00  1.0000000e+00 -1.4818039e+00 ...  7.7864373e-01\n",
      "   2.7485163e+00  4.8664623e-01]\n",
      " [ 1.2041836e+00  1.0000000e+00 -1.4817865e+00 ...  6.1351085e-01\n",
      "   3.9138215e+00  1.6020477e-01]\n",
      " ...\n",
      " [ 1.2041740e+00  1.0000000e+00 -1.4818144e+00 ... -2.8480539e+01\n",
      "   2.1780397e+02  1.0163502e+02]\n",
      " [ 1.2041225e+00  1.0000000e+00 -1.4818840e+00 ... -2.0604735e+02\n",
      "  -1.6622725e+01  2.8853127e+01]\n",
      " [ 1.2040176e+00  1.0000000e+00 -1.4819241e+00 ...  3.1675217e+02\n",
      "   8.4015511e+01  1.3366731e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.6031761    1.          -1.0372009  ... -21.215591    13.991808\n",
      "   54.851166  ]\n",
      " [  1.6032047    1.          -1.0371237  ...  -0.35430276   5.320545\n",
      "    1.2196451 ]\n",
      " [  1.6032639    1.          -1.0370929  ...   0.752751     6.909721\n",
      "   -1.1287866 ]\n",
      " ...\n",
      " [  1.6032581    1.          -1.0371332  ...  20.172266    31.73723\n",
      "  -28.310276  ]\n",
      " [  1.6032219    1.          -1.0372066  ...  24.034319   -19.402103\n",
      "  -22.057613  ]\n",
      " [  1.6031466    1.          -1.0372658  ...   1.4946208  101.67409\n",
      "  -43.512466  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8452835     1.           -0.49102402 ...   20.131512\n",
      "    25.351742      4.334273  ]\n",
      " [   1.8452969     1.           -0.4909172  ...    0.5824145\n",
      "     9.225974     -0.5417334 ]\n",
      " [   1.8453159     1.           -0.4908654  ...   -1.0420809\n",
      "    -7.405341      1.8025584 ]\n",
      " ...\n",
      " [   1.845314      1.           -0.49092484 ...   88.987434\n",
      "  -105.71467     -26.31074   ]\n",
      " [   1.8453007     1.           -0.49102974 ...   77.94354\n",
      "    35.192566    -27.49328   ]\n",
      " [   1.845274      1.           -0.4910946  ... -124.77408\n",
      "   175.12222    -462.3092    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.2       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90672016e+00  1.00000000e+00  1.03313446e-01 ... -7.09470081e+00\n",
      "  -1.39936584e+02  8.37005157e+01]\n",
      " [ 1.90671062e+00  1.00000000e+00  1.03427887e-01 ... -6.14991784e-01\n",
      "   2.62035084e+00  2.95447707e-01]\n",
      " [ 1.90673828e+00  1.00000000e+00  1.03487238e-01 ... -7.03218460e-01\n",
      "  -2.62365055e+00  6.48128688e-01]\n",
      " ...\n",
      " [ 1.90674782e+00  1.00000000e+00  1.03420258e-01 ... -6.25545959e+01\n",
      "   2.52007236e+01 -8.08630371e+01]\n",
      " [ 1.90674973e+00  1.00000000e+00  1.03307724e-01 ... -1.15133118e+03\n",
      "   3.57294922e+02 -9.47161865e+01]\n",
      " [ 1.90673447e+00  1.00000000e+00  1.03242874e-01 ...  9.82492599e+01\n",
      "  -2.18486671e+01  1.83466522e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.3       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.1       0.        0.        0.        0.\n",
      " 0.1       1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7813683e+00  1.0000000e+00  6.8791008e-01 ... -5.5618141e+01\n",
      "  -1.4240190e+02  2.3036157e+02]\n",
      " [ 1.7813263e+00  1.0000000e+00  6.8798828e-01 ... -1.2546766e+00\n",
      "   8.8631563e+00  9.1201246e-01]\n",
      " [ 1.7813301e+00  1.0000000e+00  6.8804818e-01 ... -1.1174438e+00\n",
      "   5.1284605e-01 -2.5689143e-01]\n",
      " ...\n",
      " [ 1.7813511e+00  1.0000000e+00  6.8798065e-01 ... -1.2211971e+03\n",
      "  -1.3038417e+02 -2.6350467e+02]\n",
      " [ 1.7813778e+00  1.0000000e+00  6.8790436e-01 ... -1.2860375e+01\n",
      "  -5.8795513e+01  1.6859706e+02]\n",
      " [ 1.7814054e+00  1.0000000e+00  6.8784904e-01 ... -9.8148450e+02\n",
      "   1.1741901e+02 -5.1951477e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.9000001 0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4816608e+00  1.0000000e+00  1.2045288e+00 ... -1.8553647e+02\n",
      "   5.2694523e+01 -2.2725517e+01]\n",
      " [ 1.4816008e+00  1.0000000e+00  1.2046051e+00 ... -1.0945337e+00\n",
      "   2.2446413e+00  3.1519347e-01]\n",
      " [ 1.4815845e+00  1.0000000e+00  1.2046402e+00 ...  5.0040609e-01\n",
      "   2.0143473e-01  1.1958048e+00]\n",
      " ...\n",
      " [ 1.4816189e+00  1.0000000e+00  1.2045975e+00 ... -2.6974103e+02\n",
      "   4.1774807e+01 -6.1364361e+01]\n",
      " [ 1.4816723e+00  1.0000000e+00  1.2045231e+00 ... -1.9397560e+03\n",
      "   2.7293086e+03 -7.1489825e+02]\n",
      " [ 1.4817228e+00  1.0000000e+00  1.2044773e+00 ... -1.2746078e+03\n",
      "   8.7376593e+02 -1.0973792e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        1.0000001 0.        0.9000001 0.        0.        0.1\n",
      " 0.        0.        0.        0.        0.5       0.        0.\n",
      " 1.0000001 0.6       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0367880e+00  1.0000000e+00  1.6035271e+00 ...  3.4940735e+01\n",
      "   2.7945190e+02 -5.5672791e+01]\n",
      " [ 1.0367165e+00  1.0000000e+00  1.6036158e+00 ... -3.6524353e+00\n",
      "   5.8444343e+00  1.0353664e+00]\n",
      " [ 1.0366936e+00  1.0000000e+00  1.6036270e+00 ...  5.4489636e-01\n",
      "   3.6860180e-01  1.1816601e+00]\n",
      " ...\n",
      " [ 1.0367317e+00  1.0000000e+00  1.6036091e+00 ... -5.8645273e+03\n",
      "   3.7429821e+01  4.6674707e+03]\n",
      " [ 1.0368023e+00  1.0000000e+00  1.6035213e+00 ...  5.3138019e+02\n",
      "  -5.2640610e+01 -1.9204706e+02]\n",
      " [ 1.0368719e+00  1.0000000e+00  1.6034908e+00 ...  6.8691632e+02\n",
      "  -2.0789487e+02  1.4888351e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.1       0.        1.0000001 0.        0.        0.\n",
      " 0.8000001 0.        0.        0.        0.        0.        0.\n",
      " 0.4       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.9021816e-01  1.0000000e+00  1.8455372e+00 ... -2.7155078e+02\n",
      "  -2.3396138e+02 -2.3018665e+02]\n",
      " [ 4.9013615e-01  1.0000000e+00  1.8456221e+00 ... -4.5805483e+00\n",
      "   6.4894471e+00  4.6264607e-01]\n",
      " [ 4.9014473e-01  1.0000000e+00  1.8456221e+00 ... -6.8239713e-01\n",
      "   8.8798738e-01  7.5548291e-01]\n",
      " ...\n",
      " [ 4.9019051e-01  1.0000000e+00  1.8456163e+00 ... -2.9513623e+03\n",
      "   5.6084454e+02 -9.1077740e+02]\n",
      " [ 4.9026680e-01  1.0000000e+00  1.8455315e+00 ...  3.6678189e+02\n",
      "  -1.5995847e+02  5.3903729e+02]\n",
      " [ 4.9031448e-01  1.0000000e+00  1.8455296e+00 ...  1.4869125e+02\n",
      "  -5.6619831e+01 -6.3857007e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.03868484e-01  1.00000000e+00  1.90671730e+00 ...  6.71311722e+01\n",
      "  -9.36006317e+01  9.57008133e+01]\n",
      " [-1.03955269e-01  1.00000000e+00  1.90678692e+00 ...  2.64071178e+00\n",
      "  -1.66174841e+00  4.71819758e-01]\n",
      " [-1.03927612e-01  1.00000000e+00  1.90677702e+00 ...  1.53634119e+00\n",
      "  -1.49910760e+00  4.71604615e-01]\n",
      " ...\n",
      " [-1.03881836e-01  1.00000000e+00  1.90678406e+00 ...  3.45252838e+02\n",
      "  -4.31602386e+02  3.47255829e+02]\n",
      " [-1.03805542e-01  1.00000000e+00  1.90671349e+00 ... -5.48713440e+02\n",
      "  -1.98407425e+02  6.79980698e+01]\n",
      " [-1.03770256e-01  1.00000000e+00  1.90674019e+00 ... -9.64732056e+02\n",
      "  -1.88181921e+03  5.19449280e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.8824196e-01  1.0000000e+00  1.7812901e+00 ... -1.7632566e+01\n",
      "  -1.3202472e+02  1.0798939e+02]\n",
      " [-6.8832302e-01  1.0000000e+00  1.7812796e+00 ...  5.3312721e+00\n",
      "  -4.9041243e+00  1.2902983e+00]\n",
      " [-6.8829346e-01  1.0000000e+00  1.7812781e+00 ...  4.4122863e+00\n",
      "  -5.1467705e+00 -6.7787015e-01]\n",
      " ...\n",
      " [-6.8825531e-01  1.0000000e+00  1.7812824e+00 ...  2.0546960e+02\n",
      "  -1.9746336e+02 -5.3457214e+02]\n",
      " [-6.8818283e-01  1.0000000e+00  1.7812901e+00 ...  7.7176239e+02\n",
      "  -9.6778271e+02 -8.0888531e+02]\n",
      " [-6.8815041e-01  1.0000000e+00  1.7813416e+00 ... -1.5378749e+02\n",
      "  -1.2632593e+01  6.7882774e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.20513439e+00  1.00000000e+00  1.48133850e+00 ...  4.41567444e+02\n",
      "  -2.64802113e+01 -2.45408493e+02]\n",
      " [-1.20520115e+00  1.00000000e+00  1.48133087e+00 ... -1.58907354e-01\n",
      "  -3.61611032e+00  2.93332458e-01]\n",
      " [-1.20517921e+00  1.00000000e+00  1.48131776e+00 ...  3.67142463e+00\n",
      "  -4.40109682e+00 -1.37221113e-01]\n",
      " ...\n",
      " [-1.20514679e+00  1.00000000e+00  1.48133850e+00 ...  1.19176025e+02\n",
      "  -4.47139969e+01 -5.15311432e+01]\n",
      " [-1.20509529e+00  1.00000000e+00  1.48135376e+00 ...  4.89366951e+01\n",
      "  -4.12317314e+01 -4.07276077e+01]\n",
      " [-1.20505905e+00  1.00000000e+00  1.48143387e+00 ... -1.10198303e+02\n",
      "  -4.87964172e+01 -8.10326862e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6038628e+00  1.0000000e+00  1.0365658e+00 ...  1.3137730e+02\n",
      "  -7.9669884e+01  1.2010937e+02]\n",
      " [-1.6039152e+00  1.0000000e+00  1.0365467e+00 ... -6.7159474e-02\n",
      "  -3.6363971e+00  5.3627741e-01]\n",
      " [-1.6038570e+00  1.0000000e+00  1.0365263e+00 ...  2.5912440e+00\n",
      "  -3.1866093e+00 -1.9651926e+00]\n",
      " ...\n",
      " [-1.6038322e+00  1.0000000e+00  1.0365582e+00 ... -7.1580498e+01\n",
      "   4.3299722e+02  2.6106506e+02]\n",
      " [-1.6037941e+00  1.0000000e+00  1.0365868e+00 ... -4.2472260e+01\n",
      "   3.2987366e+01  6.0623287e+01]\n",
      " [-1.6038094e+00  1.0000000e+00  1.0366821e+00 ...  7.7438736e+01\n",
      "   1.7902650e+02 -1.2813936e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.4 0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8457623e+00  1.0000000e+00  4.8994827e-01 ... -4.0049582e+02\n",
      "  -4.1765163e+01  5.9126550e+02]\n",
      " [-1.8457918e+00  1.0000000e+00  4.8992920e-01 ... -1.6747872e+00\n",
      "  -2.2842386e+00 -2.9764402e-01]\n",
      " [-1.8457298e+00  1.0000000e+00  4.8990816e-01 ...  5.1349294e-01\n",
      "  -7.9925227e-01 -8.1291187e-01]\n",
      " ...\n",
      " [-1.8457127e+00  1.0000000e+00  4.8994350e-01 ... -4.4201636e+02\n",
      "  -3.1473779e+02  2.7004883e+02]\n",
      " [-1.8457088e+00  1.0000000e+00  4.8997307e-01 ...  4.8181944e+00\n",
      "  -1.1381424e+02 -9.5412758e+01]\n",
      " [-1.8457365e+00  1.0000000e+00  4.9007416e-01 ... -1.4439320e+01\n",
      "  -2.1548246e+01 -5.2234818e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         1.0000001  0.70000005 0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90683174e+00  1.00000000e+00 -1.04541779e-01 ...  2.54890549e+02\n",
      "   1.98807888e+01 -2.67727325e+02]\n",
      " [-1.90683556e+00  1.00000000e+00 -1.04568481e-01 ... -2.15321732e+02\n",
      "   6.63803711e+01  2.10989243e+02]\n",
      " [-1.90676689e+00  1.00000000e+00 -1.04583099e-01 ... -8.12153339e-01\n",
      "  -2.07730150e+00 -4.60124135e-01]\n",
      " ...\n",
      " [-1.90676308e+00  1.00000000e+00 -1.04553223e-01 ...  1.35607901e+01\n",
      "  -1.03863831e+01  1.06417625e+02]\n",
      " [-1.90678215e+00  1.00000000e+00 -1.04515076e-01 ...  1.86464310e+01\n",
      "  -4.13029594e+01 -1.76788387e+01]\n",
      " [-1.90684509e+00  1.00000000e+00 -1.04412079e-01 ...  2.35486710e+02\n",
      "  -1.45162338e+02  5.62964783e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.9000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7812004e+00  1.0000000e+00 -6.8867302e-01 ... -8.2448318e+01\n",
      "  -1.7363997e+01  6.6872337e+01]\n",
      " [-1.7811861e+00  1.0000000e+00 -6.8869305e-01 ... -6.1141083e+02\n",
      "  -3.8901665e+01  4.8046112e+02]\n",
      " [-1.7811260e+00  1.0000000e+00 -6.8870872e-01 ... -2.9230732e-01\n",
      "  -8.3307552e-01 -5.0595969e-01]\n",
      " ...\n",
      " [-1.7811317e+00  1.0000000e+00 -6.8868065e-01 ...  3.1120644e+00\n",
      "  -3.3280174e+01  3.5810429e+01]\n",
      " [-1.7811661e+00  1.0000000e+00 -6.8864822e-01 ...  6.7320189e+00\n",
      "   5.1653801e+01  4.3335106e+01]\n",
      " [-1.7812462e+00  1.0000000e+00 -6.8855095e-01 ...  1.7864144e-01\n",
      "  -1.8634010e+01 -1.3137912e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.3\n",
      " 0.        0.        0.6       0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4811316e+00  1.0000000e+00 -1.2054443e+00 ...  2.2721218e+01\n",
      "  -1.8629366e+02  1.4216799e+01]\n",
      " [-1.4810982e+00  1.0000000e+00 -1.2054920e+00 ...  1.4168880e+02\n",
      "  -4.5654572e+02 -1.1921608e+03]\n",
      " [-1.4810219e+00  1.0000000e+00 -1.2055006e+00 ...  9.9309921e-02\n",
      "  -1.0323119e+01  4.9974608e-01]\n",
      " ...\n",
      " [-1.4810352e+00  1.0000000e+00 -1.2054844e+00 ...  5.0522873e+01\n",
      "  -3.1162374e+01 -2.1771345e+01]\n",
      " [-1.4810867e+00  1.0000000e+00 -1.2054291e+00 ... -4.0196765e+02\n",
      "  -1.5854180e+03 -1.9518834e+02]\n",
      " [-1.4812059e+00  1.0000000e+00 -1.2053452e+00 ...  5.9550519e+02\n",
      "  -4.1926935e+02 -7.1670853e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.  0.  0.5 0.  0.  0.  0.5 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0362339e+00  1.0000000e+00 -1.6040859e+00 ...  4.1570869e+01\n",
      "   1.2852563e+00  4.7274891e+01]\n",
      " [-1.0361872e+00  1.0000000e+00 -1.6041059e+00 ... -7.8625412e+00\n",
      "  -2.7247659e+02 -6.1650012e+02]\n",
      " [-1.0361099e+00  1.0000000e+00 -1.6040932e+00 ... -2.8901846e+00\n",
      "  -1.5462249e+01  3.6473200e-01]\n",
      " ...\n",
      " [-1.0361328e+00  1.0000000e+00 -1.6041040e+00 ...  1.2332058e+01\n",
      "   3.7103039e+01 -2.4822867e+02]\n",
      " [-1.0361996e+00  1.0000000e+00 -1.6040745e+00 ...  1.6696762e+01\n",
      "   7.3886230e+01 -1.0841216e+02]\n",
      " [-1.0363302e+00  1.0000000e+00 -1.6040192e+00 ...  3.0081628e+02\n",
      "  -1.7213474e+02  1.8151030e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8931408e-01  1.0000000e+00 -1.8457031e+00 ...  3.9621136e+01\n",
      "   7.8856194e+01 -2.5484970e+02]\n",
      " [-4.8926258e-01  1.0000000e+00 -1.8456964e+00 ... -9.1434021e+01\n",
      "   6.8657928e+01  5.5250717e+01]\n",
      " [-4.8915672e-01  1.0000000e+00 -1.8456628e+00 ... -2.4584231e+00\n",
      "  -1.2827181e+01 -4.3469125e-01]\n",
      " ...\n",
      " [-4.8918533e-01  1.0000000e+00 -1.8456945e+00 ...  1.0785024e+03\n",
      "  -2.0891692e+03  2.3979426e+03]\n",
      " [-4.8925781e-01  1.0000000e+00 -1.8456993e+00 ... -4.1733742e+01\n",
      "   3.8757083e+00 -1.2998299e+01]\n",
      " [-4.8942089e-01  1.0000000e+00 -1.8456688e+00 ... -5.1382923e+01\n",
      "  -6.2920212e+01 -6.2340245e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.05109215e-01  1.00000000e+00 -1.90639687e+00 ...  1.04452477e+02\n",
      "  -9.12695160e+01 -1.29849014e+01]\n",
      " [ 1.05163574e-01  1.00000000e+00 -1.90633678e+00 ...  8.30844666e+02\n",
      "  -8.33229065e+01  5.20224121e+02]\n",
      " [ 1.05279922e-01  1.00000000e+00 -1.90629745e+00 ... -1.50590098e+00\n",
      "  -8.57253075e+00  8.86848569e-01]\n",
      " ...\n",
      " [ 1.05249405e-01  1.00000000e+00 -1.90633106e+00 ...  4.14870789e+02\n",
      "  -5.25710205e+02 -2.38149765e+02]\n",
      " [ 1.05173111e-01  1.00000000e+00 -1.90639687e+00 ... -1.28999100e+02\n",
      "   1.14074638e+02 -6.92938156e+01]\n",
      " [ 1.04999542e-01  1.00000000e+00 -1.90640831e+00 ...  8.87682571e+01\n",
      "   2.31892128e+01  2.29427223e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.8919659e-01  1.0000000e+00 -1.7805901e+00 ... -8.9038269e+01\n",
      "  -3.8953693e+02 -1.2785373e+02]\n",
      " [ 6.8924809e-01  1.0000000e+00 -1.7805395e+00 ... -1.1023740e+02\n",
      "   3.0683811e+01 -1.0657210e+02]\n",
      " [ 6.8937683e-01  1.0000000e+00 -1.7804731e+00 ...  4.4012305e-01\n",
      "  -6.6785202e+00 -6.5477923e-02]\n",
      " ...\n",
      " [ 6.8934631e-01  1.0000000e+00 -1.7805233e+00 ... -1.0532690e+03\n",
      "  -7.8836798e+02  6.1225702e+02]\n",
      " [ 6.8927765e-01  1.0000000e+00 -1.7805901e+00 ... -6.6221420e+01\n",
      "  -1.7536340e+02 -1.8226219e+02]\n",
      " [ 6.8908978e-01  1.0000000e+00 -1.7806396e+00 ...  9.6423714e+01\n",
      "   1.0479668e+02  2.6186026e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2059345e+00  1.0000000e+00 -1.4803104e+00 ...  3.1039999e+02\n",
      "   1.6858279e+02  1.3003249e+02]\n",
      " [ 1.2059765e+00  1.0000000e+00 -1.4802790e+00 ... -3.0320332e+00\n",
      "  -1.8190170e+01  7.1856213e+00]\n",
      " [ 1.2060719e+00  1.0000000e+00 -1.4801818e+00 ... -2.8851017e-01\n",
      "   1.0167928e+00 -1.8237765e-01]\n",
      " ...\n",
      " [ 1.2060432e+00  1.0000000e+00 -1.4802456e+00 ...  1.8804845e+02\n",
      "   3.8684582e+01  1.3836691e+02]\n",
      " [ 1.2059975e+00  1.0000000e+00 -1.4803028e+00 ...  4.2084206e+01\n",
      "   3.6619260e+02  1.0841577e+02]\n",
      " [ 1.2058487e+00  1.0000000e+00 -1.4803905e+00 ...  1.7131302e+01\n",
      "  -6.7525573e+00 -2.6424683e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.60442924e+00  1.00000000e+00 -1.03516579e+00 ... -1.41877151e+02\n",
      "   3.07633057e+02  5.57026243e+00]\n",
      " [ 1.60445499e+00  1.00000000e+00 -1.03512955e+00 ...  5.18567017e+02\n",
      "  -3.73959900e+02  2.83052856e+02]\n",
      " [ 1.60453033e+00  1.00000000e+00 -1.03502476e+00 ... -1.22666836e+00\n",
      "   3.52154076e-01 -7.33146191e-01]\n",
      " ...\n",
      " [ 1.60451698e+00  1.00000000e+00 -1.03508759e+00 ... -7.85432434e+01\n",
      "   7.98325348e+01  1.58332593e+03]\n",
      " [ 1.60449219e+00  1.00000000e+00 -1.03515244e+00 ... -1.22538986e+02\n",
      "   9.00310211e+01  3.04311035e+02]\n",
      " [ 1.60436726e+00  1.00000000e+00 -1.03528595e+00 ... -3.50417442e+01\n",
      "  -1.20468475e+02 -6.66308899e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.2       0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84587955e+00  1.00000000e+00 -4.88597870e-01 ...  7.64963388e-01\n",
      "  -1.83031216e+01 -1.86333961e+01]\n",
      " [ 1.84589291e+00  1.00000000e+00 -4.88572121e-01 ... -9.07875977e+01\n",
      "  -2.66432678e+02 -3.69878357e+02]\n",
      " [ 1.84597969e+00  1.00000000e+00 -4.88439888e-01 ...  4.61383492e-01\n",
      "   2.47305679e+00 -4.66452777e-01]\n",
      " ...\n",
      " [ 1.84597588e+00  1.00000000e+00 -4.88524437e-01 ...  7.03564606e+01\n",
      "  -8.05245056e+01 -9.67951965e+00]\n",
      " [ 1.84596825e+00  1.00000000e+00 -4.88580704e-01 ... -4.17956299e+02\n",
      "   9.35650574e+02  1.42526016e+02]\n",
      " [ 1.84585094e+00  1.00000000e+00 -4.88731384e-01 ... -1.08421364e+02\n",
      "   2.21202698e+02  1.68183868e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90658188e+00  1.00000000e+00  1.05913162e-01 ...  9.79812378e+02\n",
      "  -9.00421509e+02  7.71901794e+02]\n",
      " [ 1.90657616e+00  1.00000000e+00  1.05960846e-01 ...  2.98886833e+01\n",
      "   9.94852829e+01 -2.25333771e+02]\n",
      " [ 1.90666389e+00  1.00000000e+00  1.06110863e-01 ...  1.59628582e+00\n",
      "   5.89865160e+00 -5.96994162e-01]\n",
      " ...\n",
      " [ 1.90668297e+00  1.00000000e+00  1.06008530e-01 ... -5.90754738e+01\n",
      "   1.16334099e+02 -1.45101547e+02]\n",
      " [ 1.90669441e+00  1.00000000e+00  1.05932236e-01 ...  5.68186279e+02\n",
      "   2.08004608e+02  4.71757416e+02]\n",
      " [ 1.90658569e+00  1.00000000e+00  1.05775833e-01 ...  2.66926514e+02\n",
      "  -1.95883408e+01  3.66237144e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.2       0.        0.        0.        0.        0.9000001 0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7805462e+00  1.0000000e+00  6.9006729e-01 ... -4.5401495e+02\n",
      "   7.9516144e+02 -1.7213333e+03]\n",
      " [ 1.7805233e+00  1.0000000e+00  6.9011021e-01 ...  1.6447830e+02\n",
      "   1.1084945e+02  9.0266747e+01]\n",
      " [ 1.7805595e+00  1.0000000e+00  6.9025964e-01 ...  1.2456803e+00\n",
      "   9.9719419e+00  1.2059886e+00]\n",
      " ...\n",
      " [ 1.7805920e+00  1.0000000e+00  6.9015598e-01 ...  6.3065323e+01\n",
      "  -5.4479473e+01 -4.6179375e+01]\n",
      " [ 1.7806168e+00  1.0000000e+00  6.9008255e-01 ...  6.6735680e+01\n",
      "  -5.8474144e+01 -5.2391731e+01]\n",
      " [ 1.7805834e+00  1.0000000e+00  6.8993378e-01 ...  5.0794559e+01\n",
      "   7.2190666e+01  3.6482407e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.48019505e+00  1.00000000e+00  1.20655251e+00 ...  1.78056488e+01\n",
      "  -1.78513088e+01 -1.51331625e+01]\n",
      " [ 1.48015881e+00  1.00000000e+00  1.20662975e+00 ...  8.12901764e+01\n",
      "  -8.96426086e+01  8.99439468e+01]\n",
      " [ 1.48015404e+00  1.00000000e+00  1.20674038e+00 ...  2.45695305e+00\n",
      "   9.08606529e+00 -4.10506606e-01]\n",
      " ...\n",
      " [ 1.48021317e+00  1.00000000e+00  1.20666599e+00 ... -4.18044464e+02\n",
      "  -1.13767120e+02 -1.20381584e+02]\n",
      " [ 1.48026085e+00  1.00000000e+00  1.20656586e+00 ... -5.19761780e+02\n",
      "   3.79681885e+02  2.59497108e+01]\n",
      " [ 1.48026848e+00  1.00000000e+00  1.20643044e+00 ... -4.89792442e+01\n",
      "  -1.88413788e+02 -5.78903076e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.1\n",
      " 1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0348644e+00  1.0000000e+00  1.6049538e+00 ...  5.3800663e+01\n",
      "  -1.5745799e+01  8.9927574e+01]\n",
      " [ 1.0348110e+00  1.0000000e+00  1.6050282e+00 ...  7.5408554e+00\n",
      "   1.6647146e+01 -4.4766459e+00]\n",
      " [ 1.0347633e+00  1.0000000e+00  1.6051092e+00 ...  4.6591580e-01\n",
      "   1.4908457e+00  2.2813600e-01]\n",
      " ...\n",
      " [ 1.0348339e+00  1.0000000e+00  1.6050539e+00 ... -7.6451210e+01\n",
      "   2.5026253e+02 -4.4982263e+02]\n",
      " [ 1.0348911e+00  1.0000000e+00  1.6049671e+00 ... -8.4613060e+01\n",
      "  -5.3007069e+01 -6.0465519e+01]\n",
      " [ 1.0349684e+00  1.0000000e+00  1.6048603e+00 ... -5.5866998e+02\n",
      "  -1.8942347e+01  4.0582138e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.1       0.        0.        0.        0.        1.0000001 0.\n",
      " 0.3       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.88135338e-01  1.00000000e+00  1.84627533e+00 ... -6.47294067e+02\n",
      "  -1.55376129e+02 -6.25964172e+02]\n",
      " [ 4.88075256e-01  1.00000000e+00  1.84630394e+00 ...  2.64809494e+01\n",
      "  -1.78169479e+01  4.39111938e+01]\n",
      " [ 4.88040924e-01  1.00000000e+00  1.84635329e+00 ...  4.39286900e+00\n",
      "   1.38696737e+01 -3.59407574e-01]\n",
      " ...\n",
      " [ 4.88121033e-01  1.00000000e+00  1.84632301e+00 ... -6.80431396e+02\n",
      "  -1.16258644e+02  2.67007599e+02]\n",
      " [ 4.88185883e-01  1.00000000e+00  1.84628487e+00 ... -5.72142258e+01\n",
      "   3.42349167e+01 -1.11096954e+02]\n",
      " [ 4.88251686e-01  1.00000000e+00  1.84621429e+00 ... -7.42940903e+01\n",
      "   4.88405304e+01  1.55018330e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.06182098e-01  1.00000000e+00  1.90675163e+00 ... -5.70214844e+02\n",
      "  -8.25369339e+01 -2.70226154e+01]\n",
      " [-1.06245995e-01  1.00000000e+00  1.90676022e+00 ...  8.39588928e+00\n",
      "  -2.53628407e+01 -2.26587620e+01]\n",
      " [-1.06279373e-01  1.00000000e+00  1.90677667e+00 ... -3.35429311e+00\n",
      "  -6.79372787e+00  7.81114876e-01]\n",
      " ...\n",
      " [-1.06197357e-01  1.00000000e+00  1.90677834e+00 ...  1.46698288e+02\n",
      "   2.26132088e+01  1.84307037e+02]\n",
      " [-1.06124878e-01  1.00000000e+00  1.90676117e+00 ...  3.54697876e+02\n",
      "  -6.52299438e+02 -2.05537399e+02]\n",
      " [-1.06062889e-01  1.00000000e+00  1.90674019e+00 ...  1.70592651e+01\n",
      "  -4.03691216e+01 -2.96109295e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.9012928e-01  1.0000000e+00  1.7806416e+00 ...  1.4171498e+02\n",
      "  -7.5991254e+02  3.4958798e+02]\n",
      " [-6.9018745e-01  1.0000000e+00  1.7806263e+00 ...  1.3329301e+02\n",
      "   8.2143593e+00 -1.0750191e+02]\n",
      " [-6.9019318e-01  1.0000000e+00  1.7806261e+00 ... -1.1320903e+00\n",
      "  -3.5326014e+00  5.0490928e-01]\n",
      " ...\n",
      " [-6.9012070e-01  1.0000000e+00  1.7806406e+00 ...  8.5992744e+01\n",
      "  -5.7760167e+00 -8.2140022e+01]\n",
      " [-6.9005775e-01  1.0000000e+00  1.7806587e+00 ... -3.9931255e+01\n",
      "  -8.8111588e+01  6.6133965e+01]\n",
      " [-6.9001389e-01  1.0000000e+00  1.7806644e+00 ... -1.4904030e+02\n",
      "  -1.6914557e+02  2.9741800e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.2068653e+00  1.0000000e+00  1.4799786e+00 ...  8.7944092e+03\n",
      "  -4.1035742e+03 -1.2211295e+03]\n",
      " [-1.2069101e+00  1.0000000e+00  1.4799871e+00 ...  2.6590006e+01\n",
      "  -1.2765645e+01 -2.8966290e+02]\n",
      " [-1.2069225e+00  1.0000000e+00  1.4799573e+00 ...  5.7159925e-01\n",
      "  -4.7831364e+00  5.3706694e-01]\n",
      " ...\n",
      " [-1.2068615e+00  1.0000000e+00  1.4799986e+00 ...  4.2608933e+00\n",
      "   2.7253955e+02  3.1771561e+01]\n",
      " [-1.2068214e+00  1.0000000e+00  1.4800034e+00 ... -1.0401449e+02\n",
      "  -1.1678959e+02 -3.7905035e+02]\n",
      " [-1.2067699e+00  1.0000000e+00  1.4800510e+00 ...  5.3898209e+01\n",
      "   1.6785173e+01  8.0745682e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6050882e+00  1.0000000e+00  1.0346737e+00 ...  2.3094419e+01\n",
      "   1.7825012e+02 -2.1128372e+01]\n",
      " [-1.6051178e+00  1.0000000e+00  1.0346642e+00 ... -4.9829693e+01\n",
      "  -3.2375195e+01 -4.3831413e+01]\n",
      " [-1.6051216e+00  1.0000000e+00  1.0346129e+00 ...  1.7589238e+00\n",
      "  -3.5108447e-01  9.4765460e-01]\n",
      " ...\n",
      " [-1.6050720e+00  1.0000000e+00  1.0346737e+00 ... -7.1322281e+01\n",
      "   1.6244073e+02 -1.0924176e+02]\n",
      " [-1.6050510e+00  1.0000000e+00  1.0347023e+00 ... -6.8285812e+01\n",
      "  -1.5930458e+02  4.6550089e+02]\n",
      " [-1.6050091e+00  1.0000000e+00  1.0347691e+00 ...  1.4658368e+02\n",
      "  -1.3058328e+02 -2.1705711e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.3 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8462086e+00  1.0000000e+00  4.8803902e-01 ... -4.5520191e+01\n",
      "   5.1652622e+01 -1.3987297e+02]\n",
      " [-1.8462286e+00  1.0000000e+00  4.8802662e-01 ...  1.4819573e+03\n",
      "  -1.0562592e+03 -1.8815093e+03]\n",
      " [-1.8462391e+00  1.0000000e+00  4.8794839e-01 ...  1.3836365e+00\n",
      "   5.9933710e-01 -3.5118723e-01]\n",
      " ...\n",
      " [-1.8462029e+00  1.0000000e+00  4.8803616e-01 ...  4.3264539e+02\n",
      "   4.3446103e+02  2.0077521e+02]\n",
      " [-1.8461914e+00  1.0000000e+00  4.8807144e-01 ...  2.9747114e+03\n",
      "  -6.8545233e+02 -1.5876797e+03]\n",
      " [-1.8461685e+00  1.0000000e+00  4.8814774e-01 ... -5.9156094e+01\n",
      "  -7.1136360e+01 -1.0315261e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.4       0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90663910e+00  1.00000000e+00 -1.06536865e-01 ... -6.94030380e+01\n",
      "   2.35453014e+01 -1.27127251e+02]\n",
      " [-1.90663815e+00  1.00000000e+00 -1.06573105e-01 ... -1.58533234e+02\n",
      "  -2.61066895e+02  1.43785507e+02]\n",
      " [-1.90661621e+00  1.00000000e+00 -1.06640063e-01 ...  1.14629340e+00\n",
      "   8.54541540e-01 -5.38364649e-01]\n",
      " ...\n",
      " [-1.90660477e+00  1.00000000e+00 -1.06563568e-01 ...  7.93202820e+02\n",
      "   2.75196114e+01  8.73600197e+00]\n",
      " [-1.90662956e+00  1.00000000e+00 -1.06504440e-01 ...  2.80942657e+02\n",
      "   1.80559119e+03 -5.51171837e+01]\n",
      " [-1.90662861e+00  1.00000000e+00 -1.06428146e-01 ...  8.50769501e+01\n",
      "  -1.55517731e+02 -2.33851929e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.1       0.        1.0000001 0.        0.\n",
      " 0.        0.        0.9000001 0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7804108e+00  1.0000000e+00 -6.9049454e-01 ... -1.2310360e+02\n",
      "   2.4687334e+01  9.4130096e+01]\n",
      " [-1.7803946e+00  1.0000000e+00 -6.9054699e-01 ...  3.8417072e+02\n",
      "  -2.6834650e+02 -1.3967117e+02]\n",
      " [-1.7803574e+00  1.0000000e+00 -6.9060451e-01 ...  1.0523653e+00\n",
      "   3.2480931e+00  2.2380304e-01]\n",
      " ...\n",
      " [-1.7803745e+00  1.0000000e+00 -6.9053745e-01 ...  3.2587628e+02\n",
      "   2.9334573e+02  1.2463118e+02]\n",
      " [-1.7804222e+00  1.0000000e+00 -6.9046402e-01 ...  3.7989019e+03\n",
      "  -4.7736245e+03 -6.4272319e+03]\n",
      " [-1.7804375e+00  1.0000000e+00 -6.9039536e-01 ... -4.9690784e+02\n",
      "   2.3002234e+02 -3.1310025e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.6       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4799690e+00  1.0000000e+00 -1.2067776e+00 ...  1.0018669e+02\n",
      "   5.0461464e+01 -1.1903076e+01]\n",
      " [-1.4799385e+00  1.0000000e+00 -1.2068062e+00 ...  5.1950788e+00\n",
      "   1.6148577e+01  5.0926442e+00]\n",
      " [-1.4798985e+00  1.0000000e+00 -1.2068540e+00 ...  1.5198903e+00\n",
      "   2.5023675e+00  1.9231534e-01]\n",
      " ...\n",
      " [-1.4799213e+00  1.0000000e+00 -1.2067966e+00 ...  1.7505856e+00\n",
      "   1.2483303e+01  1.0903229e+01]\n",
      " [-1.4799957e+00  1.0000000e+00 -1.2067509e+00 ...  1.5539250e+03\n",
      "   1.4809071e+02  1.3276545e+03]\n",
      " [-1.4800329e+00  1.0000000e+00 -1.2066936e+00 ...  4.9327015e+01\n",
      "  -9.3422371e+01  1.3018375e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.3       0.        0.8000001 0.        0.6\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.9000001 0.        0.        0.        0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0343628     1.           -1.6051693  ...    7.173826\n",
      "   -22.511604     13.58841   ]\n",
      " [  -1.0343218     1.           -1.6052055  ... -108.39833\n",
      "  -791.9627     -595.32324   ]\n",
      " [  -1.0342865     1.           -1.6052322  ...    0.95856094\n",
      "    -0.8803853    -1.7220004 ]\n",
      " ...\n",
      " [  -1.0343208     1.           -1.605197   ...   -1.3811841\n",
      "   -21.714893      4.76264   ]\n",
      " [  -1.0344124     1.           -1.6051483  ...   83.77201\n",
      "   280.27603     257.83838   ]\n",
      " [  -1.0344496     1.           -1.6051121  ...  -16.792244\n",
      "   -75.511086    -86.85014   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.2       0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8761749e-01  1.0000000e+00 -1.8462601e+00 ...  2.4285786e+01\n",
      "  -5.7505302e+00  2.6665388e+01]\n",
      " [-4.8756886e-01  1.0000000e+00 -1.8462877e+00 ... -7.0557532e+02\n",
      "   1.8103789e+03 -1.5852439e+03]\n",
      " [-4.8750114e-01  1.0000000e+00 -1.8462940e+00 ...  1.2783127e+01\n",
      "   3.5147649e-01 -1.6248939e+00]\n",
      " ...\n",
      " [-4.8754692e-01  1.0000000e+00 -1.8462801e+00 ...  3.0945183e+01\n",
      "   1.2223853e+01  1.9968830e+01]\n",
      " [-4.8764610e-01  1.0000000e+00 -1.8462582e+00 ... -4.0674036e+02\n",
      "   5.9824890e+02 -3.4998267e+02]\n",
      " [-4.8772049e-01  1.0000000e+00 -1.8462467e+00 ...  8.4307739e+01\n",
      "  -8.6981702e-01  4.6442131e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.06879234e-01  1.00000000e+00 -1.90652084e+00 ... -6.10448364e+02\n",
      "  -4.83362488e+02  8.47667053e+02]\n",
      " [ 1.06927872e-01  1.00000000e+00 -1.90656281e+00 ... -2.53614471e+02\n",
      "   1.11078804e+02  2.22381042e+02]\n",
      " [ 1.07027054e-01  1.00000000e+00 -1.90653861e+00 ...  4.80845785e+00\n",
      "  -8.76094997e-02  1.62953854e-01]\n",
      " ...\n",
      " [ 1.06979370e-01  1.00000000e+00 -1.90655327e+00 ... -4.76653748e+01\n",
      "  -7.79950790e+01 -7.56854477e+01]\n",
      " [ 1.06880188e-01  1.00000000e+00 -1.90653229e+00 ...  6.88542252e+01\n",
      "  -3.27870903e+01 -1.92799149e+02]\n",
      " [ 1.06772423e-01  1.00000000e+00 -1.90654182e+00 ... -3.13557739e+02\n",
      "  -2.90833099e+02 -6.86652710e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.9088745e-01  1.0000000e+00 -1.7801151e+00 ... -2.1698570e+02\n",
      "   5.1508038e+01 -1.8619642e+01]\n",
      " [ 6.9093513e-01  1.0000000e+00 -1.7801819e+00 ... -6.4379346e+02\n",
      "   2.5101634e+02 -1.2925195e+02]\n",
      " [ 6.9101524e-01  1.0000000e+00 -1.7801336e+00 ... -3.1382227e+00\n",
      "  -1.6621081e+00 -5.2529424e-01]\n",
      " ...\n",
      " [ 6.9097328e-01  1.0000000e+00 -1.7801676e+00 ...  1.1780522e+02\n",
      "  -2.3974861e+01 -1.6119667e+01]\n",
      " [ 6.9087791e-01  1.0000000e+00 -1.7801323e+00 ...  4.1180412e+01\n",
      "  -6.4635536e+01  9.6368912e+01]\n",
      " [ 6.9078922e-01  1.0000000e+00 -1.7801743e+00 ...  9.3191583e+02\n",
      "  -2.6108494e+03  1.8159753e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2074652e+00  1.0000000e+00 -1.4792519e+00 ... -1.4559532e+01\n",
      "   4.0638123e+00 -1.5723904e+02]\n",
      " [ 1.2075062e+00  1.0000000e+00 -1.4793444e+00 ...  2.5482637e+01\n",
      "  -6.1897633e+01 -1.5631259e+02]\n",
      " [ 1.2075958e+00  1.0000000e+00 -1.4792705e+00 ...  3.4618711e+00\n",
      "  -3.8406426e-01  2.0422995e+00]\n",
      " ...\n",
      " [ 1.2075672e+00  1.0000000e+00 -1.4793215e+00 ...  8.3517654e+01\n",
      "   4.9233298e+00  3.5795483e+01]\n",
      " [ 1.2074909e+00  1.0000000e+00 -1.4792747e+00 ... -9.9273272e+00\n",
      "   9.9009666e+00  4.3606850e+01]\n",
      " [ 1.2073870e+00  1.0000000e+00 -1.4793415e+00 ... -9.1350922e+02\n",
      "   8.6674187e+01 -7.8283789e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.6053743     1.           -1.0339527  ...  -42.690453\n",
      "    72.30814      38.093224  ]\n",
      " [   1.6054077     1.           -1.0340347  ...  -37.332485\n",
      "    66.36611      53.38531   ]\n",
      " [   1.6054707     1.           -1.0339388  ...    2.0854998\n",
      "     1.72791       0.49153757]\n",
      " ...\n",
      " [   1.6054592     1.           -1.0340061  ... -351.2621\n",
      "   -43.585304     14.899504  ]\n",
      " [   1.605402      1.           -1.0339832  ...  142.97934\n",
      "   161.78581    -170.04169   ]\n",
      " [   1.6053181     1.           -1.0340786  ...   31.085302\n",
      "    -2.2403154    -7.8000507 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.0000001  0.         0.         0.70000005 0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8464375e+00  1.0000000e+00 -4.8695183e-01 ... -6.0266431e+02\n",
      "  -4.3685300e+02  7.9812134e+01]\n",
      " [ 1.8464651e+00  1.0000000e+00 -4.8700619e-01 ... -1.7245911e+02\n",
      "   8.2681897e+02  6.2439903e+01]\n",
      " [ 1.8465004e+00  1.0000000e+00 -4.8690313e-01 ...  2.0504451e+00\n",
      "   2.6245499e+00  5.2937657e-01]\n",
      " ...\n",
      " [ 1.8465061e+00  1.0000000e+00 -4.8697090e-01 ... -8.4011284e+01\n",
      "  -6.8164223e+01 -8.0878496e+00]\n",
      " [ 1.8464756e+00  1.0000000e+00 -4.8698235e-01 ... -8.3030890e+02\n",
      "  -1.4171552e+02 -8.5531628e+02]\n",
      " [ 1.8464203e+00  1.0000000e+00 -4.8709679e-01 ...  8.0406807e+01\n",
      "  -8.1276512e-01 -4.1392860e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90658092e+00  1.00000000e+00  1.07538223e-01 ... -3.57811089e+01\n",
      "  -1.25068649e+02 -1.16044350e+02]\n",
      " [ 1.90660667e+00  1.00000000e+00  1.07455254e-01 ...  2.43972305e+02\n",
      "  -2.58455780e+02  4.56454437e+02]\n",
      " [ 1.90663147e+00  1.00000000e+00  1.07554570e-01 ...  1.48265314e+00\n",
      "   2.05505657e+00  6.46966934e-01]\n",
      " ...\n",
      " [ 1.90664673e+00  1.00000000e+00  1.07490540e-01 ... -2.65796204e+02\n",
      "   1.49420746e+02 -4.07542816e+02]\n",
      " [ 1.90663147e+00  1.00000000e+00  1.07507706e-01 ...  2.12882019e+02\n",
      "  -1.35596899e+03 -1.12441544e+02]\n",
      " [ 1.90660095e+00  1.00000000e+00  1.07393265e-01 ...  7.02976227e+01\n",
      "   9.86623917e+01 -1.11666718e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 1.0000001  0.         0.4        1.0000001  0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7799311e+00  1.0000000e+00  6.9169235e-01 ...  1.2246437e+02\n",
      "   1.5221779e+02  2.8289923e+02]\n",
      " [ 1.7799597e+00  1.0000000e+00  6.9160652e-01 ...  6.5948004e+02\n",
      "   6.1063892e+02 -3.5959148e+01]\n",
      " [ 1.7799797e+00  1.0000000e+00  6.9170427e-01 ...  1.5434260e+00\n",
      "   1.7102752e+00  3.9271069e-01]\n",
      " ...\n",
      " [ 1.7800198e+00  1.0000000e+00  6.9163990e-01 ...  1.4912653e+02\n",
      "   3.2487179e+01 -2.8801584e+02]\n",
      " [ 1.7800217e+00  1.0000000e+00  6.9166565e-01 ...  2.6955136e+02\n",
      "  -5.5771660e+01 -6.5961603e+02]\n",
      " [ 1.7799902e+00  1.0000000e+00  6.9155312e-01 ...  9.1193680e+01\n",
      "  -6.2540947e+01 -1.0086494e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4792595e+00  1.0000000e+00  1.2076492e+00 ... -1.4297821e+01\n",
      "  -2.0957504e+01  1.3791022e+02]\n",
      " [ 1.4792957e+00  1.0000000e+00  1.2075510e+00 ...  3.5368787e+02\n",
      "   4.3165677e+02 -2.8533508e+02]\n",
      " [ 1.4792786e+00  1.0000000e+00  1.2076410e+00 ...  2.4972901e+00\n",
      "   2.6580977e+00  3.4090328e-01]\n",
      " ...\n",
      " [ 1.4793415e+00  1.0000000e+00  1.2075796e+00 ...  6.8779102e+02\n",
      "  -2.3771379e+02  4.2694553e+01]\n",
      " [ 1.4793434e+00  1.0000000e+00  1.2076283e+00 ... -4.1983450e+02\n",
      "  -3.8054724e+02 -3.5798233e+02]\n",
      " [ 1.4793568e+00  1.0000000e+00  1.2075329e+00 ... -2.5163872e+01\n",
      "   5.5260811e+01  5.7074730e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.5       0.\n",
      " 1.0000001 0.3       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.033597      1.            1.6056824  ...  -49.937893\n",
      "   -14.546068     22.49339   ]\n",
      " [   1.0336428     1.            1.605588   ...  -28.78304\n",
      "  -504.36917     -40.891975  ]\n",
      " [   1.0336208     1.            1.6056601  ...    4.6007376\n",
      "    12.404208     -0.57184213]\n",
      " ...\n",
      " [   1.0336952     1.            1.6056137  ...  -79.52258\n",
      "   273.89532     -12.029063  ]\n",
      " [   1.033699      1.            1.6056652  ...  -56.04997\n",
      "   -96.233765    128.11465   ]\n",
      " [   1.0337238     1.            1.6055946  ...  -88.34687\n",
      "    20.563189    -88.898224  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.8643017e-01  1.0000000e+00  1.8466301e+00 ... -2.0256901e+01\n",
      "  -2.1245876e+01  2.5201263e+01]\n",
      " [ 4.8647881e-01  1.0000000e+00  1.8465500e+00 ...  1.7450343e+02\n",
      "   5.8388702e+02  1.3797246e+02]\n",
      " [ 4.8644829e-01  1.0000000e+00  1.8465897e+00 ...  2.4224379e+00\n",
      "   9.3765736e+00  1.7913625e+00]\n",
      " ...\n",
      " [ 4.8652649e-01  1.0000000e+00  1.8465672e+00 ... -7.6519371e+01\n",
      "  -1.0489126e+01  7.8457130e+01]\n",
      " [ 4.8653221e-01  1.0000000e+00  1.8466206e+00 ...  9.1902847e+01\n",
      "   1.3705028e+02  1.4263858e+02]\n",
      " [ 4.8656654e-01  1.0000000e+00  1.8465748e+00 ... -8.9757217e+01\n",
      "  -3.7204571e+01 -8.9844025e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.07973099e-01  1.00000000e+00  1.90656281e+00 ... -8.94369049e+01\n",
      "   4.18941784e+00  7.39338989e+01]\n",
      " [-1.07921600e-01  1.00000000e+00  1.90649319e+00 ...  1.39535248e+02\n",
      "  -3.70333649e+02 -9.12280273e+00]\n",
      " [-1.07944489e-01  1.00000000e+00  1.90652049e+00 ... -4.39492404e-01\n",
      "   3.44030547e+00  7.93846369e-01]\n",
      " ...\n",
      " [-1.07864380e-01  1.00000000e+00  1.90650749e+00 ...  1.65919922e+02\n",
      "   1.44498734e+02  1.92199554e+02]\n",
      " [-1.07858658e-01  1.00000000e+00  1.90657234e+00 ... -1.56489490e+03\n",
      "  -3.39454773e+02  1.75914062e+02]\n",
      " [-1.07829094e-01  1.00000000e+00  1.90655518e+00 ... -1.46566177e+02\n",
      "   2.86255417e+01  1.10626083e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.9218540e-01  1.0000000e+00  1.7797852e+00 ... -2.7915083e+01\n",
      "   2.6670601e+01  7.4884354e+01]\n",
      " [-6.9213676e-01  1.0000000e+00  1.7797222e+00 ...  9.4995262e+01\n",
      "   9.6731407e+01 -3.9748358e+02]\n",
      " [-6.9214630e-01  1.0000000e+00  1.7797270e+00 ... -1.3304922e-01\n",
      "   8.0203114e+00  1.7143149e+00]\n",
      " ...\n",
      " [-6.9207191e-01  1.0000000e+00  1.7797384e+00 ... -3.6739491e+01\n",
      "  -1.3289333e+01 -2.5407719e+01]\n",
      " [-6.9206619e-01  1.0000000e+00  1.7798100e+00 ... -4.2266638e+02\n",
      "   6.1365936e+02 -7.6746082e+02]\n",
      " [-6.9204998e-01  1.0000000e+00  1.7798214e+00 ...  1.3626477e+02\n",
      "  -2.0249335e+02 -1.8725203e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.20795059e+00  1.00000000e+00  1.47883034e+00 ...  1.26696312e+02\n",
      "   5.93505669e+00  1.95855582e+00]\n",
      " [-1.20790863e+00  1.00000000e+00  1.47880173e+00 ... -4.83232605e+02\n",
      "   9.43985107e+02  7.47899780e+01]\n",
      " [-1.20792580e+00  1.00000000e+00  1.47878087e+00 ... -1.11958516e+00\n",
      "   2.36445665e+00  1.06002188e+00]\n",
      " ...\n",
      " [-1.20785713e+00  1.00000000e+00  1.47881985e+00 ... -3.96077871e+00\n",
      "  -1.46739388e+01  8.90355396e+00]\n",
      " [-1.20785332e+00  1.00000000e+00  1.47887802e+00 ... -6.97997513e+01\n",
      "  -1.15317604e+02 -1.55070892e+02]\n",
      " [-1.20783234e+00  1.00000000e+00  1.47890663e+00 ...  3.20021782e+01\n",
      "   6.05981731e+00  7.22692966e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6061239e+00  1.0000000e+00  1.0327606e+00 ...  1.4778494e+02\n",
      "   4.6091888e+01  2.5967041e+01]\n",
      " [-1.6060925e+00  1.0000000e+00  1.0327244e+00 ... -9.3876600e+00\n",
      "   1.9096434e+01 -1.0856192e+02]\n",
      " [-1.6061172e+00  1.0000000e+00  1.0327009e+00 ... -7.3218453e-01\n",
      "  -1.8481970e-01  5.6387365e-01]\n",
      " ...\n",
      " [-1.6060658e+00  1.0000000e+00  1.0327473e+00 ...  1.6819035e+02\n",
      "  -2.4410034e+02  1.0549035e+02]\n",
      " [-1.6060677e+00  1.0000000e+00  1.0328178e+00 ...  1.5093179e+02\n",
      "   5.8955765e+00  1.0920121e+02]\n",
      " [-1.6060381e+00  1.0000000e+00  1.0328560e+00 ...  2.0203972e+01\n",
      "  -3.8936707e+02 -2.1879642e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.8000001 0.2\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8466148e+00  1.0000000e+00  4.8610878e-01 ...  9.2196404e+01\n",
      "   2.6486978e+02 -4.7664295e+01]\n",
      " [-1.8465948e+00  1.0000000e+00  4.8606014e-01 ... -3.3779095e+01\n",
      "   3.3054327e+02  1.4663115e+02]\n",
      " [-1.8466339e+00  1.0000000e+00  4.8604420e-01 ...  1.0765173e+00\n",
      "  -3.4704208e-03  6.8987596e-01]\n",
      " ...\n",
      " [-1.8465958e+00  1.0000000e+00  4.8608589e-01 ... -7.1837555e+01\n",
      "  -6.1187981e+01  5.1105919e+01]\n",
      " [-1.8466015e+00  1.0000000e+00  4.8618317e-01 ... -4.8983603e+02\n",
      "   1.2546873e+03 -1.4044155e+03]\n",
      " [-1.8465681e+00  1.0000000e+00  4.8623276e-01 ... -1.6985735e+02\n",
      "   9.5505363e+01 -5.8798595e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90647030e+00  1.00000000e+00 -1.08430862e-01 ...  1.61974213e+02\n",
      "  -6.89840088e+01  1.85689774e+01]\n",
      " [-1.90645409e+00  1.00000000e+00 -1.08478546e-01 ...  1.63435455e+02\n",
      "  -3.85966797e+01  2.34826965e+02]\n",
      " [-1.90648651e+00  1.00000000e+00 -1.08496025e-01 ... -3.82976675e+00\n",
      "   3.74893808e+00  1.17565560e+00]\n",
      " ...\n",
      " [-1.90647316e+00  1.00000000e+00 -1.08452797e-01 ...  6.17357941e+01\n",
      "  -1.80240707e+02 -3.14030313e+00]\n",
      " [-1.90649223e+00  1.00000000e+00 -1.08356476e-01 ...  3.86459259e+02\n",
      "   1.58051853e+01  4.57756287e+02]\n",
      " [-1.90647316e+00  1.00000000e+00 -1.08304977e-01 ... -1.12360954e+02\n",
      "   1.44583511e+02  4.01122070e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         1.0000001\n",
      " 1.0000001  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7795992e+00  1.0000000e+00 -6.9248581e-01 ...  3.1178204e+01\n",
      "   7.9218239e+01  4.1062397e+01]\n",
      " [-1.7795877e+00  1.0000000e+00 -6.9253635e-01 ... -1.1980497e+02\n",
      "   2.9664206e+02 -2.4051370e+02]\n",
      " [-1.7796631e+00  1.0000000e+00 -6.9254470e-01 ...  8.0402002e+00\n",
      "  -4.4840350e+00  2.5800437e-01]\n",
      " ...\n",
      " [-1.7796669e+00  1.0000000e+00 -6.9251060e-01 ...  6.5832495e+02\n",
      "   2.6562578e+01 -1.2562971e+02]\n",
      " [-1.7797031e+00  1.0000000e+00 -6.9241905e-01 ... -5.9537826e+00\n",
      "   9.6996908e+00 -1.6771778e+01]\n",
      " [-1.7796469e+00  1.0000000e+00 -6.9237328e-01 ...  6.3869915e+01\n",
      "   5.3392677e+01 -1.5857059e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        0.        0.        0.        0.8000001 1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.5       1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.47840214e+00  1.00000000e+00 -1.20871162e+00 ... -2.43492584e+01\n",
      "   3.04117012e+01  4.82037354e+01]\n",
      " [-1.47839165e+00  1.00000000e+00 -1.20876217e+00 ... -8.39380188e+01\n",
      "  -1.01824196e+02 -8.47265091e+01]\n",
      " [-1.47847366e+00  1.00000000e+00 -1.20876729e+00 ...  1.98115873e+00\n",
      "  -7.51782656e-01 -5.77475309e-01]\n",
      " ...\n",
      " [-1.47849083e+00  1.00000000e+00 -1.20874119e+00 ...  8.21347839e+02\n",
      "   3.43550453e+01 -2.82128326e+02]\n",
      " [-1.47853279e+00  1.00000000e+00 -1.20866013e+00 ... -8.15186890e+02\n",
      "   1.31157303e+02 -3.22662537e+02]\n",
      " [-1.47848797e+00  1.00000000e+00 -1.20861626e+00 ...  1.28146021e+03\n",
      "  -7.48602417e+02  1.38075781e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0324640e+00  1.0000000e+00 -1.6064453e+00 ...  5.0768514e+00\n",
      "   8.4335075e+01 -1.6509752e+02]\n",
      " [-1.0324516e+00  1.0000000e+00 -1.6064978e+00 ...  2.1737048e+02\n",
      "  -2.2587429e+01 -1.9368713e+02]\n",
      " [-1.0325050e+00  1.0000000e+00 -1.6064930e+00 ...  5.4514170e-01\n",
      "   2.3243666e-01 -3.2193696e-01]\n",
      " ...\n",
      " [-1.0325375e+00  1.0000000e+00 -1.6064854e+00 ...  1.7440926e+02\n",
      "   4.3031952e+01 -1.2826358e+02]\n",
      " [-1.0325890e+00  1.0000000e+00 -1.6064148e+00 ...  4.0361404e+02\n",
      "   2.0117682e+02  7.8812347e+01]\n",
      " [-1.0325766e+00  1.0000000e+00 -1.6063824e+00 ... -7.2902448e+02\n",
      "  -1.0356238e+03  4.2582851e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.5 0.4 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8561859e-01  1.0000000e+00 -1.8468342e+00 ...  7.1693901e+01\n",
      "   1.7965514e+02  2.4151875e+01]\n",
      " [-4.8560524e-01  1.0000000e+00 -1.8468971e+00 ... -3.7958863e+00\n",
      "   4.0904194e+01  3.9411079e+01]\n",
      " [-4.8566055e-01  1.0000000e+00 -1.8468909e+00 ...  2.8700972e-01\n",
      "  -2.5097537e-01 -8.1881166e-02]\n",
      " ...\n",
      " [-4.8569489e-01  1.0000000e+00 -1.8468952e+00 ...  3.3368094e+02\n",
      "  -2.6615932e+01 -3.1420013e+02]\n",
      " [-4.8575783e-01  1.0000000e+00 -1.8468342e+00 ... -1.9381255e+02\n",
      "  -1.8142275e+01  1.1914041e+02]\n",
      " [-4.8574924e-01  1.0000000e+00 -1.8468132e+00 ...  5.6456070e+01\n",
      "   5.9608923e+02  1.4632669e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0919571e-01  1.0000000e+00 -1.9064922e+00 ... -1.8242361e+02\n",
      "  -8.5655922e+01  4.9054398e+01]\n",
      " [ 1.0920906e-01  1.0000000e+00 -1.9065266e+00 ... -1.5608715e+01\n",
      "  -2.6387143e+02 -1.4688480e+02]\n",
      " [ 1.0915375e-01  1.0000000e+00 -1.9065375e+00 ...  7.1753979e-02\n",
      "  -3.1281519e-01  1.5684724e-02]\n",
      " ...\n",
      " [ 1.0911751e-01  1.0000000e+00 -1.9065428e+00 ... -3.0512897e+01\n",
      "  -2.7462107e+01  1.9232231e+01]\n",
      " [ 1.0904884e-01  1.0000000e+00 -1.9065247e+00 ... -5.6852887e+02\n",
      "  -6.7260522e+02  5.3137157e+01]\n",
      " [ 1.0906029e-01  1.0000000e+00 -1.9065094e+00 ...  4.4167576e+01\n",
      "  -5.2907417e+01 -2.4636389e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.9290257e-01  1.0000000e+00 -1.7794285e+00 ...  2.3429639e+00\n",
      "   6.9646980e+01  2.2010180e+01]\n",
      " [ 6.9291592e-01  1.0000000e+00 -1.7794943e+00 ... -9.6879486e+01\n",
      "   2.9324169e+01  5.1857841e+01]\n",
      " [ 6.9287872e-01  1.0000000e+00 -1.7794957e+00 ... -4.4935703e-02\n",
      "  -1.7653036e-01  3.1814158e-02]\n",
      " ...\n",
      " [ 6.9284821e-01  1.0000000e+00 -1.7795343e+00 ...  7.9959976e+01\n",
      "  -4.7125931e+01 -2.4908069e+02]\n",
      " [ 6.9278526e-01  1.0000000e+00 -1.7795143e+00 ...  2.4934907e+03\n",
      "  -1.8737617e+03  9.1236823e+02]\n",
      " [ 6.9277477e-01  1.0000000e+00 -1.7795029e+00 ... -4.3779761e+03\n",
      "  -2.7933428e+03  6.1884448e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2089205e+00  1.0000000e+00 -1.4781990e+00 ...  4.2275977e+02\n",
      "  -2.6644592e+02  1.1184180e+02]\n",
      " [ 1.2089329e+00  1.0000000e+00 -1.4782600e+00 ...  1.7926294e+02\n",
      "  -3.7414925e+01  5.7365910e+01]\n",
      " [ 1.2088795e+00  1.0000000e+00 -1.4782851e+00 ...  1.2833695e+00\n",
      "  -1.1431482e+00 -2.4100304e-02]\n",
      " ...\n",
      " [ 1.2088547e+00  1.0000000e+00 -1.4783278e+00 ... -1.6923977e+01\n",
      "  -6.4572205e+01 -3.5663158e+01]\n",
      " [ 1.2088013e+00  1.0000000e+00 -1.4783268e+00 ...  8.2723480e+02\n",
      "   5.8980090e+02 -3.6210635e+02]\n",
      " [ 1.2088079e+00  1.0000000e+00 -1.4783211e+00 ...  4.3226324e+02\n",
      "  -3.0084790e+02 -8.1059088e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.6066732    1.          -1.0319805 ...  805.07135   -732.9423\n",
      "   997.7769   ]\n",
      " [   1.6066856    1.          -1.032033  ...  257.2512      44.345253\n",
      "    -4.3962035]\n",
      " [   1.6066437    1.          -1.0320716 ...    5.1899037   -4.529208\n",
      "    -1.090593 ]\n",
      " ...\n",
      " [   1.6066227    1.          -1.0321169 ... -318.4734     -34.016674\n",
      "   132.86969  ]\n",
      " [   1.6065922    1.          -1.0321331 ...  -32.79339   -101.06774\n",
      "   119.21559  ]\n",
      " [   1.6066027    1.          -1.0321331 ...  -24.412977  -104.61432\n",
      "   184.03903  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8468523e+00  1.0000000e+00 -4.8475838e-01 ...  9.2347374e+01\n",
      "  -4.4194412e+02 -5.6078815e+01]\n",
      " [ 1.8468676e+00  1.0000000e+00 -4.8479366e-01 ... -1.1919055e+02\n",
      "   9.5994957e+01 -2.1655149e+01]\n",
      " [ 1.8468094e+00  1.0000000e+00 -4.8484153e-01 ...  2.4574511e+00\n",
      "  -2.8776300e+00 -3.9716130e-01]\n",
      " ...\n",
      " [ 1.8467941e+00  1.0000000e+00 -4.8488712e-01 ...  7.8092354e+01\n",
      "  -3.3095852e+01  5.9767013e+00]\n",
      " [ 1.8467846e+00  1.0000000e+00 -4.8492432e-01 ...  1.0656982e+02\n",
      "  -5.1700523e+01  9.2834572e+01]\n",
      " [ 1.8468390e+00  1.0000000e+00 -4.8492813e-01 ... -4.0359844e+01\n",
      "   8.0941107e+02  6.7676935e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.5       0.        0.        0.        0.2\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:4, Score:6.01, Best Score:15.21, Average Score:10.10, Best Avg Score:12.04\n",
      "Episode number: 5\n",
      "Hand Exit\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7f2d81ed9970>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7791328e+00  1.0000000e+00  6.9354057e-01 ... -3.8095070e+01\n",
      "  -9.7722839e+01 -8.1560684e+01]\n",
      " [ 1.7791510e+00  1.0000000e+00  6.9346428e-01 ...  4.1005630e+02\n",
      "  -1.5856955e+02  2.2551500e+02]\n",
      " [ 1.7791061e+00  1.0000000e+00  6.9342452e-01 ... -2.4781084e-01\n",
      "  -3.3147750e+00 -1.2480264e+00]\n",
      " ...\n",
      " [ 1.7791100e+00  1.0000000e+00  6.9337368e-01 ...  1.7096741e+02\n",
      "   4.2828992e+02  7.8159137e+02]\n",
      " [ 1.7791309e+00  1.0000000e+00  6.9336891e-01 ... -3.3514320e+01\n",
      "   3.4900795e+01  3.1100178e+01]\n",
      " [ 1.7792158e+00  1.0000000e+00  6.9336510e-01 ...  1.6097394e+02\n",
      "   9.4778542e+01 -9.1821434e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.70000005 0.         0.         0.2\n",
      " 0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.47789383e+00  1.00000000e+00  1.20929337e+00 ... -1.52294312e+01\n",
      "   4.28489914e+01 -3.20528564e+01]\n",
      " [ 1.47791386e+00  1.00000000e+00  1.20922184e+00 ... -6.97597809e+01\n",
      "  -1.70363190e+02 -2.18869125e+02]\n",
      " [ 1.47786331e+00  1.00000000e+00  1.20918334e+00 ... -5.84371209e-01\n",
      "  -1.08933907e+01 -2.24154210e+00]\n",
      " ...\n",
      " [ 1.47787476e+00  1.00000000e+00  1.20914459e+00 ...  2.64035217e+02\n",
      "   2.27658951e+02  1.20549170e+03]\n",
      " [ 1.47790146e+00  1.00000000e+00  1.20913696e+00 ...  1.17086517e+02\n",
      "   1.49745956e+02  1.03879196e+02]\n",
      " [ 1.47801590e+00  1.00000000e+00  1.20913506e+00 ...  8.79741440e+01\n",
      "   6.30699272e+01  6.38498840e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0315323e+00  1.0000000e+00  1.6070766e+00 ...  6.0382584e+01\n",
      "   1.1844330e+01 -3.1909641e+01]\n",
      " [ 1.0315571e+00  1.0000000e+00  1.6069984e+00 ...  8.7417259e+01\n",
      "  -3.9928387e+01 -3.2613376e+02]\n",
      " [ 1.0315056e+00  1.0000000e+00  1.6069627e+00 ... -3.6268258e-01\n",
      "   1.3510380e+00  2.1959639e-01]\n",
      " ...\n",
      " [ 1.0315266e+00  1.0000000e+00  1.6069374e+00 ...  3.2076108e+02\n",
      "  -4.9497051e+00 -7.1579614e+02]\n",
      " [ 1.0315609e+00  1.0000000e+00  1.6069527e+00 ...  2.8393116e+00\n",
      "   1.2018421e+00 -3.3158165e+01]\n",
      " [ 1.0316877e+00  1.0000000e+00  1.6069527e+00 ...  7.0202087e+01\n",
      "   2.8799937e+00 -1.6265512e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.8463726e-01  1.0000000e+00  1.8471584e+00 ... -3.9186611e+01\n",
      "   9.1532249e+00 -2.2304411e+01]\n",
      " [ 4.8466778e-01  1.0000000e+00  1.8470831e+00 ... -7.4228722e+01\n",
      "   2.2605783e+02 -3.1027750e+02]\n",
      " [ 4.8460197e-01  1.0000000e+00  1.8470502e+00 ... -1.3983274e-01\n",
      "  -2.4780927e+00 -5.3324050e-01]\n",
      " ...\n",
      " [ 4.8463058e-01  1.0000000e+00  1.8470373e+00 ...  3.1711040e+01\n",
      "   6.6267322e+02 -7.8327797e+01]\n",
      " [ 4.8466873e-01  1.0000000e+00  1.8470840e+00 ... -6.3138351e+00\n",
      "  -8.2432384e+00  3.0031276e-01]\n",
      " [ 4.8481941e-01  1.0000000e+00  1.8470936e+00 ...  1.4425367e+01\n",
      "  -9.3297607e+01  8.3423491e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.10089302e-01  1.00000000e+00  1.90651894e+00 ...  1.56775141e+01\n",
      "   6.63397980e+01 -8.60451584e+01]\n",
      " [-1.10058784e-01  1.00000000e+00  1.90641975e+00 ...  5.51786766e+01\n",
      "  -7.37087097e+01  1.82459564e+01]\n",
      " [-1.10113144e-01  1.00000000e+00  1.90640616e+00 ...  5.20106077e-01\n",
      "   1.47387886e+00  1.90673172e-01]\n",
      " ...\n",
      " [-1.10082626e-01  1.00000000e+00  1.90639210e+00 ...  1.33931238e+03\n",
      "  -7.86840430e+03  8.87513965e+03]\n",
      " [-1.10042572e-01  1.00000000e+00  1.90647697e+00 ... -4.68862915e+02\n",
      "   2.71183662e+01  3.03738739e+02]\n",
      " [-1.09902382e-01  1.00000000e+00  1.90650558e+00 ...  1.44001633e+02\n",
      "  -3.05994682e+01  1.89172562e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.6938877     1.            1.7791557  ...  -34.171417\n",
      "    88.48252    -166.49333   ]\n",
      " [  -0.6938591     1.            1.7790966  ...   -7.28491\n",
      "   -20.421135    -18.665413  ]\n",
      " [  -0.693882      1.            1.7790865  ...    2.8061767\n",
      "     9.673418      0.9576157 ]\n",
      " ...\n",
      " [  -0.6938553     1.            1.7790833  ...  232.9993\n",
      "  -658.1243      221.25111   ]\n",
      " [  -0.69381523    1.            1.7791462  ...   75.9177\n",
      "   -14.1160345   -55.68293   ]\n",
      " [  -0.6937094     1.            1.779192   ...  104.230385\n",
      "    35.456306   -128.4399    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.2098074    1.           1.4775543 ...   69.78558    415.6827\n",
      "  -161.05347  ]\n",
      " [  -1.2097826    1.           1.4775295 ...  -13.702817    69.8955\n",
      "   -49.800755 ]\n",
      " [  -1.2097816    1.           1.4775132 ...    1.555459     6.1728077\n",
      "     2.3093028]\n",
      " ...\n",
      " [  -1.2097569    1.           1.4775286 ... -466.13223    324.67776\n",
      "   200.60619  ]\n",
      " [  -1.2097206    1.           1.4775658 ...  -56.0131    -129.9455\n",
      "  -151.07658  ]\n",
      " [  -1.2096596    1.           1.4776268 ... -144.59149   -233.9256\n",
      "   249.66568  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6072855e+00  1.0000000e+00  1.0312290e+00 ...  3.2997913e+01\n",
      "   6.8268074e+01 -1.6338420e+02]\n",
      " [-1.6072655e+00  1.0000000e+00  1.0311775e+00 ...  2.3661699e+01\n",
      "  -5.6582458e+01  1.6758862e+00]\n",
      " [-1.6072826e+00  1.0000000e+00  1.0311722e+00 ... -1.0307169e+00\n",
      "   4.1069369e+00  1.3935301e+00]\n",
      " ...\n",
      " [-1.6072674e+00  1.0000000e+00  1.0311823e+00 ...  6.7449268e+02\n",
      "   1.0087537e+03  9.6397485e+02]\n",
      " [-1.6072407e+00  1.0000000e+00  1.0312672e+00 ...  8.3769550e+00\n",
      "  -2.3190974e+02  4.3525061e+02]\n",
      " [-1.6071739e+00  1.0000000e+00  1.0313416e+00 ... -4.8962460e+02\n",
      "   5.5952222e+02 -4.9345969e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8473272     1.            0.48389816 ...  138.38132\n",
      "  -449.02194      19.662294  ]\n",
      " [  -1.8473091     1.            0.48384285 ...  -38.566814\n",
      "   339.17688     314.78027   ]\n",
      " [  -1.8473244     1.            0.48384494 ...   -0.9582569\n",
      "     6.628416      1.0692325 ]\n",
      " ...\n",
      " [  -1.8473244     1.            0.48385334 ...  -61.27228\n",
      "   302.7216     -387.18964   ]\n",
      " [  -1.8473091     1.            0.48394775 ...  -11.799385\n",
      "   -12.25705     122.64084   ]\n",
      " [  -1.8472462     1.            0.4840374  ...   10.882817\n",
      "    13.676506    -22.438358  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90647030e+00  1.00000000e+00 -1.10626221e-01 ... -2.61642731e+02\n",
      "   1.20502769e+02 -7.61150131e+01]\n",
      " [-1.90645027e+00  1.00000000e+00 -1.10678673e-01 ...  2.65610905e+01\n",
      "  -3.35029373e+01  3.58150635e+01]\n",
      " [-1.90642929e+00  1.00000000e+00 -1.10688969e-01 ... -3.39212871e+00\n",
      "   6.09715796e+00  1.75415790e+00]\n",
      " ...\n",
      " [-1.90643692e+00  1.00000000e+00 -1.10667229e-01 ...  8.06085892e+01\n",
      "  -6.83748627e+01 -5.65927172e+00]\n",
      " [-1.90644264e+00  1.00000000e+00 -1.10572815e-01 ...  5.18092285e+03\n",
      "   4.02737866e+03 -1.12965808e+03]\n",
      " [-1.90643692e+00  1.00000000e+00 -1.10477448e-01 ...  1.98868774e+02\n",
      "   2.85971588e+02 -1.86160995e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.3       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7790222e+00  1.0000000e+00 -6.9422722e-01 ...  7.4444870e+01\n",
      "   3.9948524e+01 -2.2933743e+01]\n",
      " [-1.7789993e+00  1.0000000e+00 -6.9429016e-01 ...  2.9602440e+01\n",
      "   5.8807877e+01  8.5443047e+01]\n",
      " [-1.7789536e+00  1.0000000e+00 -6.9429934e-01 ... -3.5782957e-01\n",
      "   1.7867060e+00  4.5280057e-01]\n",
      " ...\n",
      " [-1.7789688e+00  1.0000000e+00 -6.9427872e-01 ...  3.3380711e+01\n",
      "  -6.1870155e+01 -1.6004944e+01]\n",
      " [-1.7789898e+00  1.0000000e+00 -6.9417953e-01 ... -3.0303738e+03\n",
      "   6.4920923e+02 -3.9020081e+03]\n",
      " [-1.7790422e+00  1.0000000e+00 -6.9409370e-01 ...  6.5415894e+01\n",
      "  -1.1927968e+01  8.0170143e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.9000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4772072e+00  1.0000000e+00 -1.2102013e+00 ... -6.3545307e+01\n",
      "   3.7860775e+01 -7.4123344e+01]\n",
      " [-1.4771776e+00  1.0000000e+00 -1.2102833e+00 ...  6.2226521e+01\n",
      "  -1.1528817e+02  2.4611694e+02]\n",
      " [-1.4771042e+00  1.0000000e+00 -1.2102846e+00 ...  1.5212498e+00\n",
      "  -1.6972065e+00 -4.6349931e-01]\n",
      " ...\n",
      " [-1.4771366e+00  1.0000000e+00 -1.2102766e+00 ... -1.6860497e+01\n",
      "   4.7495960e+01  3.4461563e+01]\n",
      " [-1.4771786e+00  1.0000000e+00 -1.2101612e+00 ...  5.7863690e+02\n",
      "  -4.2844955e+02  1.3444482e+03]\n",
      " [-1.4772730e+00  1.0000000e+00 -1.2100887e+00 ...  1.8306525e+02\n",
      "  -2.2803606e+02 -1.7292671e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.1       0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0311794e+00  1.0000000e+00 -1.6072350e+00 ... -5.0754093e+01\n",
      "   5.3990952e+01  1.4971677e+02]\n",
      " [-1.0311451e+00  1.0000000e+00 -1.6073551e+00 ...  2.1275148e+01\n",
      "  -7.3241272e+01  6.7226974e+01]\n",
      " [-1.0310230e+00  1.0000000e+00 -1.6073476e+00 ... -1.3823116e+00\n",
      "   2.0915956e+00  7.5958586e-01]\n",
      " ...\n",
      " [-1.0310631e+00  1.0000000e+00 -1.6073503e+00 ... -2.3007280e+02\n",
      "  -7.7371063e+01 -2.3541862e+01]\n",
      " [-1.0311184e+00  1.0000000e+00 -1.6072102e+00 ... -6.7963242e+01\n",
      "   7.3498642e+01  3.2670544e+01]\n",
      " [-1.0312738e+00  1.0000000e+00 -1.6071587e+00 ... -7.8526080e+02\n",
      "   2.0551157e+02 -8.5596735e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.2 0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8373222e-01  1.0000000e+00 -1.8472538e+00 ... -2.6372965e+01\n",
      "   1.6001915e+01 -3.7995361e+01]\n",
      " [-4.8369312e-01  1.0000000e+00 -1.8473330e+00 ... -5.8096539e+01\n",
      "  -2.7093334e+01  3.1352345e+01]\n",
      " [-4.8356247e-01  1.0000000e+00 -1.8473345e+00 ... -7.9813738e+00\n",
      "   9.7484016e+00  2.1854930e+00]\n",
      " ...\n",
      " [-4.8361015e-01  1.0000000e+00 -1.8473263e+00 ...  5.0487331e+01\n",
      "   1.2843648e+02 -2.5263081e+00]\n",
      " [-4.8367119e-01  1.0000000e+00 -1.8472443e+00 ... -5.0201942e+01\n",
      "  -1.1451162e+01  4.5841321e+02]\n",
      " [-4.8385048e-01  1.0000000e+00 -1.8472176e+00 ...  3.3646561e+02\n",
      "   4.9958713e+02 -2.4583533e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.11054420e-01  1.00000000e+00 -1.90626335e+00 ... -1.07792404e+02\n",
      "   9.09802933e+01  2.71431580e+02]\n",
      " [ 1.11095428e-01  1.00000000e+00 -1.90633011e+00 ... -9.86240768e+01\n",
      "  -1.24292984e+02  1.35751907e+02]\n",
      " [ 1.11257553e-01  1.00000000e+00 -1.90631592e+00 ... -3.39137554e-01\n",
      "   1.22731113e+00 -1.98946089e-01]\n",
      " ...\n",
      " [ 1.11209869e-01  1.00000000e+00 -1.90631676e+00 ...  1.35018890e+02\n",
      "   1.16585579e+02 -1.74856918e+02]\n",
      " [ 1.11145020e-01  1.00000000e+00 -1.90625763e+00 ...  1.26529922e+01\n",
      "   8.21625671e+01 -1.27519180e+02]\n",
      " [ 1.10933304e-01  1.00000000e+00 -1.90627098e+00 ... -8.57062149e+01\n",
      "  -3.36968201e+02  5.44409363e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.9481659e-01  1.0000000e+00 -1.7785721e+00 ... -1.2497551e+01\n",
      "   3.8422986e+03 -2.8211557e+02]\n",
      " [ 6.9485378e-01  1.0000000e+00 -1.7786236e+00 ...  2.3180228e+02\n",
      "  -8.7782879e+00 -3.6695881e+01]\n",
      " [ 6.9502640e-01  1.0000000e+00 -1.7785777e+00 ... -1.4287221e-01\n",
      "   3.8995352e+00 -1.2085885e+00]\n",
      " ...\n",
      " [ 6.9497871e-01  1.0000000e+00 -1.7785931e+00 ... -1.5214378e+02\n",
      "   5.0596241e+01 -1.4624988e+02]\n",
      " [ 6.9491577e-01  1.0000000e+00 -1.7785645e+00 ... -2.1052947e+03\n",
      "  -1.7409518e+02 -4.8770679e+03]\n",
      " [ 6.9470501e-01  1.0000000e+00 -1.7786312e+00 ...  4.2265967e+02\n",
      "  -4.4346657e+01  1.7408194e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2102671e+00  1.0000000e+00 -1.4769382e+00 ... -3.6242908e+03\n",
      "   1.4560947e+03  1.4750725e+02]\n",
      " [ 1.2102995e+00  1.0000000e+00 -1.4769878e+00 ... -4.0789886e+01\n",
      "   1.3862367e+01 -3.0612371e+01]\n",
      " [ 1.2104588e+00  1.0000000e+00 -1.4769071e+00 ... -9.7285217e-01\n",
      "   3.5480721e+00 -1.2681894e+00]\n",
      " ...\n",
      " [ 1.2104168e+00  1.0000000e+00 -1.4769354e+00 ...  2.4622499e+02\n",
      "   1.6388708e+02 -5.6547409e+01]\n",
      " [ 1.2103558e+00  1.0000000e+00 -1.4769287e+00 ...  6.6464099e+02\n",
      "  -3.0054077e+02  3.0676727e+02]\n",
      " [ 1.2101822e+00  1.0000000e+00 -1.4770336e+00 ...  1.9173248e-01\n",
      "   3.4482620e+01 -4.4065552e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6074610e+00  1.0000000e+00 -1.0306015e+00 ... -3.8814868e+03\n",
      "  -1.3241659e+03 -4.9113544e+02]\n",
      " [ 1.6074867e+00  1.0000000e+00 -1.0306940e+00 ...  9.6400024e+01\n",
      "  -5.6354901e+02 -8.2853491e+02]\n",
      " [ 1.6076164e+00  1.0000000e+00 -1.0305923e+00 ...  3.4064498e+00\n",
      "   1.3993103e+01  1.2008569e+00]\n",
      " ...\n",
      " [ 1.6075840e+00  1.0000000e+00 -1.0306282e+00 ...  2.6659297e+03\n",
      "  -1.9723329e+03  2.5362632e+03]\n",
      " [ 1.6075401e+00  1.0000000e+00 -1.0305843e+00 ...  8.3446198e+00\n",
      "   4.2412656e+02 -2.1680020e+02]\n",
      " [ 1.6074133e+00  1.0000000e+00 -1.0307236e+00 ...  7.4369427e+02\n",
      "  -3.1801080e+02  5.1587000e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8474236e+00  1.0000000e+00 -4.8300362e-01 ...  6.7062726e+02\n",
      "   1.4351468e+02 -3.9215872e+02]\n",
      " [ 1.8474445e+00  1.0000000e+00 -4.8307800e-01 ... -8.3232246e+01\n",
      "   4.7575757e+02 -3.3466843e+01]\n",
      " [ 1.8475361e+00  1.0000000e+00 -4.8296678e-01 ... -1.3586506e-02\n",
      "  -3.1422176e+00 -1.1286339e-01]\n",
      " ...\n",
      " [ 1.8475170e+00  1.0000000e+00 -4.8300171e-01 ... -2.8498404e+02\n",
      "  -2.0395298e+03  2.9807523e+02]\n",
      " [ 1.8474865e+00  1.0000000e+00 -4.8298264e-01 ...  1.1038943e+02\n",
      "   5.0872509e+01  1.3405194e+02]\n",
      " [ 1.8474226e+00  1.0000000e+00 -4.8314476e-01 ... -7.4257446e+01\n",
      "  -1.7609858e+01  2.4376581e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90629101e+00  1.00000000e+00  1.11265182e-01 ...  1.81190094e+02\n",
      "  -4.63717316e+02 -7.59510620e+02]\n",
      " [ 1.90631104e+00  1.00000000e+00  1.11203194e-01 ... -1.85094742e+02\n",
      "   1.47701691e+02 -1.44210464e+02]\n",
      " [ 1.90638161e+00  1.00000000e+00  1.11320011e-01 ... -1.23429012e+00\n",
      "  -4.16662693e+00 -5.84655821e-01]\n",
      " ...\n",
      " [ 1.90636444e+00  1.00000000e+00  1.11281395e-01 ...  1.59112366e+03\n",
      "   7.67029877e+01 -1.18636975e+03]\n",
      " [ 1.90634346e+00  1.00000000e+00  1.11286163e-01 ...  1.05751396e+02\n",
      "  -4.89933434e+01  8.77007065e+01]\n",
      " [ 1.90632915e+00  1.00000000e+00  1.11125946e-01 ...  1.04816994e+02\n",
      "   1.09002449e+02 -4.71042557e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7785091e+00  1.0000000e+00  6.9528961e-01 ...  5.2081512e+02\n",
      "   6.7513843e+02  7.2738245e+02]\n",
      " [ 1.7785320e+00  1.0000000e+00  6.9522381e-01 ...  1.9812162e+01\n",
      "   1.0809074e+02 -4.5002388e+01]\n",
      " [ 1.7785797e+00  1.0000000e+00  6.9532621e-01 ...  1.7613933e+00\n",
      "   1.9937749e+00 -1.1223302e+00]\n",
      " ...\n",
      " [ 1.7785702e+00  1.0000000e+00  6.9529724e-01 ...  2.1925760e+02\n",
      "   2.9475824e+02  8.2343201e+01]\n",
      " [ 1.7785606e+00  1.0000000e+00  6.9530869e-01 ...  2.9943869e+01\n",
      "  -8.3816483e+01 -2.0857862e+01]\n",
      " [ 1.7785940e+00  1.0000000e+00  6.9515419e-01 ... -4.9100796e+01\n",
      "   1.0542551e+02 -1.5953894e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.2       1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4766998e+00  1.0000000e+00  1.2107506e+00 ...  2.3170010e+02\n",
      "  -2.1709530e+01  1.1099746e+02]\n",
      " [ 1.4767275e+00  1.0000000e+00  1.2107115e+00 ... -5.6599531e+00\n",
      "  -2.9713127e+01  1.4096975e-02]\n",
      " [ 1.4767418e+00  1.0000000e+00  1.2107921e+00 ...  6.4801788e-01\n",
      "   1.2129831e+00  1.9122234e-01]\n",
      " ...\n",
      " [ 1.4767342e+00  1.0000000e+00  1.2107782e+00 ... -1.9061931e+02\n",
      "  -2.9112915e+01  3.0134686e+02]\n",
      " [ 1.4767399e+00  1.0000000e+00  1.2107639e+00 ...  3.5499744e+02\n",
      "  -6.3810879e+01 -3.2537436e+02]\n",
      " [ 1.4768238e+00  1.0000000e+00  1.2106400e+00 ... -3.2537689e+01\n",
      "   9.9584167e+01  7.0538315e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.9000001 1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0301390e+00  1.0000000e+00  1.6079941e+00 ... -2.9812881e+02\n",
      "  -4.6127522e+01 -1.4765915e+02]\n",
      " [ 1.0301704e+00  1.0000000e+00  1.6078959e+00 ...  6.0735688e+00\n",
      "  -1.7780828e-01 -7.2376647e+00]\n",
      " [ 1.0301685e+00  1.0000000e+00  1.6079656e+00 ...  4.3265033e-01\n",
      "   4.5950699e-01  1.8228209e-01]\n",
      " ...\n",
      " [ 1.0301685e+00  1.0000000e+00  1.6079493e+00 ...  2.9549249e+02\n",
      "  -1.3396695e+02  7.6110992e+01]\n",
      " [ 1.0301819e+00  1.0000000e+00  1.6079998e+00 ...  1.8129961e+02\n",
      "   8.2437218e+01  3.1199390e+02]\n",
      " [ 1.0303020e+00  1.0000000e+00  1.6079159e+00 ...  6.7755447e+01\n",
      "   1.2588962e+02 -7.7919487e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.8257446e-01  1.0000000e+00  1.8477135e+00 ... -1.3464598e+02\n",
      "   4.3703163e+01 -1.5954742e+02]\n",
      " [ 4.8261070e-01  1.0000000e+00  1.8476219e+00 ...  9.9129280e+01\n",
      "   7.5687874e+01  2.9926744e+01]\n",
      " [ 4.8259544e-01  1.0000000e+00  1.8476602e+00 ... -3.1283784e-01\n",
      "   1.0872073e+00  1.3669133e-02]\n",
      " ...\n",
      " [ 4.8259735e-01  1.0000000e+00  1.8476591e+00 ...  1.7583397e+01\n",
      "  -5.5196766e+01  8.6317268e+01]\n",
      " [ 4.8261261e-01  1.0000000e+00  1.8477192e+00 ...  6.2420040e+01\n",
      "  -5.6997852e+02 -7.2024376e+01]\n",
      " [ 4.8276043e-01  1.0000000e+00  1.8476734e+00 ... -2.2817604e+01\n",
      "  -7.0557343e+01  1.3106592e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.11884117e-01  1.00000000e+00  1.90637207e+00 ...  5.19200020e+01\n",
      "  -2.95419254e+01  7.08068848e+01]\n",
      " [-1.11843109e-01  1.00000000e+00  1.90631676e+00 ... -1.17018715e+02\n",
      "   6.47910385e+01 -1.95595230e+02]\n",
      " [-1.11841202e-01  1.00000000e+00  1.90631986e+00 ... -6.15086555e-02\n",
      "   1.63730669e+00  2.96119452e-02]\n",
      " ...\n",
      " [-1.11839294e-01  1.00000000e+00  1.90633106e+00 ... -1.15696968e+02\n",
      "  -6.52637756e+02 -1.30602066e+02]\n",
      " [-1.11822128e-01  1.00000000e+00  1.90637779e+00 ...  4.04935837e+01\n",
      "   3.86087036e+01  5.55873566e+01]\n",
      " [-1.11684799e-01  1.00000000e+00  1.90637207e+00 ...  9.94343338e+01\n",
      "  -4.53061157e+02 -6.23812622e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.9565201e-01  1.0000000e+00  1.7784367e+00 ... -9.1971930e+02\n",
      "   3.8534875e+02  9.5408350e+02]\n",
      " [-6.9561195e-01  1.0000000e+00  1.7783985e+00 ...  4.7778894e+02\n",
      "   2.8304950e+02 -2.3045135e+02]\n",
      " [-6.9557571e-01  1.0000000e+00  1.7783777e+00 ...  2.6142039e+00\n",
      "   8.4381018e+00  1.4299810e-02]\n",
      " ...\n",
      " [-6.9557381e-01  1.0000000e+00  1.7784033e+00 ... -5.2548016e+02\n",
      "  -4.8437399e+02 -3.4109302e+02]\n",
      " [-6.9555664e-01  1.0000000e+00  1.7784405e+00 ... -2.5090723e+02\n",
      "   5.8097333e+02  2.9587775e+02]\n",
      " [-6.9546986e-01  1.0000000e+00  1.7784958e+00 ...  1.1966582e+02\n",
      "   2.2377357e+02 -2.4638510e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.2110758e+00  1.0000000e+00  1.4765396e+00 ... -7.8652935e+00\n",
      "   8.9697208e+00  5.8342786e+00]\n",
      " [-1.2110415e+00  1.0000000e+00  1.4764700e+00 ...  2.3109619e+02\n",
      "   4.2280331e+02 -5.2867932e+02]\n",
      " [-1.2109985e+00  1.0000000e+00  1.4764502e+00 ... -2.0224824e+00\n",
      "  -1.3120602e+01 -8.0450034e-01]\n",
      " ...\n",
      " [-1.2109985e+00  1.0000000e+00  1.4764700e+00 ... -7.5646744e+00\n",
      "   7.2469803e+01  3.5996948e+01]\n",
      " [-1.2109814e+00  1.0000000e+00  1.4765491e+00 ... -1.2401350e+01\n",
      "   1.3781976e+01 -4.0634781e+01]\n",
      " [-1.2109127e+00  1.0000000e+00  1.4766541e+00 ...  1.1511738e+03\n",
      "   3.8780548e+02  6.5266418e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.60808754e+00  1.00000000e+00  1.03001595e+00 ... -1.72368884e+03\n",
      "  -5.15345312e+03  5.09730566e+03]\n",
      " [-1.60806179e+00  1.00000000e+00  1.02995014e+00 ...  6.99592285e+01\n",
      "  -2.28760788e+02  3.06360687e+02]\n",
      " [-1.60799408e+00  1.00000000e+00  1.02991605e+00 ... -2.18027329e+00\n",
      "  -9.15545559e+00 -1.19072080e-01]\n",
      " ...\n",
      " [-1.60800171e+00  1.00000000e+00  1.02994823e+00 ...  3.58731956e+01\n",
      "   1.05302956e+02  2.87958279e+01]\n",
      " [-1.60798454e+00  1.00000000e+00  1.03003120e+00 ... -1.41470306e+02\n",
      "  -4.34168282e+01 -4.63971786e+01]\n",
      " [-1.60797119e+00  1.00000000e+00  1.03016472e+00 ...  2.20170832e+00\n",
      "  -1.48325211e+02  5.92839600e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.4       0.6\n",
      " 0.        0.        0.8000001 0.        0.        0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8477449e+00  1.0000000e+00  4.8241043e-01 ...  3.4402244e+03\n",
      "   4.4714261e+02  3.3200684e+03]\n",
      " [-1.8477240e+00  1.0000000e+00  4.8230553e-01 ...  7.2084557e+01\n",
      "   8.8609703e+01  7.7257128e+00]\n",
      " [-1.8476505e+00  1.0000000e+00  4.8227802e-01 ... -2.6185575e+00\n",
      "  -1.2141455e+01  1.0327697e-01]\n",
      " ...\n",
      " [-1.8476696e+00  1.0000000e+00  4.8230362e-01 ... -2.5624216e+00\n",
      "  -5.8649879e+01 -1.0519733e+02]\n",
      " [-1.8476677e+00  1.0000000e+00  4.8243332e-01 ...  2.9952045e+01\n",
      "   6.7771332e+01 -9.3806015e+01]\n",
      " [-1.8476896e+00  1.0000000e+00  4.8258400e-01 ... -6.9383568e+01\n",
      "  -8.6454071e+01 -2.1076958e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.70000005 0.         0.         1.0000001\n",
      " 0.3        0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         1.0000001  0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90635300e+00  1.00000000e+00 -1.12257004e-01 ...  6.53281128e+02\n",
      "  -3.27034416e+01 -1.85868311e+03]\n",
      " [-1.90632915e+00  1.00000000e+00 -1.12392426e-01 ... -1.83656906e+02\n",
      "   1.28817795e+02 -3.04192047e+02]\n",
      " [-1.90624809e+00  1.00000000e+00 -1.12406000e-01 ... -5.10275960e-01\n",
      "  -2.40087700e+00 -2.69882917e-01]\n",
      " ...\n",
      " [-1.90627670e+00  1.00000000e+00 -1.12394333e-01 ...  7.73851624e+01\n",
      "  -8.85972214e+01  6.43991947e+00]\n",
      " [-1.90628815e+00  1.00000000e+00 -1.12234116e-01 ... -1.16620293e+02\n",
      "  -2.71266308e+01 -1.79594755e+00]\n",
      " [-1.90635014e+00  1.00000000e+00 -1.12081528e-01 ... -3.03264103e+01\n",
      "   2.13152409e+00 -1.57725267e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        1.0000001 0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.77820301e+00  1.00000000e+00 -6.95682526e-01 ...  1.42319690e+03\n",
      "  -3.35367645e+02  2.60690948e+02]\n",
      " [-1.77816963e+00  1.00000000e+00 -6.95840836e-01 ... -5.03456078e+01\n",
      "  -8.13403320e+01 -1.00909256e+02]\n",
      " [-1.77804947e+00  1.00000000e+00 -6.95849359e-01 ... -3.26174498e-01\n",
      "  -6.86073399e+00  2.90756643e-01]\n",
      " ...\n",
      " [-1.77808380e+00  1.00000000e+00 -6.95842743e-01 ... -9.70310364e+02\n",
      "   2.47128937e+02  2.59681763e+02]\n",
      " [-1.77811241e+00  1.00000000e+00 -6.95659637e-01 ...  1.15498589e+02\n",
      "  -1.40024887e+02 -3.65553017e+01]\n",
      " [-1.77825165e+00  1.00000000e+00 -6.95522308e-01 ...  1.08010605e+02\n",
      "   5.41443901e+01 -1.86581192e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        1.0000001 0.\n",
      " 0.        0.        0.9000001 0.        0.        0.4       0.\n",
      " 0.6       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.4757566     1.           -1.2114487  ... -168.93393\n",
      "    16.657347   -325.8904    ]\n",
      " [  -1.4757099     1.           -1.211668   ... -337.3899\n",
      "  -319.43195     127.15403   ]\n",
      " [  -1.475605      1.           -1.2116596  ...   -2.3670197\n",
      "   -10.224791      0.88454807]\n",
      " ...\n",
      " [  -1.4756393     1.           -1.2116671  ...   -8.203396\n",
      "    38.60775      -8.332527  ]\n",
      " [  -1.475708      1.           -1.2114372  ...   13.640155\n",
      "   -10.980559    130.84407   ]\n",
      " [  -1.475852      1.           -1.2113266  ...    4.9977756\n",
      "    92.25411      72.826996  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.5       0.        0.4       0.        0.\n",
      " 0.        0.1       0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0289097e+00  1.0000000e+00 -1.6080856e+00 ...  1.2176469e+03\n",
      "  -9.6755066e+02  9.5993250e+02]\n",
      " [-1.0288477e+00  1.0000000e+00 -1.6082659e+00 ...  1.1163396e+02\n",
      "   3.3164948e+01 -7.9946953e+01]\n",
      " [-1.0287228e+00  1.0000000e+00 -1.6082441e+00 ...  1.8331707e-01\n",
      "  -4.5850182e+00  1.4262500e+00]\n",
      " ...\n",
      " [-1.0287590e+00  1.0000000e+00 -1.6082602e+00 ... -5.2011200e+01\n",
      "  -2.0720712e+02  2.2256467e+02]\n",
      " [-1.0288544e+00  1.0000000e+00 -1.6080780e+00 ... -3.8754282e+02\n",
      "   6.2602882e+01 -8.4960309e+02]\n",
      " [-1.0290346e+00  1.0000000e+00 -1.6079979e+00 ...  3.5934708e+01\n",
      "  -1.5512939e+01 -4.9995770e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8157501e-01  1.0000000e+00 -1.8474216e+00 ...  1.9323399e+02\n",
      "   4.1038409e+02 -7.0113907e+01]\n",
      " [-4.8150349e-01  1.0000000e+00 -1.8475609e+00 ... -2.1410963e+02\n",
      "   6.3139886e+02  3.6047168e+02]\n",
      " [-4.8141670e-01  1.0000000e+00 -1.8475326e+00 ... -6.9630375e+00\n",
      "  -2.5536591e+02 -2.1398304e+01]\n",
      " ...\n",
      " [-4.8145294e-01  1.0000000e+00 -1.8475485e+00 ...  1.1419439e+01\n",
      "   1.9955023e+02  1.3936656e+02]\n",
      " [-4.8156548e-01  1.0000000e+00 -1.8474140e+00 ...  5.5161476e-02\n",
      "   6.5226517e+00 -1.0353987e+01]\n",
      " [-4.8172379e-01  1.0000000e+00 -1.8473721e+00 ...  2.3848500e+02\n",
      "   1.4547086e+02  3.4736202e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.13102913e-01  1.00000000e+00 -1.90587616e+00 ...  3.33183624e+02\n",
      "  -2.01256760e+02  3.89797150e+02]\n",
      " [ 1.13176346e-01  1.00000000e+00 -1.90599537e+00 ...  4.89145111e+02\n",
      "  -1.91247009e+02  1.35338486e+02]\n",
      " [ 1.13286972e-01  1.00000000e+00 -1.90595353e+00 ...  1.16952744e+02\n",
      "  -1.15441914e+01  8.46646690e+00]\n",
      " ...\n",
      " [ 1.13250732e-01  1.00000000e+00 -1.90598011e+00 ... -1.16227612e+03\n",
      "  -1.70237500e+03  4.41088348e+02]\n",
      " [ 1.13130569e-01  1.00000000e+00 -1.90586853e+00 ...  4.14690125e+02\n",
      "   4.75786400e+01  5.04749054e+02]\n",
      " [ 1.12949371e-01  1.00000000e+00 -1.90589714e+00 ... -3.20901871e+01\n",
      "  -5.47152252e+01  1.20473991e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.96660042e-01  1.00000000e+00 -1.77772522e+00 ...  4.98913307e+01\n",
      "  -6.94699554e+01  5.15466881e+01]\n",
      " [ 6.96723938e-01  1.00000000e+00 -1.77780437e+00 ...  8.57085144e+02\n",
      "  -8.24923279e+02 -6.53297791e+02]\n",
      " [ 6.96819305e-01  1.00000000e+00 -1.77774644e+00 ... -1.99325924e+01\n",
      "   1.20187569e+02 -1.83060364e+02]\n",
      " ...\n",
      " [ 6.96783066e-01  1.00000000e+00 -1.77777576e+00 ...  3.60506104e+02\n",
      "  -2.01580963e+02 -1.28642059e+02]\n",
      " [ 6.96668625e-01  1.00000000e+00 -1.77771568e+00 ...  1.91160717e+01\n",
      "   2.62682705e+01  2.14953251e+01]\n",
      " [ 6.96516991e-01  1.00000000e+00 -1.77779579e+00 ... -1.34568970e+02\n",
      "   7.50437698e+01  1.03227036e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.2122564     1.           -1.4752579  ...   34.040154\n",
      "  -115.31081     -82.532616  ]\n",
      " [   1.2123156     1.           -1.475338   ... -133.46005\n",
      "    -0.29066992 -121.02543   ]\n",
      " [   1.2123947     1.           -1.4752619  ...   24.770767\n",
      "    36.731106     29.971928  ]\n",
      " ...\n",
      " [   1.2123623     1.           -1.4752989  ...  -39.348907\n",
      "    66.58735      82.86477   ]\n",
      " [   1.2122688     1.           -1.4752483  ...   43.706593\n",
      "    -6.808101    -64.14905   ]\n",
      " [   1.2121372     1.           -1.47538    ...  -96.55175\n",
      "  -162.40015    -144.6307    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.6089067    1.          -1.0285053 ...  265.78152    -83.97603\n",
      "    75.68281  ]\n",
      " [   1.6089582    1.          -1.0285873 ...  -46.37491    100.56215\n",
      "   138.68854  ]\n",
      " [   1.6090069    1.          -1.0284739 ... -165.56514    -43.157776\n",
      "   276.36853  ]\n",
      " ...\n",
      " [   1.6089802    1.          -1.0285358 ...    5.453058  -324.49234\n",
      "   511.28415  ]\n",
      " [   1.6089039    1.          -1.0284958 ...   93.721344   -56.20465\n",
      "  -134.45493  ]\n",
      " [   1.6088219    1.          -1.0286636 ...   79.33168    -61.647602\n",
      "   -62.654255 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.9000001 0.        0.\n",
      " 0.        0.        0.1       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Forearm\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84809875e+00  1.00000000e+00 -4.80976105e-01 ... -5.38054085e+01\n",
      "  -8.14536209e+01  6.13111572e+01]\n",
      " [ 1.84814167e+00  1.00000000e+00 -4.81053352e-01 ... -7.37521362e+01\n",
      "   8.11807404e+01  2.44543343e+01]\n",
      " [ 1.84820747e+00  1.00000000e+00 -4.80934829e-01 ... -4.33408905e+02\n",
      "   1.28924004e+04  1.66278357e+03]\n",
      " ...\n",
      " [ 1.84818840e+00  1.00000000e+00 -4.80996132e-01 ... -7.44526733e+02\n",
      "   2.10008325e+03 -1.28079590e+03]\n",
      " [ 1.84812546e+00  1.00000000e+00 -4.80966568e-01 ...  1.33857849e+02\n",
      "  -1.35243195e+02  1.46134537e+02]\n",
      " [ 1.84806919e+00  1.00000000e+00 -4.81159210e-01 ... -1.69561783e+02\n",
      "   6.40909500e+01  1.22888664e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90635204e+00  1.00000000e+00  1.13739014e-01 ...  4.84174156e+00\n",
      "   4.17112112e+00  4.15223694e+00]\n",
      " [ 1.90638065e+00  1.00000000e+00  1.13698006e-01 ...  5.17060242e+01\n",
      "  -7.36790848e+01 -2.43697845e+02]\n",
      " [ 1.90642738e+00  1.00000000e+00  1.13818966e-01 ...  8.84093079e+02\n",
      "  -1.20518591e+03 -8.18360535e+02]\n",
      " ...\n",
      " [ 1.90641403e+00  1.00000000e+00  1.13757133e-01 ...  1.13384558e+03\n",
      "  -2.05434937e+03  2.00248810e+02]\n",
      " [ 1.90636635e+00  1.00000000e+00  1.13748550e-01 ...  6.22308540e+01\n",
      "   1.17618752e+01  9.66644669e+01]\n",
      " [ 1.90637589e+00  1.00000000e+00  1.13550186e-01 ...  1.12353874e+02\n",
      "  -7.60743475e+00 -2.04695023e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7779436e+00  1.0000000e+00  6.9728088e-01 ...  9.5390093e-01\n",
      "   6.4903121e+00  1.0699015e+00]\n",
      " [ 1.7779655e+00  1.0000000e+00  6.9721413e-01 ... -3.3304588e+01\n",
      "   1.0629119e+01 -4.9837101e+01]\n",
      " [ 1.7780304e+00  1.0000000e+00  6.9733715e-01 ...  9.0645337e+02\n",
      "   1.2196851e+03 -4.4011920e+02]\n",
      " ...\n",
      " [ 1.7780304e+00  1.0000000e+00  6.9726753e-01 ...  1.3218185e+03\n",
      "   3.5470813e+02 -1.2776221e+03]\n",
      " [ 1.7780037e+00  1.0000000e+00  6.9729042e-01 ... -7.0872970e+00\n",
      "  -6.5958037e+00 -2.8623835e+01]\n",
      " [ 1.7780361e+00  1.0000000e+00  6.9709396e-01 ... -1.2003016e+02\n",
      "  -3.5573204e+01 -2.1884909e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.8000001 0.\n",
      " 0.        0.        0.1       0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4753122e+00  1.0000000e+00  1.2128162e+00 ...  1.6117990e-02\n",
      "   1.2205117e+01 -1.5200831e+00]\n",
      " [ 1.4753342e+00  1.0000000e+00  1.2127495e+00 ... -4.7300816e+01\n",
      "  -8.6193138e+01  6.5620399e+01]\n",
      " [ 1.4753532e+00  1.0000000e+00  1.2128516e+00 ... -5.3675831e+01\n",
      "  -2.3724627e+01  3.2183401e+02]\n",
      " ...\n",
      " [ 1.4753685e+00  1.0000000e+00  1.2127905e+00 ...  1.5887466e+02\n",
      "   1.9598097e+02 -1.0065386e+02]\n",
      " [ 1.4753475e+00  1.0000000e+00  1.2128258e+00 ... -7.0328955e+02\n",
      "   5.0777548e+02  1.8819753e+02]\n",
      " [ 1.4754419e+00  1.0000000e+00  1.2126522e+00 ...  9.3010921e+00\n",
      "   5.8259396e+01 -2.3360929e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0283813e+00  1.0000000e+00  1.6093102e+00 ... -3.7684172e-01\n",
      "  -2.4851384e+00  1.8535703e-01]\n",
      " [ 1.0284052e+00  1.0000000e+00  1.6092377e+00 ... -8.0367332e+01\n",
      "  -1.1725067e+02  1.1753456e+02]\n",
      " [ 1.0284004e+00  1.0000000e+00  1.6093127e+00 ... -4.5800247e+02\n",
      "   4.7282022e+02 -9.0385036e+00]\n",
      " ...\n",
      " [ 1.0284233e+00  1.0000000e+00  1.6092663e+00 ...  2.0903278e+02\n",
      "  -1.5551208e+02 -1.6649702e+02]\n",
      " [ 1.0284042e+00  1.0000000e+00  1.6093197e+00 ... -2.4663555e+01\n",
      "   7.3450996e+01 -6.9318054e+01]\n",
      " [ 1.0285482e+00  1.0000000e+00  1.6091938e+00 ...  1.1445334e+01\n",
      "   8.0433741e+00  9.5485001e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.1       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.80871201e-01  1.00000000e+00  1.84827614e+00 ... -1.42445946e+00\n",
      "   3.00105739e+00  5.93167424e-01]\n",
      " [ 4.80895996e-01  1.00000000e+00  1.84821892e+00 ...  1.27596531e+01\n",
      "   3.11954346e+02  5.40471802e+02]\n",
      " [ 4.80905533e-01  1.00000000e+00  1.84826219e+00 ...  2.06573303e+02\n",
      "   6.71589050e+01 -1.47967163e+02]\n",
      " ...\n",
      " [ 4.80936050e-01  1.00000000e+00  1.84823990e+00 ... -3.47173119e+00\n",
      "   5.72707253e+01  1.14990585e+02]\n",
      " [ 4.80924606e-01  1.00000000e+00  1.84828568e+00 ...  2.46513863e+01\n",
      "   1.03984566e+02 -6.70256653e+01]\n",
      " [ 4.81062889e-01  1.00000000e+00  1.84820557e+00 ... -9.13787964e+02\n",
      "   1.15040210e+03  1.52451050e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.13931656e-01  1.00000000e+00  1.90634346e+00 ... -1.23922825e+00\n",
      "   3.67905378e+00  8.22774053e-01]\n",
      " [-1.13905907e-01  1.00000000e+00  1.90629482e+00 ...  4.05732994e+01\n",
      "  -1.62450546e+02 -1.13940720e+02]\n",
      " [-1.13937378e-01  1.00000000e+00  1.90632367e+00 ...  1.12044945e+01\n",
      "   1.72850250e+02  1.15828796e+02]\n",
      " ...\n",
      " [-1.13904953e-01  1.00000000e+00  1.90631104e+00 ...  2.34455399e+02\n",
      "   2.02670639e+02  5.83129761e+02]\n",
      " [-1.13916397e-01  1.00000000e+00  1.90635681e+00 ... -2.17116165e+02\n",
      "   6.97259903e+01  7.89550171e+02]\n",
      " [-1.13735199e-01  1.00000000e+00  1.90634918e+00 ... -7.78229218e+01\n",
      "   5.98375671e+02  2.54711182e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.6977434     1.            1.7776928  ...   -1.8048697\n",
      "     4.533722     -0.46783125]\n",
      " [  -0.6977215     1.            1.7776384  ...  195.69394\n",
      "   126.592834   -259.8412    ]\n",
      " [  -0.6977329     1.            1.7776461  ...  277.85666\n",
      "   170.68575     190.06404   ]\n",
      " ...\n",
      " [  -0.6977043     1.            1.7776527  ... -101.30384\n",
      "    89.45588      62.58674   ]\n",
      " [  -0.69771576    1.            1.77771    ... -214.87099\n",
      "   129.48509      35.18743   ]\n",
      " [  -0.6975603     1.            1.7777481  ...   49.956375\n",
      "   273.1164      121.82247   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.2128286e+00  1.0000000e+00  1.4751225e+00 ... -7.5214767e-01\n",
      "   2.1787252e+00  7.5112522e-02]\n",
      " [-1.2128105e+00  1.0000000e+00  1.4750404e+00 ... -2.1594843e+02\n",
      "  -6.5375641e+01 -1.0881301e+02]\n",
      " [-1.2128105e+00  1.0000000e+00  1.4750487e+00 ...  1.0719899e+02\n",
      "  -7.2683014e+01  2.0484898e+02]\n",
      " ...\n",
      " [-1.2127876e+00  1.0000000e+00  1.4750538e+00 ... -2.6532158e+01\n",
      "   9.8249435e+01 -5.1330399e+01]\n",
      " [-1.2127991e+00  1.0000000e+00  1.4751472e+00 ... -4.0126572e+01\n",
      "   9.1687881e+01 -3.7177299e+01]\n",
      " [-1.2126751e+00  1.0000000e+00  1.4752350e+00 ...  5.0782204e+01\n",
      "   3.3049377e+02  9.1788681e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.60936642e+00  1.00000000e+00  1.02790260e+00 ... -1.94239616e-01\n",
      "   2.41428685e+00 -8.30122709e-01]\n",
      " [-1.60935116e+00  1.00000000e+00  1.02785015e+00 ...  4.78168678e+01\n",
      "   1.74687958e+01  8.48150253e+01]\n",
      " [-1.60932541e+00  1.00000000e+00  1.02785099e+00 ... -1.01225967e+02\n",
      "   1.45885191e+01 -4.23444580e+02]\n",
      " ...\n",
      " [-1.60931206e+00  1.00000000e+00  1.02786350e+00 ...  1.75998268e+01\n",
      "  -1.27639713e+01 -1.34099722e+00]\n",
      " [-1.60933113e+00  1.00000000e+00  1.02793121e+00 ...  6.75618458e+00\n",
      "   5.86337738e+01 -1.09700645e+02]\n",
      " [-1.60925674e+00  1.00000000e+00  1.02805901e+00 ...  3.26367521e+00\n",
      "  -1.31675701e+01 -6.12752390e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.84810925e+00  1.00000000e+00  4.80377197e-01 ... -2.51481533e-01\n",
      "   1.03377581e+00 -9.84342933e-01]\n",
      " [-1.84809399e+00  1.00000000e+00  4.80342865e-01 ... -1.14575874e+02\n",
      "   3.30096313e+02 -1.34927628e+02]\n",
      " [-1.84810257e+00  1.00000000e+00  4.80338842e-01 ... -7.31349030e+01\n",
      "  -1.85905487e+02  9.61701965e+01]\n",
      " ...\n",
      " [-1.84809113e+00  1.00000000e+00  4.80356216e-01 ...  3.71299713e+02\n",
      "   1.04799728e+02 -2.84137543e+02]\n",
      " [-1.84811401e+00  1.00000000e+00  4.80407715e-01 ... -6.15375595e+01\n",
      "  -2.09046051e+02  1.03538376e+02]\n",
      " [-1.84805012e+00  1.00000000e+00  4.80562210e-01 ... -5.35658836e+01\n",
      "   9.06902771e+01  9.29910736e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90608788e+00  1.00000000e+00 -1.14358902e-01 ... -9.37120557e-01\n",
      "   5.70928097e+00  6.05095744e-01]\n",
      " [-1.90607262e+00  1.00000000e+00 -1.14376068e-01 ... -1.15786530e+02\n",
      "   1.56194885e+02  6.54548569e+01]\n",
      " [-1.90608025e+00  1.00000000e+00 -1.14397079e-01 ... -5.77740356e+02\n",
      "   2.04250275e+02  3.74428101e+02]\n",
      " ...\n",
      " [-1.90608406e+00  1.00000000e+00 -1.14362717e-01 ...  9.97308960e+01\n",
      "   1.35911423e+02  1.44985825e+02]\n",
      " [-1.90611458e+00  1.00000000e+00 -1.14328384e-01 ...  6.10999584e-01\n",
      "   4.25539886e+02  3.65037847e+00]\n",
      " [-1.90609074e+00  1.00000000e+00 -1.14164352e-01 ...  4.19085175e+02\n",
      "   5.66040588e+02  2.14932999e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7773857e+00  1.0000000e+00 -6.9803238e-01 ... -1.5820392e+00\n",
      "   5.9005203e+00 -1.9690266e+00]\n",
      " [-1.7773676e+00  1.0000000e+00 -6.9801903e-01 ...  7.1970566e+01\n",
      "  -3.9268384e+02 -5.1561016e+01]\n",
      " [-1.7773533e+00  1.0000000e+00 -6.9804180e-01 ... -1.2233898e+03\n",
      "   4.0137585e+03  3.3618711e+03]\n",
      " ...\n",
      " [-1.7773628e+00  1.0000000e+00 -6.9800568e-01 ... -3.2737160e+01\n",
      "  -4.4300903e+01 -4.6917377e+01]\n",
      " [-1.7774029e+00  1.0000000e+00 -6.9800186e-01 ...  4.4793158e+02\n",
      "  -2.1389263e+01  7.2564148e+02]\n",
      " [-1.7774525e+00  1.0000000e+00 -6.9784737e-01 ... -4.2307178e+02\n",
      "  -2.6820663e+02 -3.3276514e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.4745798    1.          -1.2132473 ...   -2.8780766    9.505959\n",
      "     4.0850477]\n",
      " [  -1.4745617    1.          -1.2132626 ...  181.27129     10.503614\n",
      "   363.73483  ]\n",
      " [  -1.4745483    1.          -1.2132757 ... -112.74242   -439.60315\n",
      "   438.88504  ]\n",
      " ...\n",
      " [  -1.4745579    1.          -1.2132492 ...  182.07251    555.9877\n",
      "  -263.70822  ]\n",
      " [  -1.4746056    1.          -1.2132263 ...  -72.12574    -35.68481\n",
      "    67.108444 ]\n",
      " [  -1.4747019    1.          -1.2130985 ... -294.81815   -591.2442\n",
      "  -236.96234  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.         0.2        0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0277195     1.           -1.6093788  ...   -0.20270467\n",
      "     2.7061179     1.0778416 ]\n",
      " [  -1.0276995     1.           -1.6093969  ...  171.36197\n",
      "  -129.8483     -124.86757   ]\n",
      " [  -1.0276814     1.           -1.6094059  ... -107.29911\n",
      "   -70.07382     188.19594   ]\n",
      " ...\n",
      " [  -1.0276947     1.           -1.6093855  ...  -15.20401\n",
      "  -106.38401      21.11234   ]\n",
      " [  -1.0277462     1.           -1.6093597  ...  125.208405\n",
      "    38.620872    -55.768696  ]\n",
      " [  -1.0278778     1.           -1.6092701  ...   79.063934\n",
      "    14.227874    -21.05989   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.7984791e-01  1.0000000e+00 -1.8482571e+00 ... -3.5478613e+00\n",
      "   1.2501397e+00  3.5994044e-01]\n",
      " [-4.7982788e-01  1.0000000e+00 -1.8482800e+00 ...  1.8464195e+02\n",
      "   3.9121295e+02 -4.5128595e+02]\n",
      " [-4.7982216e-01  1.0000000e+00 -1.8482851e+00 ... -7.9057678e+02\n",
      "   1.1344467e+03 -8.6260541e+02]\n",
      " ...\n",
      " [-4.7983742e-01  1.0000000e+00 -1.8482685e+00 ... -1.0659106e+01\n",
      "  -9.8800987e+01  5.9855076e+01]\n",
      " [-4.7989273e-01  1.0000000e+00 -1.8482475e+00 ... -1.0688103e+02\n",
      "  -1.2393137e+01 -3.1726349e+01]\n",
      " [-4.8003960e-01  1.0000000e+00 -1.8482113e+00 ... -2.0826035e+03\n",
      "   1.2410980e+03 -4.8459180e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.15093231e-01  1.00000000e+00 -1.90603638e+00 ... -1.74900937e+00\n",
      "   2.61903143e+00  9.26366329e-01]\n",
      " [ 1.15114212e-01  1.00000000e+00 -1.90606594e+00 ...  2.87229639e+03\n",
      "  -2.02701123e+03 -2.36952515e+03]\n",
      " [ 1.15091324e-01  1.00000000e+00 -1.90605903e+00 ... -2.42341385e+01\n",
      "  -7.19982605e+02  4.46626373e+02]\n",
      " ...\n",
      " [ 1.15076065e-01  1.00000000e+00 -1.90605450e+00 ...  2.79955177e+01\n",
      "   9.83005047e+00 -1.71980225e+02]\n",
      " [ 1.15016937e-01  1.00000000e+00 -1.90603065e+00 ... -3.97307587e+02\n",
      "  -5.05551834e+01  1.95356232e+02]\n",
      " [ 1.14892960e-01  1.00000000e+00 -1.90604782e+00 ...  1.43325367e+01\n",
      "  -6.67217484e+01  6.23044128e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.98656082e-01  1.00000000e+00 -1.77716064e+00 ... -1.02480721e+00\n",
      "   1.61838293e+00  2.00433493e-01]\n",
      " [ 6.98677063e-01  1.00000000e+00 -1.77714825e+00 ...  5.74667244e+01\n",
      "  -1.16605789e+02 -1.03167694e+02]\n",
      " [ 6.98604584e-01  1.00000000e+00 -1.77714980e+00 ...  8.29445953e+01\n",
      "   2.47559769e+02  1.71783691e+02]\n",
      " ...\n",
      " [ 6.98591232e-01  1.00000000e+00 -1.77714157e+00 ...  1.84063358e+01\n",
      "  -7.25403061e+01  7.76423340e+01]\n",
      " [ 6.98535919e-01  1.00000000e+00 -1.77716446e+00 ...  3.69156586e+02\n",
      "   1.24749794e+02 -3.28774597e+02]\n",
      " [ 6.98466301e-01  1.00000000e+00 -1.77723885e+00 ... -2.60157318e+01\n",
      "   4.77182274e+01  6.91951418e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.21342182e+00  1.00000000e+00 -1.47450066e+00 ... -5.16017437e-01\n",
      "   8.84026051e-01  1.26583338e-01]\n",
      " [ 1.21343803e+00  1.00000000e+00 -1.47446346e+00 ... -8.51412678e+00\n",
      "  -1.26578949e+02 -6.64685364e+01]\n",
      " [ 1.21340370e+00  1.00000000e+00 -1.47449005e+00 ...  4.24485397e+00\n",
      "   3.06466599e+01  5.75340919e+01]\n",
      " ...\n",
      " [ 1.21339226e+00  1.00000000e+00 -1.47446823e+00 ...  1.38484314e+02\n",
      "   3.16487549e+02  2.61971313e+02]\n",
      " [ 1.21333885e+00  1.00000000e+00 -1.47451973e+00 ...  4.55691895e+02\n",
      "  -2.00586441e+02  2.44557541e+02]\n",
      " [ 1.21326160e+00  1.00000000e+00 -1.47465324e+00 ...  1.11101524e+02\n",
      "   2.55243057e+02  5.12376251e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.6098013     1.           -1.0271492  ...   -0.49964952\n",
      "     1.1176848     0.51308656]\n",
      " [   1.6098118     1.           -1.0271301  ... -474.3592\n",
      "   175.74261     207.36044   ]\n",
      " [   1.6098099     1.           -1.0271682  ...   76.60163\n",
      "   -20.074348    -89.81854   ]\n",
      " ...\n",
      " [   1.6098061     1.           -1.0271425  ...  -36.674118\n",
      "    -6.2706375   -16.244148  ]\n",
      " [   1.6097698     1.           -1.0271778  ... -222.0126\n",
      "  -308.4863     -445.25638   ]\n",
      " [   1.6096992     1.           -1.0273438  ...  200.034\n",
      "    46.30758     167.61217   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.6       0.\n",
      " 1.0000001 0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84849262e+00  1.00000000e+00 -4.79240417e-01 ... -5.08387947e+00\n",
      "   4.47015810e+00 -2.14529216e-01]\n",
      " [ 1.84849930e+00  1.00000000e+00 -4.79256630e-01 ... -4.58281189e+02\n",
      "   7.70678345e+02 -8.85685654e+01]\n",
      " [ 1.84848404e+00  1.00000000e+00 -4.79277849e-01 ...  1.96823387e+01\n",
      "   1.63173656e+01  9.54620647e+00]\n",
      " ...\n",
      " [ 1.84848404e+00  1.00000000e+00 -4.79275703e-01 ...  1.63942535e+02\n",
      "   8.20282669e+01 -4.80903664e+01]\n",
      " [ 1.84845924e+00  1.00000000e+00 -4.79278564e-01 ... -7.35915771e+02\n",
      "  -1.02030347e+03  3.13238281e+02]\n",
      " [ 1.84844494e+00  1.00000000e+00 -4.79459763e-01 ... -9.22610397e+01\n",
      "   1.03439667e+02  3.63237000e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.2       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.3       0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90612698e+00  1.00000000e+00  1.15701675e-01 ... -5.11614895e+00\n",
      "   2.88786459e+00  7.23798931e-01]\n",
      " [ 1.90613079e+00  1.00000000e+00  1.15693092e-01 ...  1.22725235e+02\n",
      "  -2.03643227e+00 -2.26557770e+02]\n",
      " [ 1.90612984e+00  1.00000000e+00  1.15669593e-01 ...  7.44784546e+01\n",
      "  -1.58181442e+02 -2.78562298e+01]\n",
      " ...\n",
      " [ 1.90612984e+00  1.00000000e+00  1.15674019e-01 ...  1.82273922e+01\n",
      "  -4.59227791e+01  5.03738451e+00]\n",
      " [ 1.90611649e+00  1.00000000e+00  1.15663528e-01 ... -6.26986542e+01\n",
      "   8.52993103e+02 -3.64086792e+02]\n",
      " [ 1.90615368e+00  1.00000000e+00  1.15480423e-01 ...  8.55239410e+01\n",
      "  -2.82365845e+02 -4.16391815e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.6       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7771788e+00  1.0000000e+00  6.9886017e-01 ... -2.7039528e-01\n",
      "  -2.6609969e-01  6.3053727e-02]\n",
      " [ 1.7771788e+00  1.0000000e+00  6.9882584e-01 ...  5.8384772e+02\n",
      "   2.2293495e+02  2.5158035e+02]\n",
      " [ 1.7771854e+00  1.0000000e+00  6.9880760e-01 ...  3.3830585e+01\n",
      "   4.4710155e+01  5.1350044e+01]\n",
      " ...\n",
      " [ 1.7771873e+00  1.0000000e+00  6.9880772e-01 ... -7.3814232e+01\n",
      "   7.7834785e+01  5.5029152e+01]\n",
      " [ 1.7771797e+00  1.0000000e+00  6.9882202e-01 ... -2.7912537e+01\n",
      "   1.0910430e+02  1.3363716e+02]\n",
      " [ 1.7772732e+00  1.0000000e+00  6.9866180e-01 ...  7.1658417e+01\n",
      "  -4.0896686e+02  1.4602272e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.4       0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.1       0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4739428e+00  1.0000000e+00  1.2139816e+00 ... -1.9497037e+00\n",
      "  -2.9087949e-01  8.0427289e-02]\n",
      " [ 1.4739418e+00  1.0000000e+00  1.2139797e+00 ... -5.5418472e+00\n",
      "  -8.8722755e+01 -5.2930145e+00]\n",
      " [ 1.4739895e+00  1.0000000e+00  1.2139583e+00 ... -1.5097666e+00\n",
      "   7.4271255e+01  4.8928970e+01]\n",
      " ...\n",
      " [ 1.4739933e+00  1.0000000e+00  1.2139654e+00 ... -7.6751419e+01\n",
      "   9.9315872e+01 -1.1329066e+02]\n",
      " [ 1.4739933e+00  1.0000000e+00  1.2139492e+00 ...  2.4020543e+02\n",
      "  -1.3562781e+02 -1.1250785e+01]\n",
      " [ 1.4741049e+00  1.0000000e+00  1.2138214e+00 ... -1.7359439e+02\n",
      "   2.6977657e+01  1.3041428e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.8000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0265923e+00  1.0000000e+00  1.6099396e+00 ... -9.5079947e-01\n",
      "  -7.5318766e-01  9.6829653e-02]\n",
      " [ 1.0265913e+00  1.0000000e+00  1.6099253e+00 ... -2.6386777e+02\n",
      "   5.6445194e+01  6.3033859e+01]\n",
      " [ 1.0265980e+00  1.0000000e+00  1.6099185e+00 ... -2.0219826e+01\n",
      "   2.0638472e+01 -4.1694038e+01]\n",
      " ...\n",
      " [ 1.0266018e+00  1.0000000e+00  1.6099167e+00 ... -1.2396569e+02\n",
      "  -3.2626312e+01  6.7236618e+01]\n",
      " [ 1.0266037e+00  1.0000000e+00  1.6099167e+00 ... -2.0038009e+00\n",
      "   2.2815395e+01 -1.7570684e+01]\n",
      " [ 1.0267954e+00  1.0000000e+00  1.6098289e+00 ... -5.4641449e+01\n",
      "   6.1906122e+02  4.9418539e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.78503227e-01  1.00000000e+00  1.84846878e+00 ...  4.86350060e-02\n",
      "   1.60932541e-01 -7.37897158e-02]\n",
      " [ 4.78503227e-01  1.00000000e+00  1.84847450e+00 ... -8.97170067e-01\n",
      "   1.21131134e+01  4.19091263e+01]\n",
      " [ 4.78561401e-01  1.00000000e+00  1.84846735e+00 ...  1.92642242e+02\n",
      "  -4.18651581e+02 -1.13341408e+02]\n",
      " ...\n",
      " [ 4.78567123e-01  1.00000000e+00  1.84847832e+00 ...  4.54425335e+00\n",
      "  -2.87187309e+01 -2.21145309e+02]\n",
      " [ 4.78569031e-01  1.00000000e+00  1.84845924e+00 ...  9.57796192e+00\n",
      "  -6.51028776e+00 -1.94847584e+00]\n",
      " [ 4.78743553e-01  1.00000000e+00  1.84842300e+00 ...  1.09693985e+02\n",
      "   5.90104942e+01  9.74312878e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.16303444e-01  1.00000000e+00  1.90586853e+00 ...  5.95396996e-01\n",
      "   8.05810213e-01 -1.85313880e-01]\n",
      " [-1.16303444e-01  1.00000000e+00  1.90590191e+00 ...  2.60905762e+01\n",
      "  -9.42878265e+01 -8.53273926e+01]\n",
      " [-1.16264343e-01  1.00000000e+00  1.90589237e+00 ...  1.76324295e+02\n",
      "  -7.15173279e+02  8.35353699e+01]\n",
      " ...\n",
      " [-1.16258621e-01  1.00000000e+00  1.90591621e+00 ... -2.29746384e+02\n",
      "   1.75050262e+02 -2.23782520e+01]\n",
      " [-1.16256714e-01  1.00000000e+00  1.90586662e+00 ...  6.07013321e+01\n",
      "  -5.74543571e+01  5.73572426e+01]\n",
      " [-1.16053581e-01  1.00000000e+00  1.90589142e+00 ... -1.03378632e+02\n",
      "   1.14698708e+02  7.80882568e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.9934940e-01  1.0000000e+00  1.7767563e+00 ...  4.5344572e+00\n",
      "  -5.9890866e-01 -1.5906489e-01]\n",
      " [-6.9934845e-01  1.0000000e+00  1.7768316e+00 ... -1.8749306e+02\n",
      "   1.3160904e+02 -2.1985283e+01]\n",
      " [-6.9930458e-01  1.0000000e+00  1.7768062e+00 ... -3.4911053e+02\n",
      "  -1.2079228e+02 -7.5161292e+02]\n",
      " ...\n",
      " [-6.9930077e-01  1.0000000e+00  1.7768641e+00 ...  2.3122517e+02\n",
      "  -3.2152637e+01  9.9603836e+01]\n",
      " [-6.9929886e-01  1.0000000e+00  1.7767677e+00 ... -9.4670725e+00\n",
      "  -6.8196311e+00 -8.9375076e+00]\n",
      " [-6.9911766e-01  1.0000000e+00  1.7768497e+00 ...  6.2694927e+01\n",
      "  -2.8931545e+01  7.5682297e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.2143059     1.            1.4736805  ...    6.5955644\n",
      "    -1.0938263     0.72908795]\n",
      " [  -1.2143049     1.            1.4737158  ...  137.24628\n",
      "   148.76028    -251.72293   ]\n",
      " [  -1.214323      1.            1.4737056  ...   67.32928\n",
      "    50.64861       5.498309  ]\n",
      " ...\n",
      " [  -1.2143211     1.            1.4737606  ...   -8.607743\n",
      "  -153.84631     307.10492   ]\n",
      " [  -1.2143192     1.            1.4737072  ...   -3.094822\n",
      "     6.8846154   -14.484532  ]\n",
      " [  -1.2141209     1.            1.4738388  ...   95.50889\n",
      "  -301.6438     -558.34985   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6104240e+00  1.0000000e+00  1.0261784e+00 ...  2.0379887e+00\n",
      "  -6.7670494e-02 -4.7944254e-01]\n",
      " [-1.6104212e+00  1.0000000e+00  1.0262146e+00 ...  1.6437581e+02\n",
      "  -4.1378372e+01 -7.0119171e+02]\n",
      " [-1.6104393e+00  1.0000000e+00  1.0261930e+00 ...  3.6413662e+00\n",
      "   5.7670601e+01 -3.7160435e+01]\n",
      " ...\n",
      " [-1.6104431e+00  1.0000000e+00  1.0262623e+00 ...  3.8878534e+02\n",
      "   7.4124554e+02  3.5202396e+01]\n",
      " [-1.6104412e+00  1.0000000e+00  1.0262146e+00 ... -9.7121477e+00\n",
      "   5.7255020e+00  4.7997441e+00]\n",
      " [-1.6102839e+00  1.0000000e+00  1.0263748e+00 ...  7.7554407e+00\n",
      "  -3.1060093e+01  3.9619370e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.2       0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.84861565e+00  1.00000000e+00  4.78151321e-01 ... -5.15345812e-01\n",
      "   2.72966623e-01 -3.17333698e-01]\n",
      " [-1.84861279e+00  1.00000000e+00  4.78164673e-01 ...  4.00452042e+01\n",
      "   1.21574036e+02 -9.11404190e+01]\n",
      " [-1.84861755e+00  1.00000000e+00  4.78153467e-01 ... -3.85886993e+01\n",
      "  -2.44189396e+01  7.52965317e+01]\n",
      " ...\n",
      " [-1.84862709e+00  1.00000000e+00  4.78213310e-01 ...  5.37754364e+01\n",
      "  -3.98595657e+01  1.41030331e+01]\n",
      " [-1.84862137e+00  1.00000000e+00  4.78193283e-01 ... -2.10964813e+02\n",
      "   6.70774002e+01  4.59358856e+02]\n",
      " [-1.84854412e+00  1.00000000e+00  4.78382111e-01 ...  5.57472878e+01\n",
      "  -1.33149246e+02 -4.95667229e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.2       0.1       0.        1.0000001 0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.6       0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90594673e+00  1.00000000e+00 -1.16807938e-01 ... -2.29660511e-01\n",
      "   1.44000936e+00  9.23564434e-02]\n",
      " [-1.90594769e+00  1.00000000e+00 -1.16778374e-01 ... -2.87787533e+01\n",
      "   1.73298836e+01  1.90789032e+01]\n",
      " [-1.90594101e+00  1.00000000e+00 -1.16780534e-01 ... -2.53055286e+01\n",
      "   2.61496353e+01 -1.00793129e+02]\n",
      " ...\n",
      " [-1.90596199e+00  1.00000000e+00 -1.16727829e-01 ... -3.19043922e+01\n",
      "  -5.10549469e+01  6.30121269e+01]\n",
      " [-1.90594864e+00  1.00000000e+00 -1.16764069e-01 ...  1.51577042e+02\n",
      "   2.61957111e+01 -1.61629593e+02]\n",
      " [-1.90595436e+00  1.00000000e+00 -1.16567612e-01 ...  1.37215698e+02\n",
      "   3.30222893e+01  3.45186310e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        1.0000001\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7765512e+00  1.0000000e+00 -7.0029259e-01 ...  3.4126735e-01\n",
      "   3.3243895e-02 -4.3596768e-01]\n",
      " [-1.7765532e+00  1.0000000e+00 -7.0023155e-01 ...  2.0730769e+02\n",
      "  -6.9791039e+01  2.0884160e+02]\n",
      " [-1.7764816e+00  1.0000000e+00 -7.0025623e-01 ... -1.5401109e+02\n",
      "   3.5505787e+01 -1.3452922e+02]\n",
      " ...\n",
      " [-1.7765198e+00  1.0000000e+00 -7.0018196e-01 ...  1.7023293e+02\n",
      "   4.4871742e+01  2.2183670e+02]\n",
      " [-1.7765007e+00  1.0000000e+00 -7.0025253e-01 ... -4.8771439e+02\n",
      "  -2.4530222e+02  5.7106964e+02]\n",
      " [-1.7766285e+00  1.0000000e+00 -7.0007324e-01 ...  1.4111430e+02\n",
      "  -2.7586649e+01 -6.9304245e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.3\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4731731e+00  1.0000000e+00 -1.2150974e+00 ... -1.1268220e+00\n",
      "  -7.6835048e-01 -4.7002196e-02]\n",
      " [-1.4731750e+00  1.0000000e+00 -1.2150755e+00 ...  1.3410412e+01\n",
      "   1.2496961e+02 -8.9806656e+01]\n",
      " [-1.4731007e+00  1.0000000e+00 -1.2150851e+00 ...  3.5274998e+01\n",
      "  -5.6887592e+02  2.8626344e+00]\n",
      " ...\n",
      " [-1.4731579e+00  1.0000000e+00 -1.2150316e+00 ...  1.7145982e+02\n",
      "   2.8247647e+02 -7.5317200e+01]\n",
      " [-1.4731293e+00  1.0000000e+00 -1.2150650e+00 ...  1.2489371e+02\n",
      "   4.2659889e+01  1.7807707e+01]\n",
      " [-1.4733114e+00  1.0000000e+00 -1.2149258e+00 ... -1.9190688e+02\n",
      "   8.3402580e+01 -4.5975529e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.4       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.02581882e+00  1.00000000e+00 -1.61076355e+00 ... -1.30842829e+00\n",
      "  -6.37743592e-01  3.92699242e-03]\n",
      " [-1.02582550e+00  1.00000000e+00 -1.61072826e+00 ... -1.34278498e+01\n",
      "   1.30608711e+01  6.12191811e+01]\n",
      " [-1.02574730e+00  1.00000000e+00 -1.61073744e+00 ... -1.27825470e+02\n",
      "  -5.63516350e+01 -5.00531387e+01]\n",
      " ...\n",
      " [-1.02581406e+00  1.00000000e+00 -1.61069393e+00 ... -1.54274551e+02\n",
      "  -5.68503456e+01  1.63512955e+02]\n",
      " [-1.02577972e+00  1.00000000e+00 -1.61073685e+00 ...  2.42672943e+02\n",
      "  -1.13397896e+02  4.07405663e+01]\n",
      " [-1.02600384e+00  1.00000000e+00 -1.61064339e+00 ... -7.08621750e+01\n",
      "   2.37381220e+00 -1.92255341e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.7743320e-01  1.0000000e+00 -1.8490238e+00 ... -7.4356184e+00\n",
      "   1.4410751e+00 -1.0267850e+00]\n",
      " [-4.7744083e-01  1.0000000e+00 -1.8489971e+00 ...  2.3017756e+01\n",
      "  -7.1866617e+00  2.9660416e+01]\n",
      " [-4.7734261e-01  1.0000000e+00 -1.8490086e+00 ... -5.2436951e+02\n",
      "   2.1167346e+02 -1.8045091e+02]\n",
      " ...\n",
      " [-4.7742081e-01  1.0000000e+00 -1.8489752e+00 ...  5.4746639e+01\n",
      "  -6.0921837e+01  1.2257002e+02]\n",
      " [-4.7737694e-01  1.0000000e+00 -1.8489990e+00 ... -3.2500348e+00\n",
      "   2.0466469e+01  1.2254695e+00]\n",
      " [-4.7764587e-01  1.0000000e+00 -1.8489609e+00 ...  1.2935754e+02\n",
      "  -5.4266426e+01 -1.4440120e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.17079735e-01  1.00000000e+00 -1.90606308e+00 ... -6.74543428e+00\n",
      "   1.66946995e+00 -9.71767843e-01]\n",
      " [ 1.17071152e-01  1.00000000e+00 -1.90605068e+00 ...  1.86434769e+02\n",
      "  -1.32968781e+02  5.43587097e+02]\n",
      " [ 1.17189407e-01  1.00000000e+00 -1.90605366e+00 ...  1.67058594e+02\n",
      "  -1.16615547e+02 -1.46442795e+02]\n",
      " ...\n",
      " [ 1.17109299e-01  1.00000000e+00 -1.90603828e+00 ...  1.36369507e+02\n",
      "   7.06835556e+01 -2.82446625e+02]\n",
      " [ 1.17153168e-01  1.00000000e+00 -1.90603828e+00 ... -1.89778519e+02\n",
      "   7.80912109e+02 -1.45222644e+03]\n",
      " [ 1.16863251e-01  1.00000000e+00 -1.90607452e+00 ... -2.37544205e+02\n",
      "   1.78405334e+02 -1.09649406e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.0054722e-01  1.0000000e+00 -1.7762356e+00 ... -2.0265012e+00\n",
      "   4.1331935e-01 -8.1186056e-02]\n",
      " [ 7.0054054e-01  1.0000000e+00 -1.7762842e+00 ...  1.5209041e+01\n",
      "  -4.9504391e+01  1.5212479e+00]\n",
      " [ 7.0065117e-01  1.0000000e+00 -1.7762600e+00 ... -3.6297356e+01\n",
      "   8.1993065e+01  9.0213556e+00]\n",
      " ...\n",
      " [ 7.0057297e-01  1.0000000e+00 -1.7762718e+00 ... -2.1310649e+02\n",
      "   1.1009715e+02 -8.5307884e+01]\n",
      " [ 7.0061684e-01  1.0000000e+00 -1.7762089e+00 ... -8.9209122e+01\n",
      "  -1.5421912e+02  1.1912865e+02]\n",
      " [ 7.0034313e-01  1.0000000e+00 -1.7763119e+00 ... -4.3279028e+02\n",
      "   1.6282298e+02  2.0315504e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.2154064     1.           -1.4726639  ...    0.44538146\n",
      "    -0.48802614    3.1480262 ]\n",
      " [   1.2153997     1.           -1.4726915  ...  152.0715\n",
      "  -104.520424   -205.71346   ]\n",
      " [   1.2154789     1.           -1.4726722  ...  226.8499\n",
      "   -22.614872    -54.083405  ]\n",
      " ...\n",
      " [   1.2154121     1.           -1.4726782  ...    3.7775543\n",
      "    79.30387     118.142586  ]\n",
      " [   1.2154484     1.           -1.47262    ...   43.91716\n",
      "  -375.70453     126.09005   ]\n",
      " [   1.2152405     1.           -1.4727726  ... -184.11356\n",
      "   287.5531       25.36593   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6109028e+00  1.0000000e+00 -1.0250778e+00 ...  7.5999165e-01\n",
      "  -1.6484547e-01  5.9249735e-01]\n",
      " [ 1.6108971e+00  1.0000000e+00 -1.0250950e+00 ...  6.4978889e+01\n",
      "  -6.4818619e+01 -1.8212934e+01]\n",
      " [ 1.6109657e+00  1.0000000e+00 -1.0250746e+00 ...  5.0995670e+01\n",
      "   3.8720600e+01  3.1181074e+01]\n",
      " ...\n",
      " [ 1.6109104e+00  1.0000000e+00 -1.0250816e+00 ... -2.6386034e+01\n",
      "  -8.7549858e+01  1.9212221e+02]\n",
      " [ 1.6109390e+00  1.0000000e+00 -1.0250282e+00 ... -4.3345108e+01\n",
      "  -9.9170830e+01 -1.2865593e+02]\n",
      " [ 1.6107903e+00  1.0000000e+00 -1.0252266e+00 ...  4.3560650e+01\n",
      "  -5.7634878e-01 -5.0380287e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.9000001 0.        0.\n",
      " 0.        0.2       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8488932     1.           -0.47717476 ...    1.5084004\n",
      "     0.9408916     0.40847993]\n",
      " [   1.8488922     1.           -0.47715187 ...   14.457355\n",
      "   -51.777817    -22.02908   ]\n",
      " [   1.8489285     1.           -0.47711766 ... -171.90121\n",
      "  -142.6936      294.97464   ]\n",
      " ...\n",
      " [   1.8488903     1.           -0.47713757 ...  110.33002\n",
      "    37.084675     42.188786  ]\n",
      " [   1.8489037     1.           -0.47711945 ...  109.11161\n",
      "   140.8059       26.74952   ]\n",
      " [   1.8488388     1.           -0.47735023 ... -306.6112\n",
      "    67.61948     -41.36096   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90569782e+00  1.00000000e+00  1.18011475e-01 ... -4.11815643e-02\n",
      "   1.64061308e+00  8.89340043e-02]\n",
      " [ 1.90569687e+00  1.00000000e+00  1.18027687e-01 ...  7.42889643e-01\n",
      "   2.96018753e+01  3.17961388e+01]\n",
      " [ 1.90571022e+00  1.00000000e+00  1.18066728e-01 ... -4.58765078e+00\n",
      "   4.89884834e+01 -2.47605095e+01]\n",
      " ...\n",
      " [ 1.90568542e+00  1.00000000e+00  1.18041992e-01 ...  2.43006485e+02\n",
      "   1.55790970e+02 -1.06538391e+02]\n",
      " [ 1.90568542e+00  1.00000000e+00  1.18066788e-01 ...  6.19327164e+01\n",
      "   7.45580139e+01  8.52428818e+01]\n",
      " [ 1.90570164e+00  1.00000000e+00  1.17834091e-01 ...  8.10202148e+02\n",
      "   1.92663647e+03  5.67352661e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.1       0.        0.        1.0000001 0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7759571e+00  1.0000000e+00  7.0113945e-01 ...  6.4329243e-01\n",
      "   1.3452778e+00  6.2598705e-02]\n",
      " [ 1.7759571e+00  1.0000000e+00  7.0117378e-01 ...  5.7019123e+01\n",
      "  -1.9856349e+02 -4.5900681e+01]\n",
      " [ 1.7759438e+00  1.0000000e+00  7.0121473e-01 ... -2.4655606e+02\n",
      "  -8.8930962e+01 -1.7182281e+02]\n",
      " ...\n",
      " [ 1.7759209e+00  1.0000000e+00  7.0118809e-01 ... -1.4306433e+02\n",
      "  -5.2271751e+01  9.7788933e+01]\n",
      " [ 1.7759190e+00  1.0000000e+00  7.0119095e-01 ...  2.0007262e+02\n",
      "  -1.9829556e+01  2.4389574e+02]\n",
      " [ 1.7760191e+00  1.0000000e+00  7.0098305e-01 ...  9.4266113e+02\n",
      "   6.6519397e+02 -8.4339569e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.6       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.4723969     1.            1.2157135  ...    0.7420645\n",
      "     0.29829693   -0.2673453 ]\n",
      " [   1.4723969     1.            1.2157593  ...  133.6604\n",
      "    51.260433    218.82756   ]\n",
      " [   1.4723415     1.            1.215783   ...  192.80373\n",
      "    40.70413    -151.41219   ]\n",
      " ...\n",
      " [   1.4723358     1.            1.2157688  ...   38.404522\n",
      "    40.05272    -140.65897   ]\n",
      " [   1.4723186     1.            1.2157593  ...  222.33803\n",
      "  -176.85805      15.982583  ]\n",
      " [   1.4725122     1.            1.2155876  ... -131.43655\n",
      "    13.095849     95.158615  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.70000005 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0245171e+00  1.0000000e+00  1.6113415e+00 ... -2.6044426e+00\n",
      "  -1.8305812e+00  2.4656892e-02]\n",
      " [ 1.0245161e+00  1.0000000e+00  1.6114397e+00 ... -1.2683410e+02\n",
      "   6.4703674e+01  9.0705280e+00]\n",
      " [ 1.0244236e+00  1.0000000e+00  1.6114262e+00 ...  7.4254143e+01\n",
      "   8.8305542e+01  4.5392891e+01]\n",
      " ...\n",
      " [ 1.0244389e+00  1.0000000e+00  1.6114473e+00 ... -5.8467987e+01\n",
      "  -8.4404724e+01 -2.3236698e+01]\n",
      " [ 1.0244045e+00  1.0000000e+00  1.6113701e+00 ... -1.2404721e+02\n",
      "  -2.2505605e+02 -5.8333073e+01]\n",
      " [ 1.0246658e+00  1.0000000e+00  1.6112556e+00 ... -7.1119125e+01\n",
      "   1.4220768e+01  5.1151073e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.7609043e-01  1.0000000e+00  1.8491135e+00 ... -9.0725622e+00\n",
      "  -3.9738719e+00 -7.7153996e-02]\n",
      " [ 4.7608662e-01  1.0000000e+00  1.8492088e+00 ... -3.6235042e+00\n",
      "   7.5758801e+00 -2.2839514e+01]\n",
      " [ 4.7602844e-01  1.0000000e+00  1.8491769e+00 ...  5.2818478e+01\n",
      "  -3.2128071e+01 -6.8297606e+00]\n",
      " ...\n",
      " [ 4.7604561e-01  1.0000000e+00  1.8492107e+00 ... -3.6130802e+01\n",
      "  -3.6477303e+01 -9.1456268e+01]\n",
      " [ 4.7600937e-01  1.0000000e+00  1.8491249e+00 ... -9.0830544e+01\n",
      "  -1.3108707e+02  6.9571781e+00]\n",
      " [ 4.7627449e-01  1.0000000e+00  1.8490829e+00 ...  8.7683067e+01\n",
      "  -3.3445691e+02  1.2783314e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.18463516e-01  1.00000000e+00  1.90582275e+00 ... -4.66836882e+00\n",
      "  -2.27608609e+00 -7.75812864e-02]\n",
      " [-1.18467331e-01  1.00000000e+00  1.90589046e+00 ...  4.60256767e+01\n",
      "  -1.99548477e+02 -5.93797350e+00]\n",
      " [-1.18535995e-01  1.00000000e+00  1.90585589e+00 ... -9.86129684e+01\n",
      "   7.79380417e+01 -1.77324343e+00]\n",
      " ...\n",
      " [-1.18515015e-01  1.00000000e+00  1.90587807e+00 ... -9.18776627e+01\n",
      "   5.61476379e+02 -1.40442825e+02]\n",
      " [-1.18555069e-01  1.00000000e+00  1.90579796e+00 ... -1.75290430e+03\n",
      "   3.61495392e+02  4.15512543e+02]\n",
      " [-1.18277550e-01  1.00000000e+00  1.90584946e+00 ... -2.71616211e+02\n",
      "   1.00958154e+03 -2.38707855e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.0191956e-01  1.0000000e+00  1.7758312e+00 ...  2.2487075e+00\n",
      "   2.5809169e-01  1.8815215e+00]\n",
      " [-7.0192242e-01  1.0000000e+00  1.7758512e+00 ... -2.9928320e+02\n",
      "   3.2481834e+01 -2.4418758e+02]\n",
      " [-7.0202827e-01  1.0000000e+00  1.7757992e+00 ...  7.3013550e+01\n",
      "  -1.2686115e+01  4.4394329e+01]\n",
      " ...\n",
      " [-7.0200920e-01  1.0000000e+00  1.7758274e+00 ... -5.0164824e+00\n",
      "   1.5915838e+01 -7.6124220e+00]\n",
      " [-7.0204735e-01  1.0000000e+00  1.7757835e+00 ...  1.2844469e+03\n",
      "  -5.7023492e+02 -1.1943538e+03]\n",
      " [-7.0174599e-01  1.0000000e+00  1.7759018e+00 ... -1.5088792e+02\n",
      "   1.2248375e+02  7.4862877e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.2165880e+00  1.0000000e+00  1.4718914e+00 ...  1.6340590e-01\n",
      "   9.0766835e-01 -7.8543067e-02]\n",
      " [-1.2165899e+00  1.0000000e+00  1.4719086e+00 ... -1.1959257e+03\n",
      "  -6.6832257e+02 -1.0061733e+02]\n",
      " [-1.2166710e+00  1.0000000e+00  1.4718430e+00 ...  1.4673398e+02\n",
      "   2.4299202e+02  7.3597069e+01]\n",
      " ...\n",
      " [-1.2166519e+00  1.0000000e+00  1.4718771e+00 ...  6.9363123e+02\n",
      "   3.4938492e+02 -8.6520386e+00]\n",
      " [-1.2166882e+00  1.0000000e+00  1.4718132e+00 ... -8.2193871e+01\n",
      "   3.2313742e+02  3.5244391e+02]\n",
      " [-1.2164383e+00  1.0000000e+00  1.4720173e+00 ...  3.0883286e+02\n",
      "  -2.1538107e+01 -1.3449258e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6118336e+00  1.0000000e+00  1.0240898e+00 ...  2.4648523e-01\n",
      "   1.3714166e+00  2.0644590e-02]\n",
      " [-1.6118326e+00  1.0000000e+00  1.0241041e+00 ...  2.9624307e+02\n",
      "   5.7237651e+03 -1.9126200e+03]\n",
      " [-1.6119213e+00  1.0000000e+00  1.0240262e+00 ... -6.6602840e+00\n",
      "   6.2541660e+01  3.7254326e+01]\n",
      " ...\n",
      " [-1.6119041e+00  1.0000000e+00  1.0240679e+00 ...  1.2407814e+02\n",
      "   1.8837880e+02  1.8708020e+02]\n",
      " [-1.6119308e+00  1.0000000e+00  1.0239925e+00 ...  6.4650177e+01\n",
      "   9.9029289e+01  1.9775909e+02]\n",
      " [-1.6117163e+00  1.0000000e+00  1.0242519e+00 ...  3.9349472e+02\n",
      "   3.1569885e+02  9.7532700e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8495255e+00  1.0000000e+00  4.7560883e-01 ... -1.4467192e-01\n",
      "   5.3398180e-01  9.5075667e-02]\n",
      " [-1.8495226e+00  1.0000000e+00  4.7563648e-01 ...  1.0797614e+03\n",
      "  -4.1073859e+02  7.8452576e+02]\n",
      " [-1.8495770e+00  1.0000000e+00  4.7555000e-01 ... -2.0428905e+01\n",
      "  -6.0468365e+01  2.1309886e+02]\n",
      " ...\n",
      " [-1.8495617e+00  1.0000000e+00  4.7559452e-01 ...  1.4317805e+02\n",
      "   8.5515692e+02  6.7786629e+01]\n",
      " [-1.8495808e+00  1.0000000e+00  4.7550011e-01 ... -4.1334940e+02\n",
      "  -1.4321122e+03 -1.5060248e+03]\n",
      " [-1.8494492e+00  1.0000000e+00  4.7578812e-01 ...  6.1559673e+03\n",
      "  -3.7043293e+03 -3.0055989e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.5       0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90600395e+00  1.00000000e+00 -1.19270325e-01 ... -1.51625633e-01\n",
      "  -3.39947939e-01 -3.68010998e-03]\n",
      " [-1.90600014e+00  1.00000000e+00 -1.19268417e-01 ... -9.01169495e+02\n",
      "  -3.44480933e+03  1.57322217e+03]\n",
      " [-1.90605545e+00  1.00000000e+00 -1.19346678e-01 ...  1.89083588e+02\n",
      "  -1.78885574e+02 -7.14370651e+01]\n",
      " ...\n",
      " [-1.90605354e+00  1.00000000e+00 -1.19312286e-01 ...  1.12435028e+02\n",
      "  -7.42713776e+01 -1.52143280e+02]\n",
      " [-1.90605354e+00  1.00000000e+00 -1.19380951e-01 ... -1.47202423e+02\n",
      "  -3.08820190e+02  1.33216202e+02]\n",
      " [-1.90597916e+00  1.00000000e+00 -1.19087219e-01 ... -1.26399707e+03\n",
      "  -8.49774353e+02  1.72711319e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.77583027e+00  1.00000000e+00 -7.02508926e-01 ... -9.53069687e-01\n",
      "  -1.17728496e+00  3.84769738e-02]\n",
      " [-1.77582455e+00  1.00000000e+00 -7.02513695e-01 ... -1.50408252e+03\n",
      "  -1.21666565e+02 -6.08500385e+00]\n",
      " [-1.77586746e+00  1.00000000e+00 -7.02585459e-01 ... -9.16811447e+01\n",
      "   3.97242470e+01  3.70298386e+01]\n",
      " ...\n",
      " [-1.77586937e+00  1.00000000e+00 -7.02551842e-01 ... -2.37555737e+03\n",
      "   1.39868115e+03 -1.03187549e+03]\n",
      " [-1.77585793e+00  1.00000000e+00 -7.02615738e-01 ... -6.87515163e+00\n",
      "   2.16992661e+02 -1.58457733e+02]\n",
      " [-1.77585697e+00  1.00000000e+00 -7.02335358e-01 ...  1.89642960e+02\n",
      "  -6.49368835e+02  1.58180005e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.8000001 0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4719753e+00  1.0000000e+00 -1.2167320e+00 ... -3.1785059e-01\n",
      "  -8.5749483e-01  1.9821462e-01]\n",
      " [-1.4719667e+00  1.0000000e+00 -1.2167206e+00 ...  3.0051767e+02\n",
      "   3.2982413e+02 -9.0730591e+01]\n",
      " [-1.4719887e+00  1.0000000e+00 -1.2167743e+00 ...  1.9540421e+02\n",
      "  -1.2504258e+02 -6.2134621e+01]\n",
      " ...\n",
      " [-1.4719925e+00  1.0000000e+00 -1.2167521e+00 ... -1.8701841e+03\n",
      "   5.9995605e+03 -3.3087261e+01]\n",
      " [-1.4719772e+00  1.0000000e+00 -1.2168217e+00 ... -5.0920536e+01\n",
      "   4.3819859e+01  2.2610603e+01]\n",
      " [-1.4720430e+00  1.0000000e+00 -1.2165833e+00 ... -2.3718961e+02\n",
      "   6.2335388e+02  1.2727076e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.02366829e+00  1.00000000e+00 -1.61221695e+00 ...  1.04109907e+00\n",
      "   3.55856419e-01  2.26134837e-01]\n",
      " [-1.02365971e+00  1.00000000e+00 -1.61217213e+00 ...  2.35073761e+02\n",
      "   4.93342361e+01 -1.06330086e+02]\n",
      " [-1.02367020e+00  1.00000000e+00 -1.61222482e+00 ... -3.59468628e+02\n",
      "   1.74468735e+02 -8.98823013e+01]\n",
      " ...\n",
      " [-1.02368546e+00  1.00000000e+00 -1.61219883e+00 ... -2.15775314e+02\n",
      "   2.03781128e+03 -1.21915833e+03]\n",
      " [-1.02365112e+00  1.00000000e+00 -1.61229134e+00 ... -1.70757427e+01\n",
      "  -4.98831444e+01 -9.23968048e+01]\n",
      " [-1.02376461e+00  1.00000000e+00 -1.61211777e+00 ... -9.83506622e+01\n",
      "  -3.83835236e+02 -2.28456619e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.7555065e-01  1.0000000e+00 -1.8494072e+00 ...  1.4837790e-01\n",
      "   1.0989013e+00  4.8164040e-02]\n",
      " [-4.7554207e-01  1.0000000e+00 -1.8493500e+00 ... -5.4551495e+02\n",
      "   5.4596185e+02 -2.7873875e+01]\n",
      " [-4.7554398e-01  1.0000000e+00 -1.8493879e+00 ... -9.7000931e+01\n",
      "   1.4449994e+02  1.4581171e+02]\n",
      " ...\n",
      " [-4.7556496e-01  1.0000000e+00 -1.8493633e+00 ...  9.2387524e+02\n",
      "   4.9388889e+02 -6.2062775e+02]\n",
      " [-4.7552299e-01  1.0000000e+00 -1.8494492e+00 ... -6.9296501e+01\n",
      "  -6.7324615e+01 -1.2819153e+02]\n",
      " [-4.7568035e-01  1.0000000e+00 -1.8493595e+00 ... -2.7243237e+02\n",
      "  -2.6568271e+02  1.2381510e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1933613e-01  1.0000000e+00 -1.9057903e+00 ...  1.2298312e+00\n",
      "   1.5708835e+00 -6.1962187e-02]\n",
      " [ 1.1934471e-01  1.0000000e+00 -1.9057636e+00 ...  1.3113957e+02\n",
      "   3.2626028e+00 -9.1820938e+01]\n",
      " [ 1.1933708e-01  1.0000000e+00 -1.9057912e+00 ... -4.7342327e+01\n",
      "  -4.5256279e+01  1.7650607e+02]\n",
      " ...\n",
      " [ 1.1931610e-01  1.0000000e+00 -1.9057722e+00 ... -5.8126381e+01\n",
      "  -7.0145288e+02  3.4255176e+02]\n",
      " [ 1.1935806e-01  1.0000000e+00 -1.9058094e+00 ...  3.9491608e+01\n",
      "  -1.5834169e+02 -3.6845197e+02]\n",
      " [ 1.1920071e-01  1.0000000e+00 -1.9057941e+00 ...  2.1307571e+01\n",
      "  -4.6502064e+01  4.4731586e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.0269012e-01  1.0000000e+00 -1.7755299e+00 ...  1.0479341e+00\n",
      "   1.7572248e+00  4.3172455e-01]\n",
      " [ 7.0269775e-01  1.0000000e+00 -1.7754927e+00 ...  1.4015674e+02\n",
      "   3.1002896e+02 -1.6665903e+02]\n",
      " [ 7.0269966e-01  1.0000000e+00 -1.7755120e+00 ...  4.9489666e+01\n",
      "   6.9308136e+01  6.5708280e+00]\n",
      " ...\n",
      " [ 7.0267868e-01  1.0000000e+00 -1.7754984e+00 ... -6.4397540e+02\n",
      "  -3.1202237e+02 -5.0583194e+02]\n",
      " [ 7.0271873e-01  1.0000000e+00 -1.7755108e+00 ...  8.4202366e+00\n",
      "   9.8356544e+01 -1.3046007e+02]\n",
      " [ 7.0256615e-01  1.0000000e+00 -1.7755775e+00 ... -2.5137965e+02\n",
      "   5.9531042e+02 -2.2713106e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2170267e+00  1.0000000e+00 -1.4714489e+00 ... -8.7444878e-01\n",
      "  -1.0422707e+00  3.4236848e-01]\n",
      " [ 1.2170343e+00  1.0000000e+00 -1.4713869e+00 ... -1.6106429e+02\n",
      "   8.8872169e+01 -1.7290880e+02]\n",
      " [ 1.2170429e+00  1.0000000e+00 -1.4714079e+00 ... -1.6555511e+01\n",
      "  -7.4174576e+01  8.1854668e+01]\n",
      " ...\n",
      " [ 1.2170277e+00  1.0000000e+00 -1.4713917e+00 ... -2.1929979e+02\n",
      "   7.8667389e+01 -4.6134052e+02]\n",
      " [ 1.2170563e+00  1.0000000e+00 -1.4714069e+00 ... -1.5884798e+02\n",
      "  -5.1431675e+01  1.8061840e+01]\n",
      " [ 1.2169313e+00  1.0000000e+00 -1.4715252e+00 ...  5.8001642e+02\n",
      "   1.8246205e+03 -6.0194641e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.61212254e+00  1.00000000e+00 -1.02338600e+00 ... -1.78166008e+00\n",
      "  -3.49085522e+00  3.11105251e-01]\n",
      " [ 1.61212444e+00  1.00000000e+00 -1.02334690e+00 ...  1.14982040e+02\n",
      "  -1.02905922e+01  6.02379684e+01]\n",
      " [ 1.61211395e+00  1.00000000e+00 -1.02336335e+00 ... -1.16321251e+02\n",
      "  -5.19741783e+01  8.12935944e+01]\n",
      " ...\n",
      " [ 1.61210251e+00  1.00000000e+00 -1.02335167e+00 ... -7.43790359e+01\n",
      "  -7.92331009e+01 -1.21824448e+02]\n",
      " [ 1.61211014e+00  1.00000000e+00 -1.02333069e+00 ... -1.43828125e+01\n",
      "   1.76951180e+01 -1.12150513e+02]\n",
      " [ 1.61205864e+00  1.00000000e+00 -1.02349854e+00 ... -3.22280487e+02\n",
      "  -2.95675842e+02  1.96969254e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.8000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8495522     1.           -0.4749775  ...   -4.50006\n",
      "    -6.64418       1.721848  ]\n",
      " [   1.8495398     1.           -0.47489452 ...   24.799274\n",
      "   147.35046     121.19137   ]\n",
      " [   1.8495407     1.           -0.47492138 ...  386.37537\n",
      "   135.77864      -0.821733  ]\n",
      " ...\n",
      " [   1.8495293     1.           -0.4748993  ...  -50.563232\n",
      "   -66.92277      42.830173  ]\n",
      " [   1.8495255     1.           -0.47491264 ... -404.7714\n",
      "    -4.884359   -136.08493   ]\n",
      " [   1.8495216     1.           -0.4751053  ...  106.23093\n",
      "   224.32513      73.63822   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:5, Score:4.62, Best Score:15.21, Average Score:9.00, Best Avg Score:12.04\n",
      "\n",
      "Total training time = 1.7 min\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent \n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from workspace_utils import active_session\n",
    "import os\n",
    "\n",
    "# env_name = \"/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux.x86_64\" # Path to robotics reaching exe\n",
    "env_name = \"/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux.x86_64\" # Path to robotics reaching exe wihtout script debugging\n",
    "\n",
    "# Ensure the executable has the necessary permissions \n",
    "os.chmod(env_name, 0o755)\n",
    "\n",
    "try: \n",
    "    # Launch unity environment\n",
    "    env = UnityEnvironment(file_name=env_name,seed=1, side_channels=[], worker_id=5)\n",
    "\n",
    "    # Start the environment \n",
    "    env.reset()\n",
    "\n",
    "    # Get behaviour names \n",
    "    behaviour_names = env.behavior_specs.keys()\n",
    "\n",
    "    # Check that behaviour names have been retrieved from the environment\n",
    "    if not behaviour_names:\n",
    "        print(\"No behaviours found. Ensure that the unity environment has agents with behaviours\")\n",
    "    else:\n",
    "        behaviour_name = list(env.behavior_specs.keys())[0]\n",
    "        print(f\"Behaviour name: {behaviour_name}\")\n",
    "\n",
    "        # Get what actions the environment expects and the required shape\n",
    "        behaviour_spec = env.behavior_specs[behaviour_name]\n",
    "        print(f\"Behaviour specifications: {behaviour_spec.action_spec}\")\n",
    "\n",
    "        # Get the number of agents \n",
    "        decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "        num_agents = len(decisionSteps) + len(terminalSteps)\n",
    "        print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing environment: {e}\")\n",
    "    behavior_name = None\n",
    "\n",
    "\n",
    "agent = Agent(state_size=52, action_size=4, random_seed=2) # Altered from origional to fit new environment\n",
    "\n",
    "def ddpg(n_episodes=5, max_t=1000):\n",
    "    \n",
    "    print(\"Enter ddpg...\\n\")\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    actions = []\n",
    "    best_score = 0\n",
    "    best_average_score = 0\n",
    "    # try:\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        print(f\"Episode number: {i_episode}\")\n",
    "        avg_score = 0\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset()\n",
    "\n",
    "        #get the decision and terminal steps\n",
    "        decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "        print(\"Printing decisionSteps: \" )\n",
    "        print(decisionSteps)\n",
    "        print(type(decisionSteps))\n",
    "\n",
    "        # get number of agents\n",
    "        num_agents = len(decisionSteps)\n",
    "        print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "        # get number of continuous actions\n",
    "        num_continuous_actions = env.behavior_specs[behaviour_name].action_spec.continuous_size\n",
    "\n",
    "        # create 2D numpy array of continuous actions \n",
    "        continuous_actions = np.random.rand(num_agents, num_continuous_actions).astype(np.float32)\n",
    "\n",
    "        # create actiontuple\n",
    "        action_tuple = ActionTuple(continuous=continuous_actions)\n",
    "\n",
    "        # get the states vector\n",
    "        stateVector = decisionSteps.obs[0]\n",
    "\n",
    "        #init score agents\n",
    "        scores_agents = np.zeros(num_agents)\n",
    "        print(\"scores_agents type: \", type(scores_agents))\n",
    "        print(\"scores_agents shape: \", scores_agents.shape)\n",
    "\n",
    "        score = 0\n",
    "        agent.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "\n",
    "            try:\n",
    "                # Checkpoint to ensure it's not getting stuck\n",
    "                if t % 100 == 0: \n",
    "                    print(f\"Progressing at step {t}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "            # set the actions for the behaviour and step the environment\n",
    "            env.set_actions(behavior_name=behaviour_name, action=action_tuple)\n",
    "\n",
    "            # Step the environment to get the next states \n",
    "            env.step()\n",
    "\n",
    "            # get the next states\n",
    "            decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "            print(f\"Step {t}, Decision steps: {len(decisionSteps)}, Terminal steps: {len(terminalSteps)}\")\n",
    "\n",
    "            # Check if all agents are in terminal state\n",
    "            if len(decisionSteps)==0 and len(terminalSteps)>0:\n",
    "                print(f\"All agents are in terminal states at step {t}. Ending episode early.\")\n",
    "                break\n",
    "\n",
    "            # extract the next states vector from the decision steps \n",
    "            next_state_vector = decisionSteps.obs[0]\n",
    "            print(\"Next state vector: \", next_state_vector)\n",
    "\n",
    "            # get the rewards\n",
    "            rewards = decisionSteps.reward\n",
    "            print(\"rewards type: \", type(rewards))\n",
    "            print(\"rewards shape: \", rewards.shape)\n",
    "            print(\"rewards: \", rewards)\n",
    "\n",
    "            episode_finished = np.array([len(terminalSteps) > 0] * num_agents) # episode_fiished values must be passed into agent.step function as an array\n",
    "            print(\"Episode finished: \", episode_finished)\n",
    "\n",
    "            # see if episode has finished\n",
    "            if next_state_vector is not None: \n",
    "                agent.step(stateVector, actions, rewards, next_state_vector, episode_finished)\n",
    "                stateVector = next_state_vector\n",
    "            #Check if scores-agents and rewards are compatible for addition\n",
    "            scores_agents = np.add(scores_agents, rewards)\n",
    "            # scores_agents += rewards\n",
    "            if np.any(episode_finished):\n",
    "                break\n",
    "\n",
    "        # mean score of 20 agents in this episode\n",
    "        score = np.mean(scores_agents)\n",
    "        scores_deque.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        scores.append(score)\n",
    "\n",
    "        #refresh the best agent score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "        #refresh the best average score    \n",
    "        if avg_score > best_average_score:\n",
    "            best_average_score = avg_score\n",
    "        \n",
    "        #print current episode\n",
    "        print(\"Episode:{}, Score:{:.2f}, Best Score:{:.2f}, Average Score:{:.2f}, Best Avg Score:{:.2f}\".format(\n",
    "            i_episode, score, best_score, avg_score, best_average_score))\n",
    "        if (avg_score >= 32):\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_solved.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_solved.pth')\n",
    "            break\n",
    "    # finally:\n",
    "        # Save the solved paths \n",
    "    torch.save(agent.actor_local.state_dict(), 'actor_solved.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), 'critic_solved.pth')\n",
    "        # env.close() # Ensure env.close() is alwyas called    \n",
    "    return scores\n",
    "\n",
    "start = time.time()\n",
    "with active_session():\n",
    "    scores = ddpg()\n",
    "end = time.time()\n",
    "print('\\nTotal training time = {:.1f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot of the training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing using the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "state size: 52\n",
      "action size: 4\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Upperarm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(0, 52)\n",
      "Total score (averaged over agents) this episode: 3.6750003308057786\n"
     ]
    }
   ],
   "source": [
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "env.reset() # reset the unity environment\n",
    "behaviour_name = list(env.behavior_specs.keys())[0] # get behviour name\n",
    "decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name) #get the decision and terminal steps\n",
    "stateVector = decisionSteps.obs[0] # get the current state for each agent\n",
    "agent = Agent(state_size=52, action_size=4, random_seed=2)\n",
    "num_agents = len(decisionSteps) + len(terminalSteps)# get the number of agents\n",
    "scores = np.zeros(num_agents) # initialize the score for each agent \n",
    "agent.actor_local.load_state_dict(torch.load('actor_solved.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('critic_solved.pth'))\n",
    "agent.reset()\n",
    "\n",
    "# Get behavior specs\n",
    "behavior_specs = env.behavior_specs[behaviour_name]\n",
    "action_size = behavior_specs.action_spec.continuous_size\n",
    "state_size = behavior_specs.observation_specs[0].shape[0]\n",
    "\n",
    "print(f\"state size: {state_size}\")\n",
    "print(f\"action size: {action_size}\")\n",
    "\n",
    "while True:\n",
    "    num_continuous_actions = env.behavior_specs[behaviour_name].action_spec.continuous_size # get number of continuous actions\n",
    "    continuous_actions = np.random.rand(num_agents, num_continuous_actions).astype(np.float32) # select actions \n",
    "    action_tuple = ActionTuple(continuous=continuous_actions) # create action tuple for continuous actions \n",
    "    env.set_actions(behavior_name=behaviour_name, action=action_tuple) #send the actions to the environemnt\n",
    "    env.step() # step the environment\n",
    "\n",
    "    decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "    next_state_vector = decisionSteps.obs[0] # get next state vector\n",
    "\n",
    "    rewards = decisionSteps.reward # get rewards \n",
    "\n",
    "    episode_finished = np.array([len(terminalSteps) > 0] * num_agents) # episode_fiished values must be passed into agent.step function as an array\n",
    "    if rewards.shape[0] == num_agents: \n",
    "        scores = np.add(scores, rewards) # update the score\n",
    "    if next_state_vector is not None: # roll over states to next timestep \n",
    "        print(next_state_vector.shape)\n",
    "        agent.step(stateVector, action_tuple.continuous, rewards, next_state_vector, episode_finished)\n",
    "        stateVector = next_state_vector\n",
    "    if np.any(episode_finished): # Exit loop if episode finshed \n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 719169 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 128 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 134.6 KB\n",
      "      Overflow Count 4\n",
      "    [ALLOC_TEMP_Background Job.Worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 6\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 11\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 199 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 27\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [8.0 MB-16.0 MB]: 719170 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 12.8 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 719170 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 719169 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 65.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 719170 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 64.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 719170 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [256.0 KB-0.5 MB]: 719169 frames, [1.0 MB-2.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 1.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 719170 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 96 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [0-1.0 KB]: 719170 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 0\n",
      "      Peak Allocated memory 0 B\n",
      "      Peak Large allocation bytes 0 B\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DDPG Docker)",
   "language": "python",
   "name": "ddpg_docker_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
