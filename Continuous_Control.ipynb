{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: './python': Expected package name at the start of dependency specifier\n",
      "    ./python\n",
      "    ^\n",
      "Hint: It looks like a path. File './python' does not exist.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/Managed'\n",
      "Mono config path = '/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/MonoBleedingEdge/etc'\n",
      "Starting managed debugger on port 56982\n",
      "Using monoOptions --debugger-agent=transport=dt_socket,embedding=1,server=y,suspend=n,address=0.0.0.0:56982\n",
      "Preloaded 'lib_burst_generated.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Found 1 interfaces on host : 0) 172.18.0.2\n",
      "Player connection [140037753343872] Multi-casting \"[IP] 172.18.0.2 [Port] 55000 [Flags] 2 [Guid] 1335706982 [EditorId] 2560969981 [Version] 1048832 [Id] LinuxServer(43,172.18.0.2) [Debug] 1 [PackageName] LinuxServer [ProjectName] robotics_reaching_environment\" to [225.0.0.222:54997]...\n",
      "\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "Initialize engine version: 2022.3.36f1 (95a4219250e5)\n",
      "[Subsystems] Discovering subsystems at path /root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux_Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; threaded=0; jobified=0\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Loaded All Assemblies, in  0.125 seconds\n",
      "- Finished resetting the current domain, in  0.003 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "There is no texture data available to upload.\n",
      "[PhysX] Initialized MultithreadedTaskDispatcher with 12 workers.\n",
      "UnloadTime: 0.905422 ms\n"
     ]
    }
   ],
   "source": [
    "import mlagents\n",
    "import mlagents_envs\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux.x86_64', seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:562)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Behaviour name: HandAgent?team=0\n"
     ]
    }
   ],
   "source": [
    "# Start the environment\n",
    "env.reset()\n",
    "\n",
    "# Get behaviour names\n",
    "behaviour_names = env.behavior_specs.keys()\n",
    "behaviour_name = list(env.behavior_specs.keys())[0]\n",
    "print(f\"Behaviour name: {behaviour_name}\")\n",
    "\n",
    "# # get the default brain\n",
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Forearm\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "UnityEngine.MonoBehaviour:print (object)\n",
      "colourChange:OnTriggerEnter (UnityEngine.Collider) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/colourChange.cs:16)\n",
      "\n",
      "Printing decisionSteps:  <mlagents_envs.base_env.DecisionSteps object at 0x7fa63808b970>\n",
      "Number of agents: 20\n",
      "Continuous action size: 4\n",
      "Discrete action size: 0\n",
      "There are 20 agents. Each observes a state with length: 52\n",
      "The state for the first agent looks like: [ 1.9981155e+00  1.0000000e+00  5.9970856e-02  0.0000000e+00\n",
      " -1.5707318e-02 -0.0000000e+00  9.9987668e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  4.3590826e-01  0.0000000e+00\n",
      " -1.0999048e-10 -4.4527781e-17  5.8420269e-15  1.0000000e+00\n",
      " -1.1516385e-06 -3.2951250e+00 -1.8607207e-09  1.1294162e-09\n",
      " -4.1197904e-15  2.2469334e-13  0.0000000e+00  3.5122869e+00\n",
      "  0.0000000e+00  1.2247365e-09  1.4922823e-16  4.2405148e-08\n",
      "  1.0000000e+00  2.8249647e-06 -1.4555795e+00 -1.2519078e-07\n",
      " -2.2184851e-07  1.3806863e-14 -2.6890327e-06  0.0000000e+00\n",
      "  1.5322870e+00  0.0000000e+00  1.3035524e-10 -2.4315641e-17\n",
      " -0.0000000e+00  1.0000000e+00  1.1615396e-06 -1.4555788e+00\n",
      "  0.0000000e+00  7.1078759e-08  1.6745450e-14  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Get the decsion and terminal steps\n",
    "decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "print(\"Printing decisionSteps: \", decisionSteps)\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(decisionSteps)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# size of each action\n",
    "behaviour_spec = env.behavior_specs[behaviour_name]\n",
    "action_spec = behaviour_spec.action_spec\n",
    "\n",
    "# continuious and discrete action size\n",
    "continuious_action_size = env.behavior_specs[behaviour_name].action_spec.continuous_size\n",
    "discrete_actions_size = env.behavior_specs[behaviour_name].action_spec.discrete_size\n",
    "print(f\"Continuous action size: {continuious_action_size}\")\n",
    "print(f\"Discrete action size: {discrete_actions_size}\")\n",
    "\n",
    "#examine the state space\n",
    "states = decisionSteps.obs[0]\n",
    "state_size = states.shape[1] if states.ndim > 1 else states.shape[0]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "EPISODE START\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnEpisodeBegin () (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:39)\n",
      "Unity.MLAgents.Agent:_AgentReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1299)\n",
      "Unity.MLAgents.Academy:ForcedFullReset () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:548)\n",
      "Unity.MLAgents.Academy:OnResetCommand () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:504)\n",
      "Unity.MLAgents.RpcCommunicator:SendCommandEvent (Unity.MLAgents.CommunicatorObjects.CommandProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:298)\n",
      "Unity.MLAgents.RpcCommunicator:UpdateEnvironmentWithInput (Unity.MLAgents.CommunicatorObjects.UnityRLInputProto) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:218)\n",
      "Unity.MLAgents.RpcCommunicator:SendBatchedMessageHelper () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:408)\n",
      "Unity.MLAgents.RpcCommunicator:DecideBatch () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Communicator/RpcCommunicator.cs:320)\n",
      "Unity.MLAgents.Policies.RemotePolicy:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Policies/RemotePolicy.cs:67)\n",
      "Unity.MLAgents.Agent:DecideAction () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1360)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:578)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     10\u001b[0m action_tuple \u001b[38;5;241m=\u001b[39m ActionTuple(continuous\u001b[38;5;241m=\u001b[39mactions)                                      \u001b[38;5;66;03m# all actions between -1 and 1\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehavior_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbehaviour_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# send all actions to tne environment\u001b[39;00m\n\u001b[1;32m     12\u001b[0m env\u001b[38;5;241m.\u001b[39mstep()                                                                 \u001b[38;5;66;03m# step the environment to get the next states\u001b[39;00m\n\u001b[1;32m     13\u001b[0m decisionSteps, terminalSteps \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_steps(behavior_name\u001b[38;5;241m=\u001b[39mbehaviour_name)  \u001b[38;5;66;03m# get next state (for each agent)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/mlagents_envs/environment.py:373\u001b[0m, in \u001b[0;36mUnityEnvironment.set_actions\u001b[0;34m(self, behavior_name, action)\u001b[0m\n\u001b[1;32m    371\u001b[0m action_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_specs[behavior_name]\u001b[38;5;241m.\u001b[39maction_spec\n\u001b[1;32m    372\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_state[behavior_name][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 373\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43maction_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions[behavior_name] \u001b[38;5;241m=\u001b[39m action\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/mlagents_envs/base_env.py:420\u001b[0m, in \u001b[0;36mActionSpec._validate_action\u001b[0;34m(self, actions, n_agents, name)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03mValidates that action has the correct action dim\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03mfor the correct number of agents and ensures the type.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m _expected_shape \u001b[38;5;241m=\u001b[39m (n_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_size)\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m _expected_shape:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m needs a continuous input of dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_expected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for (<number of agents>, <action size>) but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived input of dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactions\u001b[38;5;241m.\u001b[39mcontinuous\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m _expected_shape \u001b[38;5;241m=\u001b[39m (n_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscrete_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'continuous'"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "env.reset()                                                                    # reset the environment \n",
    "stateVector = decisionSteps.obs[0]                                             # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                                                  # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, continuious_action_size)             # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1) \n",
    "    action_tuple = ActionTuple(continuous=actions)                                      # all actions between -1 and 1\n",
    "    env.set_actions(behavior_name=behaviour_name, action=actions)               # send all actions to tne environment\n",
    "    env.step()                                                                 # step the environment to get the next states\n",
    "    decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)  # get next state (for each agent)\n",
    "    next_states = decisionSteps.obs[0]                                         # get next state (for each agent)\n",
    "    rewards = decisionSteps.reward                                             # get reward (for each agent)\n",
    "    dones = [agent_id in terminalSteps for agent_id in decisionSteps.agent_id] # see if episode finished\n",
    "    scores += rewards                                                          # update the score (for each agent)\n",
    "    stateVector = next_states                                                  # roll over states to next time step\n",
    "    if np.any(dones):                                                          # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "Current reward: 0\n",
      "UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/build/output/unity/unity/Runtime/Export/Scripting/StackTrace.cs:37)\n",
      "UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
      "UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
      "UnityEngine.Debug:Log (object)\n",
      "HandAgent:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at C:/Users/Amanda/Documents/MouseNet Research/robotics_reaching_2/Assets/HandAgent.cs:98)\n",
      "Unity.MLAgents.Actuators.VectorActuator:OnActionReceived (Unity.MLAgents.Actuators.ActionBuffers) (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/VectorActuator.cs:76)\n",
      "Unity.MLAgents.Actuators.ActuatorManager:ExecuteActions () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Actuators/ActuatorManager.cs:295)\n",
      "Unity.MLAgents.Agent:AgentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Agent.cs:1344)\n",
      "Unity.MLAgents.Academy:EnvironmentStep () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:589)\n",
      "Unity.MLAgents.AcademyFixedUpdateStepper:FixedUpdate () (at ./Library/PackageCache/com.unity.ml-agents@2.0.1/Runtime/Academy.cs:43)\n",
      "\n",
      "PlayerConnection::CleanupMemory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 2 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AUDIO_FMOD mixer thread]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AUDIO_FMOD stream thread]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 199 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 134.9 KB\n",
      "      Overflow Count 4\n",
      "    [ALLOC_TEMP_Background Job.Worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Profiler.Dispatcher]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 11\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 6\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 128 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_MEMORYPROFILER]\n",
      "  Peak usage frame count: [64.0 KB-128.0 KB]: 3 frames\n",
      "  Requested Block Size 1.0 MB\n",
      "  Peak Block count 1\n",
      "  Peak Allocated memory 103.9 KB\n",
      "  Peak Large allocation bytes 0 B\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 27\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [8.0 MB-16.0 MB]: 3 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 13.1 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 3 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 2 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 66.8 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 3 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 64.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 3 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [256.0 KB-0.5 MB]: 2 frames, [1.0 MB-2.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 1.1 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.0 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 3 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 104 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [0-1.0 KB]: 3 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 0\n",
      "      Peak Allocated memory 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_PROFILER]\n",
      "  Peak usage frame count: [32.0 KB-64.0 KB]: 3 frames\n",
      "  Requested Block Size 16.0 MB\n",
      "  Peak Block count 1\n",
      "  Peak Allocated memory 38.9 KB\n",
      "  Peak Large allocation bytes 0 B\n",
      "    [ALLOC_PROFILER_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 0.6 KB\n",
      "##utp:{\"type\":\"MemoryLeaks\",\"version\":2,\"phase\":\"Immediate\",\"time\":1723145766982,\"processId\":546203,\"allocatedMemory\":2333795,\"memoryLabels\":[{\"Default\":18969},{\"Permanent\":1248},{\"NewDelete\":22787},{\"Thread\":34676},{\"Manager\":16427},{\"VertexData\":12},{\"Geometry\":280},{\"Texture\":16},{\"Shader\":68914},{\"Material\":24},{\"GfxDevice\":33536},{\"Animation\":344},{\"Audio\":3976},{\"FontEngine\":336},{\"Physics\":512},{\"Serialization\":276},{\"Input\":8960},{\"JobScheduler\":33008},{\"Mono\":40},{\"ScriptingNativeRuntime\":248},{\"BaseObject\":1617444},{\"Resource\":528},{\"Renderer\":2352},{\"Transform\":48},{\"File\":800},{\"WebCam\":40},{\"Culling\":40},{\"Terrain\":953},{\"Wind\":24},{\"String\":5534},{\"DynamicArray\":33008},{\"HashMap\":7680},{\"Utility\":285380},{\"PoolAlloc\":1128},{\"TypeTree\":4176},{\"ScriptManager\":80},{\"RuntimeInitializeOnLoadManager\":80},{\"SpriteAtlas\":128},{\"GI\":3584},{\"Director\":7872},{\"WebRequest\":720},{\"VR\":45529},{\"SceneManager\":424},{\"Video\":32},{\"LazyScriptCache\":40},{\"NativeArray\":12},{\"Camera\":25},{\"Secure\":1},{\"SerializationCache\":1576},{\"APIUpdating\":11856},{\"Subsystems\":392},{\"VirtualTexturing\":57552},{\"CoreBusinessMetrics\":128},{\"AssetReference\":40}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debugger-agent: Unable to listen on 6\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. DDPG Algorithm training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing environment: Couldn't start socket communication because worker number 1 is still in use. You may need to manually close a previously opened environment or use a different worker number.\n",
      "Enter ddpg...\n",
      "\n",
      "Episode number: 1\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c4e29a0>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.6089134e-01  1.0000000e+00  1.8774700e+00 ...  6.6673668e+01\n",
      "   8.2359192e+01  2.3093668e+01]\n",
      " [-3.6064243e-01  1.0000000e+00  1.8775358e+00 ... -5.4268238e+01\n",
      "  -5.8678510e+02  3.8146755e+01]\n",
      " [-3.6005592e-01  1.0000000e+00  1.8776621e+00 ... -3.7365669e-01\n",
      "  -1.8225275e+01 -1.8846086e+02]\n",
      " ...\n",
      " [-3.6032104e-01  1.0000000e+00  1.8775902e+00 ... -1.5628018e+03\n",
      "  -1.2455449e+03  1.8728696e+03]\n",
      " [-3.6091042e-01  1.0000000e+00  1.8774643e+00 ...  2.0004301e+02\n",
      "  -5.3148922e+01 -1.4683752e+02]\n",
      " [-3.6112690e-01  1.0000000e+00  1.8774109e+00 ... -5.4129340e+02\n",
      "  -1.2232679e+01 -1.0744814e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.30061722e+00  1.00000000e+00  1.04027748e+00 ...  2.82796570e+02\n",
      "  -1.38023453e+02 -7.52918930e+01]\n",
      " [-1.30048180e+00  1.00000000e+00  1.04047489e+00 ... -1.87352722e+02\n",
      "   2.76517639e+02  2.27250427e+02]\n",
      " [-1.30014420e+00  1.00000000e+00  1.04088712e+00 ... -3.18957886e+02\n",
      "   1.12100229e+01 -3.98230972e+01]\n",
      " ...\n",
      " [-1.30029297e+00  1.00000000e+00  1.04068279e+00 ... -1.70761523e+03\n",
      "   1.13161354e+02  1.78859290e+03]\n",
      " [-1.30062675e+00  1.00000000e+00  1.04025269e+00 ...  5.02557983e+01\n",
      "   6.00411911e+01  7.62651539e+00]\n",
      " [-1.30074120e+00  1.00000000e+00  1.04010391e+00 ...  1.65561996e+02\n",
      "   4.09529419e+02 -1.18707954e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6159801e+00  1.0000000e+00  5.3299713e-01 ... -7.9586611e+00\n",
      "   9.6541969e+01 -1.2456096e+02]\n",
      " [-1.6159143e+00  1.0000000e+00  5.3324890e-01 ... -5.0531338e+03\n",
      "   3.5492131e+03 -3.6885393e+03]\n",
      " [-1.6157207e+00  1.0000000e+00  5.3374958e-01 ...  2.7125546e+02\n",
      "   1.4046608e+02 -1.4240683e+02]\n",
      " ...\n",
      " [-1.6157913e+00  1.0000000e+00  5.3350830e-01 ... -1.2812155e+02\n",
      "   1.1732398e+02 -2.3909837e+02]\n",
      " [-1.6159649e+00  1.0000000e+00  5.3296089e-01 ...  2.4777480e+02\n",
      "  -2.0660600e+02 -2.7600775e+02]\n",
      " [-1.6160402e+00  1.0000000e+00  5.3277016e-01 ...  8.3496729e+02\n",
      "  -9.7021497e+02 -2.3819448e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7507086e+00  1.0000000e+00 -4.8929214e-02 ... -4.0423141e+01\n",
      "  -7.3365715e+01  1.1964460e+02]\n",
      " [-1.7507229e+00  1.0000000e+00 -4.8646927e-02 ... -5.0952835e+02\n",
      "  -1.8736427e+03 -7.2001733e+02]\n",
      " [-1.7506828e+00  1.0000000e+00 -4.8105642e-02 ... -5.5596027e+02\n",
      "   7.2784538e+01  4.2086237e+02]\n",
      " ...\n",
      " [-1.7506638e+00  1.0000000e+00 -4.8360825e-02 ...  2.0362877e+01\n",
      "  -2.7083569e+02 -3.2845947e+02]\n",
      " [-1.7506771e+00  1.0000000e+00 -4.8965454e-02 ... -3.6100186e+02\n",
      "  -3.0941902e+01 -1.0718225e+02]\n",
      " [-1.7506819e+00  1.0000000e+00 -4.9169540e-02 ... -3.5720825e+01\n",
      "  -1.5520672e+01  5.4385147e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.6884470e+00  1.0000000e+00 -6.4291954e-01 ...  2.0318645e+01\n",
      "  -6.7449036e+01  2.0959842e+02]\n",
      " [-1.6885443e+00  1.0000000e+00 -6.4264202e-01 ... -8.5622498e+02\n",
      "  -1.0958821e+02  1.9189645e+02]\n",
      " [-1.6886902e+00  1.0000000e+00 -6.4211440e-01 ... -1.7560266e+02\n",
      "   6.6620132e+01 -9.3627213e+01]\n",
      " ...\n",
      " [-1.6885891e+00  1.0000000e+00 -6.4236164e-01 ...  6.8926079e+01\n",
      "  -6.4531502e+01  1.1713516e+02]\n",
      " [-1.6884251e+00  1.0000000e+00 -6.4295387e-01 ... -6.4228920e+01\n",
      "   8.4010002e+01 -2.1205954e+01]\n",
      " [-1.6883373e+00  1.0000000e+00 -6.4315605e-01 ...  1.3959343e+01\n",
      "   1.3271443e+02  5.5891563e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4355478e+00  1.0000000e+00 -1.1841736e+00 ...  3.9919141e+02\n",
      "   5.4340790e+02 -3.7877018e+01]\n",
      " [-1.4357204e+00  1.0000000e+00 -1.1839266e+00 ...  9.1923073e+01\n",
      "  -2.7266941e+03 -1.0050234e+01]\n",
      " [-1.4360256e+00  1.0000000e+00 -1.1834668e+00 ... -4.3772606e+01\n",
      "  -4.6239986e+01  1.8944734e+02]\n",
      " ...\n",
      " [-1.4358253e+00  1.0000000e+00 -1.1836901e+00 ...  1.2921530e+02\n",
      "  -2.3965364e+00  6.4197983e+01]\n",
      " [-1.4354820e+00  1.0000000e+00 -1.1842079e+00 ...  1.2903250e+02\n",
      "   3.2379956e+00  6.6881170e+00]\n",
      " [-1.4353657e+00  1.0000000e+00 -1.1843872e+00 ...  3.5023842e+01\n",
      "  -2.0832827e+01 -1.1814169e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0203123e+00  1.0000000e+00 -1.6135979e+00 ... -2.8282779e+02\n",
      "   8.7156387e+01  7.1506122e+02]\n",
      " [-1.0205507e+00  1.0000000e+00 -1.6133833e+00 ... -7.9962280e+02\n",
      "  -4.5524130e+02  7.4655646e+02]\n",
      " [-1.0209446e+00  1.0000000e+00 -1.6130614e+00 ... -1.6107496e+01\n",
      "  -1.3486480e+01  1.8833625e+00]\n",
      " ...\n",
      " [-1.0206890e+00  1.0000000e+00 -1.6132221e+00 ...  5.5598465e+01\n",
      "   2.4980559e+02  3.7048218e+01]\n",
      " [-1.0201836e+00  1.0000000e+00 -1.6136246e+00 ...  3.4949398e+01\n",
      "   4.8138538e+01  2.3097968e+00]\n",
      " [-1.0200758e+00  1.0000000e+00 -1.6137543e+00 ... -1.6076840e+03\n",
      "  -1.5610712e+03  1.6024163e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8871422e-01  1.0000000e+00 -1.8855686e+00 ...  3.7389139e+02\n",
      "   9.5056824e+02  1.8152654e+03]\n",
      " [-4.8899937e-01  1.0000000e+00 -1.8854036e+00 ... -3.3115219e+01\n",
      "   1.3695674e+02 -1.3908202e+02]\n",
      " [-4.8951340e-01  1.0000000e+00 -1.8852507e+00 ... -8.6552905e+02\n",
      "   7.3878320e+02 -3.2811795e+02]\n",
      " ...\n",
      " [-4.8921967e-01  1.0000000e+00 -1.8853168e+00 ... -9.6404037e+01\n",
      "   1.2015057e+02  1.0515672e+02]\n",
      " [-4.8861504e-01  1.0000000e+00 -1.8855782e+00 ... -5.8269257e+01\n",
      "  -1.3647211e+02 -1.0493952e+02]\n",
      " [-4.8843861e-01  1.0000000e+00 -1.8856564e+00 ... -2.2885204e+02\n",
      "   9.5456665e+01 -3.9576605e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0215092e-01  1.0000000e+00 -1.9736443e+00 ...  2.8214070e+03\n",
      "   1.4444117e+03 -1.9713391e+03]\n",
      " [ 1.0185337e-01  1.0000000e+00 -1.9735632e+00 ... -7.5779900e+01\n",
      "   2.7497787e+02  1.9924554e+02]\n",
      " [ 1.0127449e-01  1.0000000e+00 -1.9735951e+00 ... -4.1841016e+04\n",
      "   2.7864963e+04 -7.4591460e+03]\n",
      " ...\n",
      " [ 1.0158348e-01  1.0000000e+00 -1.9735661e+00 ... -1.2487114e+01\n",
      "   6.0912289e+01 -2.0672923e+02]\n",
      " [ 1.0222244e-01  1.0000000e+00 -1.9736290e+00 ... -1.3608720e+02\n",
      "   3.5742851e+01  2.6326168e+01]\n",
      " [ 1.0243416e-01  1.0000000e+00 -1.9736462e+00 ... -5.0661853e+02\n",
      "   1.4463174e+03 -1.8183904e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.9082069e-01  1.0000000e+00 -1.8702393e+00 ... -7.1690991e+02\n",
      "   4.6444070e+02 -2.3665798e+03]\n",
      " [ 6.9053841e-01  1.0000000e+00 -1.8702507e+00 ... -1.7384167e+02\n",
      "  -1.4946963e+01  5.7771328e+01]\n",
      " [ 6.8998909e-01  1.0000000e+00 -1.8704460e+00 ... -6.3355298e+02\n",
      "  -3.6355935e+03  7.6417114e+03]\n",
      " ...\n",
      " [ 6.9028282e-01  1.0000000e+00 -1.8703318e+00 ...  5.9560398e+01\n",
      "  -5.4632195e+01 -5.8676567e+00]\n",
      " [ 6.9089699e-01  1.0000000e+00 -1.8701992e+00 ... -5.0046101e+01\n",
      "   6.3370361e+01  2.6570999e+02]\n",
      " [ 6.9109344e-01  1.0000000e+00 -1.8701706e+00 ...  1.0790817e+02\n",
      "  -3.2259556e+01 -4.7677689e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.2172146e+00  1.0000000e+00 -1.5880051e+00 ... -6.4086719e+02\n",
      "  -2.0516895e+03  4.9903900e+02]\n",
      " [ 1.2169704e+00  1.0000000e+00 -1.5880728e+00 ... -1.8489998e+01\n",
      "  -7.7516732e+00  3.4914474e+01]\n",
      " [ 1.2165108e+00  1.0000000e+00 -1.5884327e+00 ... -4.6399719e+02\n",
      "   5.1627893e+02  8.4297552e+02]\n",
      " ...\n",
      " [ 1.2167568e+00  1.0000000e+00 -1.5882349e+00 ... -1.7600725e+02\n",
      "   9.1809120e+00 -6.2176571e+01]\n",
      " [ 1.2172623e+00  1.0000000e+00 -1.5879345e+00 ... -8.7200882e+01\n",
      "  -1.2622224e+02  2.9400827e+01]\n",
      " [ 1.2174463e+00  1.0000000e+00 -1.5878677e+00 ...  5.6028533e+00\n",
      "   5.6847469e+01  1.6445158e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6303310e+00  1.0000000e+00 -1.1562023e+00 ...  5.0804291e+01\n",
      "   5.2130200e+02  1.3480878e+02]\n",
      " [ 1.6301403e+00  1.0000000e+00 -1.1562986e+00 ...  4.9900720e+02\n",
      "   8.7377205e+00  1.6176346e+02]\n",
      " [ 1.6298065e+00  1.0000000e+00 -1.1567761e+00 ... -9.0116980e+02\n",
      "   4.9218559e+01  1.3326414e+03]\n",
      " ...\n",
      " [ 1.6299877e+00  1.0000000e+00 -1.1565180e+00 ...  7.7950079e+02\n",
      "   3.3159705e+02 -3.6051654e+02]\n",
      " [ 1.6303329e+00  1.0000000e+00 -1.1561222e+00 ... -7.2290085e+01\n",
      "   2.3250660e+01 -4.2778400e+01]\n",
      " [ 1.6305151e+00  1.0000000e+00 -1.1560059e+00 ... -6.6508732e+00\n",
      "   2.0916510e+02  9.3349533e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8908825e+00  1.0000000e+00 -6.1803246e-01 ...  2.1829547e+02\n",
      "  -3.2765878e+02  9.7844124e+01]\n",
      " [ 1.8907652e+00  1.0000000e+00 -6.1814785e-01 ...  1.6070811e+02\n",
      "   6.7824440e+01 -1.6695923e+02]\n",
      " [ 1.8905869e+00  1.0000000e+00 -6.1871916e-01 ... -1.1710577e+03\n",
      "   9.7068591e+02  1.1902170e+03]\n",
      " ...\n",
      " [ 1.8906898e+00  1.0000000e+00 -6.1840820e-01 ... -1.8668173e+01\n",
      "  -1.2472650e+02  1.2314014e+02]\n",
      " [ 1.8908787e+00  1.0000000e+00 -6.1794090e-01 ...  9.4553406e+01\n",
      "  -1.2442444e+02  9.2038544e+01]\n",
      " [ 1.8910046e+00  1.0000000e+00 -6.1778450e-01 ... -1.6993657e+02\n",
      "   3.9371823e+02 -1.7138821e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9748363e+00  1.0000000e+00 -2.6550293e-02 ... -2.0708418e+02\n",
      "  -9.9992996e+01  2.7222427e+02]\n",
      " [ 1.9747896e+00  1.0000000e+00 -2.6696205e-02 ...  6.3640118e+01\n",
      "   6.8330856e+01  6.5232117e+01]\n",
      " [ 1.9747925e+00  1.0000000e+00 -2.7286647e-02 ...  8.0848920e+02\n",
      "   2.0494957e+01  1.3865147e+02]\n",
      " ...\n",
      " [ 1.9747868e+00  1.0000000e+00 -2.6967049e-02 ...  1.9256100e+02\n",
      "  -3.8899408e+02 -8.3429596e+02]\n",
      " [ 1.9748039e+00  1.0000000e+00 -2.6456833e-02 ...  2.3037292e+02\n",
      "   1.5009839e+02  1.8787006e+02]\n",
      " [ 1.9748783e+00  1.0000000e+00 -2.6283264e-02 ... -6.1395340e+01\n",
      "   8.5311556e+00  9.0338028e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.87504101e+00  1.00000000e+00  5.62625885e-01 ...  7.48847961e+01\n",
      "  -1.51041422e+01  9.26087570e+01]\n",
      " [ 1.87506390e+00  1.00000000e+00  5.62494278e-01 ... -1.11129517e+02\n",
      "   1.04900047e+02 -3.29460869e+01]\n",
      " [ 1.87524605e+00  1.00000000e+00  5.61930954e-01 ... -1.57378597e+01\n",
      "   4.80041924e+01  1.16557625e+02]\n",
      " ...\n",
      " [ 1.87515640e+00  1.00000000e+00  5.62238693e-01 ... -4.72798004e+01\n",
      "   4.59131660e+01  1.48593262e+02]\n",
      " [ 1.87502098e+00  1.00000000e+00  5.62717438e-01 ...  9.25843239e+00\n",
      "  -2.27319360e+00  3.18219337e+01]\n",
      " [ 1.87500477e+00  1.00000000e+00  5.62875748e-01 ...  1.75824265e+02\n",
      "  -1.74673023e+01 -5.94499878e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6009893e+00  1.0000000e+00  1.0941410e+00 ...  5.0849603e+02\n",
      "   2.4525486e+03 -1.6127521e+03]\n",
      " [ 1.6010685e+00  1.0000000e+00  1.0940599e+00 ...  2.2867193e+01\n",
      "   1.1771820e+02 -6.0360657e+01]\n",
      " [ 1.6014595e+00  1.0000000e+00  1.0935607e+00 ... -2.5809714e+02\n",
      "   6.0257202e+01 -3.3580127e+02]\n",
      " ...\n",
      " [ 1.6012821e+00  1.0000000e+00  1.0938263e+00 ...  1.2223043e+02\n",
      "   5.5511845e+01 -9.0234901e+01]\n",
      " [ 1.6009903e+00  1.0000000e+00  1.0942249e+00 ...  2.1415671e+02\n",
      "   5.5951324e+01 -1.4593236e+02]\n",
      " [ 1.6008835e+00  1.0000000e+00  1.0943604e+00 ...  2.9986383e+02\n",
      "   2.8502280e+02  1.5260970e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1788502e+00  1.0000000e+00  1.5171795e+00 ...  2.4464492e+03\n",
      "  -9.8274231e+02  6.0091846e+02]\n",
      " [ 1.1789665e+00  1.0000000e+00  1.5171623e+00 ... -1.8374192e+03\n",
      "  -2.9417676e+03 -5.7526367e+03]\n",
      " [ 1.1794872e+00  1.0000000e+00  1.5167888e+00 ... -2.2844966e+02\n",
      "   4.4167828e+02 -5.4140246e-01]\n",
      " ...\n",
      " [ 1.1792316e+00  1.0000000e+00  1.5169973e+00 ... -3.4586517e+01\n",
      "   5.9655422e+01  5.8240158e+01]\n",
      " [ 1.1788311e+00  1.0000000e+00  1.5172462e+00 ... -3.4972958e+02\n",
      "   2.5787601e+02  2.7786267e+02]\n",
      " [ 1.1786757e+00  1.0000000e+00  1.5173359e+00 ...  3.1861368e+02\n",
      "  -1.6351285e+02 -1.5215585e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.4841843e-01  1.0000000e+00  1.7919998e+00 ...  5.7548602e+02\n",
      "  -5.6081616e+02  7.4188892e+02]\n",
      " [ 6.4856148e-01  1.0000000e+00  1.7920208e+00 ... -7.0665228e+02\n",
      "   3.5402427e+03 -1.8943442e+03]\n",
      " [ 6.4913559e-01  1.0000000e+00  1.7917947e+00 ...  5.5871625e+02\n",
      "   1.8991575e+01 -1.4299039e+03]\n",
      " ...\n",
      " [ 6.4882660e-01  1.0000000e+00  1.7919312e+00 ... -5.3950891e+02\n",
      "   3.1829180e+03 -6.2354819e+03]\n",
      " [ 6.4835167e-01  1.0000000e+00  1.7920513e+00 ...  3.4287539e+00\n",
      "  -3.2253769e+01 -6.2829277e+01]\n",
      " [ 6.4821815e-01  1.0000000e+00  1.7920818e+00 ...  6.1596859e+01\n",
      "  -7.9483376e+01  1.8253920e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.90705872e-02  1.00000000e+00  1.89227295e+00 ... -4.88950684e+02\n",
      "  -3.37044525e+02 -2.32004608e+02]\n",
      " [ 5.92212677e-02  1.00000000e+00  1.89233303e+00 ... -8.37438477e+02\n",
      "   3.23360413e+02 -5.61844711e+01]\n",
      " [ 5.98297119e-02  1.00000000e+00  1.89228868e+00 ...  1.09313576e+02\n",
      "  -2.82053165e+01 -1.13116409e+02]\n",
      " ...\n",
      " [ 5.95111847e-02  1.00000000e+00  1.89233303e+00 ...  7.01446819e+00\n",
      "  -2.16247803e+02 -1.02199135e+02]\n",
      " [ 5.90038300e-02  1.00000000e+00  1.89230347e+00 ...  1.23107315e+02\n",
      "   2.26764618e+02 -8.42320740e+02]\n",
      " [ 5.88560104e-02  1.00000000e+00  1.89228439e+00 ...  2.70571899e+00\n",
      "  -1.86208191e+02  6.48286057e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.3238010e-01  1.0000000e+00  1.8075943e+00 ...  1.6904823e+03\n",
      "   4.3848761e+02 -9.9098877e+02]\n",
      " [-5.3223991e-01  1.0000000e+00  1.8077202e+00 ...  7.6140002e+02\n",
      "  -1.3544012e+03  5.2017070e+03]\n",
      " [-5.3164864e-01  1.0000000e+00  1.8078637e+00 ...  1.8672026e+02\n",
      "   4.2906998e+01  9.5504173e+01]\n",
      " ...\n",
      " [-5.3195763e-01  1.0000000e+00  1.8078098e+00 ... -8.5838837e+01\n",
      "  -1.2079071e+02 -7.2511559e+01]\n",
      " [-5.3244781e-01  1.0000000e+00  1.8076038e+00 ... -3.2621986e+01\n",
      "  -7.0558772e+00  3.4756138e+01]\n",
      " [-5.3258610e-01  1.0000000e+00  1.8075275e+00 ...  2.5971775e+01\n",
      "  -1.9444858e+02  4.5040920e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0693817e+00  1.0000000e+00  1.5455132e+00 ...  1.7326112e+02\n",
      "  -9.9691290e+02  7.0932416e+02]\n",
      " [-1.0692606e+00  1.0000000e+00  1.5456896e+00 ...  8.1152545e+02\n",
      "   1.8369826e+02  5.8926794e+02]\n",
      " [-1.0687218e+00  1.0000000e+00  1.5460176e+00 ... -8.1725082e+01\n",
      "   3.2302835e+02  2.9909998e+01]\n",
      " ...\n",
      " [-1.0690022e+00  1.0000000e+00  1.5458527e+00 ... -2.1959773e+02\n",
      "   1.3861028e+02 -6.3723540e-01]\n",
      " [-1.0694294e+00  1.0000000e+00  1.5454998e+00 ... -4.5111980e+01\n",
      "  -2.4428857e+02  8.1309509e+01]\n",
      " [-1.0695543e+00  1.0000000e+00  1.5453835e+00 ... -2.2370432e+01\n",
      "  -1.1563654e+01  1.0247061e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.4995556    1.           1.1307468 ...  230.38501     82.32727\n",
      "    69.5903   ]\n",
      " [  -1.499464     1.           1.1309576 ... -398.5627     -83.1566\n",
      "  -464.84003  ]\n",
      " [  -1.499012     1.           1.1314174 ...  216.45792   -185.20021\n",
      "   190.83954  ]\n",
      " ...\n",
      " [  -1.4992161    1.           1.1311846 ... -321.59158   -216.36864\n",
      "   364.53543  ]\n",
      " [  -1.4995403    1.           1.1307182 ...   15.779012   -13.087124\n",
      "   -11.899796 ]\n",
      " [  -1.4996729    1.           1.1305733 ...  -10.855294   -57.793964\n",
      "   -38.215916 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.7804012     1.            0.6033821  ...   -6.152859\n",
      "    -4.6295757    -2.3673224 ]\n",
      " [  -1.7803583     1.            0.6036205  ... -157.49513\n",
      "   293.90207    -337.7174    ]\n",
      " [  -1.7800655     1.            0.604176   ...   84.53026\n",
      "   -37.359756    -46.772873  ]\n",
      " ...\n",
      " [  -1.7801857     1.            0.60389423 ...  -15.227443\n",
      "   262.08554     -22.350441  ]\n",
      " [  -1.7803497     1.            0.6033497  ...   14.10281\n",
      "     3.5634966     4.844949  ]\n",
      " [  -1.7804556     1.            0.60318947 ...   85.68806\n",
      "   -39.805496   -137.49564   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Forearm\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.88393593e+00  1.00000000e+00  1.48925781e-02 ...  1.63237915e+02\n",
      "   1.10968056e+01  1.91183186e+00]\n",
      " [-1.88395309e+00  1.00000000e+00  1.51538849e-02 ... -1.38507904e+02\n",
      "  -2.77412598e+02  9.07971420e+01]\n",
      " [-1.88384438e+00  1.00000000e+00  1.57524273e-02 ...  4.06610565e+01\n",
      "   3.41739410e+02 -2.10973167e+01]\n",
      " ...\n",
      " [-1.88385201e+00  1.00000000e+00  1.54418945e-02 ...  2.54680252e+01\n",
      "  -2.00832233e+01  7.01652985e+01]\n",
      " [-1.88386917e+00  1.00000000e+00  1.48525238e-02 ... -9.70646439e+01\n",
      "  -1.26720711e+02 -5.17771416e+01]\n",
      " [-1.88391876e+00  1.00000000e+00  1.46770477e-02 ...  8.08721161e+00\n",
      "  -3.32280846e+01  1.97281017e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79954338e+00  1.00000000e+00 -5.76553345e-01 ...  1.13507751e+02\n",
      "   1.43558655e+01  2.84747131e+02]\n",
      " [-1.79963589e+00  1.00000000e+00 -5.76270103e-01 ... -1.92282013e+02\n",
      "  -3.45574585e+02 -6.05625458e+01]\n",
      " [-1.79973793e+00  1.00000000e+00 -5.75713575e-01 ... -1.76771362e+02\n",
      "  -1.90428360e+02 -4.48586945e+02]\n",
      " ...\n",
      " [-1.79966927e+00  1.00000000e+00 -5.75999260e-01 ...  1.31363358e+02\n",
      "  -2.34570328e+02 -1.94081650e+02]\n",
      " [-1.79951859e+00  1.00000000e+00 -5.76591492e-01 ... -1.33385792e+01\n",
      "  -2.54865173e+02  1.57701233e+02]\n",
      " [-1.79946136e+00  1.00000000e+00 -5.76763153e-01 ... -7.27854980e+02\n",
      "  -2.15622217e+03  2.07473853e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5351982    1.          -1.1128178 ... -101.77458   -221.33342\n",
      "    58.540306 ]\n",
      " [  -1.5353603    1.          -1.1125517 ... -396.43958   -453.95197\n",
      "  -539.59674  ]\n",
      " [  -1.5356026    1.          -1.1120673 ... -267.36197   -286.21396\n",
      "    30.134003 ]\n",
      " ...\n",
      " [  -1.5354443    1.          -1.1123123 ...    1.8115325    1.8435926\n",
      "    -4.8159466]\n",
      " [  -1.5351467    1.          -1.1128483 ...   36.578587  -117.593124\n",
      "   477.6706   ]\n",
      " [  -1.5350485    1.          -1.1129932 ...   26.444601   -15.580518\n",
      "   -16.915703 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1173534e+00  1.0000000e+00 -1.5398998e+00 ...  1.8000126e-01\n",
      "  -5.1501508e+00  3.2108781e+00]\n",
      " [-1.1175766e+00  1.0000000e+00 -1.5397129e+00 ... -2.2648788e+02\n",
      "  -8.6301727e+01  1.5155206e+02]\n",
      " [-1.1179142e+00  1.0000000e+00 -1.5393584e+00 ... -4.2483009e+01\n",
      "  -3.0309086e+01 -7.0729993e+02]\n",
      " ...\n",
      " [-1.1176834e+00  1.0000000e+00 -1.5395298e+00 ... -1.6710406e+02\n",
      "  -3.2778708e+02 -2.9386782e+01]\n",
      " [-1.1172562e+00  1.0000000e+00 -1.5399208e+00 ... -2.5242152e+00\n",
      "  -5.4897285e+01  1.2696232e+02]\n",
      " [-1.1171408e+00  1.0000000e+00 -1.5400333e+00 ...  3.0792819e+02\n",
      "   1.4003766e+02  6.6031213e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4 0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.8752632e-01  1.0000000e+00 -1.8157730e+00 ...  1.9201181e+00\n",
      "   1.4840783e+00  6.9351854e+00]\n",
      " [-5.8779430e-01  1.0000000e+00 -1.8156300e+00 ...  1.5549285e+01\n",
      "   1.3303230e+02 -8.6006042e+01]\n",
      " [-5.8816910e-01  1.0000000e+00 -1.8154448e+00 ... -3.5378668e+02\n",
      "  -3.0922290e+02  1.3489410e+03]\n",
      " ...\n",
      " [-5.8790588e-01  1.0000000e+00 -1.8155222e+00 ... -3.1561386e+02\n",
      "   3.1083524e+02  3.0719363e+02]\n",
      " [-5.8739662e-01  1.0000000e+00 -1.8157768e+00 ... -5.3528668e+02\n",
      "   1.0914875e+03  3.9884504e+03]\n",
      " [-5.8728218e-01  1.0000000e+00 -1.8158398e+00 ... -1.2876418e+02\n",
      "  -1.5436671e+02 -1.2966159e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9674301e-03  1.0000000e+00 -1.9133739e+00 ...  4.4788961e+00\n",
      "   4.9052864e-01  1.9549321e+00]\n",
      " [ 1.6889572e-03  1.0000000e+00 -1.9133186e+00 ...  6.5757166e+02\n",
      "   3.9601483e+02  3.5294534e+02]\n",
      " [ 1.2664795e-03  1.0000000e+00 -1.9133050e+00 ... -3.6734177e+01\n",
      "   9.3273933e+01  1.8270824e+01]\n",
      " ...\n",
      " [ 1.5563965e-03  1.0000000e+00 -1.9132776e+00 ... -2.6320990e+02\n",
      "   5.2903156e+02 -9.8353540e+02]\n",
      " [ 2.0885468e-03  1.0000000e+00 -1.9133568e+00 ... -1.6560783e+02\n",
      "   2.0996550e+02  9.7793785e+01]\n",
      " [ 2.2239685e-03  1.0000000e+00 -1.9133720e+00 ... -3.9772186e+01\n",
      "  -8.0260765e+01  2.7607676e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.92832565e-01  1.00000000e+00 -1.82309151e+00 ...  5.50754738e+00\n",
      "   5.30632544e+00  1.49774528e+00]\n",
      " [ 5.92562675e-01  1.00000000e+00 -1.82315350e+00 ...  1.14767224e+03\n",
      "   2.29169006e+02 -4.48193604e+02]\n",
      " [ 5.92138290e-01  1.00000000e+00 -1.82329297e+00 ...  6.06504860e+01\n",
      "  -1.87299042e+01  3.90434570e+01]\n",
      " ...\n",
      " [ 5.92414856e-01  1.00000000e+00 -1.82318115e+00 ... -2.01386322e+02\n",
      "  -8.44664001e+01 -6.92055130e+01]\n",
      " [ 5.92922211e-01  1.00000000e+00 -1.82305527e+00 ... -1.23826904e+02\n",
      "   2.21616928e+02  4.69807053e+01]\n",
      " [ 5.93079567e-01  1.00000000e+00 -1.82302856e+00 ...  7.74210835e+00\n",
      "   1.39594879e+02 -1.08231482e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1260929e+00  1.0000000e+00 -1.5543709e+00 ...  3.3405268e+00\n",
      "   6.4994812e+00  2.5933942e-01]\n",
      " [ 1.1258640e+00  1.0000000e+00 -1.5545235e+00 ...  8.8768024e+02\n",
      "   7.8431433e+02  1.0248726e+03]\n",
      " [ 1.1254768e+00  1.0000000e+00 -1.5548013e+00 ...  1.7702635e+02\n",
      "  -4.7715649e+01  2.9222651e+01]\n",
      " ...\n",
      " [ 1.1257229e+00  1.0000000e+00 -1.5546045e+00 ... -1.3321194e+01\n",
      "  -1.8250314e+02 -3.0787439e+02]\n",
      " [ 1.1261578e+00  1.0000000e+00 -1.5543098e+00 ... -1.1502002e+03\n",
      "   2.4873666e+02  7.8483398e+02]\n",
      " [ 1.1262970e+00  1.0000000e+00 -1.5542469e+00 ...  4.3215820e+02\n",
      "   3.2105640e+02 -6.1243823e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.55035305e+00  1.00000000e+00 -1.13349533e+00 ...  7.93706656e-01\n",
      "   1.15968275e+00  2.32730341e+00]\n",
      " [ 1.55017853e+00  1.00000000e+00 -1.13366604e+00 ...  7.72607346e+01\n",
      "  -1.97763397e+02 -7.18679047e+01]\n",
      " [ 1.54991913e+00  1.00000000e+00 -1.13407946e+00 ... -4.96142006e+01\n",
      "  -2.49736977e+01  3.75757942e+01]\n",
      " ...\n",
      " [ 1.55010414e+00  1.00000000e+00 -1.13379383e+00 ...  2.34293613e+01\n",
      "  -8.57983780e+00 -8.95581284e+01]\n",
      " [ 1.55041695e+00  1.00000000e+00 -1.13340950e+00 ...  6.25833496e+02\n",
      "  -1.66043860e+03  1.45347595e+01]\n",
      " [ 1.55050373e+00  1.00000000e+00 -1.13331413e+00 ... -1.92119781e+02\n",
      "   1.22868323e+03  2.75531921e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8236179e+00  1.0000000e+00 -6.0206032e-01 ...  2.6331431e-01\n",
      "   1.1282206e-02  1.1774850e+00]\n",
      " [ 1.8235207e+00  1.0000000e+00 -6.0226727e-01 ... -8.3002026e+02\n",
      "   1.1728531e+03 -4.9826763e+01]\n",
      " [ 1.8234024e+00  1.0000000e+00 -6.0274440e-01 ...  5.6871243e+01\n",
      "  -7.2256210e+01 -5.7966663e+01]\n",
      " ...\n",
      " [ 1.8235016e+00  1.0000000e+00 -6.0242462e-01 ...  6.9921700e+01\n",
      "   3.3496439e-01 -1.0126757e+02]\n",
      " [ 1.8237019e+00  1.0000000e+00 -6.0196495e-01 ... -3.9267853e+02\n",
      "   2.4868997e+02  2.2297897e+02]\n",
      " [ 1.8237019e+00  1.0000000e+00 -6.0184097e-01 ... -2.6548801e+02\n",
      "  -1.9495973e+02 -1.0783870e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9193554e+00  1.0000000e+00 -1.2243271e-02 ... -8.0542201e-01\n",
      "   1.8162839e+00  6.2037516e-01]\n",
      " [ 1.9193497e+00  1.0000000e+00 -1.2468338e-02 ... -5.5240780e+01\n",
      "  -6.7338661e+01 -2.0393042e+02]\n",
      " [ 1.9194069e+00  1.0000000e+00 -1.2963529e-02 ... -2.9885925e+01\n",
      "   4.1415741e+01  3.7623001e+01]\n",
      " ...\n",
      " [ 1.9194050e+00  1.0000000e+00 -1.2630463e-02 ... -2.4653110e+02\n",
      "   7.4638829e+00 -8.0603943e+01]\n",
      " [ 1.9194221e+00  1.0000000e+00 -1.2144089e-02 ...  4.6518585e+01\n",
      "   3.2379070e+01  5.5147644e+01]\n",
      " [ 1.9193821e+00  1.0000000e+00 -1.2008667e-02 ...  9.5735107e+01\n",
      "  -3.0489923e+02 -1.4401752e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8284359     1.            0.57828903 ...   -1.6751311\n",
      "     2.1037126     0.41414216]\n",
      " [   1.8285065     1.            0.5780611  ...  -78.06216\n",
      "    -2.308339   -160.5807    ]\n",
      " [   1.8287182     1.            0.5775983  ...   72.44323\n",
      "    49.3386      -41.75507   ]\n",
      " ...\n",
      " [   1.8286018     1.            0.57790947 ...  211.19588\n",
      "    26.578033    229.92897   ]\n",
      " [   1.8284798     1.            0.5783844  ... -206.83867\n",
      "  -335.8287     -261.80698   ]\n",
      " [   1.8283882     1.            0.5785141  ... -193.81519\n",
      "   -32.013042    398.4893    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5595465e+00  1.0000000e+00  1.1119938e+00 ... -1.2349439e+00\n",
      "   8.5283041e-01  3.9572465e-01]\n",
      " [ 1.5596876e+00  1.0000000e+00  1.1118641e+00 ...  5.3168686e+01\n",
      "   2.0801155e+02  3.1975412e+00]\n",
      " [ 1.5600281e+00  1.0000000e+00  1.1114622e+00 ...  2.6257672e+02\n",
      "   1.9426376e+01  9.1660149e+01]\n",
      " ...\n",
      " [ 1.5598183e+00  1.0000000e+00  1.1117287e+00 ...  2.6182275e+02\n",
      "   1.7475943e+02  5.1055809e+01]\n",
      " [ 1.5595512e+00  1.0000000e+00  1.1120739e+00 ... -6.2967590e+02\n",
      "   7.8077094e+02 -3.5634641e+02]\n",
      " [ 1.5594234e+00  1.0000000e+00  1.1121788e+00 ...  1.2967339e+01\n",
      "   4.2352230e+01  1.3066438e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.13945484e+00  1.00000000e+00  1.53678894e+00 ...  1.38381734e-01\n",
      "  -1.83137298e-01  1.16287977e-01]\n",
      " [ 1.13964844e+00  1.00000000e+00  1.53671741e+00 ... -2.63422546e+01\n",
      "   6.08085403e+01  1.88290424e+01]\n",
      " [ 1.14005852e+00  1.00000000e+00  1.53643012e+00 ... -2.70244263e+02\n",
      "  -4.53403778e+02  9.55510437e+02]\n",
      " ...\n",
      " [ 1.13978386e+00  1.00000000e+00  1.53661633e+00 ...  7.11563416e+01\n",
      "  -1.69600010e+01  8.45574951e+01]\n",
      " [ 1.13939857e+00  1.00000000e+00  1.53683281e+00 ...  1.10173996e+02\n",
      "  -3.91550522e+01 -3.34221649e+02]\n",
      " [ 1.13927364e+00  1.00000000e+00  1.53690910e+00 ... -9.68246078e+01\n",
      "   1.31349701e+02 -1.48021488e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.0871792e-01  1.0000000e+00  1.8114185e+00 ...  8.7239647e-01\n",
      "  -7.3152083e-01 -3.5174459e-02]\n",
      " [ 6.0894966e-01  1.0000000e+00  1.8114004e+00 ... -1.2023944e+02\n",
      "  -3.3378403e+01 -1.5524284e+02]\n",
      " [ 6.0942841e-01  1.0000000e+00  1.8112453e+00 ...  4.3566345e+01\n",
      "   8.9286621e+01  4.4054432e+01]\n",
      " ...\n",
      " [ 6.0911751e-01  1.0000000e+00  1.8113499e+00 ... -1.9317911e+02\n",
      "  -3.0362497e+02  4.2904153e+02]\n",
      " [ 6.0866737e-01  1.0000000e+00  1.8114376e+00 ... -1.6478905e+02\n",
      "  -1.5010877e+01  4.8153675e+01]\n",
      " [ 6.0850811e-01  1.0000000e+00  1.8114586e+00 ... -6.5741492e+02\n",
      "  -7.5692467e+01 -2.7999692e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9037247e-02  1.0000000e+00  1.9090061e+00 ...  5.9573853e-01\n",
      "  -3.5431182e-01 -2.6987925e-02]\n",
      " [ 1.9280434e-02  1.0000000e+00  1.9090223e+00 ... -6.7553291e+01\n",
      "  -1.3158894e+02  2.2965115e+01]\n",
      " [ 1.9775391e-02  1.0000000e+00  1.9090346e+00 ... -1.8137938e+02\n",
      "   1.4099657e+02 -1.7447433e+02]\n",
      " ...\n",
      " [ 1.9451141e-02  1.0000000e+00  1.9090252e+00 ...  2.2360298e+02\n",
      "  -6.1695255e+01 -9.8201691e+01]\n",
      " [ 1.8981934e-02  1.0000000e+00  1.9090042e+00 ... -5.5332462e+01\n",
      "  -1.8304872e+02  1.4552460e+02]\n",
      " [ 1.8817902e-02  1.0000000e+00  1.9089775e+00 ... -4.1822061e+02\n",
      "   1.7969179e+02 -3.0189709e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.7176304e-01  1.0000000e+00  1.8199215e+00 ...  2.1871351e-02\n",
      "  -8.9800954e-02  3.7197188e-02]\n",
      " [-5.7152939e-01  1.0000000e+00  1.8199730e+00 ... -3.1637695e+02\n",
      "  -9.8257896e+01  2.0415990e+02]\n",
      " [-5.7101822e-01  1.0000000e+00  1.8201616e+00 ...  1.1534731e+02\n",
      "  -5.1412439e+02 -2.3598482e+02]\n",
      " ...\n",
      " [-5.7132912e-01  1.0000000e+00  1.8200235e+00 ...  1.7598886e+00\n",
      "  -2.6046696e+00  9.4509721e-01]\n",
      " [-5.7178497e-01  1.0000000e+00  1.8198967e+00 ...  8.4032356e+01\n",
      "   8.1704727e+01  3.1126961e+01]\n",
      " [-5.7197571e-01  1.0000000e+00  1.8198128e+00 ...  6.4538368e+01\n",
      "  -3.7149364e+01  1.3197093e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1065340e+00  1.0000000e+00  1.5525742e+00 ... -1.0326227e+01\n",
      "   3.5568356e+01 -2.0771940e+00]\n",
      " [-1.1063204e+00  1.0000000e+00  1.5527287e+00 ...  3.6650439e+02\n",
      "  -3.4000259e+02  8.9235870e+01]\n",
      " [-1.1058941e+00  1.0000000e+00  1.5530580e+00 ... -1.1447212e+01\n",
      "   2.8308348e+01  2.4029072e+01]\n",
      " ...\n",
      " [-1.1061592e+00  1.0000000e+00  1.5528278e+00 ...  1.9153132e+03\n",
      "  -1.8585176e+03 -1.6323586e+03]\n",
      " [-1.1065350e+00  1.0000000e+00  1.5525284e+00 ... -8.6413391e+01\n",
      "   2.1255341e+02  2.2097717e+01]\n",
      " [-1.1067123e+00  1.0000000e+00  1.5524025e+00 ... -7.5716507e+01\n",
      "   9.6061905e+01  4.0289617e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5322275e+00  1.0000000e+00  1.1335850e+00 ...  5.7356229e+00\n",
      "  -3.3498645e+00  5.8273487e+00]\n",
      " [-1.5320635e+00  1.0000000e+00  1.1337795e+00 ...  1.3392917e+02\n",
      "   5.9679398e+01 -2.7222972e+00]\n",
      " [-1.5317497e+00  1.0000000e+00  1.1342176e+00 ... -5.5243907e+00\n",
      "   4.7301277e+01 -9.4258347e+00]\n",
      " ...\n",
      " [-1.5319252e+00  1.0000000e+00  1.1339149e+00 ... -2.7591472e+03\n",
      "   1.1768672e+03  4.2970316e+02]\n",
      " [-1.5322170e+00  1.0000000e+00  1.1335258e+00 ...  8.2087708e+00\n",
      "   1.0855869e+01  2.1639408e+01]\n",
      " [-1.5323648e+00  1.0000000e+00  1.1333523e+00 ...  2.0600065e+01\n",
      "   1.8468069e+02  2.3233633e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8078632e+00  1.0000000e+00  6.0307693e-01 ... -1.2357025e+00\n",
      "   2.1294302e-01 -4.3828797e-01]\n",
      " [-1.8077726e+00  1.0000000e+00  6.0336876e-01 ...  1.3982799e+02\n",
      "   4.3112808e+01 -1.1265579e+02]\n",
      " [-1.8076000e+00  1.0000000e+00  6.0385954e-01 ... -2.1821466e+02\n",
      "   1.5254515e+02 -9.0273193e+01]\n",
      " ...\n",
      " [-1.8076572e+00  1.0000000e+00  6.0353088e-01 ... -1.8754398e+02\n",
      "   3.9572628e+01  1.5243652e+01]\n",
      " [-1.8078308e+00  1.0000000e+00  6.0301018e-01 ...  3.2614493e+02\n",
      "   5.6831348e+01  8.7350906e+01]\n",
      " [-1.8079338e+00  1.0000000e+00  6.0281181e-01 ...  4.8042847e+02\n",
      "  -8.1952202e+01  1.7594095e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90597534e+00  1.00000000e+00  1.37004852e-02 ...  1.96221542e+00\n",
      "  -1.27751160e+00  1.53504860e+00]\n",
      " [-1.90596962e+00  1.00000000e+00  1.40132904e-02 ...  3.97068970e+02\n",
      "  -9.97963028e+01 -1.14008286e+02]\n",
      " [-1.90596962e+00  1.00000000e+00  1.45225637e-02 ... -9.65029373e+01\n",
      "   9.32364655e+01  6.80429230e+01]\n",
      " ...\n",
      " [-1.90590858e+00  1.00000000e+00  1.41849518e-02 ...  1.60574554e+02\n",
      "   1.12926361e+02 -2.64915497e+02]\n",
      " [-1.90594673e+00  1.00000000e+00  1.36337280e-02 ... -3.05948830e+01\n",
      "  -3.99080620e+01  1.05495224e+02]\n",
      " [-1.90595722e+00  1.00000000e+00  1.34201050e-02 ... -4.08470688e+01\n",
      "   6.33864975e+01 -2.53872070e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81717396e+00  1.00000000e+00 -5.76908112e-01 ... -1.78423882e-01\n",
      "   9.71658528e-01 -1.97571516e-01]\n",
      " [-1.81724644e+00  1.00000000e+00 -5.76602936e-01 ... -1.17240204e+02\n",
      "  -3.47234650e+01  6.67065964e+01]\n",
      " [-1.81739235e+00  1.00000000e+00 -5.76105356e-01 ...  1.27478325e+02\n",
      "  -7.34408035e+01  1.67540421e+02]\n",
      " ...\n",
      " [-1.81722260e+00  1.00000000e+00 -5.76441765e-01 ...  1.33112091e+02\n",
      "  -2.23163376e+02 -3.17119446e+02]\n",
      " [-1.81711578e+00  1.00000000e+00 -5.76972961e-01 ...  6.63604279e+01\n",
      "   6.29807234e+00 -1.29076782e+02]\n",
      " [-1.81705379e+00  1.00000000e+00 -5.77180862e-01 ...  4.76634491e+02\n",
      "   7.87635193e+02  1.74316483e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5498819e+00  1.0000000e+00 -1.1114616e+00 ... -3.3799219e-01\n",
      "   1.5622607e-01  2.7331054e+00]\n",
      " [-1.5500383e+00  1.0000000e+00 -1.1111937e+00 ... -3.0418428e+01\n",
      "   3.6287819e+01 -1.0281819e+02]\n",
      " [-1.5503368e+00  1.0000000e+00 -1.1107696e+00 ... -3.4723160e+01\n",
      "   1.3684431e+02  3.1411348e+01]\n",
      " ...\n",
      " [-1.5500622e+00  1.0000000e+00 -1.1110573e+00 ... -3.0848267e+02\n",
      "  -2.6728616e+02  7.5033981e+01]\n",
      " [-1.5498142e+00  1.0000000e+00 -1.1115170e+00 ... -1.6944534e+03\n",
      "  -1.8724001e+02  1.4786354e+03]\n",
      " [-1.5496855e+00  1.0000000e+00 -1.1117020e+00 ...  2.9101223e+01\n",
      "  -9.4659592e+01  9.3519081e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.13048649e+00  1.00000000e+00 -1.53703308e+00 ... -1.87722826e+00\n",
      "  -4.15971994e-01  3.76489878e-01]\n",
      " [-1.13071060e+00  1.00000000e+00 -1.53681946e+00 ...  6.85709033e+03\n",
      "   3.99411914e+03 -8.54919128e+02]\n",
      " [-1.13113785e+00  1.00000000e+00 -1.53651059e+00 ...  1.74655170e+01\n",
      "  -2.73437866e+02  7.17104340e+01]\n",
      " ...\n",
      " [-1.13075066e+00  1.00000000e+00 -1.53672886e+00 ...  2.63740265e+02\n",
      "   1.13574646e+02 -2.12686127e+02]\n",
      " [-1.13040924e+00  1.00000000e+00 -1.53706932e+00 ...  5.36111328e+02\n",
      "   3.44083069e+02 -3.41256836e+02]\n",
      " [-1.13023281e+00  1.00000000e+00 -1.53720665e+00 ... -6.69095459e+01\n",
      "   7.65608139e+01  1.78734756e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.599926      1.           -1.8120251  ...   -3.39753\n",
      "    -1.6013851     0.96981525]\n",
      " [  -0.60019207    1.           -1.8118792  ... -379.48917\n",
      "   -98.78421     -57.77315   ]\n",
      " [  -0.6006851     1.           -1.8117274  ...  145.89053\n",
      "   -14.788101   -124.23086   ]\n",
      " ...\n",
      " [  -0.60025215    1.           -1.81184    ...  -60.304302\n",
      "   -96.213425   -374.16428   ]\n",
      " [  -0.599844      1.           -1.8120403  ... -181.04808\n",
      "    11.929137    115.15024   ]\n",
      " [  -0.5996399     1.           -1.8121185  ... -110.24173\n",
      "  -138.20764     -21.758907  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.03845596e-02  1.00000000e+00 -1.90942383e+00 ...  1.21185493e+00\n",
      "   4.90809679e-01 -2.02864349e-01]\n",
      " [-1.06611252e-02  1.00000000e+00 -1.90935040e+00 ... -2.90536102e+02\n",
      "   3.49212433e+02 -2.47600967e+02]\n",
      " [-1.11656189e-02  1.00000000e+00 -1.90936244e+00 ...  1.32248322e+02\n",
      "  -7.71117477e+01 -1.14990425e+01]\n",
      " ...\n",
      " [-1.07173920e-02  1.00000000e+00 -1.90936852e+00 ...  1.13763702e+02\n",
      "  -6.37722397e+01  4.98621483e+01]\n",
      " [-1.02844238e-02  1.00000000e+00 -1.90942192e+00 ... -1.33783569e+02\n",
      "  -4.89398224e+02  1.87927856e+02]\n",
      " [-1.00879669e-02  1.00000000e+00 -1.90943146e+00 ...  9.26576004e+01\n",
      "  -3.52572365e+01  1.09416954e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.80232620e-01  1.00000000e+00 -1.81959343e+00 ... -1.09610605e+00\n",
      "  -1.56416893e-01  3.05570602e-01]\n",
      " [ 5.79963684e-01  1.00000000e+00 -1.81963158e+00 ...  2.00982227e+01\n",
      "   1.00330191e+01  3.50838127e+01]\n",
      " [ 5.79486847e-01  1.00000000e+00 -1.81978977e+00 ...  2.68535900e+01\n",
      "  -1.55602894e+01  5.01518669e+01]\n",
      " ...\n",
      " [ 5.79919815e-01  1.00000000e+00 -1.81968689e+00 ... -2.64417297e+02\n",
      "   2.46155731e+02 -2.00341919e+02]\n",
      " [ 5.80331802e-01  1.00000000e+00 -1.81957245e+00 ...  4.33171356e+02\n",
      "   3.33919983e+02 -1.19436966e+02]\n",
      " [ 5.80517769e-01  1.00000000e+00 -1.81952095e+00 ...  6.32250244e+02\n",
      "  -2.61133026e+02 -1.79698883e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.11448765e+00  1.00000000e+00 -1.55154800e+00 ... -1.74559116e-01\n",
      "  -1.16611481e-01  4.42832708e-02]\n",
      " [ 1.11425114e+00  1.00000000e+00 -1.55167198e+00 ...  6.53902710e+02\n",
      "   6.99279114e+02 -6.81244263e+02]\n",
      " [ 1.11383820e+00  1.00000000e+00 -1.55196536e+00 ... -1.99066284e+02\n",
      "   3.48760254e+02  3.43403809e+02]\n",
      " ...\n",
      " [ 1.11420822e+00  1.00000000e+00 -1.55176449e+00 ... -1.70424671e+01\n",
      "  -1.61762024e+02 -1.17368546e+02]\n",
      " [ 1.11456299e+00  1.00000000e+00 -1.55151749e+00 ...  1.00921425e+02\n",
      "  -2.73837921e+02  3.81117020e+01]\n",
      " [ 1.11472988e+00  1.00000000e+00 -1.55141068e+00 ... -9.06324768e+02\n",
      "   3.47632355e+02 -5.28017517e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.53944778e+00  1.00000000e+00 -1.13168335e+00 ... -1.23850346e-01\n",
      "  -7.17286110e-01 -2.87259817e-02]\n",
      " [ 1.53927898e+00  1.00000000e+00 -1.13188648e+00 ... -7.32637024e+02\n",
      "   8.47496521e+02  1.17859131e+03]\n",
      " [ 1.53896523e+00  1.00000000e+00 -1.13230276e+00 ...  9.87673340e+01\n",
      "   3.28207016e+00 -1.48824692e+02]\n",
      " ...\n",
      " [ 1.53923798e+00  1.00000000e+00 -1.13200855e+00 ...  1.37792892e+02\n",
      "  -1.39585663e+02 -1.15111923e+02]\n",
      " [ 1.53951073e+00  1.00000000e+00 -1.13163948e+00 ... -1.27543861e+02\n",
      "  -2.89912292e+02  1.29471008e+02]\n",
      " [ 1.53963089e+00  1.00000000e+00 -1.13149643e+00 ... -1.84072449e+02\n",
      "  -1.80077118e+02  2.54325394e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81401157e+00  1.00000000e+00 -6.00486755e-01 ...  9.30438042e-02\n",
      "  -5.18783092e-01 -2.89839432e-02]\n",
      " [ 1.81392479e+00  1.00000000e+00 -6.00771904e-01 ... -1.96538406e+02\n",
      "  -1.94836060e+02 -1.54966049e+02]\n",
      " [ 1.81378937e+00  1.00000000e+00 -6.01252675e-01 ...  4.65059662e+02\n",
      "  -1.06459460e+03 -1.06796106e+03]\n",
      " ...\n",
      " [ 1.81393433e+00  1.00000000e+00 -6.00910187e-01 ...  7.54688416e+01\n",
      "   1.65016235e+02 -9.55811920e+01]\n",
      " [ 1.81405640e+00  1.00000000e+00 -6.00433350e-01 ... -8.65580902e+01\n",
      "  -1.08086975e+02  2.46049210e+02]\n",
      " [ 1.81411648e+00  1.00000000e+00 -6.00263596e-01 ...  1.83198746e+02\n",
      "  -8.42632675e+01  4.72744873e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.91081524e+00  1.00000000e+00 -1.08585358e-02 ...  3.38401961e+00\n",
      "   4.04322100e+00  9.92673755e-01]\n",
      " [ 1.91081810e+00  1.00000000e+00 -1.11398697e-02 ... -2.42061829e+02\n",
      "   1.53704178e+02  2.81058868e+02]\n",
      " [ 1.91085625e+00  1.00000000e+00 -1.16594704e-02 ... -6.39814087e+02\n",
      "   5.86317505e+02  4.70468445e+02]\n",
      " ...\n",
      " [ 1.91089439e+00  1.00000000e+00 -1.12876892e-02 ... -1.19105125e+02\n",
      "  -2.42499298e+02  3.05376068e+02]\n",
      " [ 1.91086769e+00  1.00000000e+00 -1.08032227e-02 ...  2.39069347e+01\n",
      "   1.22177715e+01 -1.09673141e+02]\n",
      " [ 1.91083145e+00  1.00000000e+00 -1.06182098e-02 ...  6.44121027e+00\n",
      "   6.18789482e+01  2.25148277e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.82074738e+00  1.00000000e+00  5.79565048e-01 ...  1.93851089e+00\n",
      "   1.44771886e+00  3.47587079e-01]\n",
      " [ 1.82083130e+00  1.00000000e+00  5.79296112e-01 ...  5.49639854e+01\n",
      "   3.34795151e+01 -3.87918711e+00]\n",
      " [ 1.82106590e+00  1.00000000e+00  5.78807235e-01 ... -1.39056747e+02\n",
      "  -1.32424026e+02  5.45746651e+01]\n",
      " ...\n",
      " [ 1.82098770e+00  1.00000000e+00  5.79155922e-01 ... -1.37233772e+01\n",
      "  -4.26624268e+02  7.89031982e+01]\n",
      " [ 1.82080460e+00  1.00000000e+00  5.79616547e-01 ...  4.15543488e+02\n",
      "   5.09199867e+01  1.44832825e+02]\n",
      " [ 1.82068825e+00  1.00000000e+00  5.79792023e-01 ...  1.11129654e+02\n",
      "   1.85518494e+01  1.30929245e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5524826e+00  1.0000000e+00  1.1136398e+00 ...  2.8758955e-01\n",
      "   3.0226202e+00  7.3476648e-01]\n",
      " [ 1.5526476e+00  1.0000000e+00  1.1134205e+00 ...  1.0714470e+02\n",
      "   6.3801537e+01  1.9784444e+02]\n",
      " [ 1.5529957e+00  1.0000000e+00  1.1130017e+00 ...  1.1852596e+02\n",
      "   4.0690250e+01  1.9715692e+02]\n",
      " ...\n",
      " [ 1.5528145e+00  1.0000000e+00  1.1133051e+00 ... -4.1770275e+01\n",
      "   1.2009873e+02 -2.3058096e+02]\n",
      " [ 1.5525150e+00  1.0000000e+00  1.1136837e+00 ...  1.4240909e+02\n",
      "   1.5527878e+01  6.4180023e+01]\n",
      " [ 1.5523472e+00  1.0000000e+00  1.1138306e+00 ...  7.6554382e+02\n",
      "   5.6131116e+02  1.0657954e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1323929     1.            1.5385323  ...    0.73112637\n",
      "     5.493765      1.219776  ]\n",
      " [   1.1326103     1.            1.5383854  ...   12.513873\n",
      "   392.41354     227.05333   ]\n",
      " [   1.1330948     1.            1.5380796  ...  -36.30553\n",
      "   243.19421    -271.2873    ]\n",
      " ...\n",
      " [   1.1328201     1.            1.5382996  ... -527.1979\n",
      "  -404.68854     171.25453   ]\n",
      " [   1.1323986     1.            1.5385609  ...   -1.7738262\n",
      "     6.7953477    10.07614   ]\n",
      " [   1.1322069     1.            1.5386734  ...   18.339987\n",
      "  -108.22455      63.501488  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.01596832e-01  1.00000000e+00  1.81282425e+00 ...  2.99339056e-01\n",
      "   1.37297487e+01 -9.68132555e-01]\n",
      " [ 6.01854324e-01  1.00000000e+00  1.81280994e+00 ... -1.00808098e+02\n",
      "   1.33017960e+02  3.30092102e+02]\n",
      " [ 6.02405548e-01  1.00000000e+00  1.81265032e+00 ...  2.04765472e+01\n",
      "   1.56881733e+01 -1.17478348e+02]\n",
      " ...\n",
      " [ 6.02071762e-01  1.00000000e+00  1.81276798e+00 ... -6.48002338e+00\n",
      "  -2.05668655e+02 -2.66129456e+01]\n",
      " [ 6.01570129e-01  1.00000000e+00  1.81283760e+00 ...  2.17238007e+01\n",
      "   3.06367626e+01  5.24472809e+01]\n",
      " [ 6.01377487e-01  1.00000000e+00  1.81291008e+00 ...  1.92541904e+01\n",
      "  -1.22062485e+02 -5.47180367e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.16987228e-02  1.00000000e+00  1.90973663e+00 ... -1.48922175e-01\n",
      "   4.19778728e+00  5.92652000e-02]\n",
      " [ 1.19695663e-02  1.00000000e+00  1.90979767e+00 ... -9.38944016e+01\n",
      "  -3.81757996e+02  1.21941025e+02]\n",
      " [ 1.25846863e-02  1.00000000e+00  1.90980232e+00 ... -2.03082916e+02\n",
      "   2.74092529e+02  2.44920242e+02]\n",
      " ...\n",
      " [ 1.22299194e-02  1.00000000e+00  1.90981865e+00 ... -2.46190033e+01\n",
      "   9.45639114e+01  2.91762104e+01]\n",
      " [ 1.16882324e-02  1.00000000e+00  1.90973663e+00 ... -1.45493454e+02\n",
      "   3.91428032e+01 -1.19386337e+02]\n",
      " [ 1.14650726e-02  1.00000000e+00  1.90972710e+00 ...  6.58958664e+01\n",
      "   4.33862839e+01  6.86255646e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.7903862e-01  1.0000000e+00  1.8196640e+00 ...  4.2713910e-02\n",
      "   1.7544589e+00  2.1869917e-01]\n",
      " [-5.7878113e-01  1.0000000e+00  1.8197813e+00 ... -5.0294347e+00\n",
      "  -1.3016757e+00  3.8590832e+01]\n",
      " [-5.7816315e-01  1.0000000e+00  1.8199576e+00 ... -2.5358891e+01\n",
      "   1.5896259e+01 -6.8683586e+01]\n",
      " ...\n",
      " [-5.7850456e-01  1.0000000e+00  1.8198605e+00 ...  5.0326300e+00\n",
      "   1.9142078e+01  2.8025057e+01]\n",
      " [-5.7902527e-01  1.0000000e+00  1.8196526e+00 ...  1.6857599e+01\n",
      "  -6.5751793e+01 -4.9531590e+01]\n",
      " [-5.7926083e-01  1.0000000e+00  1.8195763e+00 ... -2.1717764e+03\n",
      "   2.4961559e+02 -2.1090559e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1129503e+00  1.0000000e+00  1.5515079e+00 ...  1.1095999e+00\n",
      "   2.3381245e+00 -5.4351956e-01]\n",
      " [-1.1127319e+00  1.0000000e+00  1.5516977e+00 ... -5.5621643e+01\n",
      "   4.5699927e+02 -4.8592152e+01]\n",
      " [-1.1122246e+00  1.0000000e+00  1.5520295e+00 ...  1.0560615e+03\n",
      "  -1.8764137e+03  2.2063008e+03]\n",
      " ...\n",
      " [-1.1125259e+00  1.0000000e+00  1.5518351e+00 ...  3.4715213e+02\n",
      "   1.1054513e+02 -4.0707421e+00]\n",
      " [-1.1129589e+00  1.0000000e+00  1.5514812e+00 ... -2.0412631e+02\n",
      "  -5.4496689e+01  1.0150540e+02]\n",
      " [-1.1131382e+00  1.0000000e+00  1.5513496e+00 ...  4.1060992e+02\n",
      "   4.9992474e+02 -7.4719867e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.53777504e+00  1.00000000e+00  1.13162231e+00 ...  1.07392526e+00\n",
      "   3.53205752e+00  3.19961309e-02]\n",
      " [-1.53760719e+00  1.00000000e+00  1.13185215e+00 ...  1.23960175e+02\n",
      "  -1.23960266e+02 -6.44357071e+01]\n",
      " [-1.53723526e+00  1.00000000e+00  1.13233054e+00 ... -2.30294285e+01\n",
      "  -3.82712059e+01 -3.92930023e+02]\n",
      " ...\n",
      " [-1.53747749e+00  1.00000000e+00  1.13205242e+00 ... -1.63636810e+02\n",
      "   1.83268204e+01 -1.92981291e+00]\n",
      " [-1.53781319e+00  1.00000000e+00  1.13158798e+00 ... -1.21755226e+02\n",
      "  -2.99311256e+01 -1.29476685e+02]\n",
      " [-1.53790665e+00  1.00000000e+00  1.13140869e+00 ... -4.07054504e+02\n",
      "  -6.34484802e+02  6.13365967e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81223488e+00  1.00000000e+00  6.00744247e-01 ...  4.51845884e-01\n",
      "   2.67471457e+00  7.51135349e-02]\n",
      " [-1.81214237e+00  1.00000000e+00  6.00998878e-01 ... -9.25711288e+01\n",
      "   2.40231644e+02  1.52600876e+02]\n",
      " [-1.81193924e+00  1.00000000e+00  6.01560533e-01 ... -1.50361343e+02\n",
      "   5.45771713e+01  8.41102295e+01]\n",
      " ...\n",
      " [-1.81206894e+00  1.00000000e+00  6.01233482e-01 ... -2.04868576e+02\n",
      "   6.59986734e+00 -3.59421229e+00]\n",
      " [-1.81224632e+00  1.00000000e+00  6.00700378e-01 ...  9.31940842e+01\n",
      "  -1.19305275e+02 -5.15989532e+01]\n",
      " [-1.81228924e+00  1.00000000e+00  6.00494385e-01 ... -9.71732635e+01\n",
      "  -2.31854202e+02 -1.51821686e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2 0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9089680e+00  1.0000000e+00  1.1110306e-02 ...  2.9005480e-01\n",
      "   3.0429697e+00  4.2444587e-02]\n",
      " [-1.9089613e+00  1.0000000e+00  1.1386871e-02 ...  2.3999516e+01\n",
      "  -1.9766760e+02 -1.6311636e+02]\n",
      " [-1.9089508e+00  1.0000000e+00  1.1985536e-02 ... -5.6821594e+00\n",
      "   5.4306412e-01  1.6146034e+01]\n",
      " ...\n",
      " [-1.9089680e+00  1.0000000e+00  1.1634827e-02 ... -5.7419598e+02\n",
      "  -4.8781531e+02  1.9560567e+02]\n",
      " [-1.9089832e+00  1.0000000e+00  1.1066437e-02 ...  2.9429913e+01\n",
      "   8.8443268e+01 -3.4523766e+01]\n",
      " [-1.9089518e+00  1.0000000e+00  1.0843277e-02 ... -1.9973955e+02\n",
      "   2.4185553e+02 -1.6395093e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8187046e+00  1.0000000e+00 -5.7969093e-01 ...  2.4592257e+00\n",
      "   5.9068394e+00  5.6055009e-02]\n",
      " [-1.8187885e+00  1.0000000e+00 -5.7940006e-01 ... -5.5351013e+01\n",
      "   1.3623276e+02 -1.4205589e+02]\n",
      " [-1.8189335e+00  1.0000000e+00 -5.7884181e-01 ... -1.0848457e+02\n",
      "  -2.0665167e+02 -1.3847417e+02]\n",
      " ...\n",
      " [-1.8188457e+00  1.0000000e+00 -5.7916641e-01 ...  2.5671094e+02\n",
      "   1.3933379e+03  1.3980580e+02]\n",
      " [-1.8186970e+00  1.0000000e+00 -5.7973099e-01 ...  1.2989563e+02\n",
      "  -1.1704427e+02  6.4150177e+01]\n",
      " [-1.8186083e+00  1.0000000e+00 -5.7995033e-01 ... -5.6251591e+01\n",
      "  -4.0600986e+01 -7.8240356e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5503702    1.          -1.1137657 ...    2.4240663    3.6533124\n",
      "    -1.0771096]\n",
      " [  -1.5505342    1.          -1.1135311 ...   23.690712     6.95885\n",
      "   -35.035675 ]\n",
      " [  -1.5508404    1.          -1.1130611 ...  205.79256   -167.05693\n",
      "  -116.99357  ]\n",
      " ...\n",
      " [  -1.5506477    1.          -1.1133308 ... -348.19934    132.62619\n",
      "  -293.3536   ]\n",
      " [  -1.5503502    1.          -1.1137924 ... -161.30402   -155.14697\n",
      "   127.91896  ]\n",
      " [  -1.5502186    1.          -1.1139984 ...   62.00732    -82.880295\n",
      "    53.091946 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1302576e+00  1.0000000e+00 -1.5385532e+00 ...  6.8244410e-01\n",
      "  -1.3913364e+00 -2.5003457e-01]\n",
      " [-1.1304817e+00  1.0000000e+00 -1.5383720e+00 ... -1.2290013e+02\n",
      "  -1.8696794e+02  2.3302242e+02]\n",
      " [-1.1309052e+00  1.0000000e+00 -1.5380324e+00 ... -2.0668196e+01\n",
      "   6.9019501e+01  9.6750648e+01]\n",
      " ...\n",
      " [-1.1306248e+00  1.0000000e+00 -1.5382156e+00 ...  2.0133826e+02\n",
      "  -2.0667749e+02 -2.4467261e+02]\n",
      " [-1.1301880e+00  1.0000000e+00 -1.5385666e+00 ... -8.5875525e+02\n",
      "   8.0534857e+02  2.9699939e+02]\n",
      " [-1.1300478e+00  1.0000000e+00 -1.5387344e+00 ...  3.6272502e+00\n",
      "   1.1794935e+01  9.1622143e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.1       0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9955406e-01  1.0000000e+00 -1.8126659e+00 ...  6.2204385e-01\n",
      "   1.7973509e+00  3.5063350e-01]\n",
      " [-5.9981441e-01  1.0000000e+00 -1.8125792e+00 ...  2.7810944e+02\n",
      "  -3.6425055e+02 -4.4392688e+02]\n",
      " [-6.0036659e-01  1.0000000e+00 -1.8123966e+00 ...  4.2814789e+01\n",
      "  -2.3786838e+02  7.7427483e+01]\n",
      " ...\n",
      " [-6.0004425e-01  1.0000000e+00 -1.8124800e+00 ...  8.8363319e+01\n",
      "  -7.9412796e+01  6.4517441e+01]\n",
      " [-5.9950638e-01  1.0000000e+00 -1.8126640e+00 ...  7.4599053e+03\n",
      "  -6.3501650e+03  7.9944204e+03]\n",
      " [-5.9929657e-01  1.0000000e+00 -1.8127728e+00 ... -5.2689758e+01\n",
      "   1.6042584e+02  1.2350452e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.86766815e-03  1.00000000e+00 -1.90937996e+00 ... -2.08572388e-01\n",
      "   4.10408115e+00  7.05650568e-01]\n",
      " [-1.01423264e-02  1.00000000e+00 -1.90937424e+00 ... -1.27301926e+02\n",
      "   2.54547638e+02 -1.81457703e+02]\n",
      " [-1.07650757e-02  1.00000000e+00 -1.90936220e+00 ...  5.62892380e+01\n",
      "  -1.90521530e+02  5.04617958e+01]\n",
      " ...\n",
      " [-1.04217529e-02  1.00000000e+00 -1.90934467e+00 ... -1.05091530e+02\n",
      "  -4.21824532e+01  2.45670462e+00]\n",
      " [-9.86099243e-03  1.00000000e+00 -1.90936661e+00 ...  9.19615631e+01\n",
      "  -4.79103546e+02  1.52775928e+03]\n",
      " [-9.58824158e-03  1.00000000e+00 -1.90940285e+00 ... -8.80911636e+01\n",
      "  -3.81113586e+01  2.17950836e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.8094978e-01  1.0000000e+00 -1.8186207e+00 ...  2.7337606e+00\n",
      "   7.6143293e+00  2.1345899e-01]\n",
      " [ 5.8068848e-01  1.0000000e+00 -1.8187265e+00 ...  5.5513683e+01\n",
      "   4.1234158e+01 -1.6299789e+02]\n",
      " [ 5.8012199e-01  1.0000000e+00 -1.8188750e+00 ...  1.1111473e+02\n",
      "   3.5303961e+02 -4.5706134e+02]\n",
      " ...\n",
      " [ 5.8044624e-01  1.0000000e+00 -1.8187666e+00 ...  4.5194420e+01\n",
      "  -2.7856827e+00  9.3704140e+01]\n",
      " [ 5.8096123e-01  1.0000000e+00 -1.8185921e+00 ...  8.5254932e+02\n",
      "   9.1989548e+01 -4.7466476e+02]\n",
      " [ 5.8121204e-01  1.0000000e+00 -1.8185635e+00 ... -8.4216751e+01\n",
      "   1.3688710e+02 -1.2208019e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1148853    1.          -1.5500412 ...   -3.2234535  -14.254126\n",
      "    -1.2588739]\n",
      " [   1.1146584    1.          -1.5502481 ...  238.23679    -93.83528\n",
      "   478.5793   ]\n",
      " [   1.1141682    1.          -1.5505373 ...   73.15774     11.48003\n",
      "   224.01375  ]\n",
      " ...\n",
      " [   1.1144581    1.          -1.550333  ...  151.30925     24.400167\n",
      "   142.51465  ]\n",
      " [   1.1149044    1.          -1.550005  ... -162.30995     68.91052\n",
      "    40.914314 ]\n",
      " [   1.1151037    1.          -1.549921  ...   12.001802   -42.332973\n",
      "    28.485315 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.5394001   1.         -1.1295395 ...   0.5135536  -1.5474658\n",
      "   -0.5460566]\n",
      " [  1.5392437   1.         -1.129796  ...  89.882225  131.15756\n",
      "   48.009567 ]\n",
      " [  1.5388565   1.         -1.1302183 ...  -2.0467525 -97.516884\n",
      "   -7.2743464]\n",
      " ...\n",
      " [  1.539053    1.         -1.12994   ...  11.35052    -2.9958704\n",
      "   20.964315 ]\n",
      " [  1.5393963   1.         -1.1295013 ...   3.7224553 -22.784822\n",
      "   15.627896 ]\n",
      " [  1.5395651   1.         -1.1293659 ... 461.9155    348.28018\n",
      "   76.36084  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81332207e+00  1.00000000e+00 -5.98497391e-01 ... -2.75937557e-01\n",
      "   1.37264204e+00 -6.33617640e-02]\n",
      " [ 1.81323338e+00  1.00000000e+00 -5.98794937e-01 ... -3.37511932e+02\n",
      "  -8.02904968e+01  1.11305916e+02]\n",
      " [ 1.81303596e+00  1.00000000e+00 -5.99302351e-01 ...  2.30510979e+01\n",
      "   2.33580566e+02 -1.35284988e+02]\n",
      " ...\n",
      " [ 1.81311035e+00  1.00000000e+00 -5.98974228e-01 ... -4.26047707e+00\n",
      "   1.28973370e+01  3.13450813e+00]\n",
      " [ 1.81333160e+00  1.00000000e+00 -5.98451614e-01 ...  7.61528492e+00\n",
      "  -2.44669676e+00 -2.35259285e+01]\n",
      " [ 1.81341553e+00  1.00000000e+00 -5.98281860e-01 ... -4.23238297e+01\n",
      "   2.72000790e+01  5.05409660e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9096518e+00  1.0000000e+00 -9.0618134e-03 ... -7.6744676e-01\n",
      "   1.6802306e+00 -2.2206783e-02]\n",
      " [ 1.9096556e+00  1.0000000e+00 -9.3927383e-03 ... -1.3777444e+02\n",
      "  -2.0655473e+02 -3.7456402e+01]\n",
      " [ 1.9096489e+00  1.0000000e+00 -9.9302232e-03 ... -9.7033669e+01\n",
      "   5.0139503e+00  9.5501595e+01]\n",
      " ...\n",
      " [ 1.9096222e+00  1.0000000e+00 -9.5815659e-03 ...  1.2304054e+01\n",
      "   7.7276085e+01  6.3826885e+01]\n",
      " [ 1.9096889e+00  1.0000000e+00 -9.0141296e-03 ... -5.7293278e+01\n",
      "  -4.7931866e+01  5.4536469e+01]\n",
      " [ 1.9096670e+00  1.0000000e+00 -8.8291168e-03 ...  3.0296108e+01\n",
      "   1.5080605e+02 -4.0079977e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8190889e+00  1.0000000e+00  5.8156776e-01 ...  5.5114102e-01\n",
      "   3.8581133e-01 -2.5687516e-01]\n",
      " [ 1.8191671e+00  1.0000000e+00  5.8127117e-01 ... -1.8833255e+02\n",
      "   1.9544954e+02 -6.1164036e+01]\n",
      " [ 1.8192863e+00  1.0000000e+00  5.8074838e-01 ...  1.6009758e+02\n",
      "   1.0735695e+02 -8.9526196e+00]\n",
      " ...\n",
      " [ 1.8191586e+00  1.0000000e+00  5.8109188e-01 ...  3.1316780e+01\n",
      "   1.1422069e+03 -2.6888831e+02]\n",
      " [ 1.8190670e+00  1.0000000e+00  5.8161545e-01 ... -2.4471737e+01\n",
      "  -2.3459174e+02 -1.0608256e+02]\n",
      " [ 1.8190250e+00  1.0000000e+00  5.8179283e-01 ... -1.2393712e+03\n",
      "   1.9002150e+01  3.6959579e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.54992962e+00  1.00000000e+00  1.11557770e+00 ...  2.23559856e+00\n",
      "   4.26605988e+00  5.13097942e-01]\n",
      " [ 1.55008125e+00  1.00000000e+00  1.11532688e+00 ...  1.02631317e+02\n",
      "   1.70547028e+02 -3.94471359e+00]\n",
      " [ 1.55037308e+00  1.00000000e+00  1.11489475e+00 ... -2.76951813e+02\n",
      "  -6.78464966e+01 -2.69174561e+02]\n",
      " ...\n",
      " [ 1.55017471e+00  1.00000000e+00  1.11517334e+00 ... -2.97573614e+00\n",
      "   1.24693054e+02 -1.96896637e+02]\n",
      " [ 1.54990959e+00  1.00000000e+00  1.11561966e+00 ... -5.34721222e+01\n",
      "  -1.03807507e+01  4.40363388e+01]\n",
      " [ 1.54979515e+00  1.00000000e+00  1.11577797e+00 ... -2.02663684e+03\n",
      "  -8.77941833e+02 -9.85535156e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1295242e+00  1.0000000e+00  1.5398750e+00 ... -1.8652246e+00\n",
      "  -1.0754777e+01 -4.7998762e-01]\n",
      " [ 1.1297407e+00  1.0000000e+00  1.5396776e+00 ...  1.6562639e+03\n",
      "   1.8282784e+02 -1.4007516e+03]\n",
      " [ 1.1301384e+00  1.0000000e+00  1.5393728e+00 ... -1.0155890e+02\n",
      "  -3.4095516e+01  9.9705582e+00]\n",
      " ...\n",
      " [ 1.1298618e+00  1.0000000e+00  1.5395737e+00 ...  5.4362503e+01\n",
      "   8.1164505e+01 -6.0079624e+01]\n",
      " [ 1.1294689e+00  1.0000000e+00  1.5399113e+00 ...  3.2137076e+02\n",
      "   1.9700156e+02 -2.4410782e+01]\n",
      " [ 1.1293383e+00  1.0000000e+00  1.5400276e+00 ...  1.1029653e+02\n",
      "   2.0808481e+01  2.5054904e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9815693e-01  1.0000000e+00  1.8136730e+00 ...  5.7570595e-01\n",
      "  -2.1173050e+00  5.7915032e-01]\n",
      " [ 5.9841061e-01  1.0000000e+00  1.8135443e+00 ... -8.4250409e+02\n",
      "  -4.6205884e+02 -1.6078685e+02]\n",
      " [ 5.9890366e-01  1.0000000e+00  1.8133836e+00 ...  2.0549796e+02\n",
      "  -1.8513255e+02  4.5685880e+02]\n",
      " ...\n",
      " [ 5.9857368e-01  1.0000000e+00  1.8134909e+00 ... -9.6707764e+01\n",
      "   4.2358372e+01  4.9582176e+01]\n",
      " [ 5.9811783e-01  1.0000000e+00  1.8136921e+00 ...  2.5078648e+01\n",
      "  -4.6418022e+01 -9.6607651e+01]\n",
      " [ 5.9793472e-01  1.0000000e+00  1.8137474e+00 ...  7.3499390e+01\n",
      "  -9.6995869e+00  1.6636911e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.4667206e-03  1.0000000e+00  1.9097652e+00 ...  1.0454398e+00\n",
      "  -8.7605798e-01  1.4896572e-01]\n",
      " [ 8.7394714e-03  1.0000000e+00  1.9097042e+00 ...  1.9327654e+02\n",
      "   2.3543207e+02  1.1566875e+00]\n",
      " [ 9.2468262e-03  1.0000000e+00  1.9097196e+00 ... -2.8980069e+02\n",
      "  -2.2081065e+02 -7.7968201e+01]\n",
      " ...\n",
      " [ 8.9054108e-03  1.0000000e+00  1.9097061e+00 ...  1.7982506e+01\n",
      "   1.5614139e+02 -6.4918854e+01]\n",
      " [ 8.4228516e-03  1.0000000e+00  1.9097633e+00 ...  1.3269997e+01\n",
      "  -1.5526494e+01  1.1971998e-01]\n",
      " [ 8.2321167e-03  1.0000000e+00  1.9097576e+00 ... -1.8946964e+01\n",
      "   2.4515369e+02 -3.9078970e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.58201313    1.            1.8189144  ...    0.7869512\n",
      "    -0.41323066    0.8549012 ]\n",
      " [  -0.5817623     1.            1.818965   ...  -40.37308\n",
      "   -29.990622    -90.99083   ]\n",
      " [  -0.5812683     1.            1.8191417  ...  -12.644815\n",
      "  -188.42218    -400.49908   ]\n",
      " ...\n",
      " [  -0.5815964     1.            1.8190212  ...  117.979195\n",
      "  -106.91598      64.10374   ]\n",
      " [  -0.5820484     1.            1.818903   ...   51.306293\n",
      "  -113.010765    103.090126  ]\n",
      " [  -0.58224106    1.            1.8188381  ...  -43.92435\n",
      "   -75.93962      76.86273   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1156406e+00  1.0000000e+00  1.5499649e+00 ...  9.8693562e-01\n",
      "  -1.4415250e+00  7.3809552e-01]\n",
      " [-1.1154337e+00  1.0000000e+00  1.5500994e+00 ...  1.2439001e+02\n",
      "   3.2578674e+02  3.4746341e+02]\n",
      " [-1.1150284e+00  1.0000000e+00  1.5504239e+00 ... -1.2439718e+04\n",
      "  -8.0977729e+03 -1.2833569e+02]\n",
      " ...\n",
      " [-1.1153126e+00  1.0000000e+00  1.5502052e+00 ... -3.2495483e+01\n",
      "   5.6895687e+01  1.6971901e+02]\n",
      " [-1.1156712e+00  1.0000000e+00  1.5499363e+00 ... -8.4755994e+02\n",
      "   7.7606530e+00 -1.4036829e+03]\n",
      " [-1.1158333e+00  1.0000000e+00  1.5498199e+00 ...  2.8081165e+02\n",
      "   3.7684494e+02 -2.3023094e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5399437e+00  1.0000000e+00  1.1290703e+00 ...  1.2338567e+00\n",
      "  -2.9426363e+00  3.7969422e-01]\n",
      " [-1.5397987e+00  1.0000000e+00  1.1292887e+00 ... -4.2004724e+02\n",
      "   2.6249860e+02  3.9199454e+02]\n",
      " [-1.5395050e+00  1.0000000e+00  1.1297274e+00 ... -2.3692234e+03\n",
      "   1.3243256e+03  1.3917503e+02]\n",
      " ...\n",
      " [-1.5397072e+00  1.0000000e+00  1.1294279e+00 ... -7.5892242e+02\n",
      "  -1.6027490e+02 -8.7460211e+02]\n",
      " [-1.5399361e+00  1.0000000e+00  1.1290321e+00 ...  1.2494860e+03\n",
      "   2.6113665e+01 -1.7249606e+03]\n",
      " [-1.5400801e+00  1.0000000e+00  1.1288834e+00 ...  8.2059033e+02\n",
      "  -2.6847150e+02 -6.0025885e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81326962e+00  1.00000000e+00  5.97774506e-01 ...  6.49263000e+00\n",
      "  -3.79851079e+00  4.70371246e-01]\n",
      " [-1.81320953e+00  1.00000000e+00  5.98044395e-01 ...  2.74445984e+02\n",
      "  -2.25327002e+03 -4.57823669e+02]\n",
      " [-1.81307602e+00  1.00000000e+00  5.98550439e-01 ...  8.26657227e+02\n",
      "  -7.93866028e+02 -5.30965332e+02]\n",
      " ...\n",
      " [-1.81317520e+00  1.00000000e+00  5.98203659e-01 ... -5.18541626e+02\n",
      "   1.13675095e+02 -5.68762207e+02]\n",
      " [-1.81327248e+00  1.00000000e+00  5.97732544e-01 ...  2.94997009e+02\n",
      "   5.86663551e+01  2.04745987e+02]\n",
      " [-1.81333065e+00  1.00000000e+00  5.97568512e-01 ... -1.99126601e+00\n",
      "   2.09931240e+01  1.61875191e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9092302e+00  1.0000000e+00  7.6923370e-03 ...  3.0427594e+00\n",
      "  -1.1430992e+00  5.6152987e-01]\n",
      " [-1.9092579e+00  1.0000000e+00  7.9908371e-03 ...  5.1491998e+02\n",
      "   5.6727814e+01 -7.9758896e+01]\n",
      " [-1.9092388e+00  1.0000000e+00  8.5250419e-03 ... -4.7309769e+02\n",
      "  -2.0087640e+02  1.1874214e+01]\n",
      " ...\n",
      " [-1.9092350e+00  1.0000000e+00  8.1605911e-03 ...  5.0334052e+02\n",
      "  -8.4317245e+01 -5.9800177e+02]\n",
      " [-1.9092064e+00  1.0000000e+00  7.6465607e-03 ... -1.5165298e+02\n",
      "  -6.2757611e+02  7.5100140e+02]\n",
      " [-1.9092150e+00  1.0000000e+00  7.4748993e-03 ... -4.1815464e+01\n",
      "  -6.2425095e+01  3.9154382e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8180475e+00  1.0000000e+00 -5.8259201e-01 ...  5.0569630e+00\n",
      "  -5.3127325e-01  5.9690118e-01]\n",
      " [-1.8181601e+00  1.0000000e+00 -5.8228779e-01 ...  1.0219742e+01\n",
      "  -2.4849495e+02 -5.7084473e+01]\n",
      " [-1.8183041e+00  1.0000000e+00 -5.8178073e-01 ...  1.2198191e+02\n",
      "  -7.7920624e+02  2.5932748e+02]\n",
      " ...\n",
      " [-1.8181820e+00  1.0000000e+00 -5.8212757e-01 ...  1.0456316e+03\n",
      "  -3.4207452e+02 -3.4564313e+02]\n",
      " [-1.8180103e+00  1.0000000e+00 -5.8263588e-01 ... -3.6181430e+02\n",
      "   1.9516733e+02  6.8569229e+01]\n",
      " [-1.8179617e+00  1.0000000e+00 -5.8279800e-01 ...  1.2183940e+02\n",
      "  -9.7127068e+01  1.2078353e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5488262e+00  1.0000000e+00 -1.1163406e+00 ...  1.6882532e+01\n",
      "  -5.8657584e+00 -1.2272081e+00]\n",
      " [-1.5490141e+00  1.0000000e+00 -1.1161156e+00 ...  4.9694351e+02\n",
      "  -2.1957286e+02  7.9931999e+01]\n",
      " [-1.5493145e+00  1.0000000e+00 -1.1156777e+00 ... -8.4060417e+01\n",
      "   7.4335107e+02 -4.4732437e+02]\n",
      " ...\n",
      " [-1.5490894e+00  1.0000000e+00 -1.1159792e+00 ... -3.0116605e+02\n",
      "   1.4029909e+02 -8.7049088e+01]\n",
      " [-1.5487747e+00  1.0000000e+00 -1.1163807e+00 ...  4.3267889e+02\n",
      "   1.4365631e+02 -4.0311984e+02]\n",
      " [-1.5486803e+00  1.0000000e+00 -1.1165142e+00 ... -4.1049780e+03\n",
      "   2.0183494e+03 -5.7618481e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1280508     1.           -1.5402374  ...    7.9760747\n",
      "    -3.149547     -0.77588445]\n",
      " [  -1.1283045     1.           -1.5400906  ...   85.778076\n",
      "    54.81109     -17.159027  ]\n",
      " [  -1.1287003     1.           -1.5397711  ...  -99.196915\n",
      "    84.76773    -124.110954  ]\n",
      " ...\n",
      " [  -1.1283932     1.           -1.5399923  ... -153.73535\n",
      "  -141.12778    -297.13544   ]\n",
      " [  -1.1279469     1.           -1.5402699  ... -100.07975\n",
      "  -244.61029    -248.1994    ]\n",
      " [  -1.1278467     1.           -1.5403595  ...    6.5035605\n",
      "   115.71801      36.39844   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.59676933    1.           -1.8136158  ...    3.987693\n",
      "    -3.5255895    -2.6968465 ]\n",
      " [  -0.5970564     1.           -1.8135567  ...   36.49153\n",
      "   -62.368927      3.234549  ]\n",
      " [  -0.59752464    1.           -1.8133793  ...   -0.822871\n",
      "   221.9607     -229.05472   ]\n",
      " ...\n",
      " [  -0.5971718     1.           -1.8135033  ... -277.6967\n",
      "   194.85532      46.07321   ]\n",
      " [  -0.5966568     1.           -1.8136349  ...  352.39966\n",
      "  -100.68573    -187.19226   ]\n",
      " [  -0.5965347     1.           -1.8136845  ... -149.98541\n",
      "    47.447903     25.355751  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.8235397e-03  1.0000000e+00 -1.9093609e+00 ...  5.0348930e+00\n",
      "  -5.0987029e+00 -1.4723772e+00]\n",
      " [-7.1249008e-03  1.0000000e+00 -1.9094362e+00 ... -3.0696196e+01\n",
      "   6.9971184e+01 -2.7922544e+01]\n",
      " [-7.6122284e-03  1.0000000e+00 -1.9094054e+00 ...  1.1734915e+02\n",
      "  -2.4246197e+01  1.5536565e+02]\n",
      " ...\n",
      " [-7.2441101e-03  1.0000000e+00 -1.9094362e+00 ... -1.3745731e+02\n",
      "  -2.0458879e+02 -5.7557919e+01]\n",
      " [-6.6890717e-03  1.0000000e+00 -1.9093647e+00 ... -6.0902796e+00\n",
      "  -1.7657894e+01 -4.1797859e+01]\n",
      " [-6.5813065e-03  1.0000000e+00 -1.9093704e+00 ...  1.5348019e+01\n",
      "   5.5960526e+01  1.1906398e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.83593369e-01  1.00000000e+00 -1.81813240e+00 ... -1.96733785e+00\n",
      "  -6.95105672e-01 -6.04554296e-01]\n",
      " [ 5.83307266e-01  1.00000000e+00 -1.81831932e+00 ... -1.45600647e+02\n",
      "  -2.25553875e+01  7.03323669e+01]\n",
      " [ 5.82864761e-01  1.00000000e+00 -1.81844449e+00 ... -1.22316605e+02\n",
      "  -1.98566849e+02 -1.56321136e+02]\n",
      " ...\n",
      " [ 5.83221436e-01  1.00000000e+00 -1.81837082e+00 ...  2.53484116e+01\n",
      "  -3.52085632e+02  2.84901337e+02]\n",
      " [ 5.83759308e-01  1.00000000e+00 -1.81811714e+00 ...  2.07043610e+02\n",
      "   2.85230896e+02 -3.27456970e+02]\n",
      " [ 5.83821297e-01  1.00000000e+00 -1.81807709e+00 ... -1.08317957e+03\n",
      "   3.19247009e+02 -6.55266724e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1171837e+00  1.0000000e+00 -1.5484715e+00 ...  1.7495286e-01\n",
      "  -2.1422710e+00 -1.0358878e+00]\n",
      " [ 1.1169405e+00  1.0000000e+00 -1.5487356e+00 ... -8.2873947e+01\n",
      "  -1.8290436e+00  2.6877878e+01]\n",
      " [ 1.1165714e+00  1.0000000e+00 -1.5490028e+00 ... -7.6040092e+01\n",
      "  -5.0344944e+01 -3.6427475e+01]\n",
      " ...\n",
      " [ 1.1168957e+00  1.0000000e+00 -1.5488396e+00 ... -1.7305841e+02\n",
      "   2.1369963e+02  1.1752352e+02]\n",
      " [ 1.1173515e+00  1.0000000e+00 -1.5484333e+00 ... -6.6090948e+02\n",
      "   7.7826561e+01  1.1186764e+03]\n",
      " [ 1.1173773e+00  1.0000000e+00 -1.5483494e+00 ... -8.3715820e+02\n",
      "  -1.4401060e+03 -2.7430740e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5411320e+00  1.0000000e+00 -1.1274910e+00 ... -5.7156434e+00\n",
      "   7.0910454e-01  1.2812847e+01]\n",
      " [ 1.5409517e+00  1.0000000e+00 -1.1278667e+00 ...  8.0821251e+01\n",
      "   4.1556454e-01 -9.9806580e+01]\n",
      " [ 1.5406551e+00  1.0000000e+00 -1.1282327e+00 ... -8.1307785e+01\n",
      "  -1.4944884e+03 -5.2993317e+02]\n",
      " ...\n",
      " [ 1.5409126e+00  1.0000000e+00 -1.1280060e+00 ...  1.7165930e+02\n",
      "  -5.7797651e+00 -6.1151681e+00]\n",
      " [ 1.5412483e+00  1.0000000e+00 -1.1274281e+00 ... -8.6660851e+01\n",
      "   4.3488688e+00 -3.4561491e-01]\n",
      " [ 1.5412683e+00  1.0000000e+00 -1.1273174e+00 ... -4.7211277e+01\n",
      "  -9.8691193e+02 -5.8604248e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8139448e+00  1.0000000e+00 -5.9603310e-01 ... -6.7187853e+00\n",
      "  -3.9602101e+00  1.0273548e+01]\n",
      " [ 1.8138599e+00  1.0000000e+00 -5.9643364e-01 ... -2.8666193e+02\n",
      "  -1.8392758e+02  3.2415508e+01]\n",
      " [ 1.8137760e+00  1.0000000e+00 -5.9687674e-01 ...  1.6048898e+02\n",
      "  -9.3643166e+01  3.1407550e+02]\n",
      " ...\n",
      " [ 1.8139477e+00  1.0000000e+00 -5.9659386e-01 ... -2.9320938e+02\n",
      "   4.0230949e+01 -4.4247482e+02]\n",
      " [ 1.8140869e+00  1.0000000e+00 -5.9595871e-01 ... -3.1042945e-01\n",
      "  -2.5394409e+01 -2.2436447e+01]\n",
      " [ 1.8140163e+00  1.0000000e+00 -5.9583473e-01 ...  1.7215387e+01\n",
      "   8.7653824e+01 -2.0917535e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9094791e+00  1.0000000e+00 -5.8059692e-03 ... -3.5649943e+00\n",
      "  -1.3201975e+00  3.6953025e+00]\n",
      " [ 1.9094973e+00  1.0000000e+00 -6.2637329e-03 ...  8.4196884e+01\n",
      "   5.4982998e+01 -5.6278992e+01]\n",
      " [ 1.9095726e+00  1.0000000e+00 -6.7162621e-03 ...  3.3386032e+02\n",
      "  -7.4595573e+01  7.0043341e+02]\n",
      " ...\n",
      " [ 1.9096203e+00  1.0000000e+00 -6.4296722e-03 ...  2.0408253e+01\n",
      "  -5.9033666e+00  5.7478905e+00]\n",
      " [ 1.9095974e+00  1.0000000e+00 -5.7239532e-03 ...  2.8509430e+01\n",
      "   1.0295262e+02  2.8139694e+01]\n",
      " [ 1.9094954e+00  1.0000000e+00 -5.5961609e-03 ... -7.4195747e+00\n",
      "   5.2791886e+00  5.8768258e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81792068e+00  1.00000000e+00  5.84623337e-01 ... -4.87154531e+00\n",
      "   6.45272923e+00  6.65005636e+00]\n",
      " [ 1.81804848e+00  1.00000000e+00  5.84218979e-01 ...  1.22641945e+01\n",
      "  -2.25405579e+01 -5.23917198e+01]\n",
      " [ 1.81825447e+00  1.00000000e+00  5.83780229e-01 ... -1.20746109e+02\n",
      "  -7.57061691e+01 -1.58814430e+01]\n",
      " ...\n",
      " [ 1.81818581e+00  1.00000000e+00  5.84056854e-01 ...  1.06206384e+03\n",
      "   8.26057922e+02  2.94444751e+03]\n",
      " [ 1.81798553e+00  1.00000000e+00  5.84705353e-01 ...  8.41112137e+00\n",
      "  -6.48145905e+01  3.12641773e+01]\n",
      " [ 1.81786728e+00  1.00000000e+00  5.84819794e-01 ...  7.19035583e+02\n",
      "  -9.57703552e+01 -5.63987732e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5483217    1.           1.117878  ...    4.4361873    5.5978184\n",
      "     1.5364656]\n",
      " [   1.5485601    1.           1.1175022 ... -319.83618   -301.83548\n",
      "  -204.6647   ]\n",
      " [   1.5488739    1.           1.1171336 ... -100.48192   -295.30496\n",
      "   292.3696   ]\n",
      " ...\n",
      " [   1.5487347    1.           1.1173573 ... -233.87561   -572.38556\n",
      "   -99.98741  ]\n",
      " [   1.5483437    1.           1.1179504 ...  252.95973   -150.99245\n",
      "    53.99873  ]\n",
      " [   1.5482016    1.           1.118042  ... -138.72145    550.00555\n",
      "   449.6905   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1272354e+00  1.0000000e+00  1.5415726e+00 ...  7.2914085e+00\n",
      "  -2.0699739e-02  2.4148283e+00]\n",
      " [ 1.1275625e+00  1.0000000e+00  1.5413141e+00 ... -4.9982980e+02\n",
      "  -3.8442059e+02  1.7314001e+01]\n",
      " [ 1.1279850e+00  1.0000000e+00  1.5410482e+00 ...  6.6547446e+00\n",
      "   3.6026196e+01 -5.3058800e+01]\n",
      " ...\n",
      " [ 1.1277885e+00  1.0000000e+00  1.5411949e+00 ...  2.2737534e+02\n",
      "   4.6154016e+02  2.9253833e+02]\n",
      " [ 1.1272488e+00  1.0000000e+00  1.5416374e+00 ...  8.1077965e+01\n",
      "  -3.1460223e+02  1.4452739e+02]\n",
      " [ 1.1270676e+00  1.0000000e+00  1.5416946e+00 ...  2.3048987e+02\n",
      "   1.5582976e+02 -2.5638699e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9545994e-01  1.0000000e+00  1.8145752e+00 ...  8.0049062e-01\n",
      "   3.1944997e+00  4.4149594e+00]\n",
      " [ 5.9584522e-01  1.0000000e+00  1.8144169e+00 ... -4.4262630e+01\n",
      "   2.8835214e+02  1.5288731e+02]\n",
      " [ 5.9631729e-01  1.0000000e+00  1.8142866e+00 ... -1.3586222e+01\n",
      "  -7.3915844e+00 -2.1491787e+00]\n",
      " ...\n",
      " [ 5.9607124e-01  1.0000000e+00  1.8143406e+00 ... -5.5460686e+01\n",
      "  -1.3044284e+02 -2.4676414e+02]\n",
      " [ 5.9547043e-01  1.0000000e+00  1.8146133e+00 ... -1.3986394e+02\n",
      "   2.7817648e+02 -2.8547052e+02]\n",
      " [ 5.9526348e-01  1.0000000e+00  1.8146248e+00 ... -7.6096295e+02\n",
      "   2.9581421e+02 -5.2149438e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.7210922e-03  1.0000000e+00  1.9097519e+00 ...  4.8626590e-01\n",
      "  -1.7583122e+00  7.4769449e-01]\n",
      " [ 6.1254501e-03  1.0000000e+00  1.9097395e+00 ... -7.2455696e+01\n",
      "  -8.1390419e+01  1.1619733e+02]\n",
      " [ 6.6223145e-03  1.0000000e+00  1.9097536e+00 ... -2.2001760e+01\n",
      "  -2.4562187e+00 -5.6352409e+01]\n",
      " ...\n",
      " [ 6.3552856e-03  1.0000000e+00  1.9097214e+00 ...  5.1537716e+01\n",
      "  -6.8706012e+02  6.2293994e+02]\n",
      " [ 5.7182312e-03  1.0000000e+00  1.9097748e+00 ...  1.8982323e+02\n",
      "  -4.0827282e+01 -1.5916476e+02]\n",
      " [ 5.5122375e-03  1.0000000e+00  1.9097500e+00 ...  1.2985278e+02\n",
      "  -1.9504355e+02  3.4578180e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:1, Score:1.11, Best Score:1.11, Average Score:1.11, Best Avg Score:1.11\n",
      "Episode number: 2\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c44a970>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1179094e+00  1.0000000e+00  1.5483418e+00 ...  1.6993527e+00\n",
      "   1.0588813e-01  3.4542018e-01]\n",
      " [-1.1175833e+00  1.0000000e+00  1.5485783e+00 ...  6.8077075e+02\n",
      "  -5.7837988e+02  5.9385266e+02]\n",
      " [-1.1171455e+00  1.0000000e+00  1.5488805e+00 ... -3.8713032e+01\n",
      "   3.0529722e+01  9.7385406e+00]\n",
      " ...\n",
      " [-1.1173439e+00  1.0000000e+00  1.5486822e+00 ...  5.0649432e+02\n",
      "   6.1036945e+02 -5.3159912e+02]\n",
      " [-1.1178570e+00  1.0000000e+00  1.5483551e+00 ... -1.9810391e+02\n",
      "  -1.4205453e+02  1.5060475e+02]\n",
      " [-1.1180792e+00  1.0000000e+00  1.5482235e+00 ...  2.9097314e+02\n",
      "   2.7647940e+02  1.7412177e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5417356    1.           1.1270161 ...    7.9192963   -0.617267\n",
      "     1.5112864]\n",
      " [  -1.5414982    1.           1.1273079 ...  -63.222397    -5.6188717\n",
      "    55.4806   ]\n",
      " [  -1.5411663    1.           1.1277283 ...  143.05933   -160.16864\n",
      "  -104.18538  ]\n",
      " ...\n",
      " [  -1.5412884    1.           1.1274614 ...   61.34194   -394.815\n",
      "  -105.62313  ]\n",
      " [  -1.5416622    1.           1.1270294 ...  -85.57006    118.29123\n",
      "   -48.421165 ]\n",
      " [  -1.5418558    1.           1.1268501 ...  128.70578   -431.98352\n",
      "   157.43163  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8144875e+00  1.0000000e+00  5.9546471e-01 ... -2.8719672e+02\n",
      "  -5.3307812e+02  3.7441254e+02]\n",
      " [-1.8143597e+00  1.0000000e+00  5.9580994e-01 ...  1.7116077e+02\n",
      "  -6.2639221e+02 -6.4035332e+01]\n",
      " [-1.8141823e+00  1.0000000e+00  5.9630358e-01 ... -1.5574274e+02\n",
      "  -6.6749748e+01 -1.3242136e+02]\n",
      " ...\n",
      " [-1.8142109e+00  1.0000000e+00  5.9599972e-01 ...  9.0392383e+02\n",
      "   2.5872964e+02 -2.9318567e+02]\n",
      " [-1.8143940e+00  1.0000000e+00  5.9547806e-01 ... -3.8225551e+00\n",
      "   7.3863701e+01  3.0950571e+01]\n",
      " [-1.8145514e+00  1.0000000e+00  5.9527016e-01 ...  8.5721169e+01\n",
      "  -1.8759773e+02 -5.1437976e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90964699e+00  1.00000000e+00  5.70869446e-03 ... -4.19563828e+01\n",
      "   5.23574829e+01 -5.38705864e+01]\n",
      " [-1.90964890e+00  1.00000000e+00  6.06441498e-03 ...  2.02205467e+01\n",
      "   2.06056881e+01  8.47465744e+01]\n",
      " [-1.90962791e+00  1.00000000e+00  6.57755835e-03 ...  1.25580025e+02\n",
      "   1.18919266e+02  7.44875793e+01]\n",
      " ...\n",
      " [-1.90957642e+00  1.00000000e+00  6.26945496e-03 ...  2.44720688e+02\n",
      "  -3.45191803e+02 -2.12500732e+02]\n",
      " [-1.90957260e+00  1.00000000e+00  5.72204590e-03 ... -1.35564972e+02\n",
      "   2.18676968e+01 -7.43815536e+01]\n",
      " [-1.90965176e+00  1.00000000e+00  5.49507141e-03 ...  2.47681107e+02\n",
      "   3.39927769e+00 -6.41937485e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.0000001  0.         1.0000001  0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8178549e+00  1.0000000e+00 -5.8487701e-01 ...  3.9264951e+02\n",
      "   5.4077099e+01  1.9244678e+02]\n",
      " [-1.8179712e+00  1.0000000e+00 -5.8454418e-01 ...  4.4987027e+02\n",
      "   5.1091281e+02  5.9903784e+02]\n",
      " [-1.8181229e+00  1.0000000e+00 -5.8406073e-01 ...  5.9995804e+01\n",
      "  -7.6814453e+01 -4.0404835e+01]\n",
      " ...\n",
      " [-1.8179893e+00  1.0000000e+00 -5.8435059e-01 ...  1.2569990e+01\n",
      "   5.6993732e+01  2.3026892e+01]\n",
      " [-1.8177719e+00  1.0000000e+00 -5.8486366e-01 ...  1.8610628e+02\n",
      "   1.2072277e+02 -2.5230641e+02]\n",
      " [-1.8177910e+00  1.0000000e+00 -5.8508110e-01 ... -6.7985176e+01\n",
      "  -4.8527519e+01 -3.8291163e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.3       0.\n",
      " 0.3       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5480003     1.           -1.1181278  ... -316.67453\n",
      "  -431.16052    -330.8355    ]\n",
      " [  -1.5482149     1.           -1.1178627  ...    0.85786533\n",
      "    77.35025       2.4864573 ]\n",
      " [  -1.5485115     1.           -1.1174475  ...  394.5307\n",
      "   155.65758     552.5914    ]\n",
      " ...\n",
      " [  -1.5483112     1.           -1.1176987  ...  -28.408028\n",
      "    72.86376     -55.206932  ]\n",
      " [  -1.5479298     1.           -1.1181145  ... -138.38953\n",
      "   -30.7233      -89.53486   ]\n",
      " [  -1.5478764     1.           -1.1183033  ... -271.85632\n",
      "  -146.90082     -92.89193   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1268091e+00  1.0000000e+00 -1.5416126e+00 ...  2.4226650e+01\n",
      "  -1.1603285e+02 -5.6305714e+01]\n",
      " [-1.1271114e+00  1.0000000e+00 -1.5414982e+00 ... -4.7269382e+00\n",
      "   3.9328365e+01  3.8824654e+01]\n",
      " [-1.1274986e+00  1.0000000e+00 -1.5411716e+00 ... -2.7743408e+01\n",
      "   6.0758812e+01 -4.7040329e+01]\n",
      " ...\n",
      " [-1.1272392e+00  1.0000000e+00 -1.5413866e+00 ... -1.7824989e+02\n",
      "  -1.7954309e+02  9.7347122e+01]\n",
      " [-1.1267414e+00  1.0000000e+00 -1.5415993e+00 ... -1.6769226e+02\n",
      "   2.3418026e+01  4.0360897e+01]\n",
      " [-1.1266375e+00  1.0000000e+00 -1.5417480e+00 ...  1.4539622e+03\n",
      "   7.1379626e+02  2.6917354e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9498024e-01  1.0000000e+00 -1.8144512e+00 ... -1.2768765e+02\n",
      "   1.5804578e+01  3.8990047e+01]\n",
      " [-5.9533310e-01  1.0000000e+00 -1.8144197e+00 ...  1.8519205e+01\n",
      "  -4.6061172e+01  3.9439442e+01]\n",
      " [-5.9578133e-01  1.0000000e+00 -1.8142298e+00 ...  2.8272311e+02\n",
      "   1.6806894e+02 -4.0162061e+02]\n",
      " ...\n",
      " [-5.9548378e-01  1.0000000e+00 -1.8143549e+00 ... -5.6013060e+00\n",
      "   9.1526833e+00  9.4092655e+00]\n",
      " [-5.9489250e-01  1.0000000e+00 -1.8144341e+00 ...  1.9647255e+01\n",
      "   6.8379402e+00  2.9799879e+01]\n",
      " [-5.9478855e-01  1.0000000e+00 -1.8145103e+00 ...  4.3000800e+02\n",
      "   1.1496408e+02  6.6300519e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.1441193e-03  1.0000000e+00 -1.9095020e+00 ...  1.0269000e+01\n",
      "   7.7225159e+01 -8.0945492e+00]\n",
      " [-5.5179596e-03  1.0000000e+00 -1.9095364e+00 ...  1.6849907e+01\n",
      "  -9.3716145e+00 -8.2188881e+01]\n",
      " [-5.9928894e-03  1.0000000e+00 -1.9095277e+00 ...  5.6846582e+02\n",
      "   3.7461555e+03  2.2494801e+02]\n",
      " ...\n",
      " [-5.6762695e-03  1.0000000e+00 -1.9095373e+00 ... -2.5797427e-01\n",
      "   3.3358874e+00  3.9159656e-02]\n",
      " [-5.0544739e-03  1.0000000e+00 -1.9094772e+00 ... -7.1092176e+00\n",
      "   6.2298943e+01 -3.3124872e+02]\n",
      " [-4.9409866e-03  1.0000000e+00 -1.9094982e+00 ... -5.8078976e+01\n",
      "   8.9793190e+01 -6.2588538e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5854664     1.           -1.8175163  ...   36.69387\n",
      "     4.355997     -9.002371  ]\n",
      " [   0.5851116     1.           -1.817668   ...  -67.97921\n",
      "   -97.394455    -56.691967  ]\n",
      " [   0.5846672     1.           -1.8178189  ...   -8.528138\n",
      "   -76.20477     -42.148926  ]\n",
      " ...\n",
      " [   0.58496094    1.           -1.8177338  ...  -15.210106\n",
      "    32.772606      0.25424588]\n",
      " [   0.58555794    1.           -1.8174839  ...    4.3419647\n",
      "    18.2352       70.35526   ]\n",
      " [   0.5856619     1.           -1.8174534  ...   95.434944\n",
      "  -215.86983      20.9648    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1185637e+00  1.0000000e+00 -1.5475521e+00 ...  9.2243843e+01\n",
      "   8.5317551e+01 -3.4788334e+01]\n",
      " [ 1.1182518e+00  1.0000000e+00 -1.5477953e+00 ...  2.7800751e+02\n",
      "   1.7011978e+02  1.4168565e+02]\n",
      " [ 1.1178818e+00  1.0000000e+00 -1.5480934e+00 ... -4.3973000e+01\n",
      "   3.8260175e+02 -4.7204794e+02]\n",
      " ...\n",
      " [ 1.1181240e+00  1.0000000e+00 -1.5479155e+00 ... -4.8160768e-01\n",
      "   2.7999678e+00  6.3536105e+00]\n",
      " [ 1.1186504e+00  1.0000000e+00 -1.5475121e+00 ...  3.5519665e+01\n",
      "  -9.6944250e+02 -1.6765131e+02]\n",
      " [ 1.1187286e+00  1.0000000e+00 -1.5474205e+00 ... -1.3321182e+03\n",
      "  -8.8210754e+01 -6.0021424e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5420876e+00  1.0000000e+00 -1.1261082e+00 ...  1.3375642e+01\n",
      "   3.2905022e+01 -2.1429243e+01]\n",
      " [ 1.5418501e+00  1.0000000e+00 -1.1264563e+00 ...  4.4213132e+02\n",
      "  -2.0218887e+03 -4.3434830e+02]\n",
      " [ 1.5415649e+00  1.0000000e+00 -1.1268519e+00 ... -1.4634921e+01\n",
      "   1.1302991e+02  1.7364807e+02]\n",
      " ...\n",
      " [ 1.5417690e+00  1.0000000e+00 -1.1266146e+00 ...  1.0872810e+00\n",
      "   1.3333647e+00 -7.4835634e-01]\n",
      " [ 1.5421848e+00  1.0000000e+00 -1.1260586e+00 ...  1.4662585e+02\n",
      "  -1.3268527e+02  3.7364099e+02]\n",
      " [ 1.5422134e+00  1.0000000e+00 -1.1259289e+00 ... -3.8474915e+01\n",
      "   4.2791710e+01  9.9067665e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8146400e+00  1.0000000e+00 -5.9436989e-01 ...  2.1976956e+02\n",
      "   4.7311679e+02 -3.6797205e+02]\n",
      " [ 1.8145084e+00  1.0000000e+00 -5.9476376e-01 ... -1.0459603e+02\n",
      "   3.8086868e+01 -1.7929901e+02]\n",
      " [ 1.8143845e+00  1.0000000e+00 -5.9524524e-01 ... -6.2810085e+01\n",
      "   1.8894496e+02  1.1391392e+02]\n",
      " ...\n",
      " [ 1.8144855e+00  1.0000000e+00 -5.9494781e-01 ... -4.6093144e+00\n",
      "   4.1773093e-01  3.6562064e+00]\n",
      " [ 1.8147373e+00  1.0000000e+00 -5.9431458e-01 ...  5.3653687e+01\n",
      "   1.5895842e+02  1.2912514e+02]\n",
      " [ 1.8147087e+00  1.0000000e+00 -5.9416771e-01 ... -1.2028234e+02\n",
      "   1.4920293e+02  4.4009132e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Upperarm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9095211e+00  1.0000000e+00 -4.6062469e-03 ... -5.2904949e+01\n",
      "  -9.8184080e+00 -3.8779678e+00]\n",
      " [ 1.9095030e+00  1.0000000e+00 -5.0096512e-03 ... -2.9004198e+01\n",
      "  -2.7716633e+01  7.1310478e+01]\n",
      " [ 1.9095535e+00  1.0000000e+00 -5.5132871e-03 ... -9.8222214e+01\n",
      "   1.6889688e+02  2.4833823e+02]\n",
      " ...\n",
      " [ 1.9095726e+00  1.0000000e+00 -5.2003860e-03 ...  2.7387867e+00\n",
      "   1.9000788e+00 -3.1392021e+00]\n",
      " [ 1.9096336e+00  1.0000000e+00 -4.5490265e-03 ... -1.0716714e+02\n",
      "  -1.2218905e+02 -1.6282310e+02]\n",
      " [ 1.9095240e+00  1.0000000e+00 -4.3945312e-03 ...  1.7120221e+03\n",
      "   6.6975543e+02 -8.9377722e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.1\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8174963e+00  1.0000000e+00  5.8581924e-01 ... -1.3190923e+02\n",
      "   6.0884827e+01  2.8417248e+01]\n",
      " [ 1.8175955e+00  1.0000000e+00  5.8543968e-01 ...  1.0394770e+02\n",
      "  -2.8411219e+01 -7.5925987e+01]\n",
      " [ 1.8177891e+00  1.0000000e+00  5.8495814e-01 ...  9.9030495e+00\n",
      "   7.0559593e+01 -7.4399689e+01]\n",
      " ...\n",
      " [ 1.8176937e+00  1.0000000e+00  5.8526230e-01 ... -2.3549943e+00\n",
      "  -3.7172902e-01  4.6026564e+00]\n",
      " [ 1.8175926e+00  1.0000000e+00  5.8587265e-01 ...  5.7209988e+01\n",
      "   5.8184616e+01  1.0455582e+01]\n",
      " [ 1.8174477e+00  1.0000000e+00  5.8601952e-01 ...  1.2192100e+03\n",
      "  -2.3906314e+02 -5.3565155e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.4       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.2       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5473461    1.           1.1190491 ...  103.41354    134.2345\n",
      "   -41.815468 ]\n",
      " [   1.5475416    1.           1.1187334 ...   44.04737   -116.47432\n",
      "   -98.53858  ]\n",
      " [   1.5479145    1.           1.1183238 ...  -73.62496      9.110924\n",
      "   166.53554  ]\n",
      " ...\n",
      " [   1.5477161    1.           1.1185722 ...    1.7790127    1.336222\n",
      "    -0.7061031]\n",
      " [   1.5473976    1.           1.1191025 ...  235.34154    270.1705\n",
      "   384.2303   ]\n",
      " [   1.5472326    1.           1.1192284 ...  -76.047935    46.43983\n",
      "   -97.24936  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.8000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1256065e+00  1.0000000e+00  1.5423794e+00 ... -2.3180573e+02\n",
      "  -6.6653864e+02 -5.1641132e+02]\n",
      " [ 1.1258955e+00  1.0000000e+00  1.5421820e+00 ...  4.9069214e-01\n",
      "  -2.2628376e+01  5.5285978e+00]\n",
      " [ 1.1263390e+00  1.0000000e+00  1.5418594e+00 ... -1.3897148e+02\n",
      "   8.2662277e+01  2.0874588e+02]\n",
      " ...\n",
      " [ 1.1260738e+00  1.0000000e+00  1.5420523e+00 ...  8.5852003e-01\n",
      "   1.3085129e+00  5.7042456e-01]\n",
      " [ 1.1256180e+00  1.0000000e+00  1.5424271e+00 ...  1.9941270e+02\n",
      "   1.3045763e+02  3.2972404e+01]\n",
      " [ 1.1254530e+00  1.0000000e+00  1.5425148e+00 ...  7.8205879e+01\n",
      "  -6.0789600e+01 -9.7603312e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.1\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5938959     1.            1.8147411  ...   28.088663\n",
      "    15.257097    181.26639   ]\n",
      " [   0.5942364     1.            1.8146534  ...    8.639195\n",
      "    12.023091     -0.48144794]\n",
      " [   0.5947666     1.            1.8144671  ...  326.9967\n",
      "  -187.96727     433.34802   ]\n",
      " ...\n",
      " [   0.5944538     1.            1.8145723  ...    2.1983223\n",
      "     1.613503      0.9185674 ]\n",
      " [   0.5939026     1.            1.8147755  ...   68.76339\n",
      "    31.312302    -99.63537   ]\n",
      " [   0.59370995    1.            1.8148308  ...    3.813043\n",
      "     3.531135    -36.75535   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.9215088e-03  1.0000000e+00  1.9094944e+00 ... -2.0091075e+02\n",
      "  -2.0930257e+02 -1.4046490e+02]\n",
      " [ 4.2829514e-03  1.0000000e+00  1.9095039e+00 ... -1.1148856e+02\n",
      "  -2.1944785e+02 -7.3600365e+01]\n",
      " [ 4.8713684e-03  1.0000000e+00  1.9094772e+00 ...  1.7181230e-01\n",
      "   9.6805328e+01 -1.3117183e+02]\n",
      " ...\n",
      " [ 4.5413971e-03  1.0000000e+00  1.9094944e+00 ...  1.1038315e+00\n",
      "   1.5072276e+01 -1.2647580e+00]\n",
      " [ 3.9615631e-03  1.0000000e+00  1.9095192e+00 ... -1.2791034e+02\n",
      "  -5.1956657e+01 -1.2044616e+02]\n",
      " [ 3.7221909e-03  1.0000000e+00  1.9095154e+00 ... -7.2356806e+00\n",
      "  -4.9910679e+01  3.5123676e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.8639050e-01  1.0000000e+00  1.8173180e+00 ... -2.6829281e+01\n",
      "   1.1365623e+02  7.0729836e+01]\n",
      " [-5.8603954e-01  1.0000000e+00  1.8173943e+00 ... -6.8421036e+01\n",
      "   4.8530602e+01  1.1936734e+02]\n",
      " [-5.8543396e-01  1.0000000e+00  1.8175414e+00 ... -1.5155103e+02\n",
      "   5.5838416e+02 -7.7692635e+01]\n",
      " ...\n",
      " [-5.8574867e-01  1.0000000e+00  1.8174572e+00 ... -3.3270180e-02\n",
      "   4.7778654e+00 -1.3065264e-01]\n",
      " [-5.8629608e-01  1.0000000e+00  1.8173313e+00 ... -8.8848976e+01\n",
      "  -5.4430566e+02  1.5620366e+02]\n",
      " [-5.8658695e-01  1.0000000e+00  1.8172646e+00 ... -9.4644958e+01\n",
      "   1.3490210e+02 -4.9454521e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1195173    1.           1.5470791 ... -432.82925    320.90808\n",
      "   -69.58026  ]\n",
      " [  -1.1192207    1.           1.5472221 ...   52.328854  -163.4162\n",
      "   100.21649  ]\n",
      " [  -1.118679     1.           1.5475287 ...   18.63124   -306.36853\n",
      "   424.27402  ]\n",
      " ...\n",
      " [  -1.118948     1.           1.5473452 ...   -1.3844795    3.774535\n",
      "     2.0293498]\n",
      " [  -1.1194267    1.           1.5470886 ...   55.70304   -104.82296\n",
      "   245.95576  ]\n",
      " [  -1.1196852    1.           1.5469627 ...   40.15592    -57.244476\n",
      "   -19.037498 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5429554    1.           1.1252728 ... -104.97881    124.67772\n",
      "   207.73152  ]\n",
      " [  -1.5427418    1.           1.125495  ...  118.79288     46.785503\n",
      "   -52.741863 ]\n",
      " [  -1.5422993    1.           1.1259162 ...   28.564823  -106.37014\n",
      "   -39.772682 ]\n",
      " ...\n",
      " [  -1.5424976    1.           1.1256704 ...   -5.6011963   11.543806\n",
      "    -1.5137599]\n",
      " [  -1.5428524    1.           1.1252842 ...   42.661953    11.54241\n",
      "    -1.344964 ]\n",
      " [  -1.5430737    1.           1.1250801 ...  237.58232   -120.66948\n",
      "    52.832146 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81492615e+00  1.00000000e+00  5.93523026e-01 ... -8.57992096e+01\n",
      "  -1.11810356e+02 -3.81492615e+01]\n",
      " [-1.81480312e+00  1.00000000e+00  5.93791008e-01 ... -7.23650208e+01\n",
      "  -1.43679905e+01 -7.60251951e+00]\n",
      " [-1.81454086e+00  1.00000000e+00  5.94290793e-01 ...  2.78413062e+03\n",
      "  -1.85560022e+03  3.68422144e+03]\n",
      " ...\n",
      " [-1.81463814e+00  1.00000000e+00  5.94017982e-01 ...  7.36733437e-01\n",
      "  -1.48787403e+00 -3.93238887e-02]\n",
      " [-1.81482887e+00  1.00000000e+00  5.93536377e-01 ...  2.78807621e+01\n",
      "   3.27707291e+00 -7.64942169e+01]\n",
      " [-1.81498146e+00  1.00000000e+00  5.93303680e-01 ... -1.07693848e+03\n",
      "   1.31295264e+03 -3.26141754e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9095335e+00  1.0000000e+00  3.4179688e-03 ...  8.1882530e+01\n",
      "   6.1472992e+01 -6.2144779e+01]\n",
      " [-1.9095392e+00  1.0000000e+00  3.6926270e-03 ...  1.6048429e+01\n",
      "   9.4808702e+00  5.4030914e+01]\n",
      " [-1.9094028e+00  1.0000000e+00  4.2325072e-03 ...  5.3742334e+02\n",
      "   1.1121207e+03 -4.0855930e+02]\n",
      " ...\n",
      " [-1.9094143e+00  1.0000000e+00  3.9396286e-03 ... -3.4357266e+00\n",
      "   6.9423132e+00  7.9332751e-01]\n",
      " [-1.9094257e+00  1.0000000e+00  3.4313202e-03 ... -3.6006626e+01\n",
      "   7.2375000e+01  3.8503485e+00]\n",
      " [-1.9095078e+00  1.0000000e+00  3.1833649e-03 ...  1.8756893e+01\n",
      "  -5.6050944e-01  8.0394726e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8170261e+00  1.0000000e+00 -5.8713722e-01 ... -9.7771019e+01\n",
      "  -2.3243547e+02  6.9321983e+01]\n",
      " [-1.8171282e+00  1.0000000e+00 -5.8687210e-01 ... -7.5510994e+01\n",
      "   2.6398426e+01 -8.6552277e+01]\n",
      " [-1.8171749e+00  1.0000000e+00 -5.8635628e-01 ...  9.2102515e+02\n",
      "  -4.2021489e+03 -2.1254775e+03]\n",
      " ...\n",
      " [-1.8171062e+00  1.0000000e+00 -5.8663845e-01 ...  3.3350620e+00\n",
      "  -4.5981069e+00 -2.8092176e-01]\n",
      " [-1.8169765e+00  1.0000000e+00 -5.8712578e-01 ...  1.8066470e+03\n",
      "   5.8103210e+02  1.1824764e+03]\n",
      " [-1.8169346e+00  1.0000000e+00 -5.8735847e-01 ... -5.4400490e+01\n",
      "   1.1611271e+02  2.2175853e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5467653e+00  1.0000000e+00 -1.1198254e+00 ...  9.6762398e+01\n",
      "   8.4773232e+01  6.5737221e+01]\n",
      " [-1.5469494e+00  1.0000000e+00 -1.1196251e+00 ...  1.9043050e+00\n",
      "  -1.8587770e+00  3.1919546e+01]\n",
      " [-1.5471344e+00  1.0000000e+00 -1.1191854e+00 ...  1.9995453e+03\n",
      "  -8.3889514e+02  3.2587512e+03]\n",
      " ...\n",
      " [-1.5469570e+00  1.0000000e+00 -1.1194277e+00 ...  2.5386593e+00\n",
      "  -4.7531009e+00 -9.2978859e-01]\n",
      " [-1.5466652e+00  1.0000000e+00 -1.1198196e+00 ...  5.2595825e+02\n",
      "  -2.6880182e+02 -3.4373819e+02]\n",
      " [-1.5466280e+00  1.0000000e+00 -1.1200104e+00 ... -8.4965797e+01\n",
      "  -1.1348967e+01  1.0088655e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1248884    1.          -1.5430946 ...   -6.803295  -102.855225\n",
      "     9.747559 ]\n",
      " [  -1.1251478    1.          -1.5429621 ...  175.7105     272.97186\n",
      "   150.67966  ]\n",
      " [  -1.1254597    1.          -1.5426406 ... -231.87039   -176.94492\n",
      "   -22.16337  ]\n",
      " ...\n",
      " [  -1.1252232    1.          -1.5428095 ...    2.2031238   -6.971492\n",
      "    -1.6508338]\n",
      " [  -1.1248341    1.          -1.5430889 ...  -54.12037     25.53601\n",
      "   -18.915752 ]\n",
      " [  -1.1247206    1.          -1.543232  ...   62.947605   194.18056\n",
      "  -167.13489  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9270477e-01  1.0000000e+00 -1.8152790e+00 ... -2.1700581e+02\n",
      "   8.0800343e+00  8.4227783e+01]\n",
      " [-5.9300327e-01  1.0000000e+00 -1.8152475e+00 ... -4.4656778e+02\n",
      "   2.3181752e+02  1.2639809e+02]\n",
      " [-5.9342003e-01  1.0000000e+00 -1.8150775e+00 ... -7.7212695e+02\n",
      "  -7.4656090e+01  1.0580393e+03]\n",
      " ...\n",
      " [-5.9313774e-01  1.0000000e+00 -1.8151693e+00 ...  2.2806454e-01\n",
      "  -1.3856893e+00 -1.3069040e-01]\n",
      " [-5.9266663e-01  1.0000000e+00 -1.8152714e+00 ...  2.5387950e+02\n",
      "   5.4112970e+02  7.9273541e+02]\n",
      " [-5.9250164e-01  1.0000000e+00 -1.8153572e+00 ... -4.6474225e+02\n",
      "   2.2065044e+03  6.1088477e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.8371811e-03  1.0000000e+00 -1.9095993e+00 ... -2.5575687e+02\n",
      "  -1.1748805e+02 -5.8830943e+00]\n",
      " [-3.1490326e-03  1.0000000e+00 -1.9096508e+00 ... -5.6988922e+02\n",
      "  -1.8982681e+02  5.8995281e+01]\n",
      " [-3.5648346e-03  1.0000000e+00 -1.9096407e+00 ... -4.2165567e+02\n",
      "  -8.6137231e+02 -5.5161518e+01]\n",
      " ...\n",
      " [-3.2711029e-03  1.0000000e+00 -1.9096394e+00 ... -8.8217497e-01\n",
      "   7.5731897e-01 -8.3358154e-02]\n",
      " [-2.7790070e-03  1.0000000e+00 -1.9095840e+00 ...  1.0448574e+02\n",
      "  -5.7272762e+01 -3.5395329e+01]\n",
      " [-2.6168823e-03  1.0000000e+00 -1.9095917e+00 ... -1.4171609e+01\n",
      "   1.3764771e+01  1.7673302e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.8739853e-01  1.0000000e+00 -1.8170013e+00 ...  6.0393005e+01\n",
      "   2.7511702e+01 -4.0943691e+01]\n",
      " [ 5.8709812e-01  1.0000000e+00 -1.8171177e+00 ...  1.3513329e+02\n",
      "  -2.0036020e+02  4.8085870e+02]\n",
      " [ 5.8670425e-01  1.0000000e+00 -1.8172638e+00 ...  1.6112630e+02\n",
      "  -9.4473030e+01 -6.4199298e+02]\n",
      " ...\n",
      " [ 5.8697701e-01  1.0000000e+00 -1.8171587e+00 ...  8.7271106e-01\n",
      "  -4.5356464e-01 -1.2024646e-01]\n",
      " [ 5.8744621e-01  1.0000000e+00 -1.8169765e+00 ... -2.9165344e+00\n",
      "   1.2436393e+02  4.2869617e+02]\n",
      " [ 5.8760738e-01  1.0000000e+00 -1.8169041e+00 ... -2.4960629e+01\n",
      "   8.8895294e+01 -2.2481262e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1201715e+00  1.0000000e+00 -1.5465260e+00 ...  5.6167572e+02\n",
      "  -4.5849188e+02 -7.8448596e+02]\n",
      " [ 1.1199121e+00  1.0000000e+00 -1.5467701e+00 ...  2.1439174e+02\n",
      "   3.2296652e+02 -4.7132198e+01]\n",
      " [ 1.1195316e+00  1.0000000e+00 -1.5470393e+00 ...  1.1276489e+02\n",
      "  -1.5008090e+02  2.8979353e+01]\n",
      " ...\n",
      " [ 1.1197529e+00  1.0000000e+00 -1.5468655e+00 ...  1.5199542e-02\n",
      "  -3.1196928e-01 -2.1981597e-02]\n",
      " [ 1.1201668e+00  1.0000000e+00 -1.5464993e+00 ... -5.4385334e+01\n",
      "  -9.8770615e+01 -8.4035149e+01]\n",
      " [ 1.1203489e+00  1.0000000e+00 -1.5463753e+00 ... -2.6114334e+01\n",
      "  -3.0206860e+02  1.1523742e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.54334831e+00  1.00000000e+00 -1.12454033e+00 ...  1.23609497e+02\n",
      "   3.54198074e+01 -1.07096436e+02]\n",
      " [ 1.54316235e+00  1.00000000e+00 -1.12489414e+00 ...  8.96352783e+02\n",
      "   2.98841461e+02  1.70441772e+02]\n",
      " [ 1.54286766e+00  1.00000000e+00 -1.12526250e+00 ... -1.84886795e+02\n",
      "  -3.25102936e+02  9.19509964e+01]\n",
      " ...\n",
      " [ 1.54302025e+00  1.00000000e+00 -1.12502193e+00 ... -7.27320790e-01\n",
      "   6.28006935e+00 -5.79903364e-01]\n",
      " [ 1.54331970e+00  1.00000000e+00 -1.12450600e+00 ... -2.82471905e+01\n",
      "   1.02868469e+02  5.35405388e+01]\n",
      " [ 1.54347324e+00  1.00000000e+00 -1.12435150e+00 ... -8.93821793e+01\n",
      "   1.53911102e+02  5.35787048e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81544590e+00  1.00000000e+00 -5.92287064e-01 ...  9.66991806e+01\n",
      "   2.14183090e+02  1.13254967e+02]\n",
      " [ 1.81534290e+00  1.00000000e+00 -5.92750549e-01 ...  1.54037704e+01\n",
      "   4.37005524e+02  3.07001404e+02]\n",
      " [ 1.81517410e+00  1.00000000e+00 -5.93163848e-01 ...  1.44839111e+02\n",
      "  -1.48741882e+03  1.68978369e+03]\n",
      " ...\n",
      " [ 1.81524658e+00  1.00000000e+00 -5.92896461e-01 ... -1.52897966e+00\n",
      "  -1.40624418e+01 -1.13954544e-01]\n",
      " [ 1.81537247e+00  1.00000000e+00 -5.92248917e-01 ...  2.11823486e+02\n",
      "   4.54463501e+02 -3.50882935e+02]\n",
      " [ 1.81550217e+00  1.00000000e+00 -5.92069626e-01 ...  3.75462151e+01\n",
      "   2.21717701e+01 -1.77031193e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.1       1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9096365e+00  1.0000000e+00 -2.2563934e-03 ...  2.9179880e+01\n",
      "  -3.2668743e+01  7.9355779e+00]\n",
      " [ 1.9096394e+00  1.0000000e+00 -2.6855469e-03 ...  3.1863955e+01\n",
      "   1.4065695e+01 -6.4774523e+00]\n",
      " [ 1.9096260e+00  1.0000000e+00 -3.1222939e-03 ...  6.2215668e+02\n",
      "   3.8272784e+02  5.9424567e+02]\n",
      " ...\n",
      " [ 1.9096146e+00  1.0000000e+00 -2.8438568e-03 ...  1.2294292e-02\n",
      "  -6.6439295e+00  4.1934702e-01]\n",
      " [ 1.9095612e+00  1.0000000e+00 -2.2163391e-03 ... -9.5084579e+01\n",
      "   6.3035103e+01 -3.7669994e+01]\n",
      " [ 1.9096327e+00  1.0000000e+00 -2.0294189e-03 ... -1.4570773e+02\n",
      "  -1.2169526e+02 -1.4718071e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.3       0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8169107e+00  1.0000000e+00  5.8786774e-01 ... -3.5092594e+01\n",
      "   1.2397840e+02  3.0428217e+02]\n",
      " [ 1.8170328e+00  1.0000000e+00  5.8745956e-01 ... -1.2178085e+02\n",
      "   2.1666032e+02  5.5775499e+00]\n",
      " [ 1.8171864e+00  1.0000000e+00  5.8704054e-01 ...  5.5344940e+02\n",
      "  -7.3042432e+02  1.4039717e+01]\n",
      " ...\n",
      " [ 1.8170719e+00  1.0000000e+00  5.8731079e-01 ... -9.9029970e-01\n",
      "  -7.4973798e+00  5.7827669e-01]\n",
      " [ 1.8168316e+00  1.0000000e+00  5.8790588e-01 ... -1.7644708e+02\n",
      "  -4.7551743e+02  6.6661365e+02]\n",
      " [ 1.8168364e+00  1.0000000e+00  5.8808517e-01 ...  1.1249782e+02\n",
      "   2.7446175e+01  7.6689194e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5461760e+00  1.0000000e+00  1.1208019e+00 ... -1.2794097e+02\n",
      "   8.7521111e+01  2.5108318e+00]\n",
      " [ 1.5464077e+00  1.0000000e+00  1.1204462e+00 ...  6.0616730e+02\n",
      "   5.5760492e+02 -4.7462448e+01]\n",
      " [ 1.5466900e+00  1.0000000e+00  1.1200832e+00 ... -2.9236200e+02\n",
      "  -4.4784294e+01  1.6994455e+01]\n",
      " ...\n",
      " [ 1.5465088e+00  1.0000000e+00  1.1203184e+00 ...  3.3691263e-01\n",
      "   1.0012856e+00 -3.8614869e-02]\n",
      " [ 1.5460815e+00  1.0000000e+00  1.1208363e+00 ... -1.6862917e+02\n",
      "  -6.2519885e+02  1.3533975e+02]\n",
      " [ 1.5460281e+00  1.0000000e+00  1.1209869e+00 ... -1.6755641e+02\n",
      "   1.2074750e+02 -4.5032669e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.6        0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1243811     1.            1.5435429  ... -162.70023\n",
      "   242.26248     178.39226   ]\n",
      " [   1.1246891     1.            1.5432768  ...  262.20996\n",
      "  -116.1132     -185.38121   ]\n",
      " [   1.1250896     1.            1.5430053  ...  125.61672\n",
      "   -28.505283    -20.765463  ]\n",
      " ...\n",
      " [   1.1248474     1.            1.5431881  ...   -1.0076035\n",
      "     1.3365712     0.71773064]\n",
      " [   1.1242809     1.            1.5435638  ...   16.325436\n",
      "    -2.7886314    44.39145   ]\n",
      " [   1.1241808     1.            1.5436707  ...  -34.988495\n",
      "    34.377316    -26.136534  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.9000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9214973e-01  1.0000000e+00  1.8154659e+00 ...  3.3984167e+03\n",
      "  -7.9482043e+02 -4.4955391e+03]\n",
      " [ 5.9250450e-01  1.0000000e+00  1.8153543e+00 ...  5.5726271e+00\n",
      "  -3.2069336e+01 -2.1397312e+01]\n",
      " [ 5.9297180e-01  1.0000000e+00  1.8152145e+00 ...  1.7066067e+03\n",
      "   6.5035968e+02  1.1533898e+03]\n",
      " ...\n",
      " [ 5.9269142e-01  1.0000000e+00  1.8153095e+00 ... -1.3755447e+00\n",
      "   7.7757339e+00 -1.3409734e-02]\n",
      " [ 5.9201813e-01  1.0000000e+00  1.8154640e+00 ... -2.6131491e+02\n",
      "   8.5269646e+01 -1.8988635e+02]\n",
      " [ 5.9191704e-01  1.0000000e+00  1.8155155e+00 ...  6.1282791e+01\n",
      "   5.3381359e+01  5.5815720e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7890930e-03  1.0000000e+00  1.9095459e+00 ... -1.4164705e+02\n",
      "   2.5636676e+02  4.0249692e+02]\n",
      " [ 2.1638870e-03  1.0000000e+00  1.9095716e+00 ...  1.4786937e+02\n",
      "  -2.5915335e+01 -1.3340025e+02]\n",
      " [ 2.6378632e-03  1.0000000e+00  1.9095688e+00 ... -1.0072727e+04\n",
      "  -3.2667876e+03 -2.1571119e+04]\n",
      " ...\n",
      " [ 2.3460388e-03  1.0000000e+00  1.9095764e+00 ... -2.4415495e+00\n",
      "   7.9253550e+00 -1.0456042e+00]\n",
      " [ 1.6460419e-03  1.0000000e+00  1.9095249e+00 ... -1.5632765e+04\n",
      "  -6.9027508e+04 -3.5530328e+04]\n",
      " [ 1.5449524e-03  1.0000000e+00  1.9095192e+00 ... -3.2238870e+02\n",
      "   6.3025525e+02 -3.1802288e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.8818722e-01  1.0000000e+00  1.8166618e+00 ...  7.7859367e+01\n",
      "  -4.8225665e+02 -3.1223468e+02]\n",
      " [-5.8782959e-01  1.0000000e+00  1.8167791e+00 ...  8.2986893e+01\n",
      "   2.8536661e+02  2.4631334e+02]\n",
      " [-5.8739662e-01  1.0000000e+00  1.8169378e+00 ...  3.9824795e+03\n",
      "  -1.9446748e+03 -7.1181494e+03]\n",
      " ...\n",
      " [-5.8766937e-01  1.0000000e+00  1.8168411e+00 ...  4.3135643e-02\n",
      "  -1.5024781e+00  1.5892100e-01]\n",
      " [-5.8833694e-01  1.0000000e+00  1.8166142e+00 ...  1.3374336e+04\n",
      "   6.0621313e+03  4.4603560e+03]\n",
      " [-5.8841228e-01  1.0000000e+00  1.8165455e+00 ...  1.6838097e+01\n",
      "  -8.0182358e+01 -2.3340657e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1210585e+00  1.0000000e+00  1.5455761e+00 ...  2.0946942e+02\n",
      "   1.3692345e+02  1.2740052e+02]\n",
      " [-1.1207619e+00  1.0000000e+00  1.5457983e+00 ... -5.4085350e+01\n",
      "   6.0604568e+01  1.4807910e+01]\n",
      " [-1.1204281e+00  1.0000000e+00  1.5460888e+00 ...  8.3908179e+02\n",
      "  -9.8246100e+02  1.5989861e+02]\n",
      " ...\n",
      " [-1.1206760e+00  1.0000000e+00  1.5459118e+00 ... -1.9058502e-01\n",
      "   2.6864767e+00 -4.0934175e-01]\n",
      " [-1.1212368e+00  1.0000000e+00  1.5455074e+00 ...  1.9238055e+02\n",
      "   7.7602534e+03  5.1266807e+03]\n",
      " [-1.1212521e+00  1.0000000e+00  1.5454025e+00 ...  3.0937567e+02\n",
      "  -1.3943651e+02 -4.8942828e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.54373837e+00  1.00000000e+00  1.12365341e+00 ... -7.95170746e+01\n",
      "  -4.15680923e+01 -8.30185623e+01]\n",
      " [-1.54352665e+00  1.00000000e+00  1.12398243e+00 ...  1.16336426e+02\n",
      "  -5.05385666e+01 -3.23735809e+01]\n",
      " [-1.54328156e+00  1.00000000e+00  1.12435675e+00 ... -1.48304886e+02\n",
      "   2.57691422e+01 -9.58744446e+02]\n",
      " ...\n",
      " [-1.54346657e+00  1.00000000e+00  1.12413692e+00 ...  9.83855546e-01\n",
      "   2.83256912e+00 -5.89818358e-01]\n",
      " [-1.54385757e+00  1.00000000e+00  1.12356377e+00 ... -4.90468359e+03\n",
      "   3.26020581e+03  9.65705078e+03]\n",
      " [-1.54387283e+00  1.00000000e+00  1.12344551e+00 ...  1.58609700e+00\n",
      "   2.07464371e+01 -6.76392441e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8155899e+00  1.0000000e+00  5.9123611e-01 ...  1.6707157e+01\n",
      "   2.5655842e+01 -5.0155071e+01]\n",
      " [-1.8154879e+00  1.0000000e+00  5.9163189e-01 ...  7.2198413e+02\n",
      "  -5.6593414e+02 -6.0312543e+02]\n",
      " [-1.8153706e+00  1.0000000e+00  5.9206599e-01 ...  2.2064069e+02\n",
      "   2.9609194e+01 -3.0595703e+01]\n",
      " ...\n",
      " [-1.8154812e+00  1.0000000e+00  5.9181309e-01 ... -2.2150025e-01\n",
      "   8.8126583e+00  3.5237420e-01]\n",
      " [-1.8157005e+00  1.0000000e+00  5.9111977e-01 ...  5.6359297e+03\n",
      "  -5.3743027e+03  3.1707859e+03]\n",
      " [-1.8156586e+00  1.0000000e+00  5.9099007e-01 ... -1.8514055e+01\n",
      "   3.2502369e+01  1.5634343e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90949059e+00  1.00000000e+00  1.01852417e-03 ... -2.45946598e+01\n",
      "  -1.26892136e+02  9.50415268e+01]\n",
      " [-1.90951347e+00  1.00000000e+00  1.44863129e-03 ... -1.46892380e+02\n",
      "  -4.03018532e+01 -9.62675934e+01]\n",
      " [-1.90953636e+00  1.00000000e+00  1.89985661e-03 ... -5.06517365e+02\n",
      "  -7.09953125e+02  6.14250916e+02]\n",
      " ...\n",
      " [-1.90954971e+00  1.00000000e+00  1.64031982e-03 ... -1.07820243e-01\n",
      "  -1.75912809e+00  1.15210742e-01]\n",
      " [-1.90955162e+00  1.00000000e+00  8.92639160e-04 ...  4.58736420e+01\n",
      "   5.10060577e+01  1.81954056e+02]\n",
      " [-1.90948200e+00  1.00000000e+00  7.57217407e-04 ... -2.00425079e+02\n",
      "   2.73991516e+02  9.94672165e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8164778e+00  1.0000000e+00 -5.8894157e-01 ...  5.0006893e+01\n",
      "  -8.3564911e+01 -3.2344727e+01]\n",
      " [-1.8166142e+00  1.0000000e+00 -5.8854103e-01 ... -2.2695155e+02\n",
      "   4.2725565e+02  1.8487709e+02]\n",
      " [-1.8167381e+00  1.0000000e+00 -5.8810484e-01 ...  1.5093046e+03\n",
      "  -1.6183884e+03  1.1902102e+03]\n",
      " ...\n",
      " [-1.8166676e+00  1.0000000e+00 -5.8835316e-01 ...  3.5222435e-01\n",
      "   1.3238977e+01 -2.9664487e-01]\n",
      " [-1.8164845e+00  1.0000000e+00 -5.8905792e-01 ...  3.1462198e+02\n",
      "  -1.4340389e+03  2.1712878e+03]\n",
      " [-1.8163967e+00  1.0000000e+00 -5.8918571e-01 ...  4.1431652e+01\n",
      "  -6.7542961e+01 -2.5662544e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.54529476e+00  1.00000000e+00 -1.12165070e+00 ...  1.04697609e+02\n",
      "  -1.30939178e+02 -3.13486115e+02]\n",
      " [-1.54553604e+00  1.00000000e+00 -1.12131023e+00 ... -3.60553093e+01\n",
      "   2.46028728e+01 -1.36685181e+02]\n",
      " [-1.54578209e+00  1.00000000e+00 -1.12093592e+00 ...  1.48185596e+03\n",
      "  -8.67921143e+01  1.17562732e+03]\n",
      " ...\n",
      " [-1.54563713e+00  1.00000000e+00 -1.12114620e+00 ... -2.26479554e+00\n",
      "  -1.12747507e+01  2.54716396e-01]\n",
      " [-1.54527283e+00  1.00000000e+00 -1.12174797e+00 ... -1.30874420e+02\n",
      "   2.44182068e+02 -1.00784706e+02]\n",
      " [-1.54514122e+00  1.00000000e+00 -1.12186050e+00 ...  2.18716431e+02\n",
      "  -1.78718857e+02 -1.26135010e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.123148     1.          -1.5442867 ... -230.7436    -435.8503\n",
      "   221.624    ]\n",
      " [  -1.1234865    1.          -1.5440283 ...   29.508291   -30.994774\n",
      "   -53.657093 ]\n",
      " [  -1.1238499    1.          -1.5437491 ...  -68.73501      8.510515\n",
      "   197.8154   ]\n",
      " ...\n",
      " [  -1.1236324    1.          -1.5438948 ...   -1.2532935   -9.484709\n",
      "     0.824842 ]\n",
      " [  -1.1230831    1.          -1.544342  ...   -3.8213687  -52.74683\n",
      "  -255.27534  ]\n",
      " [  -1.1229305    1.          -1.544426  ... -121.30506     62.647003\n",
      "  -102.36871  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9076309e-01  1.0000000e+00 -1.8157406e+00 ...  5.1585571e+02\n",
      "  -2.2225414e+02 -7.1920013e+00]\n",
      " [-5.9115887e-01  1.0000000e+00 -1.8155832e+00 ...  2.5515881e+01\n",
      "   4.4894135e+01 -7.2143547e+01]\n",
      " [-5.9156990e-01  1.0000000e+00 -1.8154238e+00 ...  2.0751762e+02\n",
      "  -6.1218095e+00 -2.2041609e+02]\n",
      " ...\n",
      " [-5.9132004e-01  1.0000000e+00 -1.8154974e+00 ... -8.9720917e-01\n",
      "  -3.5722566e+00  2.8627628e-01]\n",
      " [-5.9065056e-01  1.0000000e+00 -1.8157539e+00 ... -2.4024908e+02\n",
      "  -5.0548180e+01 -1.6232024e+02]\n",
      " [-5.9051514e-01  1.0000000e+00 -1.8158016e+00 ... -5.1014328e+02\n",
      "   7.7081779e+01  8.1662506e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.9768219e-04  1.0000000e+00 -1.9094887e+00 ... -1.3365800e+02\n",
      "   1.4290376e+02  6.4813919e+01]\n",
      " [-8.1253052e-04  1.0000000e+00 -1.9094715e+00 ... -4.8421200e+01\n",
      "   9.3757240e+01  1.1839482e+02]\n",
      " [-1.2397766e-03  1.0000000e+00 -1.9094452e+00 ... -2.0140195e+00\n",
      "  -1.2209143e+01  4.6235142e+01]\n",
      " ...\n",
      " [-9.8228455e-04  1.0000000e+00 -1.9094334e+00 ...  4.9592549e-01\n",
      "   3.2593234e+00 -2.0930043e-01]\n",
      " [-2.8800964e-04  1.0000000e+00 -1.9094791e+00 ... -5.0866428e+02\n",
      "  -4.1901691e+02  3.2510307e+01]\n",
      " [-1.3446808e-04  1.0000000e+00 -1.9094887e+00 ...  3.5018311e+02\n",
      "  -1.1705659e+02 -6.3994171e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.89904785e-01  1.00000000e+00 -1.81613159e+00 ...  1.44832812e+04\n",
      "  -1.03908340e+04 -5.38059912e+03]\n",
      " [ 5.89506149e-01  1.00000000e+00 -1.81621647e+00 ... -1.12945343e+02\n",
      "   8.81437378e+01  1.45399979e+02]\n",
      " [ 5.89084625e-01  1.00000000e+00 -1.81634009e+00 ... -1.06582275e+02\n",
      "   3.86647186e+01 -1.00695930e+02]\n",
      " ...\n",
      " [ 5.89334488e-01  1.00000000e+00 -1.81624126e+00 ... -4.43500102e-01\n",
      "   6.00308943e+00  1.14933038e+00]\n",
      " [ 5.89979172e-01  1.00000000e+00 -1.81608200e+00 ... -3.15619904e+02\n",
      "  -1.05550378e+03 -5.64836975e+02]\n",
      " [ 5.90152740e-01  1.00000000e+00 -1.81603622e+00 ... -1.64479553e+02\n",
      "  -4.06874504e+01 -5.05023422e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1223011e+00  1.0000000e+00 -1.5449772e+00 ... -4.2412115e+02\n",
      "   3.4439148e+02  1.3843294e+02]\n",
      " [ 1.1219530e+00  1.0000000e+00 -1.5452099e+00 ... -1.5294480e+01\n",
      "   1.7531998e+01  6.0780212e+01]\n",
      " [ 1.1216145e+00  1.0000000e+00 -1.5454470e+00 ...  7.3712624e+01\n",
      "  -7.0264580e+01 -9.2655739e+01]\n",
      " ...\n",
      " [ 1.1218357e+00  1.0000000e+00 -1.5452843e+00 ... -9.4333887e-03\n",
      "   2.5798779e+00 -1.0351737e+00]\n",
      " [ 1.1223755e+00  1.0000000e+00 -1.5449047e+00 ... -5.4929646e+01\n",
      "  -4.8100544e+01  1.6316721e+02]\n",
      " [ 1.1225023e+00  1.0000000e+00 -1.5448036e+00 ... -1.5314278e+01\n",
      "   3.4805168e+01  1.1446078e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.54477882e+00  1.00000000e+00 -1.12264442e+00 ...  1.91786102e+02\n",
      "  -1.40782257e+02 -8.99836182e+02]\n",
      " [ 1.54451847e+00  1.00000000e+00 -1.12303066e+00 ... -1.51813431e+02\n",
      "   8.69487305e+01  2.67378906e+02]\n",
      " [ 1.54428101e+00  1.00000000e+00 -1.12334871e+00 ... -3.34578061e+00\n",
      "  -1.80866642e+01  3.22991848e+00]\n",
      " ...\n",
      " [ 1.54443550e+00  1.00000000e+00 -1.12315178e+00 ...  1.27816260e+00\n",
      "  -1.09683075e+01 -3.31445456e-01]\n",
      " [ 1.54479980e+00  1.00000000e+00 -1.12255096e+00 ...  8.81878662e+02\n",
      "   5.54917542e+02  3.22307281e+02]\n",
      " [ 1.54492378e+00  1.00000000e+00 -1.12241554e+00 ... -1.63867474e+01\n",
      "  -6.00985813e+00  1.47663994e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81618786e+00  1.00000000e+00 -5.90190887e-01 ...  3.57200073e+02\n",
      "  -1.25128105e+02 -4.75546143e+02]\n",
      " [ 1.81604195e+00  1.00000000e+00 -5.90620995e-01 ...  1.54871155e+02\n",
      "  -8.92274551e+01  7.86055679e+01]\n",
      " [ 1.81591988e+00  1.00000000e+00 -5.91008186e-01 ...  2.79564087e+02\n",
      "  -2.79224030e+02  9.95510071e+02]\n",
      " ...\n",
      " [ 1.81600571e+00  1.00000000e+00 -5.90767860e-01 ... -5.45725286e-01\n",
      "  -6.59714222e+00 -2.33909816e-01]\n",
      " [ 1.81616974e+00  1.00000000e+00 -5.90084076e-01 ...  1.09272095e+03\n",
      "   6.91447083e+02  7.22429138e+02]\n",
      " [ 1.81625938e+00  1.00000000e+00 -5.89929581e-01 ...  3.13333988e+01\n",
      "   1.47994385e+02 -9.76478424e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9097548e+00  1.0000000e+00  4.5776367e-05 ...  4.7805841e+02\n",
      "   3.3457547e+02 -1.1161740e+02]\n",
      " [ 1.9097204e+00  1.0000000e+00 -4.6157837e-04 ... -4.8205101e+01\n",
      "   1.8257175e+02  4.4917560e-01]\n",
      " [ 1.9097137e+00  1.0000000e+00 -8.4872206e-04 ...  1.0926645e+02\n",
      "   1.1159239e+00 -8.5671661e+01]\n",
      " ...\n",
      " [ 1.9097195e+00  1.0000000e+00 -6.1893463e-04 ...  7.5116217e-01\n",
      "  -1.1362045e+01 -9.0959460e-01]\n",
      " [ 1.9096680e+00  1.0000000e+00  1.5449524e-04 ...  1.1854844e+02\n",
      "   1.5213228e+02  3.3137460e+02]\n",
      " [ 1.9097414e+00  1.0000000e+00  3.0899048e-04 ... -4.8396633e+01\n",
      "  -2.7839687e+01 -4.8463001e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8162756e+00  1.0000000e+00  5.9023476e-01 ...  3.4882538e+02\n",
      "   2.7137466e+01 -2.6478293e+02]\n",
      " [ 1.8163815e+00  1.0000000e+00  5.8976078e-01 ... -2.7678010e+00\n",
      "  -4.7987738e+00 -9.0483856e-01]\n",
      " [ 1.8165131e+00  1.0000000e+00  5.8938086e-01 ... -3.2122264e+00\n",
      "   1.3563878e+02 -8.8714890e+01]\n",
      " ...\n",
      " [ 1.8164616e+00  1.0000000e+00  5.8960915e-01 ...  1.7732372e+00\n",
      "   6.4937558e+00 -5.6420559e-01]\n",
      " [ 1.8161888e+00  1.0000000e+00  5.9033966e-01 ... -3.9332348e+01\n",
      "  -1.0527125e+03  1.4110101e+03]\n",
      " [ 1.8161755e+00  1.0000000e+00  5.9048271e-01 ... -2.1040640e+01\n",
      "  -3.5296936e+00  4.3307060e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.6       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5448599    1.           1.1227837 ...   35.584297    44.731438\n",
      "   -85.18991  ]\n",
      " [   1.5450783    1.           1.1224003 ...  235.67288    -36.136097\n",
      "    39.459892 ]\n",
      " [   1.545372     1.           1.1220739 ...   10.059301   -75.49206\n",
      "    23.140593 ]\n",
      " ...\n",
      " [   1.5452614    1.           1.1222715 ...    3.000596     8.297198\n",
      "     0.5550718]\n",
      " [   1.5447712    1.           1.1228733 ...   27.694805   -37.132725\n",
      "    13.758437 ]\n",
      " [   1.5446835    1.           1.1229973 ...  193.03018   -179.85526\n",
      "  -229.92189  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.12242413e+00  1.00000000e+00  1.54509926e+00 ... -3.40430222e+01\n",
      "   3.00716190e+01 -8.69956741e+01]\n",
      " [ 1.12273788e+00  1.00000000e+00  1.54482841e+00 ... -3.37375832e+01\n",
      "  -8.68619347e+00 -1.70708065e+01]\n",
      " [ 1.12315178e+00  1.00000000e+00  1.54459381e+00 ... -2.68216858e+02\n",
      "   1.36605316e+02  4.22130165e+01]\n",
      " ...\n",
      " [ 1.12297821e+00  1.00000000e+00  1.54473972e+00 ... -2.56491852e+00\n",
      "  -8.69649696e+00 -1.51928246e-01]\n",
      " [ 1.12231636e+00  1.00000000e+00  1.54516220e+00 ... -2.59504089e+02\n",
      "  -2.32897018e+02 -1.15322174e+02]\n",
      " [ 1.12219048e+00  1.00000000e+00  1.54524040e+00 ... -4.76360798e+00\n",
      "   6.80601168e+00  2.84832916e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.1       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5897703    1.           1.8163795 ...  -17.194477    30.665773\n",
      "   -42.275295 ]\n",
      " [   0.5901499    1.           1.8162069 ...  -70.77737     36.02716\n",
      "    -9.643387 ]\n",
      " [   0.5906067    1.           1.8161014 ...  -82.988754     5.322627\n",
      "   143.85931  ]\n",
      " ...\n",
      " [   0.590395     1.           1.8161707 ...    3.2788212   14.869844\n",
      "     1.5147266]\n",
      " [   0.5896244    1.           1.8164101 ... -129.46318    144.65717\n",
      "  -215.63206  ]\n",
      " [   0.5895014    1.           1.8164368 ...  -12.383742  -132.14308\n",
      "     7.7614756]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.0980835e-04  1.0000000e+00  1.9097042e+00 ...  1.5384027e+02\n",
      "  -1.6360175e+02 -2.4297723e+02]\n",
      " [ 1.9264221e-04  1.0000000e+00  1.9096594e+00 ... -1.0484198e+01\n",
      "   8.7534546e+01  1.2751441e+02]\n",
      " [ 6.3896179e-04  1.0000000e+00  1.9096832e+00 ...  1.2210114e+03\n",
      "  -2.3007158e+02 -1.2076599e+03]\n",
      " ...\n",
      " [ 4.1580200e-04  1.0000000e+00  1.9096842e+00 ...  9.8240292e-01\n",
      "   9.3289499e+00  5.2016068e-01]\n",
      " [-3.8337708e-04  1.0000000e+00  1.9097042e+00 ...  1.0727211e+03\n",
      "   9.6195976e+01  5.3462341e+02]\n",
      " [-4.9018860e-04  1.0000000e+00  1.9096966e+00 ... -4.5871509e+03\n",
      "   7.8972363e+03  4.6726250e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9020901e-01  1.0000000e+00  1.8161545e+00 ...  5.5964886e+02\n",
      "  -2.2533727e+02 -8.2990036e+01]\n",
      " [-5.8982658e-01  1.0000000e+00  1.8162336e+00 ... -5.0844711e+01\n",
      "  -4.1314190e+01 -1.5097710e+01]\n",
      " [-5.8938408e-01  1.0000000e+00  1.8164058e+00 ... -9.6762451e+01\n",
      "   1.8730490e+02  3.8303687e+02]\n",
      " ...\n",
      " [-5.8959961e-01  1.0000000e+00  1.8163357e+00 ...  1.4603770e+00\n",
      "  -3.4685979e+00  1.8520317e+00]\n",
      " [-5.9036636e-01  1.0000000e+00  1.8160992e+00 ...  1.5130455e+00\n",
      "   3.1333685e+01 -2.3379614e+00]\n",
      " [-5.9046936e-01  1.0000000e+00  1.8160362e+00 ...  2.9438491e+03\n",
      "  -3.2430178e+03  4.5006060e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1229057e+00  1.0000000e+00  1.5445919e+00 ...  3.9493530e+03\n",
      "  -1.8770496e+03  6.6782744e+03]\n",
      " [-1.1225939e+00  1.0000000e+00  1.5447626e+00 ... -2.1599421e+01\n",
      "  -8.3693771e+01  3.2365322e-02]\n",
      " [-1.1222267e+00  1.0000000e+00  1.5450611e+00 ... -1.9467610e+02\n",
      "   8.2945381e+01  3.5247385e+02]\n",
      " ...\n",
      " [-1.1224289e+00  1.0000000e+00  1.5449295e+00 ... -1.0035586e-01\n",
      "  -4.2282605e+00  3.0790308e-01]\n",
      " [-1.1230736e+00  1.0000000e+00  1.5445061e+00 ... -4.1563096e+03\n",
      "   3.9459958e+02 -3.8369268e+03]\n",
      " [-1.1231260e+00  1.0000000e+00  1.5444202e+00 ...  2.7901411e+03\n",
      "   5.8364124e+01 -4.3927095e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5453005e+00  1.0000000e+00  1.1219158e+00 ...  2.8674414e+02\n",
      "   2.1435619e+02  7.1688898e+02]\n",
      " [-1.5450792e+00  1.0000000e+00  1.1221581e+00 ...  2.1019615e+02\n",
      "  -2.5474443e+02 -4.8081735e+02]\n",
      " [-1.5448074e+00  1.0000000e+00  1.1225569e+00 ...  1.4353448e+02\n",
      "   2.9507574e+02 -2.1246530e+02]\n",
      " ...\n",
      " [-1.5449657e+00  1.0000000e+00  1.1223879e+00 ...  9.4588947e-01\n",
      "  -7.7504659e+00 -7.8874981e-01]\n",
      " [-1.5454559e+00  1.0000000e+00  1.1217766e+00 ... -2.3213630e+03\n",
      "   1.0266285e+03  1.8822169e+03]\n",
      " [-1.5454559e+00  1.0000000e+00  1.1216602e+00 ...  5.3403735e+02\n",
      "  -4.8261547e+01 -2.9678915e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.5 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8163366e+00  1.0000000e+00  5.8939171e-01 ... -1.6075313e+02\n",
      "   9.5438492e+01 -4.1080685e+02]\n",
      " [-1.8162117e+00  1.0000000e+00  5.8973026e-01 ... -4.6314938e+02\n",
      "  -1.8112345e+03 -8.5057635e+02]\n",
      " [-1.8161125e+00  1.0000000e+00  5.9018075e-01 ...  1.7377550e+01\n",
      "   6.8500069e+01 -1.2381155e+01]\n",
      " ...\n",
      " [-1.8162003e+00  1.0000000e+00  5.8998775e-01 ...  2.7178359e-01\n",
      "   1.1887417e+01  1.3540657e+00]\n",
      " [-1.8164635e+00  1.0000000e+00  5.8923531e-01 ... -5.9793512e+02\n",
      "   6.9732861e+02  1.1234430e+03]\n",
      " [-1.8164043e+00  1.0000000e+00  5.8909035e-01 ... -1.0617935e+04\n",
      "   3.4816914e+03 -2.8480671e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90955830e+00  1.00000000e+00 -8.22067261e-04 ...  1.99404278e+01\n",
      "   7.53721542e+01 -1.75076904e+01]\n",
      " [-1.90955544e+00  1.00000000e+00 -4.39643860e-04 ...  6.77435608e+01\n",
      "   6.63711426e+02  9.75678955e+02]\n",
      " [-1.90960312e+00  1.00000000e+00  3.22794658e-05 ...  9.35358124e+01\n",
      "  -5.87696915e+01 -8.07546158e+01]\n",
      " ...\n",
      " [-1.90963745e+00  1.00000000e+00 -1.68800354e-04 ... -4.47589934e-01\n",
      "   6.03725815e+00  7.20046759e-01]\n",
      " [-1.90966034e+00  1.00000000e+00 -9.89913940e-04 ... -4.63293518e+02\n",
      "   2.60853180e+02 -1.09758255e+02]\n",
      " [-1.90954399e+00  1.00000000e+00 -1.13677979e-03 ... -2.26473218e+03\n",
      "   6.14092285e+02  4.17706641e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8157911e+00  1.0000000e+00 -5.9105492e-01 ... -7.3486755e+01\n",
      "  -3.9045770e+02 -4.7667198e+01]\n",
      " [-1.8159056e+00  1.0000000e+00 -5.9068489e-01 ...  8.7576747e-02\n",
      "   1.5322401e+01 -1.7368792e+01]\n",
      " [-1.8161030e+00  1.0000000e+00 -5.9023249e-01 ... -3.4806965e+01\n",
      "   3.0645201e+01 -9.1992134e+01]\n",
      " ...\n",
      " [-1.8160858e+00  1.0000000e+00 -5.9042358e-01 ... -1.1321361e+00\n",
      "   8.4928379e+00  1.8107065e+00]\n",
      " [-1.8158665e+00  1.0000000e+00 -5.9121895e-01 ...  4.7749454e+01\n",
      "  -3.4103271e+01 -1.0760521e+01]\n",
      " [-1.8156919e+00  1.0000000e+00 -5.9136009e-01 ... -6.0728125e+02\n",
      "   1.2294858e+03  8.0618750e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5443211e+00  1.0000000e+00 -1.1231689e+00 ... -1.9098985e+02\n",
      "  -2.0112494e+02 -4.4917690e+01]\n",
      " [-1.5445509e+00  1.0000000e+00 -1.1228533e+00 ...  7.1162605e+01\n",
      "   1.2027631e+02 -4.9222759e+01]\n",
      " [-1.5448723e+00  1.0000000e+00 -1.1224753e+00 ...  8.4625427e+01\n",
      "   2.6407003e+01  7.8481766e+01]\n",
      " ...\n",
      " [-1.5447960e+00  1.0000000e+00 -1.1226330e+00 ...  1.1340852e+00\n",
      "  -4.5746942e+00  4.1571534e-01]\n",
      " [-1.5443687e+00  1.0000000e+00 -1.1233158e+00 ... -9.0057904e+02\n",
      "   1.9030891e+03  1.0402793e+03]\n",
      " [-1.5441389e+00  1.0000000e+00 -1.1234436e+00 ... -1.1810197e+03\n",
      "  -9.8897614e+01 -1.0391587e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1214285e+00  1.0000000e+00 -1.5455856e+00 ...  2.2628452e+01\n",
      "  -1.0522423e+00  4.1285686e+01]\n",
      " [-1.1217575e+00  1.0000000e+00 -1.5453644e+00 ...  1.6788683e+03\n",
      "  -9.3296669e+01  4.4923172e+02]\n",
      " [-1.1221867e+00  1.0000000e+00 -1.5450906e+00 ... -3.1485907e+02\n",
      "  -3.3020975e+02 -6.6808380e+01]\n",
      " ...\n",
      " [-1.1220512e+00  1.0000000e+00 -1.5451984e+00 ...  7.8746152e-01\n",
      "  -3.9332285e+00 -1.6506684e-01]\n",
      " [-1.1214485e+00  1.0000000e+00 -1.5457096e+00 ... -1.2420570e+03\n",
      "   1.3622969e+03 -7.6184509e+01]\n",
      " [-1.1211796e+00  1.0000000e+00 -1.5457935e+00 ...  7.2301080e+02\n",
      "   1.1033222e+02  7.1936505e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.8920097e-01  1.0000000e+00 -1.8163853e+00 ...  2.4801591e+02\n",
      "  -9.0044476e+02  4.0349719e+02]\n",
      " [-5.8958340e-01  1.0000000e+00 -1.8162518e+00 ...  3.5350629e+02\n",
      "   7.8920860e+01  1.3340651e+02]\n",
      " [-5.9009743e-01  1.0000000e+00 -1.8161170e+00 ... -1.1868046e+03\n",
      "  -3.3604429e+03 -2.6759167e+03]\n",
      " ...\n",
      " [-5.8993912e-01  1.0000000e+00 -1.8161669e+00 ...  3.7804673e+00\n",
      "  -1.1060724e+01 -1.1551440e-01]\n",
      " [-5.8922195e-01  1.0000000e+00 -1.8164845e+00 ... -1.7375202e+00\n",
      "  -1.0085575e+01 -1.0333958e+02]\n",
      " [-5.8891773e-01  1.0000000e+00 -1.8165112e+00 ... -6.5292259e+01\n",
      "  -1.4041280e+02  5.0743439e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.3227463e-03  1.0000000e+00 -1.9095402e+00 ... -1.4759067e+02\n",
      "   4.4172760e+02  8.4711151e+01]\n",
      " [ 9.2315674e-04  1.0000000e+00 -1.9095221e+00 ... -1.4198296e+02\n",
      "  -2.2487318e+02  2.1854726e+02]\n",
      " [ 3.8146973e-04  1.0000000e+00 -1.9095608e+00 ...  1.2593296e+02\n",
      "  -3.6499316e+03 -1.5503384e+03]\n",
      " ...\n",
      " [ 5.4550171e-04  1.0000000e+00 -1.9095507e+00 ...  8.0969167e-01\n",
      "  -4.9582214e+00 -6.6542345e-01]\n",
      " [ 1.2989044e-03  1.0000000e+00 -1.9096146e+00 ...  1.3873047e+01\n",
      "  -1.8326115e+01 -9.2996742e+01]\n",
      " [ 1.6183853e-03  1.0000000e+00 -1.9095688e+00 ... -1.4922066e+02\n",
      "   6.9519202e+02 -2.4840175e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.59123325    1.           -1.8156948  ...  -45.475037\n",
      "   -11.802479     -2.1943738 ]\n",
      " [   0.5908537     1.           -1.8158255  ... -120.12285\n",
      "   123.97396      78.6024    ]\n",
      " [   0.590353      1.           -1.8160183  ...  -83.90094\n",
      "   -37.31746     -10.863309  ]\n",
      " ...\n",
      " [   0.5905056     1.           -1.8159523  ...   -0.51143277\n",
      "     6.5447507    -1.5965424 ]\n",
      " [   0.5912266     1.           -1.815752   ...    6.677482\n",
      "  -190.43443     213.97015   ]\n",
      " [   0.59151554    1.           -1.8156433  ...   71.85009\n",
      "  -164.82649    -254.41623   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.12364197e+00  1.00000000e+00 -1.54396820e+00 ... -2.11797180e+02\n",
      "  -3.49535583e+02  1.68545856e+01]\n",
      " [ 1.12332535e+00  1.00000000e+00 -1.54417133e+00 ...  1.99660206e+01\n",
      "  -4.47321014e+01 -2.80233078e+01]\n",
      " [ 1.12291527e+00  1.00000000e+00 -1.54451334e+00 ... -3.88110657e+01\n",
      "   1.38267456e+02 -2.14612122e+02]\n",
      " ...\n",
      " [ 1.12305641e+00  1.00000000e+00 -1.54438972e+00 ...  6.27542734e-02\n",
      "  -6.87236786e-02  1.35810375e-02]\n",
      " [ 1.12365723e+00  1.00000000e+00 -1.54401207e+00 ... -5.37130117e+00\n",
      "  -2.86297226e+01  4.17556267e+01]\n",
      " [ 1.12387943e+00  1.00000000e+00 -1.54382706e+00 ... -3.71009918e+02\n",
      "  -1.18887436e+02  8.30751495e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.54571629e+00  1.00000000e+00 -1.12127113e+00 ...  1.39098364e+03\n",
      "   1.01794055e+03  1.08471924e+03]\n",
      " [ 1.54549408e+00  1.00000000e+00 -1.12157440e+00 ... -4.07281799e+01\n",
      "  -1.06609138e+02 -1.98528976e+02]\n",
      " [ 1.54519081e+00  1.00000000e+00 -1.12202644e+00 ...  1.58542145e+02\n",
      "   1.34998047e+02  1.03232468e+02]\n",
      " ...\n",
      " [ 1.54530334e+00  1.00000000e+00 -1.12187290e+00 ...  1.86023200e+00\n",
      "   2.39695978e+00 -5.38302898e-01]\n",
      " [ 1.54569817e+00  1.00000000e+00 -1.12130737e+00 ...  9.70127411e+01\n",
      "  -1.48399536e+02 -7.76861206e+02]\n",
      " [ 1.54590130e+00  1.00000000e+00 -1.12106895e+00 ...  1.00523529e+01\n",
      "  -7.66605225e+01 -4.72656403e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8166828e+00  1.0000000e+00 -5.8847427e-01 ... -8.6812653e+01\n",
      "  -1.4812831e+01 -1.4116006e+02]\n",
      " [ 1.8165674e+00  1.0000000e+00 -5.8881092e-01 ...  1.8428093e+02\n",
      "  -2.2664752e+02 -9.8962723e+01]\n",
      " [ 1.8164082e+00  1.0000000e+00 -5.8933920e-01 ... -2.3102534e+02\n",
      "  -3.4490356e+02 -5.3396820e+01]\n",
      " ...\n",
      " [ 1.8164673e+00  1.0000000e+00 -5.8914661e-01 ...  3.0798049e+00\n",
      "   9.9959297e+00  3.9442182e-02]\n",
      " [ 1.8166332e+00  1.0000000e+00 -5.8850479e-01 ... -4.0436050e+02\n",
      "   3.5746664e+02 -3.5990704e+02]\n",
      " [ 1.8167801e+00  1.0000000e+00 -5.8824348e-01 ... -6.6714211e+01\n",
      "  -5.9660851e+01  1.0846152e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9094238e+00  1.0000000e+00  1.8787384e-03 ...  1.8480502e+02\n",
      "  -6.5267159e+01 -6.1936707e+01]\n",
      " [ 1.9094343e+00  1.0000000e+00  1.5296936e-03 ... -1.3591150e+02\n",
      "  -5.8395813e+01 -1.2060293e+02]\n",
      " [ 1.9094105e+00  1.0000000e+00  9.7156613e-04 ...  9.0954376e+01\n",
      "   5.9486595e+01  6.6087723e+01]\n",
      " ...\n",
      " [ 1.9094086e+00  1.0000000e+00  1.1796951e-03 ... -5.4464684e+00\n",
      "  -1.0424726e+01  3.7213439e-01]\n",
      " [ 1.9093685e+00  1.0000000e+00  1.8501282e-03 ...  4.3400793e+02\n",
      "  -1.2076278e+03  1.2321593e+02]\n",
      " [ 1.9094410e+00  1.0000000e+00  2.1266937e-03 ... -1.6255841e+02\n",
      "   1.7982433e+01 -7.1342461e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.1       0.        0.        0.        0.\n",
      " 0.        0.        0.2       0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81536865e+00  1.00000000e+00  5.92006683e-01 ...  3.96742096e+01\n",
      "   2.52348576e+01  6.49472666e+00]\n",
      " [ 1.81550694e+00  1.00000000e+00  5.91663361e-01 ...  2.75963249e+01\n",
      "  -1.94639206e+02  1.06673805e+02]\n",
      " [ 1.81560326e+00  1.00000000e+00  5.91149688e-01 ...  5.34330702e+00\n",
      "  -8.36959152e+01  3.60477924e+00]\n",
      " ...\n",
      " [ 1.81554222e+00  1.00000000e+00  5.91333389e-01 ... -1.45540762e+00\n",
      "  -4.87441301e+00  1.11207128e-01]\n",
      " [ 1.81530380e+00  1.00000000e+00  5.91978073e-01 ... -3.84572418e+02\n",
      "   1.55455566e+03 -6.92624695e+02]\n",
      " [ 1.81529140e+00  1.00000000e+00  5.92239380e-01 ...  1.20129639e+03\n",
      "   5.25335312e+01  3.59536450e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5434046e+00  1.0000000e+00  1.1240082e+00 ... -7.6513855e+02\n",
      "   6.7741998e+02 -5.6601947e+02]\n",
      " [ 1.5436220e+00  1.0000000e+00  1.1237278e+00 ...  7.6063324e+01\n",
      "   2.6804904e+02  5.2911606e+01]\n",
      " [ 1.5439053e+00  1.0000000e+00  1.1232921e+00 ... -1.3902696e+02\n",
      "  -8.6655035e+02 -2.6496451e+02]\n",
      " ...\n",
      " [ 1.5438042e+00  1.0000000e+00  1.1234417e+00 ... -1.4515986e+00\n",
      "  -5.9250960e+00  1.1830015e+00]\n",
      " [ 1.5433750e+00  1.0000000e+00  1.1239777e+00 ...  1.4411350e+03\n",
      "   1.1551567e+02 -7.7957275e+02]\n",
      " [ 1.5432549e+00  1.0000000e+00  1.1242046e+00 ...  1.4599930e+02\n",
      "   1.1330090e+03 -7.6153705e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1205387e+00  1.0000000e+00  1.5459709e+00 ...  6.3545892e+02\n",
      "   5.0363754e+02  2.5137930e+02]\n",
      " [ 1.1208477e+00  1.0000000e+00  1.5457706e+00 ...  1.6079762e+02\n",
      "  -9.7392159e+01  6.8693756e+01]\n",
      " [ 1.1212368e+00  1.0000000e+00  1.5454602e+00 ...  1.7435844e+01\n",
      "   7.6317239e+00 -3.5218082e+01]\n",
      " ...\n",
      " [ 1.1210918e+00  1.0000000e+00  1.5455656e+00 ... -7.3034275e-01\n",
      "   1.2936595e+01  5.6716907e-01]\n",
      " [ 1.1205177e+00  1.0000000e+00  1.5459404e+00 ...  4.9097427e+01\n",
      "  -2.4906057e+01  1.6440321e+02]\n",
      " [ 1.1203165e+00  1.0000000e+00  1.5461121e+00 ...  8.1531824e+02\n",
      "  -1.2681332e+03 -1.5810600e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.87740898e-01  1.00000000e+00  1.81674576e+00 ... -5.71493164e+02\n",
      "  -2.12802460e+02 -1.22673096e+02]\n",
      " [ 5.88098526e-01  1.00000000e+00  1.81661224e+00 ... -1.57129471e+02\n",
      "  -2.37837887e+01  1.17957817e+02]\n",
      " [ 5.88584900e-01  1.00000000e+00  1.81645262e+00 ... -5.54658432e+01\n",
      "   2.07408875e+02  1.97755005e+02]\n",
      " ...\n",
      " [ 5.88407516e-01  1.00000000e+00  1.81651211e+00 ... -5.40161669e-01\n",
      "   6.87064838e+00 -3.32744062e-01]\n",
      " [ 5.87715149e-01  1.00000000e+00  1.81671524e+00 ... -4.58936310e+00\n",
      "  -1.01596542e+02  1.06682396e+02]\n",
      " [ 5.87481499e-01  1.00000000e+00  1.81679344e+00 ...  2.07435760e+02\n",
      "  -1.54901245e+03  1.87877686e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.4051666e-03  1.0000000e+00  1.9094620e+00 ... -2.4536093e+01\n",
      "   1.6925992e+02  2.5611761e+01]\n",
      " [-2.0313263e-03  1.0000000e+00  1.9094534e+00 ...  8.7998619e+00\n",
      "  -1.5399353e+02 -2.2422589e+02]\n",
      " [-1.5144348e-03  1.0000000e+00  1.9094654e+00 ... -1.8799573e+02\n",
      "  -1.3349245e+02  5.2712244e+02]\n",
      " ...\n",
      " [-1.6937256e-03  1.0000000e+00  1.9094620e+00 ... -1.1181166e+00\n",
      "   9.0360842e+00  7.5128627e-01]\n",
      " [-2.4242401e-03  1.0000000e+00  1.9094315e+00 ... -2.4743526e+01\n",
      "   2.8357098e+01  1.5698569e+01]\n",
      " [-2.6760101e-03  1.0000000e+00  1.9094448e+00 ... -4.4772575e+01\n",
      "  -3.2615536e+01  9.0762421e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9248352e-01  1.0000000e+00  1.8152847e+00 ...  7.6762077e+01\n",
      "   2.2625961e+02  7.8177704e+01]\n",
      " [-5.9212303e-01  1.0000000e+00  1.8154116e+00 ... -3.4626885e+03\n",
      "  -4.2537588e+03 -1.9269351e+03]\n",
      " [-5.9160423e-01  1.0000000e+00  1.8155785e+00 ... -8.3884857e+01\n",
      "   3.9305798e+01  1.7625987e+02]\n",
      " ...\n",
      " [-5.9177017e-01  1.0000000e+00  1.8155403e+00 ... -1.9167826e+00\n",
      "   2.6724205e+00  1.0942147e+00]\n",
      " [-5.9247780e-01  1.0000000e+00  1.8152523e+00 ...  1.5483726e+01\n",
      "   1.3930304e+02  9.8141548e+01]\n",
      " [-5.9274387e-01  1.0000000e+00  1.8152046e+00 ...  9.1847313e+01\n",
      "  -2.3533153e+02  9.5429855e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.12467670e+00  1.00000000e+00  1.54324722e+00 ...  2.67674103e+02\n",
      "   4.22255707e+02 -5.25853157e+01]\n",
      " [-1.12437057e+00  1.00000000e+00  1.54350758e+00 ... -5.06711884e+01\n",
      "   4.80351524e+01 -1.87498913e+01]\n",
      " [-1.12392616e+00  1.00000000e+00  1.54379761e+00 ...  1.56189270e+02\n",
      "  -3.54463959e+01  2.98146858e+01]\n",
      " ...\n",
      " [-1.12407303e+00  1.00000000e+00  1.54372787e+00 ... -4.44366741e+00\n",
      "   7.83226681e+00  8.43997121e-01]\n",
      " [-1.12470627e+00  1.00000000e+00  1.54321480e+00 ... -1.11893425e+02\n",
      "  -2.36384369e+02 -7.35707520e+02]\n",
      " [-1.12489700e+00  1.00000000e+00  1.54308891e+00 ...  1.29977226e-01\n",
      "   2.50046539e+01 -1.21546221e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.54651737e+00  1.00000000e+00  1.12027550e+00 ...  6.45309082e+02\n",
      "  -1.94035535e+03 -8.70175781e+02]\n",
      " [-1.54627895e+00  1.00000000e+00  1.12059402e+00 ...  1.64404163e+03\n",
      "  -4.20979462e+02 -4.01035675e+02]\n",
      " [-1.54596138e+00  1.00000000e+00  1.12101889e+00 ...  2.36410751e+02\n",
      "  -5.85644341e+01  1.46067200e+02]\n",
      " ...\n",
      " [-1.54606628e+00  1.00000000e+00  1.12089539e+00 ... -8.62425804e+00\n",
      "   1.00721302e+01  1.21509135e+00]\n",
      " [-1.54651451e+00  1.00000000e+00  1.12024117e+00 ...  2.20348206e+02\n",
      "  -1.01647064e+02 -4.50698929e+01]\n",
      " [-1.54667759e+00  1.00000000e+00  1.12005997e+00 ... -1.65063492e+02\n",
      "   3.46679535e+02  2.55853882e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8172112e+00  1.0000000e+00  5.8717728e-01 ... -2.8226675e+01\n",
      "   4.4197041e+01  3.3237617e+01]\n",
      " [-1.8170471e+00  1.0000000e+00  5.8752346e-01 ...  4.5967255e+01\n",
      "  -4.3943048e+02  2.1144019e+02]\n",
      " [-1.8169250e+00  1.0000000e+00  5.8802891e-01 ... -3.2510992e+02\n",
      "  -7.5083978e+02  2.8647940e+02]\n",
      " ...\n",
      " [-1.8169785e+00  1.0000000e+00  5.8787823e-01 ...  4.3215108e+00\n",
      "  -3.2116134e+00  3.6935210e-02]\n",
      " [-1.8172112e+00  1.0000000e+00  5.8714104e-01 ... -8.7064758e+01\n",
      "   5.8368538e+01 -3.3249496e+02]\n",
      " [-1.8172989e+00  1.0000000e+00  5.8691978e-01 ... -1.8954856e+00\n",
      "  -6.7745399e+00 -2.8094044e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9097414e+00  1.0000000e+00 -3.1223297e-03 ... -1.6755434e+01\n",
      "  -3.6691288e+01 -1.7101120e+02]\n",
      " [-1.9096889e+00  1.0000000e+00 -2.7770996e-03 ... -2.2439650e+02\n",
      "   6.7805725e+01 -1.1869638e+02]\n",
      " [-1.9097462e+00  1.0000000e+00 -2.2422590e-03 ...  4.4384818e+02\n",
      "   5.5267853e+02  4.3124060e+02]\n",
      " ...\n",
      " [-1.9097443e+00  1.0000000e+00 -2.4032593e-03 ... -1.3569360e+00\n",
      "   1.0904963e+00  2.5716907e-01]\n",
      " [-1.9097500e+00  1.0000000e+00 -3.1585693e-03 ...  3.8207520e+02\n",
      "   2.3426100e+02  1.8937180e+01]\n",
      " [-1.9097567e+00  1.0000000e+00 -3.3950806e-03 ...  2.4389393e+01\n",
      "  -5.6171860e+01 -7.2983521e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81511784e+00  1.00000000e+00 -5.93177795e-01 ...  1.24154263e+01\n",
      "   3.65300217e+01  1.91050510e+01]\n",
      " [-1.81518459e+00  1.00000000e+00 -5.92843056e-01 ...  1.14801764e-01\n",
      "  -7.38738861e+01  8.40753269e+00]\n",
      " [-1.81541252e+00  1.00000000e+00 -5.92332900e-01 ...  5.14006775e+02\n",
      "   5.91775757e+02  3.30221985e+02]\n",
      " ...\n",
      " [-1.81535339e+00  1.00000000e+00 -5.92487335e-01 ... -1.86657429e-01\n",
      "   4.29450035e-01 -4.88826632e-02]\n",
      " [-1.81511307e+00  1.00000000e+00 -5.93214035e-01 ... -2.64567451e+01\n",
      "  -3.13522778e+01 -1.18646984e+01]\n",
      " [-1.81506538e+00  1.00000000e+00 -5.93444824e-01 ... -4.04792690e+00\n",
      "   3.11518908e-01 -1.85709810e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5430155e+00  1.0000000e+00 -1.1250191e+00 ... -6.0836664e+02\n",
      "   8.5992615e+01 -8.5636035e+02]\n",
      " [-1.5432014e+00  1.0000000e+00 -1.1247330e+00 ... -6.9692619e+01\n",
      "  -7.4569611e+01  1.1131727e+02]\n",
      " [-1.5436077e+00  1.0000000e+00 -1.1242926e+00 ... -5.0397348e+00\n",
      "  -6.4868584e+01  3.1272870e+02]\n",
      " ...\n",
      " [-1.5435028e+00  1.0000000e+00 -1.1244278e+00 ... -5.6567669e-01\n",
      "   1.2343483e+00 -1.4547759e-01]\n",
      " [-1.5430603e+00  1.0000000e+00 -1.1250496e+00 ...  6.9509244e-01\n",
      "   2.6021555e+01 -1.0461117e+01]\n",
      " [-1.5428991e+00  1.0000000e+00 -1.1252499e+00 ...  2.5039957e+02\n",
      "  -2.0648239e+02 -1.2287948e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.11979294e+00  1.00000000e+00 -1.54684639e+00 ...  1.85280609e+02\n",
      "  -5.52132988e+01  1.39509857e+02]\n",
      " [-1.12007046e+00  1.00000000e+00 -1.54661465e+00 ... -1.21759895e+02\n",
      "   1.43998795e+02 -2.86550507e+02]\n",
      " [-1.12059975e+00  1.00000000e+00 -1.54629397e+00 ...  1.10483428e+04\n",
      "  -3.56562988e+03 -6.15237402e+03]\n",
      " ...\n",
      " [-1.12046051e+00  1.00000000e+00 -1.54638863e+00 ... -5.74511528e-01\n",
      "   9.90400553e-01 -1.16568804e-01]\n",
      " [-1.11981201e+00  1.00000000e+00 -1.54686928e+00 ... -3.19365479e+02\n",
      "   1.33476318e+02  2.16008865e+02]\n",
      " [-1.11961460e+00  1.00000000e+00 -1.54700851e+00 ...  7.20007782e+01\n",
      "  -1.60089157e+02  4.82369194e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.9000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.86677551e-01  1.00000000e+00 -1.81730843e+00 ... -9.78294067e+02\n",
      "  -1.24062065e+02  1.95977264e+02]\n",
      " [-5.87012291e-01  1.00000000e+00 -1.81716156e+00 ...  4.75336084e+03\n",
      "   9.46225586e+02 -9.06945215e+03]\n",
      " [-5.87572098e-01  1.00000000e+00 -1.81699729e+00 ... -1.81304321e+03\n",
      "  -9.18208374e+02 -3.27910229e+03]\n",
      " ...\n",
      " [-5.87423325e-01  1.00000000e+00 -1.81705189e+00 ...  1.56845760e+00\n",
      "  -1.75836611e+00 -4.38136578e-01]\n",
      " [-5.86668015e-01  1.00000000e+00 -1.81732559e+00 ...  4.98621483e+01\n",
      "   7.62698269e+00 -2.39753265e+01]\n",
      " [-5.86461067e-01  1.00000000e+00 -1.81740189e+00 ... -2.41760559e+01\n",
      "  -1.39154266e+02  3.75378082e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.3493042e-03  1.0000000e+00 -1.9096413e+00 ...  3.0923641e+01\n",
      "  -5.9788834e+01 -3.8082306e+02]\n",
      " [ 2.9973984e-03  1.0000000e+00 -1.9096041e+00 ... -1.1835377e+02\n",
      "   2.1059332e+02 -2.6851361e+02]\n",
      " [ 2.4147034e-03  1.0000000e+00 -1.9096159e+00 ... -9.2467499e+01\n",
      "  -2.5899549e+01  2.2562520e+02]\n",
      " ...\n",
      " [ 2.5711060e-03  1.0000000e+00 -1.9096251e+00 ...  3.9037533e+00\n",
      "  -6.3672276e+00 -6.4698046e-01]\n",
      " [ 3.3588409e-03  1.0000000e+00 -1.9096565e+00 ...  5.5557854e+01\n",
      "   3.0583559e+01  4.8059479e+01]\n",
      " [ 3.5848618e-03  1.0000000e+00 -1.9096699e+00 ...  7.7886731e+02\n",
      "  -9.5468542e+02 -2.5695813e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9365654e-01  1.0000000e+00 -1.8150177e+00 ...  2.1212490e+01\n",
      "  -9.4914665e+01  5.1358063e+01]\n",
      " [ 5.9332657e-01  1.0000000e+00 -1.8151007e+00 ... -2.0096724e+02\n",
      "   6.4214607e+01 -2.2177553e+02]\n",
      " [ 5.9278679e-01  1.0000000e+00 -1.8152866e+00 ... -5.3213432e+01\n",
      "  -1.9006365e+01 -1.9537418e+02]\n",
      " ...\n",
      " [ 5.9293175e-01  1.0000000e+00 -1.8152552e+00 ...  5.8453426e+00\n",
      "  -6.7584887e+00  5.1629114e-01]\n",
      " [ 5.9368324e-01  1.0000000e+00 -1.8150311e+00 ... -2.3913303e+01\n",
      "   5.1382885e+01  1.8153400e+01]\n",
      " [ 5.9387779e-01  1.0000000e+00 -1.8149738e+00 ... -5.6701733e+02\n",
      "  -6.5737228e+01 -5.0586243e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1254539e+00  1.0000000e+00 -1.5424881e+00 ... -1.1404061e+02\n",
      "   3.9553516e+01  2.5883600e+02]\n",
      " [ 1.1251678e+00  1.0000000e+00 -1.5426741e+00 ... -7.3287880e+01\n",
      "   8.6729298e+00  1.6741867e+02]\n",
      " [ 1.1247044e+00  1.0000000e+00 -1.5430179e+00 ...  1.5088859e+03\n",
      "  -1.8540143e+03 -1.7096650e+03]\n",
      " ...\n",
      " [ 1.1248302e+00  1.0000000e+00 -1.5429621e+00 ... -2.8416991e+00\n",
      "   5.5121112e+00  7.6621866e-01]\n",
      " [ 1.1254826e+00  1.0000000e+00 -1.5424995e+00 ...  6.7929977e+01\n",
      "  -7.0600471e+01  1.5816440e+02]\n",
      " [ 1.1256456e+00  1.0000000e+00 -1.5423698e+00 ... -2.2263362e+02\n",
      "   8.1862556e+01 -8.6834003e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5468254e+00  1.0000000e+00 -1.1194725e+00 ...  1.3819374e+02\n",
      "  -6.8132439e+01 -7.3058014e+01]\n",
      " [ 1.5466328e+00  1.0000000e+00 -1.1197376e+00 ...  5.8239819e+01\n",
      "   1.5444168e+02 -1.9377203e+02]\n",
      " [ 1.5463486e+00  1.0000000e+00 -1.1202177e+00 ...  1.5764302e+02\n",
      "   1.9673810e+02  2.3756668e+02]\n",
      " ...\n",
      " [ 1.5464268e+00  1.0000000e+00 -1.1201172e+00 ... -1.6452498e+00\n",
      "   3.5246000e+00 -7.0874351e-01]\n",
      " [ 1.5468903e+00  1.0000000e+00 -1.1194839e+00 ...  3.1447197e+01\n",
      "  -3.1467508e+01 -7.2896408e+01]\n",
      " [ 1.5469646e+00  1.0000000e+00 -1.1192970e+00 ...  1.4999374e+02\n",
      "  -2.4430017e+02 -1.3170509e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8173018e+00  1.0000000e+00 -5.8611298e-01 ...  1.9077611e+02\n",
      "   5.8334873e+01  3.5102588e+02]\n",
      " [ 1.8172016e+00  1.0000000e+00 -5.8640862e-01 ...  2.0918675e+02\n",
      "   1.4400634e+01  2.8371189e+01]\n",
      " [ 1.8170280e+00  1.0000000e+00 -5.8697498e-01 ... -4.3459338e+02\n",
      "  -1.2550404e+02 -1.0869712e+02]\n",
      " ...\n",
      " [ 1.8170776e+00  1.0000000e+00 -5.8683777e-01 ... -4.5847807e+00\n",
      "   8.8454018e+00 -3.1285867e-01]\n",
      " [ 1.8173370e+00  1.0000000e+00 -5.8612442e-01 ... -7.5394455e+01\n",
      "   8.6846268e+01  1.5926090e+01]\n",
      " [ 1.8173647e+00  1.0000000e+00 -5.8589935e-01 ...  2.7481326e+02\n",
      "   4.5155667e+02 -6.0747974e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9094925e+00  1.0000000e+00  3.9100647e-03 ... -2.1666621e+02\n",
      "   3.1414768e+01 -1.5037389e+02]\n",
      " [ 1.9094934e+00  1.0000000e+00  3.6134720e-03 ...  1.4527697e+03\n",
      "   7.7266058e+02 -3.7245176e+03]\n",
      " [ 1.9095249e+00  1.0000000e+00  3.0215965e-03 ...  1.0374411e+02\n",
      "   2.0552325e+02  2.1200064e+02]\n",
      " ...\n",
      " [ 1.9095039e+00  1.0000000e+00  3.1633377e-03 ...  1.2463641e-01\n",
      "   1.7005253e+00 -2.7654198e-01]\n",
      " [ 1.9095192e+00  1.0000000e+00  3.9005280e-03 ... -1.1635192e+03\n",
      "   1.1188110e+03 -3.1609003e+02]\n",
      " [ 1.9094925e+00  1.0000000e+00  4.1408539e-03 ...  1.5634232e+02\n",
      "   1.1533646e+02  1.5810823e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8147364     1.            0.59412384 ...  -15.814145\n",
      "   -64.07376      55.564953  ]\n",
      " [   1.8148642     1.            0.5938158  ...    9.918746\n",
      "     5.9857693   -17.826519  ]\n",
      " [   1.8150921     1.            0.5932545  ... -336.56494\n",
      "   223.70839     -34.893154  ]\n",
      " ...\n",
      " [   1.8150234     1.            0.59338856 ...    0.52875304\n",
      "     3.515695     -0.84085196]\n",
      " [   1.8147945     1.            0.5941143  ...  -76.45247\n",
      "    -4.199253     27.933084  ]\n",
      " [   1.8146715     1.            0.5943451  ... -387.29974\n",
      "   201.10312    -111.56425   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.54213715e+00  1.00000000e+00  1.12594986e+00 ... -2.15847656e+02\n",
      "  -2.61782593e+03  1.18848373e+02]\n",
      " [ 1.54236412e+00  1.00000000e+00  1.12566376e+00 ...  1.40158615e+01\n",
      "   8.13860168e+01  1.05740677e+02]\n",
      " [ 1.54274178e+00  1.00000000e+00  1.12518966e+00 ... -1.45993454e+02\n",
      "   2.65276428e+02  3.82741119e+02]\n",
      " ...\n",
      " [ 1.54262733e+00  1.00000000e+00  1.12529850e+00 ...  8.08042288e-02\n",
      "   2.01164341e+00 -8.92720222e-01]\n",
      " [ 1.54215431e+00  1.00000000e+00  1.12594032e+00 ...  2.09050064e+02\n",
      "  -7.97395859e+01 -1.29464020e+02]\n",
      " [ 1.54202080e+00  1.00000000e+00  1.12612724e+00 ... -5.51312447e+01\n",
      "  -8.20083618e+00 -6.95218945e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1188231e+00  1.0000000e+00  1.5474052e+00 ... -8.1184601e+01\n",
      "  -1.3731357e+02  4.1690198e+02]\n",
      " [ 1.1191282e+00  1.0000000e+00  1.5471811e+00 ... -6.5338074e+01\n",
      "  -4.3997566e+01  8.2758970e+00]\n",
      " [ 1.1196041e+00  1.0000000e+00  1.5468336e+00 ...  1.1455305e+02\n",
      "  -7.8738609e+01 -1.4893076e+01]\n",
      " ...\n",
      " [ 1.1194572e+00  1.0000000e+00  1.5469074e+00 ...  1.3904953e-01\n",
      "   3.0525787e+00  2.5638366e-01]\n",
      " [ 1.1188202e+00  1.0000000e+00  1.5473995e+00 ...  1.5215086e+02\n",
      "   9.2657539e+01  3.2739307e+02]\n",
      " [ 1.1186495e+00  1.0000000e+00  1.5475159e+00 ...  5.8203015e+02\n",
      "  -4.9400870e+02  7.4126453e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5858612     1.            1.8174934  ... -363.28137\n",
      "   256.69305     545.8923    ]\n",
      " [   0.5862255     1.            1.8173313  ...  272.87344\n",
      "  -123.56118      17.935284  ]\n",
      " [   0.58678627    1.            1.8171588  ...  -59.071606\n",
      "   -42.85432      -3.9621305 ]\n",
      " ...\n",
      " [   0.5866184     1.            1.8171959  ...   -1.1508031\n",
      "    -4.3630543     0.7827065 ]\n",
      " [   0.58587456    1.            1.8174877  ...  -10.916352\n",
      "   -29.070213    -13.399559  ]\n",
      " [   0.58565617    1.            1.8175354  ...  -27.079796\n",
      "   276.7112      105.46998   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.9800873e-03  1.0000000e+00  1.9096241e+00 ... -3.5852344e+03\n",
      "   9.3864978e+02 -4.8787158e+02]\n",
      " [-4.6014786e-03  1.0000000e+00  1.9095707e+00 ... -2.1241597e+02\n",
      "   8.3252777e+02  5.5402576e+02]\n",
      " [-3.9882660e-03  1.0000000e+00  1.9095824e+00 ... -5.6746845e+01\n",
      "  -3.4868028e+00 -4.9580349e+01]\n",
      " ...\n",
      " [-4.1675568e-03  1.0000000e+00  1.9095726e+00 ... -7.0847845e-01\n",
      "   1.5406460e+01 -2.5006342e-01]\n",
      " [-4.9495697e-03  1.0000000e+00  1.9096203e+00 ... -5.4413040e+01\n",
      "  -7.2564056e+01 -3.0311874e+02]\n",
      " [-5.2013397e-03  1.0000000e+00  1.9095936e+00 ... -3.2343910e+01\n",
      "  -2.1565338e+01 -1.7630119e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:2, Score:2.33, Best Score:2.33, Average Score:1.72, Best Avg Score:1.72\n",
      "Episode number: 3\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c3e8100>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.12642765e+00  1.00000000e+00  1.54179001e+00 ...  3.52875610e+02\n",
      "  -6.84964966e+02 -1.38110672e+02]\n",
      " [-1.12613010e+00  1.00000000e+00  1.54190922e+00 ... -3.70756287e+02\n",
      "   3.18185310e+01  1.92480530e+02]\n",
      " [-1.12564659e+00  1.00000000e+00  1.54230094e+00 ... -1.67820297e+02\n",
      "   3.26960602e+02 -2.69764771e+02]\n",
      " ...\n",
      " [-1.12579918e+00  1.00000000e+00  1.54219532e+00 ... -7.90106201e+01\n",
      "   2.43426704e+01 -1.09833244e+02]\n",
      " [-1.12644196e+00  1.00000000e+00  1.54179764e+00 ... -6.46011597e+02\n",
      "  -7.95290161e+02 -1.30890784e+03]\n",
      " [-1.12661266e+00  1.00000000e+00  1.54165840e+00 ...  1.03833122e+01\n",
      "   4.77886238e+01 -1.30824936e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5478354e+00  1.0000000e+00  1.1181297e+00 ... -1.8062941e+02\n",
      "  -2.3127719e+02 -3.5157360e+01]\n",
      " [-1.5476198e+00  1.0000000e+00  1.1183958e+00 ... -5.4075130e+01\n",
      "   2.3622747e+01 -1.6150540e+02]\n",
      " [-1.5472622e+00  1.0000000e+00  1.1189123e+00 ...  1.9198479e+02\n",
      "   1.0696963e+02 -1.3004191e+01]\n",
      " ...\n",
      " [-1.5473766e+00  1.0000000e+00  1.1187897e+00 ...  7.0059570e+01\n",
      "   6.7124146e+01 -1.0047213e+02]\n",
      " [-1.5478497e+00  1.0000000e+00  1.1181412e+00 ... -5.6550714e+02\n",
      "  -6.9819250e+02 -2.9303268e+02]\n",
      " [-1.5479755e+00  1.0000000e+00  1.1179543e+00 ... -3.2968884e+03\n",
      "  -2.0248954e+03  1.4542765e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8177414e+00  1.0000000e+00  5.8491707e-01 ...  1.4506084e+02\n",
      "   5.5521881e+02 -4.8620288e+02]\n",
      " [-1.8176231e+00  1.0000000e+00  5.8520412e-01 ...  8.4406113e+01\n",
      "  -1.5401723e+02 -2.6921399e+02]\n",
      " [-1.8174458e+00  1.0000000e+00  5.8582878e-01 ...  1.1224190e+02\n",
      "   1.0965948e+02  5.0164990e+02]\n",
      " ...\n",
      " [-1.8175125e+00  1.0000000e+00  5.8566284e-01 ... -1.5032159e+02\n",
      "   5.7767235e+01  2.2318541e+02]\n",
      " [-1.8177452e+00  1.0000000e+00  5.8492851e-01 ... -3.7019306e+01\n",
      "   3.5183151e+01 -7.0946622e-01]\n",
      " [-1.8178205e+00  1.0000000e+00  5.8471298e-01 ...  2.8839990e+03\n",
      "   4.5622139e+03  9.9850110e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.1       0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.9000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9095812e+00  1.0000000e+00 -5.3558350e-03 ... -9.6431740e+01\n",
      "   4.8224529e+01  2.3298082e+02]\n",
      " [-1.9095650e+00  1.0000000e+00 -5.0172806e-03 ...  1.4869596e+03\n",
      "  -2.1862336e+03  6.0696576e+02]\n",
      " [-1.9095783e+00  1.0000000e+00 -4.3811980e-03 ... -3.4389259e+01\n",
      "   3.0207193e+01  9.1020363e+01]\n",
      " ...\n",
      " [-1.9095898e+00  1.0000000e+00 -4.5328140e-03 ...  2.6859842e+02\n",
      "   9.1199207e+02 -2.5131032e+02]\n",
      " [-1.9095745e+00  1.0000000e+00 -5.3443909e-03 ...  2.5475235e+02\n",
      "   8.9100014e+01 -1.5120856e+01]\n",
      " [-1.9095917e+00  1.0000000e+00 -5.5751801e-03 ...  6.3414917e+02\n",
      "   5.9936621e+02  5.2173981e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.1       0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.81441689e+00  1.00000000e+00 -5.95598221e-01 ... -1.61570435e+01\n",
      "  -7.00202103e+01 -2.25074806e+01]\n",
      " [-1.81449413e+00  1.00000000e+00 -5.95259666e-01 ...  2.75694855e+02\n",
      "  -1.10723465e+02  7.73114471e+01]\n",
      " [-1.81466484e+00  1.00000000e+00 -5.94656408e-01 ...  6.62817871e+02\n",
      "  -3.12561859e+02  8.40225677e+01]\n",
      " ...\n",
      " [-1.81459808e+00  1.00000000e+00 -5.94808578e-01 ... -8.09368038e+00\n",
      "  -1.39104477e+02 -2.36394608e+02]\n",
      " [-1.81436348e+00  1.00000000e+00 -5.95586777e-01 ... -2.84320557e+02\n",
      "   2.44377899e+02  2.24210281e+02]\n",
      " [-1.81435013e+00  1.00000000e+00 -5.95808029e-01 ...  7.25345886e+02\n",
      "   1.14918831e+02 -1.04587860e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.1       0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5415802    1.          -1.1272182 ...  410.4748     245.08366\n",
      "  -132.89542  ]\n",
      " [  -1.541749     1.          -1.1269102 ...  103.05197    -27.372044\n",
      "    82.948944 ]\n",
      " [  -1.54212      1.          -1.126386  ...  110.60757     43.897095\n",
      "   710.8108   ]\n",
      " ...\n",
      " [  -1.5420074    1.          -1.1265202 ...   27.404327   204.93396\n",
      "   -83.295296 ]\n",
      " [  -1.5415668    1.          -1.1272068 ... -419.68018   -421.46106\n",
      "  -555.0839   ]\n",
      " [  -1.5414591    1.          -1.1274033 ... -550.1958    -571.73914\n",
      "   615.9148   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.11790466e+00  1.00000000e+00 -1.54812813e+00 ...  3.57037125e+01\n",
      "   8.90927505e+01 -1.78583237e+02]\n",
      " [-1.11814022e+00  1.00000000e+00 -1.54788303e+00 ... -6.62454773e+02\n",
      "  -7.85281189e+02  1.32007104e+03]\n",
      " [-1.11866570e+00  1.00000000e+00 -1.54749882e+00 ... -2.75598328e+02\n",
      "  -4.12715332e+02  2.37391525e+02]\n",
      " ...\n",
      " [-1.11851883e+00  1.00000000e+00 -1.54759121e+00 ...  3.06809654e+01\n",
      "   8.80169907e+01  1.55704327e+01]\n",
      " [-1.11788750e+00  1.00000000e+00 -1.54812050e+00 ...  3.91994629e+02\n",
      "   7.99102249e+01 -2.36905670e+01]\n",
      " [-1.11773968e+00  1.00000000e+00 -1.54827690e+00 ...  5.58826981e+01\n",
      "  -1.22760956e+02 -7.40494232e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.84370613e-01  1.00000000e+00 -1.81797218e+00 ...  8.02160339e+01\n",
      "  -3.42109528e+01  1.10983696e+02]\n",
      " [-5.84645271e-01  1.00000000e+00 -1.81783962e+00 ...  1.75881494e+03\n",
      "  -6.64773682e+03  1.55449512e+03]\n",
      " [-5.85220337e-01  1.00000000e+00 -1.81763971e+00 ...  1.20358452e+02\n",
      "   4.41238441e+01  1.01357285e+02]\n",
      " ...\n",
      " [-5.85056305e-01  1.00000000e+00 -1.81768990e+00 ...  8.58403931e+01\n",
      "  -3.71679726e+01 -2.04040527e+01]\n",
      " [-5.84310532e-01  1.00000000e+00 -1.81796646e+00 ... -1.18415169e+02\n",
      "   1.17501854e+02  5.22581139e+01]\n",
      " [-5.84174156e-01  1.00000000e+00 -1.81806755e+00 ...  6.60698471e+01\n",
      "  -8.20742950e+01  1.35006592e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.05678558e-03  1.00000000e+00 -1.90956879e+00 ... -1.73406509e+02\n",
      "   5.04472427e+01 -6.86136856e+01]\n",
      " [ 5.76686859e-03  1.00000000e+00 -1.90955734e+00 ... -5.01378632e+02\n",
      "  -2.81955688e+02 -5.00478630e+01]\n",
      " [ 5.15937805e-03  1.00000000e+00 -1.90955174e+00 ...  3.00272274e+00\n",
      "  -3.76035666e+00  7.57046366e+00]\n",
      " ...\n",
      " [ 5.33103943e-03  1.00000000e+00 -1.90955734e+00 ... -1.31641569e+01\n",
      "   1.13682724e+02  8.30367470e+00]\n",
      " [ 6.11114502e-03  1.00000000e+00 -1.90956688e+00 ...  8.16190414e+01\n",
      "   1.22960949e+01 -4.41847134e+00]\n",
      " [ 6.26087189e-03  1.00000000e+00 -1.90961075e+00 ... -1.16374954e+02\n",
      "   1.13053314e+02  1.15998718e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5958595     1.           -1.8142986  ...   -2.8465755\n",
      "    63.73498      17.172918  ]\n",
      " [   0.59558773    1.           -1.8143435  ...   61.123474\n",
      "  -199.33517     325.02246   ]\n",
      " [   0.59500694    1.           -1.8145381  ...   18.892622\n",
      "   -52.79806     -20.745045  ]\n",
      " ...\n",
      " [   0.59516907    1.           -1.8144922  ...    2.954598\n",
      "  -133.86586     385.97586   ]\n",
      " [   0.59591484    1.           -1.8142967  ...   94.95057\n",
      "   300.23795    -132.5241    ]\n",
      " [   0.5960531     1.           -1.8142815  ...  150.09499\n",
      "    13.922597   -104.66212   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1273098    1.          -1.5413895 ...   49.49191    -62.32376\n",
      "  -159.36359  ]\n",
      " [   1.1270914    1.          -1.5415678 ... -849.4792     535.4444\n",
      "    68.3563   ]\n",
      " [   1.1265678    1.          -1.5419286 ... -192.62964    313.27652\n",
      "    68.88729  ]\n",
      " ...\n",
      " [   1.1267128    1.          -1.5418472 ... -154.88875     52.99437\n",
      "    19.675732 ]\n",
      " [   1.1273403    1.          -1.5413857 ...  -80.785416    76.52398\n",
      "   -68.80822  ]\n",
      " [   1.1274767    1.          -1.5413189 ...  -53.089027    44.17824\n",
      "    48.558384 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5486298    1.          -1.1173    ...   56.038982    54.832947\n",
      "   -11.068916 ]\n",
      " [   1.5484915    1.          -1.1175709 ...   26.399748  -163.01672\n",
      "   122.396095 ]\n",
      " [   1.5481129    1.          -1.1180557 ... -357.0821     839.4337\n",
      "   -69.66617  ]\n",
      " ...\n",
      " [   1.5482159    1.          -1.1179571 ...  420.38837   -107.00045\n",
      "  -636.02106  ]\n",
      " [   1.5486813    1.          -1.1172924 ...  -86.1779    -775.05164\n",
      "   452.9213   ]\n",
      " [   1.5487642    1.          -1.1171703 ... -139.54733    -59.350822\n",
      "   -33.24611  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8181524e+00  1.0000000e+00 -5.8400917e-01 ...  4.9673832e+01\n",
      "  -1.1420361e+01 -3.2913623e+00]\n",
      " [ 1.8181028e+00  1.0000000e+00 -5.8430386e-01 ... -7.1264168e+01\n",
      "   5.3486261e+02  2.3879662e+02]\n",
      " [ 1.8178883e+00  1.0000000e+00 -5.8489305e-01 ...  1.0902975e+02\n",
      "   1.3323030e+02 -1.6749551e+02]\n",
      " ...\n",
      " [ 1.8179321e+00  1.0000000e+00 -5.8476543e-01 ... -3.8022202e+02\n",
      "  -8.5107513e+02 -7.8793018e+02]\n",
      " [ 1.8181572e+00  1.0000000e+00 -5.8399773e-01 ... -9.2953568e+01\n",
      "   3.2723541e+01 -6.4367966e+01]\n",
      " [ 1.8182240e+00  1.0000000e+00 -5.8384323e-01 ...  3.9509930e+02\n",
      "  -6.3558423e+02  6.6017444e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "ElbowBend\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90963840e+00  1.00000000e+00  6.23130798e-03 ...  1.50626230e+01\n",
      "   1.34778656e+02 -3.78947449e+01]\n",
      " [ 1.90969944e+00  1.00000000e+00  5.90515137e-03 ... -1.70824377e+03\n",
      "   7.41735791e+03 -2.17413818e+03]\n",
      " [ 1.90969658e+00  1.00000000e+00  5.28497156e-03 ... -3.15023766e+01\n",
      "   8.64650879e+01 -1.77605377e+02]\n",
      " ...\n",
      " [ 1.90967941e+00  1.00000000e+00  5.42449951e-03 ... -2.50154190e+02\n",
      "   5.06091614e+02  1.34065598e+02]\n",
      " [ 1.90964508e+00  1.00000000e+00  6.24465942e-03 ... -8.59031906e+01\n",
      "   3.07682285e+01 -1.74939865e+02]\n",
      " [ 1.90963745e+00  1.00000000e+00  6.40678406e-03 ...  8.45865860e+01\n",
      "   1.13710464e+02  2.18286728e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8141146e+00  1.0000000e+00  5.9645844e-01 ...  2.8696237e+02\n",
      "  -8.3733643e+01 -1.5514946e+02]\n",
      " [ 1.8142843e+00  1.0000000e+00  5.9611893e-01 ... -2.9123022e+02\n",
      "   1.6699777e+02 -1.8548669e+02]\n",
      " [ 1.8144684e+00  1.0000000e+00  5.9553450e-01 ...  3.0740366e+02\n",
      "  -1.2174241e+03 -1.4203401e+02]\n",
      " ...\n",
      " [ 1.8143902e+00  1.0000000e+00  5.9566593e-01 ... -5.2651272e+01\n",
      "  -1.3890297e+02  9.8493359e+02]\n",
      " [ 1.8140965e+00  1.0000000e+00  5.9647179e-01 ...  1.5379236e+02\n",
      "   5.7044935e-01 -2.9346048e+02]\n",
      " [ 1.8140469e+00  1.0000000e+00  5.9662056e-01 ... -4.1521011e+01\n",
      "   1.3062212e+02  2.8648016e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5410557    1.           1.1278267 ... -542.54584     40.11302\n",
      "   751.06537  ]\n",
      " [   1.5413132    1.           1.1275511 ... -588.19977   -242.4139\n",
      "   134.10323  ]\n",
      " [   1.5416737    1.           1.1270505 ...   85.25137     34.04212\n",
      "    82.317535 ]\n",
      " ...\n",
      " [   1.541544     1.           1.1271772 ...  519.09564   -273.70914\n",
      "  -248.15799  ]\n",
      " [   1.5410156    1.           1.12784   ...   43.18183     11.330831\n",
      "   -68.12464  ]\n",
      " [   1.5409384    1.           1.127964  ... -201.47798    101.45583\n",
      "  -120.07333  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1170778    1.           1.5488701 ...  -23.06873   -312.30063\n",
      "   164.34389  ]\n",
      " [   1.1174021    1.           1.5486736 ...   27.888918    -1.9027581\n",
      "   -27.562202 ]\n",
      " [   1.1179276    1.           1.5483047 ...  183.07619     21.956089\n",
      "   -16.526018 ]\n",
      " ...\n",
      " [   1.1177692    1.           1.5484133 ...  -78.77846    111.24246\n",
      "    80.770226 ]\n",
      " [   1.1170368    1.           1.5488777 ... -255.39821    -63.500336\n",
      "   268.94846  ]\n",
      " [   1.1169214    1.           1.548954  ...  306.88406   -573.1368\n",
      "  -984.90094  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.8359528e-01  1.0000000e+00  1.8182678e+00 ... -3.2347903e+02\n",
      "   5.9088371e+01 -7.0536423e+01]\n",
      " [ 5.8396435e-01  1.0000000e+00  1.8181953e+00 ... -5.1447467e+02\n",
      "   1.5010500e+02  6.4274316e+02]\n",
      " [ 5.8457756e-01  1.0000000e+00  1.8179992e+00 ...  1.4818137e+02\n",
      "   1.8653893e+02 -5.6600521e+01]\n",
      " ...\n",
      " [ 5.8440208e-01  1.0000000e+00  1.8180628e+00 ... -4.7983837e+01\n",
      "  -6.4486664e+01  7.6788658e+01]\n",
      " [ 5.8354568e-01  1.0000000e+00  1.8182716e+00 ... -2.0500286e+01\n",
      "   3.2397137e+01  5.1421082e+01]\n",
      " [ 5.8341599e-01  1.0000000e+00  1.8182983e+00 ... -3.7366692e+01\n",
      "  -1.2961090e+02  4.3850552e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.8693161e-03  1.0000000e+00  1.9094143e+00 ...  8.5972221e+01\n",
      "  -1.7799519e+02 -1.5032108e+02]\n",
      " [-6.4859390e-03  1.0000000e+00  1.9094563e+00 ... -6.7233435e+02\n",
      "   3.1586127e+02 -3.6125481e+01]\n",
      " [-5.8155060e-03  1.0000000e+00  1.9094578e+00 ...  3.4139273e+02\n",
      "  -1.0694078e+03 -1.8513784e+00]\n",
      " ...\n",
      " [-6.0043335e-03  1.0000000e+00  1.9094543e+00 ...  4.2551193e+02\n",
      "  -2.1024359e+02  9.5982330e+01]\n",
      " [-6.8969727e-03  1.0000000e+00  1.9094143e+00 ... -2.5280470e+02\n",
      "   6.0464249e+01 -3.5168997e+02]\n",
      " [-7.0562363e-03  1.0000000e+00  1.9093990e+00 ... -7.1015312e+01\n",
      "   8.7261444e+01 -7.1753845e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.9673691e-01  1.0000000e+00  1.8137474e+00 ...  1.1018930e+01\n",
      "  -9.6593712e+01  6.6035857e+00]\n",
      " [-5.9637642e-01  1.0000000e+00  1.8139267e+00 ...  2.6195050e+02\n",
      "  -2.1861430e+02 -2.5966072e+01]\n",
      " [-5.9573174e-01  1.0000000e+00  1.8141289e+00 ...  1.9180847e+01\n",
      "   2.8121506e+02  7.4550125e+01]\n",
      " ...\n",
      " [-5.9590912e-01  1.0000000e+00  1.8140650e+00 ...  2.0115681e+02\n",
      "   3.8269070e+02  1.3915038e+01]\n",
      " [-5.9676743e-01  1.0000000e+00  1.8137455e+00 ... -9.9394348e+01\n",
      "   2.1658516e+02  6.0893610e+02]\n",
      " [-5.9691525e-01  1.0000000e+00  1.8136921e+00 ... -5.1253838e+01\n",
      "  -1.3366618e+01 -1.2500154e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1281538    1.           1.5405312 ...  -10.451506    11.421593\n",
      "   -24.501087 ]\n",
      " [  -1.1278458    1.           1.5407801 ... -266.16705   -243.66997\n",
      "   594.1003   ]\n",
      " [  -1.1272831    1.           1.5411851 ... -320.62442   -123.97115\n",
      "   210.92075  ]\n",
      " ...\n",
      " [  -1.127428     1.           1.5410576 ...  -34.259388    -3.0117238\n",
      "   249.88797  ]\n",
      " [  -1.1281643    1.           1.5405178 ...  -78.85242   -115.08192\n",
      "  -172.62819  ]\n",
      " [  -1.1283026    1.           1.5404186 ...  569.5035      77.521126\n",
      "  -356.67062  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5491304    1.           1.116394  ...  -47.10476   -169.70396\n",
      "   166.3877   ]\n",
      " [  -1.5489187    1.           1.1166964 ... -253.1592     171.09233\n",
      "    46.93691  ]\n",
      " [  -1.5484734    1.           1.1172502 ... -118.0561      91.88137\n",
      "  -681.8439   ]\n",
      " ...\n",
      " [  -1.5485744    1.           1.1170826 ...    2.7944388  -47.276268\n",
      "     1.7037082]\n",
      " [  -1.5491505    1.           1.1163731 ...  156.8952     -79.524086\n",
      "   -42.760113 ]\n",
      " [  -1.5492334    1.           1.1162491 ...  113.86903   -156.29913\n",
      "    38.80517  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8182535e+00  1.0000000e+00  5.8324814e-01 ... -1.0224921e+01\n",
      "   2.5804810e+02 -9.5973190e+01]\n",
      " [-1.8181505e+00  1.0000000e+00  5.8360577e-01 ... -3.5347842e+02\n",
      "  -1.2588675e+01 -1.6747644e+01]\n",
      " [-1.8179073e+00  1.0000000e+00  5.8426279e-01 ... -2.2381180e+02\n",
      "   6.2754230e+02  6.0889209e+02]\n",
      " ...\n",
      " [-1.8179512e+00  1.0000000e+00  5.8406448e-01 ... -8.1288628e+01\n",
      "  -1.7233601e+01 -9.5534454e+01]\n",
      " [-1.8182755e+00  1.0000000e+00  5.8322525e-01 ...  1.9527184e+03\n",
      "  -7.0305231e+02 -6.2632129e+02]\n",
      " [-1.8183069e+00  1.0000000e+00  5.8308601e-01 ... -1.4058823e+02\n",
      "   6.5705969e+02  5.3426414e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9095545e+00  1.0000000e+00 -7.4272156e-03 ... -2.8965076e+01\n",
      "  -8.9506092e+00  2.2590452e+01]\n",
      " [-1.9095650e+00  1.0000000e+00 -7.0753098e-03 ...  4.5672754e+02\n",
      "   5.2753607e+02  4.3658487e+02]\n",
      " [-1.9095421e+00  1.0000000e+00 -6.3891839e-03 ...  6.2167487e+02\n",
      "  -8.6558813e+02 -4.9742657e+02]\n",
      " ...\n",
      " [-1.9095154e+00  1.0000000e+00 -6.5946579e-03 ...  1.4322546e+02\n",
      "  -1.9160350e+02  1.4706067e+01]\n",
      " [-1.9095478e+00  1.0000000e+00 -7.4501038e-03 ... -1.2729046e+02\n",
      "   1.8873419e+02 -6.8663811e+01]\n",
      " [-1.9095554e+00  1.0000000e+00 -7.5969696e-03 ... -1.2582025e+03\n",
      "  -1.4221945e+02  4.0868439e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8138304e+00  1.0000000e+00 -5.9719849e-01 ...  2.9090353e+02\n",
      "  -1.2526515e+02 -1.1760585e+02]\n",
      " [-1.8139458e+00  1.0000000e+00 -5.9683704e-01 ...  1.5804477e+02\n",
      "  -3.5906370e+03 -6.0162621e+01]\n",
      " [-1.8141003e+00  1.0000000e+00 -5.9619051e-01 ... -3.2097137e+02\n",
      "  -4.2203250e+02  3.8638947e+02]\n",
      " ...\n",
      " [-1.8140202e+00  1.0000000e+00 -5.9638405e-01 ...  3.0778931e+02\n",
      "   2.5995767e+02 -6.6041272e+02]\n",
      " [-1.8138084e+00  1.0000000e+00 -5.9722137e-01 ... -3.4957397e+01\n",
      "  -3.7936165e+01  9.8640793e+01]\n",
      " [-1.8137684e+00  1.0000000e+00 -5.9736061e-01 ...  1.2014343e+02\n",
      "   1.7948070e+02 -1.3669931e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.54031563e+00  1.00000000e+00 -1.12842941e+00 ...  3.16377075e+02\n",
      "   4.97542839e+01 -6.74114609e+01]\n",
      " [-1.54055214e+00  1.00000000e+00 -1.12808800e+00 ... -2.67137909e+01\n",
      "   4.23824234e+01  2.31304016e+01]\n",
      " [-1.54090500e+00  1.00000000e+00 -1.12754631e+00 ... -1.38999298e+02\n",
      "  -1.66170441e+02  1.27113304e+02]\n",
      " ...\n",
      " [-1.54076004e+00  1.00000000e+00 -1.12771130e+00 ... -9.50205307e+01\n",
      "  -3.14653416e+01  1.09621399e+02]\n",
      " [-1.54030991e+00  1.00000000e+00 -1.12845039e+00 ... -2.60912056e+01\n",
      "   5.45006561e+01 -3.82270264e+02]\n",
      " [-1.54022217e+00  1.00000000e+00 -1.12856865e+00 ...  3.23523132e+02\n",
      "   1.45365942e+03  1.23582733e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.11598969e+00  1.00000000e+00 -1.54924393e+00 ...  1.94008026e+02\n",
      "  -5.73496666e+01  3.09298248e+02]\n",
      " [-1.11631966e+00  1.00000000e+00 -1.54895973e+00 ... -9.19087029e+00\n",
      "   1.56595947e+02  6.73294983e+01]\n",
      " [-1.11678314e+00  1.00000000e+00 -1.54857063e+00 ...  1.30286045e+01\n",
      "  -1.07263931e+02  6.26177254e+01]\n",
      " ...\n",
      " [-1.11660385e+00  1.00000000e+00 -1.54867744e+00 ...  6.25383911e+01\n",
      "   1.98242020e+02 -4.52223969e+00]\n",
      " [-1.11597252e+00  1.00000000e+00 -1.54925728e+00 ... -9.99292676e+03\n",
      "  -4.81083105e+03  4.10586670e+03]\n",
      " [-1.11586380e+00  1.00000000e+00 -1.54933929e+00 ... -3.85275192e+01\n",
      "   4.60206223e+01  3.08040638e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.8217907e-01  1.0000000e+00 -1.8185139e+00 ...  3.8203754e+00\n",
      "   1.0780532e+02  4.7642822e+00]\n",
      " [-5.8256721e-01  1.0000000e+00 -1.8183403e+00 ...  4.3351002e+01\n",
      "   2.6494360e+02 -4.3421802e+02]\n",
      " [-5.8312607e-01  1.0000000e+00 -1.8181268e+00 ... -2.0922117e+04\n",
      "   1.2286518e+04 -6.8503877e+03]\n",
      " ...\n",
      " [-5.8292198e-01  1.0000000e+00 -1.8181839e+00 ...  5.0260910e+02\n",
      "  -2.7531009e+02 -3.2938232e+03]\n",
      " [-5.8216286e-01  1.0000000e+00 -1.8185196e+00 ...  4.4519714e+02\n",
      "   1.7674949e+03  1.1963710e+03]\n",
      " [-5.8203983e-01  1.0000000e+00 -1.8185673e+00 ...  5.7729409e+03\n",
      "  -7.1474912e+03 -5.1028623e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.3103180e-03  1.0000000e+00 -1.9094734e+00 ...  1.2817786e+02\n",
      "  -5.4999219e+02 -3.4746997e+02]\n",
      " [ 7.9097748e-03  1.0000000e+00 -1.9094152e+00 ...  1.7102759e+02\n",
      "  -8.8139038e+01  3.5988777e+01]\n",
      " [ 7.3394775e-03  1.0000000e+00 -1.9094112e+00 ... -1.5246730e+02\n",
      "  -1.2993922e+01 -1.3284947e+02]\n",
      " ...\n",
      " [ 7.5550079e-03  1.0000000e+00 -1.9094019e+00 ... -1.5899379e+02\n",
      "  -4.5733841e+02 -2.4855702e+02]\n",
      " [ 8.3580017e-03  1.0000000e+00 -1.9094753e+00 ... -4.4311203e+02\n",
      "  -4.8523950e+02 -9.8372925e+02]\n",
      " [ 8.4552765e-03  1.0000000e+00 -1.9094887e+00 ... -9.7380638e+00\n",
      "   1.0404077e+02  3.5462330e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9818459e-01  1.0000000e+00 -1.8134575e+00 ...  1.1658173e+02\n",
      "  -1.6109114e+02 -2.3233353e+02]\n",
      " [ 5.9781456e-01  1.0000000e+00 -1.8135214e+00 ... -1.2803816e+03\n",
      "   4.8197642e+03  1.4124515e+03]\n",
      " [ 5.9725761e-01  1.0000000e+00 -1.8137110e+00 ... -1.4876459e+02\n",
      "   2.5232149e+02  2.3609621e+02]\n",
      " ...\n",
      " [ 5.9745979e-01  1.0000000e+00 -1.8136301e+00 ... -7.8581750e+02\n",
      "  -2.9665500e+02  4.2849915e+02]\n",
      " [ 5.9824181e-01  1.0000000e+00 -1.8134518e+00 ...  8.9228668e+02\n",
      "  -1.7732026e+03 -1.3822587e+03]\n",
      " [ 5.9832191e-01  1.0000000e+00 -1.8134079e+00 ... -1.3921461e+00\n",
      "  -2.7113924e+01 -5.5631531e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1291037    1.          -1.5400543 ... -170.68037     21.417263\n",
      "     6.4936943]\n",
      " [   1.1287842    1.          -1.5402384 ... -193.47446    132.2191\n",
      "  -475.27875  ]\n",
      " [   1.1283112    1.          -1.5405874 ...  -83.41532    231.39343\n",
      "  -113.9044   ]\n",
      " ...\n",
      " [   1.1284904    1.          -1.5404577 ... -912.3988    -161.92055\n",
      "  -426.2428   ]\n",
      " [   1.1291466    1.          -1.5400429 ... -124.20569   -275.43747\n",
      "  -368.62054  ]\n",
      " [   1.1292181    1.          -1.5399685 ...  -76.41873     68.35161\n",
      "    59.52909  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5499458    1.          -1.1155434 ... -174.87578     59.344917\n",
      "   -24.784868 ]\n",
      " [   1.5497246    1.          -1.1158371 ...   16.621033   223.00127\n",
      "    64.84882  ]\n",
      " [   1.5493965    1.          -1.1163304 ...  -10.726082  -122.6653\n",
      "    35.98081  ]\n",
      " ...\n",
      " [   1.5495129    1.          -1.1161442 ...  334.09998    -74.83038\n",
      "   459.3526   ]\n",
      " [   1.5499802    1.          -1.1155224 ...   15.457821   186.35101\n",
      "   164.66168  ]\n",
      " [   1.5500298    1.          -1.1154346 ...   26.909443    48.963955\n",
      "   -39.1966   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81889153e+00  1.00000000e+00 -5.81922531e-01 ...  2.55461731e+01\n",
      "  -1.05403786e+02 -1.47560244e+01]\n",
      " [ 1.81876564e+00  1.00000000e+00 -5.82242012e-01 ...  3.30795624e+02\n",
      "  -2.00509232e+02  4.26277618e+02]\n",
      " [ 1.81859398e+00  1.00000000e+00 -5.82824767e-01 ... -2.95039654e+01\n",
      "  -2.03920307e+01 -1.29742188e+02]\n",
      " ...\n",
      " [ 1.81864929e+00  1.00000000e+00 -5.82602501e-01 ... -5.42886963e+02\n",
      "   4.20908241e+01 -7.91289124e+02]\n",
      " [ 1.81888580e+00  1.00000000e+00 -5.81895828e-01 ...  9.44191208e+01\n",
      "   1.06241852e+02 -3.71671677e+01]\n",
      " [ 1.81894016e+00  1.00000000e+00 -5.81790924e-01 ... -3.98764763e+01\n",
      "   3.10300636e+01  5.03649826e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9097118e+00  1.0000000e+00  8.5372925e-03 ...  8.4920139e+00\n",
      "   3.7500140e+02 -9.9865730e+01]\n",
      " [ 1.9096975e+00  1.0000000e+00  8.1815720e-03 ... -6.2847400e+02\n",
      "   1.3601365e+03  6.2273999e+02]\n",
      " [ 1.9097290e+00  1.0000000e+00  7.5796302e-03 ... -8.9331760e+00\n",
      "  -1.4993636e+02  1.0344847e+02]\n",
      " ...\n",
      " [ 1.9097099e+00  1.0000000e+00  7.7991486e-03 ...  1.2860275e+02\n",
      "  -2.8462042e+02  4.0098367e+02]\n",
      " [ 1.9097080e+00  1.0000000e+00  8.5659027e-03 ...  5.8386822e+01\n",
      "   7.7490311e+01  6.0670341e+01]\n",
      " [ 1.9097195e+00  1.0000000e+00  8.6746216e-03 ... -4.9371619e+00\n",
      "   9.8256731e-01  4.1825085e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81333733e+00  1.00000000e+00  5.98621368e-01 ...  1.05327187e+02\n",
      "  -4.69441803e+02  3.52158142e+02]\n",
      " [ 1.81344032e+00  1.00000000e+00  5.98260880e-01 ... -1.14429924e+02\n",
      "   4.56338226e+02  9.36370621e+01]\n",
      " [ 1.81365585e+00  1.00000000e+00  5.97686410e-01 ... -5.47588272e+01\n",
      "  -1.95143753e+02  8.52116585e+00]\n",
      " ...\n",
      " [ 1.81356239e+00  1.00000000e+00  5.97892761e-01 ... -1.50719559e+02\n",
      "   2.06185211e+02 -3.68620850e+02]\n",
      " [ 1.81333923e+00  1.00000000e+00  5.98648071e-01 ... -8.75320892e+01\n",
      "   2.73899021e+01  3.73765839e+02]\n",
      " [ 1.81329536e+00  1.00000000e+00  5.98749161e-01 ...  2.77559204e+02\n",
      "   1.09279503e+02  1.48991357e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5397091    1.           1.1295643 ... -161.10446    -16.50634\n",
      "  -509.41443  ]\n",
      " [   1.5399141    1.           1.1292458 ...  682.2152    -196.00612\n",
      "   398.05676  ]\n",
      " [   1.5402889    1.           1.1287574 ...  230.35812    206.01743\n",
      "  -169.4776   ]\n",
      " ...\n",
      " [   1.5401592    1.           1.128933  ...   62.583527   -22.874477\n",
      "   125.5272   ]\n",
      " [   1.5397053    1.           1.1295815 ...   59.305805   126.43228\n",
      "    -6.6306653]\n",
      " [   1.5396242    1.           1.1296711 ... -133.57587     -2.2451348\n",
      "   -21.218103 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1150103    1.           1.5503254 ...  108.20078    -54.59846\n",
      "    82.34826  ]\n",
      " [   1.115303     1.           1.5500593 ... -161.42426   -326.26096\n",
      "   172.71371  ]\n",
      " [   1.1158562    1.           1.5497159 ...  533.5818     157.92355\n",
      "   -69.105385 ]\n",
      " ...\n",
      " [   1.1156902    1.           1.5498352 ...   24.196707   -40.536797\n",
      "    69.80639  ]\n",
      " [   1.1150627    1.           1.5503349 ...   37.458096   -84.8736\n",
      "   -29.363884 ]\n",
      " [   1.1148939    1.           1.5503922 ...  -21.728874    82.97347\n",
      "   -58.836098 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.81641197e-01  1.00000000e+00  1.81896400e+00 ...  1.02334183e+02\n",
      "   4.93174629e+01  4.01496849e+01]\n",
      " [ 5.81983566e-01  1.00000000e+00  1.81877327e+00 ... -7.26844482e+02\n",
      "   3.22488159e+02 -1.12033142e+02]\n",
      " [ 5.82641602e-01  1.00000000e+00  1.81860507e+00 ...  3.18915520e+01\n",
      "  -1.81287289e+01  3.57105675e+01]\n",
      " ...\n",
      " [ 5.82443237e-01  1.00000000e+00  1.81865406e+00 ...  1.62944092e+02\n",
      "  -1.09396088e+02  6.63356247e+01]\n",
      " [ 5.81703186e-01  1.00000000e+00  1.81896782e+00 ... -9.18900681e+01\n",
      "  -1.14276176e+02 -1.82184402e+02]\n",
      " [ 5.81504822e-01  1.00000000e+00  1.81898117e+00 ... -4.73094978e+01\n",
      "   9.18322296e+01 -1.00392975e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.1562271e-03  1.0000000e+00  1.9097023e+00 ...  2.0052348e+02\n",
      "   3.4451622e+01  2.2284000e+01]\n",
      " [-8.7938309e-03  1.0000000e+00  1.9095755e+00 ...  1.3176974e+02\n",
      "   1.7268661e+01  9.1293053e+01]\n",
      " [-8.1405640e-03  1.0000000e+00  1.9096086e+00 ... -4.6652115e+02\n",
      "   4.9659412e+02 -1.6963087e+02]\n",
      " ...\n",
      " [-8.3503723e-03  1.0000000e+00  1.9095812e+00 ... -9.7460040e+02\n",
      "  -6.3469653e+02  2.4045775e+02]\n",
      " [-9.1362000e-03  1.0000000e+00  1.9097004e+00 ...  2.1229391e+01\n",
      "   2.0002239e+00  9.2615223e+00]\n",
      " [-9.2992783e-03  1.0000000e+00  1.9096622e+00 ... -1.7743137e+00\n",
      "  -7.0446869e+01  4.8289852e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.59886074    1.            1.813364   ...  -61.32896\n",
      "  -167.59001     280.81418   ]\n",
      " [  -0.59851456    1.            1.8133879  ...  114.51767\n",
      "  -333.77637    -157.54236   ]\n",
      " [  -0.59793854    1.            1.813603   ...  -93.75944\n",
      "    40.77466      98.0121    ]\n",
      " ...\n",
      " [  -0.5981426     1.            1.8135042  ...   32.988224\n",
      "     9.563856   -113.007675  ]\n",
      " [  -0.59889793    1.            1.8133545  ...  -63.086018\n",
      "   479.45807    -407.7047    ]\n",
      " [  -0.59899235    1.            1.8132877  ... -437.58936\n",
      "   -99.865715    157.4247    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1300936    1.           1.5393791 ...  -87.730675  -174.12042\n",
      "   -75.66992  ]\n",
      " [  -1.1298075    1.           1.539525  ...   85.7478     251.69975\n",
      "   181.4565   ]\n",
      " [  -1.1293316    1.           1.5399116 ...  -53.19681    -30.426775\n",
      "  -125.204636 ]\n",
      " ...\n",
      " [  -1.1295071    1.           1.5397482 ...   84.57918    -54.621758\n",
      "   -34.887215 ]\n",
      " [  -1.1301537    1.           1.5393658 ...   66.329445  -407.94095\n",
      "    83.53958  ]\n",
      " [  -1.1302109    1.           1.539278  ...  -27.672516  -245.75296\n",
      "   -83.76395  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5504999    1.           1.1148129 ...  322.9358      16.034063\n",
      "    92.81133  ]\n",
      " [  -1.5503025    1.           1.1150293 ... -156.4178     136.37099\n",
      "   -78.93514  ]\n",
      " [  -1.5499516    1.           1.1155468 ...  -90.4199     218.14394\n",
      "  -101.062874 ]\n",
      " ...\n",
      " [  -1.5500851    1.           1.1153383 ...  -99.1697     -56.042316\n",
      "   -87.09959  ]\n",
      " [  -1.5505257    1.           1.1147938 ...   -1.224928     2.4904895\n",
      "  -186.0157   ]\n",
      " [  -1.550581     1.           1.1146774 ...  274.38483     74.80573\n",
      "   230.71701  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8191366e+00  1.0000000e+00  5.8099937e-01 ... -6.7575946e+02\n",
      "   6.5301959e+02 -3.3291385e+02]\n",
      " [-1.8190451e+00  1.0000000e+00  5.8124638e-01 ... -7.1539996e+02\n",
      "   1.5197801e+01 -1.3819843e+02]\n",
      " [-1.8188667e+00  1.0000000e+00  5.8186311e-01 ...  9.5904999e+00\n",
      "   7.7727959e+01  7.2484077e+01]\n",
      " ...\n",
      " [-1.8189411e+00  1.0000000e+00  5.8160877e-01 ...  5.0057227e+02\n",
      "  -1.0819774e+01  7.6144531e+01]\n",
      " [-1.8191509e+00  1.0000000e+00  5.8096886e-01 ...  3.4774350e+02\n",
      "   2.1783844e+02 -2.2594568e+01]\n",
      " [-1.8191776e+00  1.0000000e+00  5.8083344e-01 ...  4.9043312e+00\n",
      "   2.1929319e+01  3.8076691e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9096231e+00  1.0000000e+00 -9.5329285e-03 ... -2.8745462e+02\n",
      "  -8.5058856e+02 -7.3090448e+02]\n",
      " [-1.9096317e+00  1.0000000e+00 -9.2611313e-03 ... -3.8800949e+01\n",
      "  -8.3516888e+02  4.7263467e+02]\n",
      " [-1.9096603e+00  1.0000000e+00 -8.6068278e-03 ... -7.4042091e+01\n",
      "  -2.0383835e+01  3.8648003e+01]\n",
      " ...\n",
      " [-1.9096451e+00  1.0000000e+00 -8.8634491e-03 ...  1.6218533e+01\n",
      "   1.7204935e+01 -2.6337467e+01]\n",
      " [-1.9096031e+00  1.0000000e+00 -9.5653534e-03 ...  2.8332949e+01\n",
      "   2.9060875e+01 -1.1874621e+02]\n",
      " [-1.9096107e+00  1.0000000e+00 -9.7064972e-03 ... -1.6713047e+01\n",
      "   2.2398775e+01 -1.6131142e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.1       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8132191e+00  1.0000000e+00 -5.9917450e-01 ...  7.1179272e+02\n",
      "   2.1377959e+03 -1.7372116e+02]\n",
      " [-1.8133287e+00  1.0000000e+00 -5.9892464e-01 ...  6.6450035e+01\n",
      "  -1.5712463e+02 -2.3501920e+02]\n",
      " [-1.8135281e+00  1.0000000e+00 -5.9830451e-01 ... -1.4028703e+02\n",
      "   3.9786415e+01  5.3511877e+00]\n",
      " ...\n",
      " [-1.8134212e+00  1.0000000e+00 -5.9854317e-01 ...  8.3944473e+00\n",
      "  -7.6544456e+00  4.1329036e+00]\n",
      " [-1.8131523e+00  1.0000000e+00 -5.9920502e-01 ... -6.5048995e+00\n",
      "   5.0270752e+01 -2.5019726e+01]\n",
      " [-1.8131599e+00  1.0000000e+00 -5.9933472e-01 ...  3.2022723e+02\n",
      "   1.5757086e+02 -5.6119366e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5393009    1.          -1.1301231 ...    8.767328    43.315983\n",
      "   -24.22979  ]\n",
      " [  -1.5395002    1.          -1.1299486 ... -421.03043    891.4946\n",
      "   437.02884  ]\n",
      " [  -1.5398445    1.          -1.1294236 ...   50.154747   -30.533558\n",
      "   -58.3268   ]\n",
      " ...\n",
      " [  -1.539669     1.          -1.129631  ...   38.32745     38.350887\n",
      "   -28.199965 ]\n",
      " [  -1.5392017    1.          -1.130146  ...  -64.25295     46.78198\n",
      "   182.37344  ]\n",
      " [  -1.5391912    1.          -1.1302547 ...  -98.422325    77.386734\n",
      "  -314.9213   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.11468697e+00  1.00000000e+00 -1.55047798e+00 ... -3.62979507e+01\n",
      "   1.61696625e+02 -1.02810707e+02]\n",
      " [-1.11494446e+00  1.00000000e+00 -1.55035973e+00 ... -2.64224365e+02\n",
      "  -5.26439819e+02 -4.83313110e+02]\n",
      " [-1.11542702e+00  1.00000000e+00 -1.54998195e+00 ...  4.17025681e+01\n",
      "  -6.56907883e+01  4.35642548e+01]\n",
      " ...\n",
      " [-1.11520576e+00  1.00000000e+00 -1.55013943e+00 ...  4.10193672e+01\n",
      "  -6.81477499e+00 -8.48483658e+01]\n",
      " [-1.11456680e+00  1.00000000e+00 -1.55048561e+00 ...  1.97149639e+01\n",
      "  -1.00575005e+02  1.20173779e+01]\n",
      " [-1.11453819e+00  1.00000000e+00 -1.55056953e+00 ...  1.10251343e+03\n",
      "  -5.59363098e+02  7.40491257e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.80947876e-01  1.00000000e+00 -1.81911087e+00 ... -9.26894043e+02\n",
      "  -5.77123840e+02  1.32657288e+03]\n",
      " [-5.81246376e-01  1.00000000e+00 -1.81900883e+00 ... -1.83596237e+02\n",
      "  -2.01329056e+02  9.90596294e+00]\n",
      " [-5.81830978e-01  1.00000000e+00 -1.81881344e+00 ...  5.68968582e+01\n",
      "   1.05070625e+02  1.34903198e+02]\n",
      " ...\n",
      " [-5.81579208e-01  1.00000000e+00 -1.81887913e+00 ...  4.32885933e+00\n",
      "   4.96088028e+01 -2.03034248e+01]\n",
      " [-5.80820084e-01  1.00000000e+00 -1.81909561e+00 ... -1.05945251e+02\n",
      "   1.21907644e+01  2.93453693e+01]\n",
      " [-5.80776215e-01  1.00000000e+00 -1.81913757e+00 ... -2.81051788e+02\n",
      "   9.37003021e+01 -3.94442078e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.8733902e-03  1.0000000e+00 -1.9095898e+00 ...  9.3056778e+01\n",
      "   2.1880011e+01  4.1478488e+02]\n",
      " [ 9.5624924e-03  1.0000000e+00 -1.9095497e+00 ...  2.7293832e+02\n",
      "  -1.1429126e+03  1.6206986e+03]\n",
      " [ 8.9492798e-03  1.0000000e+00 -1.9095534e+00 ... -4.2488284e+02\n",
      "   7.4774396e+02  3.8056155e+02]\n",
      " ...\n",
      " [ 9.2182159e-03  1.0000000e+00 -1.9095297e+00 ...  7.0556557e+01\n",
      "   1.5519098e+01 -5.1165363e+01]\n",
      " [ 1.0005951e-02  1.0000000e+00 -1.9095497e+00 ... -2.3637816e+02\n",
      "  -4.3208221e+01 -3.0887390e+02]\n",
      " [ 1.0055542e-02  1.0000000e+00 -1.9095631e+00 ... -3.1892587e+02\n",
      "   3.5675659e+02 -1.0118540e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.9937096e-01  1.0000000e+00 -1.8130436e+00 ... -2.8202634e+02\n",
      "  -1.7342183e+03 -7.7301674e+01]\n",
      " [ 5.9907246e-01  1.0000000e+00 -1.8131781e+00 ...  1.2286993e+02\n",
      "  -4.2060043e+01 -4.2665912e+02]\n",
      " [ 5.9848022e-01  1.0000000e+00 -1.8133457e+00 ...  2.0843118e+02\n",
      "   1.6000272e+02 -2.0644579e+02]\n",
      " ...\n",
      " [ 5.9873199e-01  1.0000000e+00 -1.8132534e+00 ...  8.6440033e+01\n",
      "   1.1584633e+02 -2.2213444e+01]\n",
      " [ 5.9948540e-01  1.0000000e+00 -1.8129902e+00 ...  1.4787566e+03\n",
      "   3.0421634e+02 -4.4446182e+02]\n",
      " [ 5.9954929e-01  1.0000000e+00 -1.8129787e+00 ... -1.2757459e+01\n",
      "  -1.4468631e+02  8.2515869e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1305084    1.          -1.538702  ...   79.725     -170.39029\n",
      "  -161.86835  ]\n",
      " [   1.1302586    1.          -1.5389042 ...  -67.995476   179.3147\n",
      "    -1.3058844]\n",
      " [   1.1297588    1.          -1.5392413 ...   18.398285    33.821503\n",
      "     3.5804136]\n",
      " ...\n",
      " [   1.1299782    1.          -1.5390759 ...  125.83389     67.633606\n",
      "    83.879745 ]\n",
      " [   1.1306286    1.          -1.5386238 ... -134.0814      20.148481\n",
      "   -72.39969  ]\n",
      " [   1.1306629    1.          -1.53858   ...    7.944203   -42.055824\n",
      "   -61.79051  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5507526    1.          -1.1139374 ...  -20.465914   -13.95014\n",
      "   150.43893  ]\n",
      " [   1.5505857    1.          -1.1142206 ...  -39.781727    34.05033\n",
      "   -28.023457 ]\n",
      " [   1.5502148    1.          -1.1146808 ...  356.3524     -34.28731\n",
      "   214.69377  ]\n",
      " ...\n",
      " [   1.5503654    1.          -1.114459  ...   28.129126  -231.3071\n",
      "   -46.138798 ]\n",
      " [   1.5508633    1.          -1.1138287 ...  -93.70942   -343.79193\n",
      "  -254.60583  ]\n",
      " [   1.5508728    1.          -1.1137695 ...  283.0728     -73.6666\n",
      "    72.17198  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8190603e+00  1.0000000e+00 -5.8021164e-01 ... -9.8011780e+01\n",
      "  -5.8188763e+00  1.3820326e+02]\n",
      " [ 1.8189745e+00  1.0000000e+00 -5.8050251e-01 ...  2.4235716e+01\n",
      "  -6.4976250e+01  5.2922398e+01]\n",
      " [ 1.8187981e+00  1.0000000e+00 -5.8105165e-01 ...  4.9962173e+02\n",
      "   2.7504956e+02 -7.4168640e+02]\n",
      " ...\n",
      " [ 1.8188686e+00  1.0000000e+00 -5.8078098e-01 ...  1.6870819e+02\n",
      "   3.1430304e+02  6.1907787e+01]\n",
      " [ 1.8191757e+00  1.0000000e+00 -5.8009338e-01 ...  1.5588670e+02\n",
      "   8.2383652e+01 -3.5409677e+02]\n",
      " [ 1.8191261e+00  1.0000000e+00 -5.8002281e-01 ... -3.5276688e+02\n",
      "  -6.2121582e+01  2.9908536e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9092264e+00  1.0000000e+00  1.0774612e-02 ... -3.4107297e+02\n",
      "   3.9635104e+02  1.4318813e+02]\n",
      " [ 1.9092283e+00  1.0000000e+00  1.0480881e-02 ... -4.7017899e+01\n",
      "  -7.0034401e+01 -4.1328629e+01]\n",
      " [ 1.9092293e+00  1.0000000e+00  9.9121649e-03 ... -7.1534229e+02\n",
      "   1.4674742e+03  7.4157074e+02]\n",
      " ...\n",
      " [ 1.9092140e+00  1.0000000e+00  1.0190010e-02 ...  3.5035156e+01\n",
      "   1.8449337e+00  4.8419060e+01]\n",
      " [ 1.9092865e+00  1.0000000e+00  1.0896683e-02 ... -7.4370903e+01\n",
      "  -2.1281590e+02  7.8484726e+01]\n",
      " [ 1.9092283e+00  1.0000000e+00  1.0971069e-02 ...  8.2673149e+01\n",
      "  -2.1084239e+02  2.4904956e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8125315e+00  1.0000000e+00  6.0019684e-01 ... -2.3593132e+02\n",
      "  -1.7332181e+02  1.7541225e+02]\n",
      " [ 1.8126316e+00  1.0000000e+00  5.9990311e-01 ... -3.1112428e+04\n",
      "  -2.3368930e+04 -3.8823828e+04]\n",
      " [ 1.8127995e+00  1.0000000e+00  5.9937662e-01 ... -6.8360468e+02\n",
      "  -3.9754068e+02  1.7814908e+02]\n",
      " ...\n",
      " [ 1.8127060e+00  1.0000000e+00  5.9963036e-01 ... -2.5127672e+01\n",
      "  -1.7056419e+02  6.2042076e+01]\n",
      " [ 1.8125381e+00  1.0000000e+00  6.0031128e-01 ... -3.8895382e+01\n",
      "  -1.3598067e+02 -7.8248611e+01]\n",
      " [ 1.8124762e+00  1.0000000e+00  6.0038376e-01 ...  1.7510544e+01\n",
      "   1.3955217e+01 -8.8166351e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.53835297e+00  1.00000000e+00  1.13110161e+00 ... -1.11750040e+01\n",
      "  -2.18580551e+01 -1.10276957e+01]\n",
      " [ 1.53853703e+00  1.00000000e+00  1.13085079e+00 ... -7.21227539e+03\n",
      "  -4.06308105e+03 -1.46101631e+04]\n",
      " [ 1.53887749e+00  1.00000000e+00  1.13040566e+00 ... -3.71020416e+02\n",
      "   3.22651825e+01  6.29915314e+01]\n",
      " ...\n",
      " [ 1.53869057e+00  1.00000000e+00  1.13062191e+00 ...  3.94077332e+02\n",
      "  -8.79488564e+00  1.49977570e+02]\n",
      " [ 1.53831100e+00  1.00000000e+00  1.13120842e+00 ... -1.20207809e+02\n",
      "   5.89308411e+02 -3.33112274e+02]\n",
      " [ 1.53825569e+00  1.00000000e+00  1.13126755e+00 ... -1.03305725e+02\n",
      "   6.53535156e+01  8.38945160e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1133232e+00  1.0000000e+00  1.5513649e+00 ...  2.4368936e+03\n",
      "   1.7047301e+02 -2.8184980e+03]\n",
      " [ 1.1135740e+00  1.0000000e+00  1.5511618e+00 ...  2.5749968e+03\n",
      "  -2.0425636e+02 -2.9854927e+03]\n",
      " [ 1.1140556e+00  1.0000000e+00  1.5508575e+00 ... -3.4324408e+02\n",
      "  -1.9148746e+02  7.8856903e+01]\n",
      " ...\n",
      " [ 1.1137924e+00  1.0000000e+00  1.5509911e+00 ... -8.3071487e+01\n",
      "   3.0035555e+01 -1.6793794e+02]\n",
      " [ 1.1132584e+00  1.0000000e+00  1.5514469e+00 ... -2.4242162e+02\n",
      "   7.7442230e+01  2.7011926e+02]\n",
      " [ 1.1131945e+00  1.0000000e+00  1.5514889e+00 ...  2.2588846e+01\n",
      "   2.2896901e+02  2.6530414e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.79326630e-01  1.00000000e+00  1.81962585e+00 ... -8.30252502e+02\n",
      "   2.08807312e+02  6.95811096e+02]\n",
      " [ 5.79627037e-01  1.00000000e+00  1.81947803e+00 ... -2.31083276e+03\n",
      "   1.95905981e+03  8.39819238e+03]\n",
      " [ 5.80196381e-01  1.00000000e+00  1.81934106e+00 ... -5.98398926e+02\n",
      "  -5.15834778e+02 -1.19482506e+02]\n",
      " ...\n",
      " [ 5.79889297e-01  1.00000000e+00  1.81939697e+00 ...  4.14946594e+02\n",
      "   1.25035973e+02 -2.51710251e+02]\n",
      " [ 5.79254150e-01  1.00000000e+00  1.81967545e+00 ... -6.44800568e+01\n",
      "   1.94384949e+02  1.59923162e+01]\n",
      " [ 5.79168320e-01  1.00000000e+00  1.81968689e+00 ...  7.73523331e+00\n",
      "  -1.79164009e+01  2.14036160e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1154175e-02  1.0000000e+00  1.9094925e+00 ...  4.3175797e+01\n",
      "  -5.5433964e+01 -1.5887140e+01]\n",
      " [-1.0838509e-02  1.0000000e+00  1.9094296e+00 ...  2.8486973e+03\n",
      "  -1.3245972e+03 -2.3492769e+03]\n",
      " [-1.0192871e-02  1.0000000e+00  1.9094621e+00 ... -2.1315414e+01\n",
      "   2.8709793e+02  4.0185645e+02]\n",
      " ...\n",
      " [-1.0519028e-02  1.0000000e+00  1.9094296e+00 ...  1.4243875e+02\n",
      "  -1.4872906e+02 -2.6121143e+02]\n",
      " [-1.1190414e-02  1.0000000e+00  1.9095116e+00 ...  6.2594181e+01\n",
      "  -1.1013271e+02  1.4405927e+02]\n",
      " [-1.1315346e-02  1.0000000e+00  1.9095135e+00 ...  8.4082191e+01\n",
      "  -5.0495472e+01 -3.1122005e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.0119629e-01  1.0000000e+00  1.8124886e+00 ... -4.1887374e+00\n",
      "   2.7885495e+02  1.7918936e+02]\n",
      " [-6.0089588e-01  1.0000000e+00  1.8125162e+00 ...  5.7790186e+02\n",
      "  -8.6552325e+02  7.6554077e+02]\n",
      " [-6.0030365e-01  1.0000000e+00  1.8127332e+00 ...  1.6385001e+02\n",
      "  -1.5615505e+01 -4.4728714e+02]\n",
      " ...\n",
      " [-6.0060120e-01  1.0000000e+00  1.8126106e+00 ...  2.0246349e+01\n",
      "  -4.5047722e+01  3.6210289e+01]\n",
      " [-6.0124016e-01  1.0000000e+00  1.8124619e+00 ... -1.1313341e+00\n",
      "  -8.6214439e+01  1.4076102e+00]\n",
      " [-6.0134888e-01  1.0000000e+00  1.8124390e+00 ...  2.4990940e+01\n",
      "  -5.0574078e+01 -2.7488691e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1316042e+00  1.0000000e+00  1.5380802e+00 ... -3.0211731e+02\n",
      "   5.1591877e+01  2.4594769e+01]\n",
      " [-1.1313486e+00  1.0000000e+00  1.5382071e+00 ... -1.0670034e+01\n",
      "  -4.1618570e+02  2.3002759e+02]\n",
      " [-1.1308784e+00  1.0000000e+00  1.5386007e+00 ...  6.4246716e+02\n",
      "   4.1572638e+02 -2.6594878e+03]\n",
      " ...\n",
      " [-1.1311340e+00  1.0000000e+00  1.5384045e+00 ...  9.0609684e+00\n",
      "   3.6401012e+01 -1.4076196e+02]\n",
      " [-1.1316643e+00  1.0000000e+00  1.5380249e+00 ...  9.3075096e+01\n",
      "   1.0126283e+02 -4.2625922e+02]\n",
      " [-1.1317301e+00  1.0000000e+00  1.5379829e+00 ... -1.4973624e+02\n",
      "  -7.7227776e+01  2.0620776e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.55173016e+00  1.00000000e+00  1.11292648e+00 ...  1.22963425e+02\n",
      "   1.68873444e+02 -1.71102219e+02]\n",
      " [-1.55154419e+00  1.00000000e+00  1.11313438e+00 ...  7.73864197e+02\n",
      "  -8.73682434e+02 -4.20230789e+01]\n",
      " [-1.55116463e+00  1.00000000e+00  1.11364448e+00 ... -1.05852393e+03\n",
      "  -5.49016699e+03  2.89547778e+03]\n",
      " ...\n",
      " [-1.55136108e+00  1.00000000e+00  1.11339283e+00 ...  5.72046738e+01\n",
      "   8.25286865e+00  1.03849297e+02]\n",
      " [-1.55177116e+00  1.00000000e+00  1.11284447e+00 ...  2.24512711e+02\n",
      "   3.71456909e+01  7.57065773e+00]\n",
      " [-1.55181217e+00  1.00000000e+00  1.11277008e+00 ...  1.74571960e+02\n",
      "   3.40443535e+01  1.61835251e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8198032e+00  1.0000000e+00  5.7870865e-01 ... -6.1512268e+01\n",
      "   1.1301200e+02  8.0327919e+01]\n",
      " [-1.8197021e+00  1.0000000e+00  5.7896614e-01 ... -7.5720154e+01\n",
      "   1.0452611e+02 -1.2692329e+01]\n",
      " [-1.8194675e+00  1.0000000e+00  5.7955384e-01 ...  9.2426740e+02\n",
      "  -3.5370574e+03  3.1058086e+03]\n",
      " ...\n",
      " [-1.8195763e+00  1.0000000e+00  5.7926655e-01 ... -3.7959328e+01\n",
      "   1.8629678e+02  2.8549283e+02]\n",
      " [-1.8198128e+00  1.0000000e+00  5.7861519e-01 ... -2.9377911e+01\n",
      "  -3.7650394e+01  1.1958748e+01]\n",
      " [-1.8198271e+00  1.0000000e+00  5.7853127e-01 ...  2.6758560e+02\n",
      "   1.8374834e+01  6.7536591e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9096222e+00  1.0000000e+00 -1.1890411e-02 ...  1.9720406e+01\n",
      "  -6.2647766e+01 -7.4752636e+00]\n",
      " [-1.9096174e+00  1.0000000e+00 -1.1610031e-02 ...  8.7983044e+02\n",
      "  -1.1897861e+03 -1.2235699e+03]\n",
      " [-1.9095478e+00  1.0000000e+00 -1.0996194e-02 ...  2.1080291e+03\n",
      "  -8.3635254e+02  1.1471022e+03]\n",
      " ...\n",
      " [-1.9095612e+00  1.0000000e+00 -1.1291504e-02 ... -9.6452026e+00\n",
      "   8.8235312e+00  1.1965075e+01]\n",
      " [-1.9096031e+00  1.0000000e+00 -1.1987686e-02 ...  1.0148975e+02\n",
      "  -2.3094170e+00 -2.1753751e+02]\n",
      " [-1.9095936e+00  1.0000000e+00 -1.2079239e-02 ... -1.7529828e+02\n",
      "   2.6923300e+02  2.0072533e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8125324     1.           -0.6013546  ...  -10.731529\n",
      "   -65.779976    -85.90182   ]\n",
      " [  -1.8126202     1.           -0.60110855 ... -298.68414\n",
      "    45.747147    337.36758   ]\n",
      " [  -1.8127289     1.           -0.6005124  ... -243.19743\n",
      "  -188.05077    -331.1516    ]\n",
      " ...\n",
      " [  -1.8126602     1.           -0.6008053  ...   67.32062\n",
      "    -6.259498   -328.3554    ]\n",
      " [  -1.8124466     1.           -0.6014519  ...  112.476395\n",
      "   124.41892     163.62733   ]\n",
      " [  -1.8124533     1.           -0.6015377  ...  -57.35119\n",
      "    59.352146     16.522617  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.53794      1.          -1.132143  ...    1.8663633  -18.705183\n",
      "    -3.6164014]\n",
      " [  -1.5381031    1.          -1.1319551 ... -213.03558    -23.872778\n",
      "   -88.32151  ]\n",
      " [  -1.5383682    1.          -1.1314368 ...  227.1321     -65.15807\n",
      "  -247.73158  ]\n",
      " ...\n",
      " [  -1.5382156    1.          -1.1316977 ... -278.44687   -204.47948\n",
      "    64.94638  ]\n",
      " [  -1.5378132    1.          -1.1322289 ...  121.03443    -82.15317\n",
      "   -55.667267 ]\n",
      " [  -1.5377979    1.          -1.1323013 ...   25.74641    -79.270004\n",
      "    94.04528  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1125526e+00  1.0000000e+00 -1.5519543e+00 ...  8.7203205e-01\n",
      "   2.0716669e+01  2.5892830e-01]\n",
      " [-1.1127663e+00  1.0000000e+00 -1.5518064e+00 ... -1.5885634e+02\n",
      "  -8.4055737e+02 -8.7462349e+01]\n",
      " [-1.1131535e+00  1.0000000e+00 -1.5514318e+00 ... -3.4364517e+02\n",
      "  -3.5294611e+02  1.7704253e+02]\n",
      " ...\n",
      " [-1.1129456e+00  1.0000000e+00 -1.5516090e+00 ... -1.0418019e+02\n",
      "   1.8501099e+02 -5.1944309e+01]\n",
      " [-1.1124058e+00  1.0000000e+00 -1.5520153e+00 ...  1.6886397e+02\n",
      "  -1.9830181e+01  7.2288536e+01]\n",
      " [-1.1123705e+00  1.0000000e+00 -1.5520611e+00 ...  1.4056879e+02\n",
      "   1.4105121e+02  5.4561802e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.7840157e-01  1.0000000e+00 -1.8198109e+00 ... -6.8046560e+00\n",
      "  -3.3663509e+01  9.7323944e+01]\n",
      " [-5.7865047e-01  1.0000000e+00 -1.8197689e+00 ...  1.0707991e+02\n",
      "   3.0856393e+02 -6.5956464e+02]\n",
      " [-5.7914162e-01  1.0000000e+00 -1.8195572e+00 ...  5.3369940e+02\n",
      "  -1.0557916e+02 -3.3960138e+02]\n",
      " ...\n",
      " [-5.7887650e-01  1.0000000e+00 -1.8196468e+00 ... -2.0142940e+02\n",
      "  -6.8808578e+01  2.5184798e+02]\n",
      " [-5.7825279e-01  1.0000000e+00 -1.8198395e+00 ... -7.0814545e+01\n",
      "  -6.6278782e+00 -2.6428825e+01]\n",
      " [-5.7818890e-01  1.0000000e+00 -1.8198586e+00 ... -3.7679832e+01\n",
      "  -4.8720547e+01 -6.7228630e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.22365952e-02  1.00000000e+00 -1.90929985e+00 ...  2.97399384e+02\n",
      "   7.72430786e+02  2.06669140e+00]\n",
      " [ 1.19771957e-02  1.00000000e+00 -1.90937614e+00 ...  3.58951569e+01\n",
      "  -8.45999718e+00 -4.28743019e+01]\n",
      " [ 1.14459991e-02  1.00000000e+00 -1.90933478e+00 ...  9.91197433e+01\n",
      "  -2.04314301e+02  4.97469521e+01]\n",
      " ...\n",
      " [ 1.17244720e-02  1.00000000e+00 -1.90934372e+00 ... -3.46429100e+01\n",
      "   4.52210464e+01 -1.06089115e+01]\n",
      " [ 1.23825073e-02  1.00000000e+00 -1.90929222e+00 ... -4.29177589e+01\n",
      "   9.55477524e+01 -5.29100752e+00]\n",
      " [ 1.24607086e-02  1.00000000e+00 -1.90928268e+00 ... -9.99189377e+01\n",
      "   2.60467339e+01 -5.56513611e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.0181999e-01  1.0000000e+00 -1.8120308e+00 ... -3.0470874e+02\n",
      "   1.5385057e+03  5.0593529e+01]\n",
      " [ 6.0157299e-01  1.0000000e+00 -1.8122044e+00 ... -1.1716959e+02\n",
      "  -7.9633303e+00  7.6837869e+00]\n",
      " [ 6.0103798e-01  1.0000000e+00 -1.8123311e+00 ...  4.9173563e+02\n",
      "   5.2427930e+02  1.5854453e+02]\n",
      " ...\n",
      " [ 6.0129166e-01  1.0000000e+00 -1.8122616e+00 ...  5.6171368e+01\n",
      "  -8.7799934e+01  8.5766243e+01]\n",
      " [ 6.0192680e-01  1.0000000e+00 -1.8119869e+00 ...  1.5403553e+01\n",
      "  -1.9452103e+02 -2.2831659e+02]\n",
      " [ 6.0203362e-01  1.0000000e+00 -1.8119545e+00 ... -4.7065884e+02\n",
      "  -4.8121685e+01  1.8872565e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1326914    1.          -1.5372124 ...    5.6082273    1.9134061\n",
      "   138.71352  ]\n",
      " [   1.1324902    1.          -1.5374079 ... -119.4642     -65.74596\n",
      "    37.469936 ]\n",
      " [   1.1320305    1.          -1.5377014 ...  -20.12655    -55.299305\n",
      "  -228.97845  ]\n",
      " ...\n",
      " [   1.1322498    1.          -1.5375605 ...   50.68103    167.57593\n",
      "    10.094097 ]\n",
      " [   1.1328125    1.          -1.537138  ...  168.72751    417.61798\n",
      "   114.73818  ]\n",
      " [   1.1328707    1.          -1.5370827 ...  113.403915   345.81436\n",
      "    33.31253  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.55225086e+00  1.00000000e+00 -1.11212540e+00 ...  2.06034985e+03\n",
      "  -3.68468903e+02  6.00046936e+02]\n",
      " [ 1.55210495e+00  1.00000000e+00 -1.11237335e+00 ...  4.67372227e+00\n",
      "   2.64458313e+02 -5.23624916e+01]\n",
      " [ 1.55174065e+00  1.00000000e+00 -1.11279440e+00 ...  4.26769066e+01\n",
      "   1.22117722e+02  9.71417313e+01]\n",
      " ...\n",
      " [ 1.55188560e+00  1.00000000e+00 -1.11259365e+00 ...  2.04206200e+01\n",
      "  -5.68368301e+01  2.13023758e+01]\n",
      " [ 1.55231094e+00  1.00000000e+00 -1.11202049e+00 ...  1.42952852e+01\n",
      "   8.27990723e+01  1.13382935e+02]\n",
      " [ 1.55237865e+00  1.00000000e+00 -1.11194420e+00 ... -1.48480911e+02\n",
      "   1.32139832e+02  1.32314865e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8200970e+00  1.0000000e+00 -5.7770920e-01 ...  6.3377576e+02\n",
      "   6.0126904e+02  2.1367082e+02]\n",
      " [ 1.8200207e+00  1.0000000e+00 -5.7800388e-01 ... -4.7080902e+01\n",
      "  -1.0168626e+02  4.9615457e+02]\n",
      " [ 1.8198109e+00  1.0000000e+00 -5.7849389e-01 ... -4.8179629e+03\n",
      "   5.1614805e+03  1.5557453e+04]\n",
      " ...\n",
      " [ 1.8198738e+00  1.0000000e+00 -5.7825756e-01 ... -7.7431934e+02\n",
      "  -1.5663800e+03  5.5196753e+02]\n",
      " [ 1.8200970e+00  1.0000000e+00 -5.7758522e-01 ...  8.2108383e+01\n",
      "   1.9355165e+02  1.0968353e+02]\n",
      " [ 1.8201733e+00  1.0000000e+00 -5.7749939e-01 ...  2.3113800e+01\n",
      "   1.3136520e+02  1.5477428e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9096041e+00  1.0000000e+00  1.2853622e-02 ...  6.5475159e+01\n",
      "  -2.4017212e+01 -2.3504373e+02]\n",
      " [ 1.9096041e+00  1.0000000e+00  1.2518883e-02 ...  3.3443436e+02\n",
      "  -1.1609085e+02 -1.4584686e+01]\n",
      " [ 1.9095688e+00  1.0000000e+00  1.2010841e-02 ... -2.3881067e+01\n",
      "   8.2048633e+02  2.3377060e+02]\n",
      " ...\n",
      " [ 1.9095478e+00  1.0000000e+00  1.2251854e-02 ... -5.3065598e+01\n",
      "   2.3814775e+01  2.6309597e+00]\n",
      " [ 1.9095535e+00  1.0000000e+00  1.2979507e-02 ... -2.7173706e+01\n",
      "  -2.9736069e+02  1.4217087e+02]\n",
      " [ 1.9096193e+00  1.0000000e+00  1.3069153e-02 ... -2.4178226e+01\n",
      "  -6.1069248e+01 -4.1355767e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8120527     1.            0.6027775  ...   17.061043\n",
      "    15.863041     -0.58140755]\n",
      " [   1.8121405     1.            0.60247517 ...   57.979996\n",
      "    48.84496     228.80222   ]\n",
      " [   1.8122654     1.            0.60199654 ...  233.22478\n",
      "   570.2456     -166.73099   ]\n",
      " ...\n",
      " [   1.81217       1.            0.6022272  ...  -53.023754\n",
      "   -10.770453     -9.561649  ]\n",
      " [   1.8119335     1.            0.60289955 ... -123.38028\n",
      "  -104.094795    175.7732    ]\n",
      " [   1.8119907     1.            0.60298157 ... -269.66376\n",
      "   119.00837      25.9202    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5372562e+00  1.0000000e+00  1.1330070e+00 ... -3.2880313e+02\n",
      "   3.3274490e+02 -7.0601227e+02]\n",
      " [ 1.5374260e+00  1.0000000e+00  1.1327705e+00 ...  1.8792805e+02\n",
      "   2.9376358e+01  4.4724823e+02]\n",
      " [ 1.5377197e+00  1.0000000e+00  1.1323632e+00 ...  5.5487384e+02\n",
      "  -7.8992233e+01 -5.9783081e+02]\n",
      " ...\n",
      " [ 1.5375557e+00  1.0000000e+00  1.1325636e+00 ... -6.6770576e+01\n",
      "  -8.7193832e+01 -1.5334663e+02]\n",
      " [ 1.5371456e+00  1.0000000e+00  1.1331177e+00 ... -2.5575687e+01\n",
      "   2.4019374e+02 -5.8073750e+00]\n",
      " [ 1.5371380e+00  1.0000000e+00  1.1331787e+00 ... -2.9891189e+03\n",
      "   1.6547513e+03 -7.8909292e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.11169052e+00  1.00000000e+00  1.55262947e+00 ... -8.24952774e+01\n",
      "  -5.52116966e+01 -1.33777664e+02]\n",
      " [ 1.11192799e+00  1.00000000e+00  1.55246353e+00 ...  1.08713722e+02\n",
      "  -1.13502464e+02  1.62909794e+01]\n",
      " [ 1.11232758e+00  1.00000000e+00  1.55217600e+00 ...  1.34339600e+03\n",
      "  -6.46887512e+02  7.83236938e+02]\n",
      " ...\n",
      " [ 1.11209869e+00  1.00000000e+00  1.55232620e+00 ...  9.90844604e+02\n",
      "   7.12875290e+01 -2.20974808e+02]\n",
      " [ 1.11152458e+00  1.00000000e+00  1.55270958e+00 ...  1.08627747e+02\n",
      "   1.09731388e+01 -1.31750458e+02]\n",
      " [ 1.11152649e+00  1.00000000e+00  1.55275536e+00 ... -5.12980286e+02\n",
      "   3.16401709e+03 -1.21932227e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.7728958e-01  1.0000000e+00  1.8201027e+00 ...  2.6375648e+01\n",
      "  -9.4370747e+00 -4.6353676e+01]\n",
      " [ 5.7756901e-01  1.0000000e+00  1.8200216e+00 ... -2.5653875e+01\n",
      "   7.2007950e+01 -5.4670410e+01]\n",
      " [ 5.7805443e-01  1.0000000e+00  1.8198802e+00 ... -1.1136108e+02\n",
      "   5.4603442e+02 -8.8376923e+01]\n",
      " ...\n",
      " [ 5.7778358e-01  1.0000000e+00  1.8199568e+00 ... -4.4362091e+02\n",
      "  -7.0926018e+01 -1.3715618e+03]\n",
      " [ 5.7711983e-01  1.0000000e+00  1.8201427e+00 ... -1.4818951e+03\n",
      "  -1.0824056e+03 -9.2383203e+02]\n",
      " [ 5.7710361e-01  1.0000000e+00  1.8201771e+00 ...  2.4131609e+03\n",
      "   6.2637933e+02  7.0071729e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.3345718e-02  1.0000000e+00  1.9094391e+00 ... -3.7242075e+02\n",
      "   3.3397073e+02  3.8989957e+02]\n",
      " [-1.3052940e-02  1.0000000e+00  1.9094181e+00 ... -1.0868310e+01\n",
      "   3.9638237e+01  1.7224098e+01]\n",
      " [-1.2506485e-02  1.0000000e+00  1.9094453e+00 ...  2.3202026e+02\n",
      "   2.3152079e+01 -1.4656004e+02]\n",
      " ...\n",
      " [-1.2796402e-02  1.0000000e+00  1.9094267e+00 ... -6.7678127e+00\n",
      "   7.0897808e+00  2.6252023e+02]\n",
      " [-1.3496399e-02  1.0000000e+00  1.9094257e+00 ...  3.4630219e+01\n",
      "   4.9715366e+00  9.4627838e+01]\n",
      " [-1.3536453e-02  1.0000000e+00  1.9094467e+00 ...  2.1342375e+03\n",
      "   2.4440007e+03 -2.4899395e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.0300827e-01  1.0000000e+00  1.8117619e+00 ... -1.2957376e+02\n",
      "   8.2174355e+01  1.6433711e+02]\n",
      " [-6.0272312e-01  1.0000000e+00  1.8118401e+00 ...  4.0503193e+01\n",
      "   7.9274702e+00  1.3790460e+02]\n",
      " [-6.0223007e-01  1.0000000e+00  1.8120472e+00 ... -5.9650505e+01\n",
      "  -1.3026469e+01  1.0206061e+01]\n",
      " ...\n",
      " [-6.0251045e-01  1.0000000e+00  1.8119230e+00 ...  3.5985211e+02\n",
      "  -5.9039612e+01 -1.4548790e+02]\n",
      " [-6.0316086e-01  1.0000000e+00  1.8117008e+00 ... -4.8797367e+01\n",
      "  -9.2930069e+01 -1.4939198e+02]\n",
      " [-6.0319328e-01  1.0000000e+00  1.8117085e+00 ...  1.8466090e+02\n",
      "  -9.9606128e+02 -7.8258636e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1336136    1.           1.5366116 ... -178.25189    312.67438\n",
      "   156.53114  ]\n",
      " [  -1.1333799    1.           1.5367708 ...  104.556526   -45.956673\n",
      "     1.8629351]\n",
      " [  -1.1329689    1.           1.5371174 ...  114.90798    -72.84329\n",
      "   100.53444  ]\n",
      " ...\n",
      " [  -1.133213     1.           1.5369186 ...  -12.2899885 -137.59378\n",
      "   -22.37306  ]\n",
      " [  -1.1337605    1.           1.5365086 ...   16.61726   -284.4107\n",
      "   -56.094795 ]\n",
      " [  -1.1337652    1.           1.5365028 ...   -9.0624075  -19.170395\n",
      "    10.35944  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5528784    1.           1.1113071 ...   42.664894  -214.04121\n",
      "     5.1319838]\n",
      " [  -1.5527105    1.           1.1115408 ...  -51.66606    100.939064\n",
      "   -51.986115 ]\n",
      " [  -1.552414     1.           1.1120054 ...   12.455905   -36.88647\n",
      "   -85.55882  ]\n",
      " ...\n",
      " [  -1.5525913    1.           1.111742  ...  206.87738    111.210304\n",
      "   -31.300846 ]\n",
      " [  -1.5529423    1.           1.1111755 ...  -13.437467   -31.049997\n",
      "    42.95679  ]\n",
      " [  -1.5529909    1.           1.1111679 ...   13.120048    45.088593\n",
      "    39.90909  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8203487e+00  1.0000000e+00  5.7699394e-01 ... -8.1315411e+02\n",
      "   6.5385376e+02  4.1610019e+02]\n",
      " [-1.8202753e+00  1.0000000e+00  5.7723999e-01 ... -1.5382360e+01\n",
      "  -9.8616692e+01 -1.6410469e+02]\n",
      " [-1.8201180e+00  1.0000000e+00  5.7779139e-01 ... -3.3627007e+01\n",
      "   2.0605148e+01 -2.3409702e+01]\n",
      " ...\n",
      " [-1.8202267e+00  1.0000000e+00  5.7747364e-01 ... -1.9614992e+01\n",
      "   4.7918980e+01 -6.8063576e+01]\n",
      " [-1.8203907e+00  1.0000000e+00  5.7683563e-01 ...  3.0245752e+01\n",
      "   2.2628065e+01  2.0257275e+01]\n",
      " [-1.8204098e+00  1.0000000e+00  5.7682037e-01 ... -7.3504593e+01\n",
      "  -8.8623428e+01  5.0180439e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9095640e+00  1.0000000e+00 -1.4123917e-02 ... -7.8264168e+01\n",
      "   1.3930227e+01 -1.5093747e+02]\n",
      " [-1.9095802e+00  1.0000000e+00 -1.3845444e-02 ... -3.2045979e+01\n",
      "   3.3763943e+01 -2.9679951e+01]\n",
      " [-1.9095955e+00  1.0000000e+00 -1.3277990e-02 ...  4.7034290e+01\n",
      "  -1.3083682e+02  3.1302631e+02]\n",
      " ...\n",
      " [-1.9095917e+00  1.0000000e+00 -1.3609886e-02 ...  7.5281898e+01\n",
      "  -1.0288771e+01  4.0075183e+02]\n",
      " [-1.9095459e+00  1.0000000e+00 -1.4289856e-02 ...  1.9250684e+02\n",
      "   6.3558548e+01  5.4371138e+00]\n",
      " [-1.9095716e+00  1.0000000e+00 -1.4310837e-02 ...  6.0635376e+01\n",
      "  -3.3263321e+01 -5.6634487e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8116951e+00  1.0000000e+00 -6.0371017e-01 ... -1.6974048e+02\n",
      "  -1.2338198e+02  8.2509811e+01]\n",
      " [-1.8118200e+00  1.0000000e+00 -6.0345364e-01 ... -1.0222980e+02\n",
      "   8.2118385e+01  7.8990112e+01]\n",
      " [-1.8119564e+00  1.0000000e+00 -6.0291338e-01 ...  1.3482919e+00\n",
      "   2.7970592e+01  1.0499740e+01]\n",
      " ...\n",
      " [-1.8118877e+00  1.0000000e+00 -6.0322666e-01 ... -1.3188089e+02\n",
      "  -1.6308490e+02 -1.9384288e+02]\n",
      " [-1.8116493e+00  1.0000000e+00 -6.0387039e-01 ... -6.1507495e+02\n",
      "   4.1672894e+01  5.7670959e+02]\n",
      " [-1.8116522e+00  1.0000000e+00 -6.0388756e-01 ... -5.0106041e+01\n",
      "  -6.9729286e+01 -7.2687614e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5364351e+00  1.0000000e+00 -1.1341152e+00 ...  6.5920334e+01\n",
      "  -9.9999336e+01  1.2202459e+01]\n",
      " [-1.5366468e+00  1.0000000e+00 -1.1338758e+00 ... -5.4057011e+01\n",
      "   1.1036712e+02 -8.1127480e+01]\n",
      " [-1.5369263e+00  1.0000000e+00 -1.1334145e+00 ... -2.5990307e+01\n",
      "   5.8780102e+01  4.8473984e+01]\n",
      " ...\n",
      " [-1.5367851e+00  1.0000000e+00 -1.1336699e+00 ... -1.4535598e+03\n",
      "  -2.1737492e+02 -9.2525897e+02]\n",
      " [-1.5363503e+00  1.0000000e+00 -1.1342564e+00 ...  6.1286713e+01\n",
      "   3.4342304e+01 -5.3456890e+01]\n",
      " [-1.5363359e+00  1.0000000e+00 -1.1342640e+00 ...  5.9545986e+01\n",
      "  -1.3049101e+02 -3.4506737e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.110857     1.          -1.5533733 ...  625.2291    -416.43274\n",
      "  -986.5697   ]\n",
      " [  -1.1111307    1.          -1.5531721 ... -192.2339    -151.55766\n",
      "   189.16199  ]\n",
      " [  -1.1115189    1.          -1.5528326 ... -103.822845  -127.10048\n",
      "   -42.313366 ]\n",
      " ...\n",
      " [  -1.1113243    1.          -1.5530128 ...  132.97177   -578.9557\n",
      "  -646.5364   ]\n",
      " [  -1.1107368    1.          -1.5534744 ...  226.4044      40.723732\n",
      "   303.12558  ]\n",
      " [  -1.1107197    1.          -1.5534744 ...    5.0563555  -37.845894\n",
      "   391.77258  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.57619095    1.           -1.8204746  ...  203.43602\n",
      "    60.556595   -224.03494   ]\n",
      " [  -0.57650757    1.           -1.8203392  ...  -53.587795\n",
      "   100.71126     267.5628    ]\n",
      " [  -0.5770054     1.           -1.820152   ...  210.03015\n",
      "   145.31427     -83.40552   ]\n",
      " ...\n",
      " [  -0.57676506    1.           -1.8202581  ... -141.45277\n",
      "    88.41423      -2.8150277 ]\n",
      " [  -0.5760708     1.           -1.8205338  ...   28.092554\n",
      "   -72.87824    -102.00388   ]\n",
      " [  -0.5760269     1.           -1.8205261  ...  225.74883\n",
      "   -93.02864    -130.98303   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.47075653e-02  1.00000000e+00 -1.90945816e+00 ...  4.09965210e+02\n",
      "   1.18904900e+02  9.43632507e+00]\n",
      " [ 1.43747330e-02  1.00000000e+00 -1.90938187e+00 ... -1.50091476e+01\n",
      "  -9.68518906e+01  8.85050201e+00]\n",
      " [ 1.38416290e-02  1.00000000e+00 -1.90936971e+00 ...  1.43956848e+03\n",
      "  -1.70507065e+02 -1.64939041e+02]\n",
      " ...\n",
      " [ 1.40953064e-02  1.00000000e+00 -1.90937901e+00 ...  2.05680756e+02\n",
      "   1.06457703e+02  2.95250092e+02]\n",
      " [ 1.48124695e-02  1.00000000e+00 -1.90946770e+00 ...  1.41129578e+02\n",
      "   3.35632629e+01  4.69894905e+01]\n",
      " [ 1.48830414e-02  1.00000000e+00 -1.90943718e+00 ... -1.79776413e+02\n",
      "  -3.51151184e+02  7.09700073e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.04180336e-01  1.00000000e+00 -1.81139755e+00 ... -1.14939026e+02\n",
      "  -1.26468430e+02 -2.85824158e+02]\n",
      " [ 6.03859901e-01  1.00000000e+00 -1.81140614e+00 ... -2.84393982e+02\n",
      "   2.32199646e+02  5.34326721e+02]\n",
      " [ 6.03340149e-01  1.00000000e+00 -1.81155610e+00 ...  1.31257278e+02\n",
      "  -5.31527686e+03 -4.65862061e+03]\n",
      " ...\n",
      " [ 6.03574753e-01  1.00000000e+00 -1.81147575e+00 ... -1.28367758e+01\n",
      "  -3.39879211e+02 -5.28403259e+02]\n",
      " [ 6.04244232e-01  1.00000000e+00 -1.81137276e+00 ...  5.13496780e+01\n",
      "   3.81911774e+01  1.82352562e+01]\n",
      " [ 6.04351997e-01  1.00000000e+00 -1.81133842e+00 ...  1.53857239e+02\n",
      "  -9.33914642e+01  5.12080536e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1345329e+00  1.0000000e+00 -1.5359497e+00 ...  2.9745935e+01\n",
      "  -3.6712456e+01 -5.9960835e+01]\n",
      " [ 1.1342573e+00  1.0000000e+00 -1.5360498e+00 ... -1.3814319e+02\n",
      "   1.7553513e+02 -1.3124991e+02]\n",
      " [ 1.1338062e+00  1.0000000e+00 -1.5363406e+00 ...  2.8271597e+03\n",
      "  -2.3696843e+03  5.7640962e+03]\n",
      " ...\n",
      " [ 1.1340122e+00  1.0000000e+00 -1.5362015e+00 ...  1.5204947e+02\n",
      "   3.0661638e+01 -2.0585705e+01]\n",
      " [ 1.1345844e+00  1.0000000e+00 -1.5358925e+00 ... -1.6934604e+02\n",
      "  -4.5887035e+01 -1.9811157e+01]\n",
      " [ 1.1346817e+00  1.0000000e+00 -1.5358448e+00 ...  1.2710117e+02\n",
      "  -1.1670684e+02  1.4505032e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5537291e+00  1.0000000e+00 -1.1101189e+00 ... -1.0861494e+03\n",
      "  -5.1403308e+02  1.4564076e+03]\n",
      " [ 1.5535250e+00  1.0000000e+00 -1.1103058e+00 ... -5.1943683e+02\n",
      "   7.5702289e+02  2.2748062e+02]\n",
      " [ 1.5532207e+00  1.0000000e+00 -1.1107035e+00 ... -6.1977534e+03\n",
      "  -2.4613663e+02  5.5461699e+03]\n",
      " ...\n",
      " [ 1.5533714e+00  1.0000000e+00 -1.1105223e+00 ... -2.7383032e+01\n",
      "   3.4068108e+00 -1.1851736e+01]\n",
      " [ 1.5537796e+00  1.0000000e+00 -1.1100368e+00 ...  2.8862885e+01\n",
      "   5.9121674e+01  5.5506153e+00]\n",
      " [ 1.5538359e+00  1.0000000e+00 -1.1099796e+00 ...  1.7974715e+02\n",
      "  -6.5873589e+01 -1.7115205e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8206701e+00  1.0000000e+00 -5.7599831e-01 ... -5.3858307e+02\n",
      "   2.3492746e+02 -4.4579941e+02]\n",
      " [ 1.8205462e+00  1.0000000e+00 -5.7619858e-01 ...  3.8598373e+01\n",
      "   1.3775812e+01  4.4858730e+01]\n",
      " [ 1.8203926e+00  1.0000000e+00 -5.7667828e-01 ... -7.4096576e+02\n",
      "  -3.8280762e+02 -5.4512201e+02]\n",
      " ...\n",
      " [ 1.8204880e+00  1.0000000e+00 -5.7645893e-01 ... -6.2116486e+01\n",
      "   3.8500072e+01 -2.7591007e+01]\n",
      " [ 1.8206902e+00  1.0000000e+00 -5.7590103e-01 ...  3.9441141e+02\n",
      "   4.8402225e+02  1.2891911e+02]\n",
      " [ 1.8207264e+00  1.0000000e+00 -5.7584000e-01 ...  1.1838313e+02\n",
      "   7.0487075e+00  5.0857254e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9095230e+00  1.0000000e+00  1.5222549e-02 ... -5.0300102e+00\n",
      "  -1.8533658e+02 -8.9616074e+01]\n",
      " [ 1.9094915e+00  1.0000000e+00  1.4995575e-02 ...  6.5459499e+00\n",
      "   1.1520060e+02  2.1514833e+01]\n",
      " [ 1.9094639e+00  1.0000000e+00  1.4506624e-02 ...  1.9787607e+02\n",
      "  -1.5715508e+02  2.3811078e+02]\n",
      " ...\n",
      " [ 1.9095020e+00  1.0000000e+00  1.4724731e-02 ...  3.8835818e+02\n",
      "   1.3566599e+02  6.1852618e+02]\n",
      " [ 1.9094925e+00  1.0000000e+00  1.5325546e-02 ... -7.3985138e+01\n",
      "  -1.3524277e+02 -4.4797127e+01]\n",
      " [ 1.9095221e+00  1.0000000e+00  1.5386581e-02 ... -9.8074725e+02\n",
      "   1.3137386e+02 -1.1126793e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.81142426e+00  1.00000000e+00  6.04539871e-01 ...  1.50244293e+02\n",
      "   3.24671745e+01  2.60729492e+02]\n",
      " [ 1.81145859e+00  1.00000000e+00  6.04299545e-01 ...  1.72219574e+02\n",
      "   4.54213829e+01  3.59595917e+02]\n",
      " [ 1.81158066e+00  1.00000000e+00  6.03838921e-01 ...  4.37896759e+02\n",
      "   1.20594885e+03 -7.45858276e+02]\n",
      " ...\n",
      " [ 1.81152534e+00  1.00000000e+00  6.04041100e-01 ... -1.09408867e+02\n",
      "  -6.72808533e+02  1.90642914e+02]\n",
      " [ 1.81129456e+00  1.00000000e+00  6.04637146e-01 ...  3.52609253e+01\n",
      "   1.01309685e+02  2.05764828e+01]\n",
      " [ 1.81136417e+00  1.00000000e+00  6.04698181e-01 ...  7.52380753e+00\n",
      "  -3.63060532e+01  8.42474747e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5359735    1.           1.1347122 ...   95.21619    -12.121455\n",
      "   -99.07854  ]\n",
      " [   1.5360737    1.           1.1345129 ... -231.87134    108.779755\n",
      "     9.413807 ]\n",
      " [   1.5363331    1.           1.1341194 ...   -7.185805   125.03023\n",
      "   127.55746  ]\n",
      " ...\n",
      " [   1.5362091    1.           1.1342964 ... -278.56964      5.7843103\n",
      "   188.71767  ]\n",
      " [   1.5357914    1.           1.1347866 ...  -54.740673  -218.32063\n",
      "   -45.48263  ]\n",
      " [   1.5358658    1.           1.1348324 ...   15.814771    56.606472\n",
      "    -4.4264116]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1100073    1.           1.5539532 ...   -7.7790494  164.03731\n",
      "    86.84698  ]\n",
      " [   1.110177     1.           1.5537882 ...  113.30615    150.34964\n",
      "  -230.15622  ]\n",
      " [   1.1105747    1.           1.5534997 ... -231.94565    568.9713\n",
      "   788.88214  ]\n",
      " ...\n",
      " [   1.1104012    1.           1.553628  ...  -66.73532   -120.28225\n",
      "  -120.8467   ]\n",
      " [   1.1098461    1.           1.5539951 ...  123.49838     45.399162\n",
      "  -155.62943  ]\n",
      " [   1.1098719    1.           1.5540333 ...   82.45174     60.780567\n",
      "   369.4527   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5753231     1.            1.820961   ...  -27.728043\n",
      "  -128.75293     149.68967   ]\n",
      " [   0.57551956    1.            1.8208752  ...  110.471596\n",
      "    68.23597      68.223595  ]\n",
      " [   0.57595634    1.            1.820729   ...  -56.083176\n",
      "   276.49622     206.13275   ]\n",
      " ...\n",
      " [   0.57574654    1.            1.820797   ... -132.7648\n",
      "  -482.66223    -548.7986    ]\n",
      " [   0.57509995    1.            1.8209782  ... -324.6212\n",
      "   116.12788     225.74773   ]\n",
      " [   0.57516193    1.            1.821001   ...   64.82369\n",
      "   -47.961468     59.008125  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.54771805e-02  1.00000000e+00  1.90961838e+00 ... -6.51742554e+02\n",
      "  -2.69934937e+02  6.34583435e+02]\n",
      " [-1.52673721e-02  1.00000000e+00  1.90958309e+00 ... -8.25512123e+00\n",
      "  -6.63755035e+00 -2.22706604e+01]\n",
      " [-1.47953033e-02  1.00000000e+00  1.90959275e+00 ...  4.52889175e+01\n",
      "  -6.91377335e+01  7.96118011e+01]\n",
      " ...\n",
      " [-1.50127411e-02  1.00000000e+00  1.90958977e+00 ...  6.04059448e+01\n",
      "   1.14047264e+02  2.56963139e+01]\n",
      " [-1.56898499e-02  1.00000000e+00  1.90958595e+00 ...  1.31436996e+01\n",
      "   1.37210129e+02  1.35373245e+02]\n",
      " [-1.56497955e-02  1.00000000e+00  1.90959930e+00 ...  3.14248047e+02\n",
      "  -1.06460220e+02  3.55747223e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:3, Score:1.06, Best Score:2.33, Average Score:1.50, Best Avg Score:1.72\n",
      "Episode number: 4\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c3e8280>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1351929e+00  1.0000000e+00  1.5354824e+00 ... -6.5880737e+01\n",
      "  -5.9966427e+01  1.8612201e+02]\n",
      " [-1.1350155e+00  1.0000000e+00  1.5356064e+00 ...  4.7244174e+02\n",
      "   3.4318600e+02  2.7224703e+01]\n",
      " [-1.1345901e+00  1.0000000e+00  1.5359118e+00 ... -6.7744278e+01\n",
      "   1.6513356e+02  1.0734684e+02]\n",
      " ...\n",
      " [-1.1347694e+00  1.0000000e+00  1.5357800e+00 ...  3.7371674e+01\n",
      "  -2.0459087e+00 -8.8801384e+01]\n",
      " [-1.1353321e+00  1.0000000e+00  1.5353470e+00 ... -3.5201245e+02\n",
      "   2.8574048e+02 -2.2967195e+03]\n",
      " [-1.1353302e+00  1.0000000e+00  1.5353527e+00 ... -2.3910841e+02\n",
      "  -1.4461760e+02 -1.6038333e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5540609    1.           1.1093426 ...   11.356176   -82.71521\n",
      "   -62.42084  ]\n",
      " [  -1.553937     1.           1.1095076 ...  -75.436264  -107.86466\n",
      "    72.66963  ]\n",
      " [  -1.5535889    1.           1.1099186 ...   72.8862     799.2336\n",
      "  -429.15964  ]\n",
      " ...\n",
      " [  -1.5537262    1.           1.1097288 ...  -80.57254     34.04888\n",
      "   -25.944061 ]\n",
      " [  -1.5541592    1.           1.1091766 ...  614.1452     536.0347\n",
      "   -89.49777  ]\n",
      " [  -1.5541649    1.           1.1091824 ...  -27.161108    28.313383\n",
      "    47.566246 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.8000001 0.        0.        0.4\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8207922e+00  1.0000000e+00  5.7497215e-01 ... -1.1130970e+02\n",
      "   1.3366063e+02 -3.1259127e+01]\n",
      " [-1.8207417e+00  1.0000000e+00  5.7518101e-01 ...  3.2055975e+02\n",
      "   3.0423599e+02  1.8390164e+02]\n",
      " [-1.8205585e+00  1.0000000e+00  5.7565153e-01 ... -1.6218488e+02\n",
      "   9.7755371e+01  3.1706662e+02]\n",
      " ...\n",
      " [-1.8206348e+00  1.0000000e+00  5.7544327e-01 ...  4.1337803e+01\n",
      "   6.8397179e+01 -1.5901236e+02]\n",
      " [-1.8208599e+00  1.0000000e+00  5.7478523e-01 ...  9.7943872e+02\n",
      "   2.5499631e+02 -7.6179102e+02]\n",
      " [-1.8208551e+00  1.0000000e+00  5.7479095e-01 ... -3.9176624e+01\n",
      "   9.4834747e+01 -5.2225426e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.70000005 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90938187e+00  1.00000000e+00 -1.60102844e-02 ... -6.56922531e+01\n",
      "  -1.23991425e+02 -8.56498566e+01]\n",
      " [-1.90939331e+00  1.00000000e+00 -1.57623291e-02 ...  1.95014755e+02\n",
      "   9.48143845e+01 -1.57713928e+02]\n",
      " [-1.90933990e+00  1.00000000e+00 -1.52750360e-02 ...  6.88400345e+01\n",
      "  -1.32032196e+02 -1.36520187e+02]\n",
      " ...\n",
      " [-1.90934563e+00  1.00000000e+00 -1.54895782e-02 ...  7.97205658e+01\n",
      "  -1.41254059e+02  7.47686234e+01]\n",
      " [-1.90935326e+00  1.00000000e+00 -1.62029266e-02 ... -1.77813660e+02\n",
      "  -6.16959095e+00  5.22996445e+01]\n",
      " [-1.90938377e+00  1.00000000e+00 -1.61972046e-02 ...  5.60113792e+01\n",
      "   1.35689592e+01  5.59850845e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8109026e+00  1.0000000e+00 -6.0574532e-01 ...  3.3689007e+01\n",
      "   5.9681759e+00 -6.2694595e+01]\n",
      " [-1.8109770e+00  1.0000000e+00 -6.0548782e-01 ...  4.9862349e+03\n",
      "   2.0607537e+03  1.1139963e+03]\n",
      " [-1.8110733e+00  1.0000000e+00 -6.0503966e-01 ... -3.7963249e+01\n",
      "   1.5443202e+00 -3.5429997e+00]\n",
      " ...\n",
      " [-1.8109970e+00  1.0000000e+00 -6.0523033e-01 ... -6.3896708e+00\n",
      "  -1.4807707e+01  4.0178084e+00]\n",
      " [-1.8107910e+00  1.0000000e+00 -6.0592842e-01 ...  1.2738178e+00\n",
      "  -1.9511822e+01 -1.0670845e+01]\n",
      " [-1.8108311e+00  1.0000000e+00 -6.0592270e-01 ...  1.4216106e+02\n",
      "   9.2126654e+02  7.7214984e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5350962e+00  1.0000000e+00 -1.1357212e+00 ...  8.5496086e+01\n",
      "   1.7824405e+02 -2.2363174e+02]\n",
      " [-1.5352306e+00  1.0000000e+00 -1.1355009e+00 ...  1.7424988e+03\n",
      "   1.3421869e+02 -1.6604994e+03]\n",
      " [-1.5354843e+00  1.0000000e+00 -1.1351246e+00 ...  5.6167343e+01\n",
      "  -5.2058264e+02  1.5618616e+02]\n",
      " ...\n",
      " [-1.5353432e+00  1.0000000e+00 -1.1352758e+00 ...  2.1381775e+01\n",
      "  -5.4372540e+02 -1.8051074e+02]\n",
      " [-1.5349751e+00  1.0000000e+00 -1.1358814e+00 ...  5.9095640e+00\n",
      "   3.7805233e+01  1.0651973e+01]\n",
      " [-1.5349855e+00  1.0000000e+00 -1.1358757e+00 ...  1.3725093e+01\n",
      "  -1.4796123e+02  1.2523977e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1086836e+00  1.0000000e+00 -1.5546055e+00 ... -2.3649651e+01\n",
      "   4.4889050e+01 -4.9375729e+01]\n",
      " [-1.1088762e+00  1.0000000e+00 -1.5544205e+00 ... -1.5523374e+03\n",
      "  -6.2387219e+02  1.1910236e+03]\n",
      " [-1.1092453e+00  1.0000000e+00 -1.5541536e+00 ...  2.0594215e+01\n",
      "   6.2833435e+02 -3.6321930e+02]\n",
      " ...\n",
      " [-1.1090374e+00  1.0000000e+00 -1.5542612e+00 ...  6.6224289e+01\n",
      "  -9.3756050e+01 -3.4922797e+02]\n",
      " [-1.1085339e+00  1.0000000e+00 -1.5547295e+00 ...  1.6594976e+01\n",
      "   2.9570635e+01 -5.6176537e+01]\n",
      " [-1.1085253e+00  1.0000000e+00 -1.5547237e+00 ...  7.3976489e+02\n",
      "  -1.0068767e+02  2.6801593e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.74284554e-01  1.00000000e+00 -1.82111931e+00 ... -1.13867855e+01\n",
      "  -1.86539993e+01 -1.60485497e+01]\n",
      " [-5.74506760e-01  1.00000000e+00 -1.82098961e+00 ...  1.68576370e+02\n",
      "   5.33636017e+01  6.24253174e+02]\n",
      " [-5.74886322e-01  1.00000000e+00 -1.82086575e+00 ... -2.54013770e+04\n",
      "  -5.08209814e+03  1.25746982e+04]\n",
      " ...\n",
      " [-5.74655533e-01  1.00000000e+00 -1.82090473e+00 ...  2.81391479e+02\n",
      "  -2.47395569e+02 -2.73856964e+02]\n",
      " [-5.74069977e-01  1.00000000e+00 -1.82117653e+00 ... -5.28315430e+02\n",
      "  -3.38145874e+02  2.79628830e+01]\n",
      " [-5.74101448e-01  1.00000000e+00 -1.82116699e+00 ... -1.74687393e+02\n",
      "   3.36517563e+01 -2.64786102e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.6730309e-02  1.0000000e+00 -1.9095020e+00 ...  2.2017281e+02\n",
      "  -1.6062709e+02  9.1357841e+01]\n",
      " [ 1.6493797e-02  1.0000000e+00 -1.9094172e+00 ... -6.5510626e+02\n",
      "  -5.2952673e+02 -2.8390231e+02]\n",
      " [ 1.6094208e-02  1.0000000e+00 -1.9094522e+00 ...  9.8509656e+02\n",
      "  -2.3845579e+03  2.7026962e+02]\n",
      " ...\n",
      " [ 1.6342163e-02  1.0000000e+00 -1.9094162e+00 ... -7.4167534e+01\n",
      "  -6.6644562e+01  1.2404553e+02]\n",
      " [ 1.6954422e-02  1.0000000e+00 -1.9094982e+00 ...  3.3802673e+01\n",
      "  -3.5551346e+01 -8.1419792e+00]\n",
      " [ 1.6924858e-02  1.0000000e+00 -1.9094887e+00 ... -7.9708939e+01\n",
      "   7.3888657e+01 -9.8712044e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.0610580e-01  1.0000000e+00 -1.8105984e+00 ... -4.0735046e+03\n",
      "   1.5083845e+03  8.3996503e+02]\n",
      " [ 6.0588169e-01  1.0000000e+00 -1.8106127e+00 ... -1.3953656e+02\n",
      "   1.2292932e+02 -2.3943455e+01]\n",
      " [ 6.0553551e-01  1.0000000e+00 -1.8107781e+00 ...  6.6832056e+02\n",
      "   4.9385559e+02 -5.9844800e+02]\n",
      " ...\n",
      " [ 6.0577011e-01  1.0000000e+00 -1.8106861e+00 ...  1.8265135e+02\n",
      "   2.1131161e+02  5.0864700e+01]\n",
      " [ 6.0634995e-01  1.0000000e+00 -1.8105240e+00 ...  2.3257123e+02\n",
      "   4.7310823e-01 -7.7929230e+01]\n",
      " [ 6.0628796e-01  1.0000000e+00 -1.8105145e+00 ...  2.5668295e+02\n",
      "   1.1474676e+02  3.0928928e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1362915    1.          -1.5344791 ...  127.54196    116.13428\n",
      "    82.11164  ]\n",
      " [   1.1361046    1.          -1.5345926 ...  179.44751   -161.4825\n",
      "   289.045    ]\n",
      " [   1.1358032    1.          -1.5348749 ...   64.25845   -247.88625\n",
      "   115.73475  ]\n",
      " ...\n",
      " [   1.1360168    1.          -1.5347261 ... -130.96375     54.128647\n",
      "   -30.745018 ]\n",
      " [   1.1365051    1.          -1.5343285 ...  -69.052185  -100.830826\n",
      "    -5.5765777]\n",
      " [   1.1364498    1.          -1.5343227 ...   49.680595   -90.07565\n",
      "    13.00397  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5547810e+00  1.0000000e+00 -1.1081791e+00 ... -1.5558203e+02\n",
      "  -5.4896777e+02 -1.3159746e+02]\n",
      " [ 1.5546427e+00  1.0000000e+00 -1.1083241e+00 ... -1.2917588e+03\n",
      "  -2.7808322e+02  6.5805750e+02]\n",
      " [ 1.5544262e+00  1.0000000e+00 -1.1087142e+00 ...  4.1816806e+02\n",
      "   2.9833707e+02  5.9041134e+01]\n",
      " ...\n",
      " [ 1.5546036e+00  1.0000000e+00 -1.1085014e+00 ... -1.7775861e+02\n",
      "   1.5997948e+02 -1.3024194e+02]\n",
      " [ 1.5549297e+00  1.0000000e+00 -1.1079769e+00 ...  1.4194661e+03\n",
      "   3.2710587e+02 -3.3586082e+03]\n",
      " [ 1.5548849e+00  1.0000000e+00 -1.1079731e+00 ...  3.6852386e+02\n",
      "  -3.4025330e+02 -2.4856067e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8211174e+00  1.0000000e+00 -5.7334900e-01 ...  1.5950687e+01\n",
      "   1.5354832e+01  5.0559250e+01]\n",
      " [ 1.8210516e+00  1.0000000e+00 -5.7355404e-01 ... -2.7404816e+02\n",
      "  -5.8238800e+02  9.0614282e+02]\n",
      " [ 1.8209381e+00  1.0000000e+00 -5.7399976e-01 ... -2.0057833e+01\n",
      "   3.3861731e+02  6.1665773e-01]\n",
      " ...\n",
      " [ 1.8210373e+00  1.0000000e+00 -5.7376385e-01 ...  1.1473364e+02\n",
      "   6.9432411e+01 -4.6555351e+01]\n",
      " [ 1.8211823e+00  1.0000000e+00 -5.7312202e-01 ...  8.7471275e+01\n",
      "   7.9278824e+01 -4.2330742e+01]\n",
      " [ 1.8211632e+00  1.0000000e+00 -5.7311821e-01 ...  1.1973985e+02\n",
      "   9.9124588e+01 -1.2125553e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9092693e+00  1.0000000e+00  1.7780304e-02 ...  1.4932130e+01\n",
      "   8.7858475e+01  9.6649460e+01]\n",
      " [ 1.9092836e+00  1.0000000e+00  1.7609596e-02 ...  1.5045486e+02\n",
      "   1.3941505e+02 -3.7064508e+02]\n",
      " [ 1.9092884e+00  1.0000000e+00  1.7135162e-02 ...  1.0240474e+02\n",
      "  -2.1067752e+02 -2.5273250e+02]\n",
      " ...\n",
      " [ 1.9093323e+00  1.0000000e+00  1.7393112e-02 ...  1.8076944e+02\n",
      "  -3.0101907e+02  5.0276538e+02]\n",
      " [ 1.9092617e+00  1.0000000e+00  1.8016815e-02 ... -4.0543850e+01\n",
      "  -5.1037411e+01 -6.6498924e+01]\n",
      " [ 1.9092531e+00  1.0000000e+00  1.8016815e-02 ...  2.7686481e+02\n",
      "  -1.7977617e+02  5.3853489e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8103561e+00  1.0000000e+00  6.0720253e-01 ... -2.1065988e+02\n",
      "   1.1865283e+02 -8.5691681e+02]\n",
      " [ 1.8104372e+00  1.0000000e+00  6.0706520e-01 ... -3.8512253e+01\n",
      "   2.0761368e+01  2.3483290e+01]\n",
      " [ 1.8105850e+00  1.0000000e+00  6.0660970e-01 ...  4.2238895e+02\n",
      "  -4.1014606e+02 -6.4906151e+01]\n",
      " ...\n",
      " [ 1.8105621e+00  1.0000000e+00  6.0685539e-01 ...  7.4511780e+01\n",
      "   2.9908929e+02 -7.3218338e+01]\n",
      " [ 1.8102779e+00  1.0000000e+00  6.0743141e-01 ... -1.6085103e+02\n",
      "   3.5269775e+01 -6.4609906e+02]\n",
      " [ 1.8102846e+00  1.0000000e+00  6.0743141e-01 ... -7.2857697e+01\n",
      "  -8.7159714e+01 -5.4070484e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5341501    1.           1.1370525 ...   98.93709      3.6355846\n",
      "     9.581903 ]\n",
      " [   1.5342951    1.           1.1369333 ...   65.54682     47.959763\n",
      "    83.185104 ]\n",
      " [   1.534584     1.           1.1365421 ... -198.34752    -59.34828\n",
      "  -345.5069   ]\n",
      " ...\n",
      " [   1.5344868    1.           1.136754  ...  111.81207   -329.54153\n",
      "  -182.26895  ]\n",
      " [   1.5340424    1.           1.1372662 ...   51.76479   -275.60352\n",
      "   500.24396  ]\n",
      " [   1.5340233    1.           1.1372662 ...  189.71754   -520.4285\n",
      "  -108.98854  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1077471    1.           1.5555077 ...  117.76328    899.7893\n",
      "   479.94183  ]\n",
      " [   1.1079445    1.           1.55546   ...  -41.95127    -55.729866\n",
      "   -61.181458 ]\n",
      " [   1.1083012    1.           1.5551674 ...  -73.1932    -134.41258\n",
      "   162.31166  ]\n",
      " ...\n",
      " [   1.1081467    1.           1.5553255 ...  136.36691     45.34404\n",
      "    34.832176 ]\n",
      " [   1.1075745    1.           1.5556793 ...  -67.30413   -232.96928\n",
      "    68.07752  ]\n",
      " [   1.1075745    1.           1.5556793 ...  297.7468     273.03592\n",
      "    92.99817  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.7277393e-01  1.0000000e+00  1.8215504e+00 ...  1.9478043e+01\n",
      "   1.7942410e+02 -5.9105232e+01]\n",
      " [ 5.7299137e-01  1.0000000e+00  1.8215704e+00 ...  1.0932680e+02\n",
      "   1.4281415e+01 -8.9540833e+01]\n",
      " [ 5.7340813e-01  1.0000000e+00  1.8214059e+00 ... -5.6710884e+01\n",
      "  -1.2399849e+03 -1.2128132e+03]\n",
      " ...\n",
      " [ 5.7322121e-01  1.0000000e+00  1.8215008e+00 ... -1.3690845e+01\n",
      "  -1.3411848e+02 -2.7481621e+01]\n",
      " [ 5.7256317e-01  1.0000000e+00  1.8216591e+00 ...  5.1328758e+01\n",
      "   2.8614047e+02 -4.7884818e+02]\n",
      " [ 5.7256413e-01  1.0000000e+00  1.8216610e+00 ...  2.2301715e+02\n",
      "   3.0049310e+02 -9.2149954e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.85070038e-02  1.00000000e+00  1.90943146e+00 ...  2.09580021e+01\n",
      "   1.00761070e+01 -5.68284569e+01]\n",
      " [-1.82819366e-02  1.00000000e+00  1.90951824e+00 ... -1.51924191e+01\n",
      "  -3.01613770e+01  8.91305923e+01]\n",
      " [-1.78413391e-02  1.00000000e+00  1.90949643e+00 ... -5.67392273e+02\n",
      "   5.40355957e+02 -9.41204712e+02]\n",
      " ...\n",
      " [-1.80397034e-02  1.00000000e+00  1.90951443e+00 ... -4.78636265e+00\n",
      "  -6.17937202e+01  1.39825325e+01]\n",
      " [-1.87206268e-02  1.00000000e+00  1.90947914e+00 ... -6.77821274e+01\n",
      "   1.83030338e+01 -9.88393631e+01]\n",
      " [-1.87244415e-02  1.00000000e+00  1.90948296e+00 ... -1.47994184e+00\n",
      "  -2.11062851e+01 -6.85589981e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.0761833e-01  1.0000000e+00  1.8100872e+00 ...  7.6359230e+01\n",
      "   2.7691684e+02 -7.9282707e+01]\n",
      " [-6.0741138e-01  1.0000000e+00  1.8101902e+00 ...  4.6100727e+01\n",
      "   7.2099937e+01  7.0098367e+00]\n",
      " [-6.0702515e-01  1.0000000e+00  1.8103049e+00 ...  1.9156206e+03\n",
      "  -5.1931881e+01 -1.8815566e+03]\n",
      " ...\n",
      " [-6.0721779e-01  1.0000000e+00  1.8102646e+00 ...  2.3152194e+02\n",
      "   2.2999072e+02  9.3708344e+01]\n",
      " [-6.0786629e-01  1.0000000e+00  1.8100529e+00 ...  2.7374243e+01\n",
      "   2.3879576e+02 -2.4491489e+02]\n",
      " [-6.0782719e-01  1.0000000e+00  1.8100586e+00 ...  8.0003311e+01\n",
      "  -4.7221443e+01 -1.1542072e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1375885e+00  1.0000000e+00  1.5335732e+00 ...  2.1685447e+03\n",
      "  -9.1183997e+02  4.3755862e+02]\n",
      " [-1.1374130e+00  1.0000000e+00  1.5337458e+00 ... -1.4989773e+05\n",
      "   5.3860473e+04 -3.5057141e+04]\n",
      " [-1.1371021e+00  1.0000000e+00  1.5339772e+00 ... -5.2714429e+00\n",
      "   2.6909969e+01 -1.0442045e+01]\n",
      " ...\n",
      " [-1.1372643e+00  1.0000000e+00  1.5338707e+00 ...  4.5166693e+02\n",
      "   4.5594415e+02 -9.8020148e+02]\n",
      " [-1.1377773e+00  1.0000000e+00  1.5334663e+00 ...  1.2193813e+01\n",
      "  -1.1859490e+02  9.4517097e+00]\n",
      " [-1.1377602e+00  1.0000000e+00  1.5334797e+00 ... -7.6395515e+01\n",
      "  -6.7771233e+01 -1.6561710e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5557671e+00  1.0000000e+00  1.1068058e+00 ...  8.1074936e+01\n",
      "  -1.1693355e+02  3.6935360e+01]\n",
      " [-1.5556335e+00  1.0000000e+00  1.1070089e+00 ... -7.2440371e+03\n",
      "  -3.9152586e+04 -4.4535526e+02]\n",
      " [-1.5554180e+00  1.0000000e+00  1.1073334e+00 ... -4.5160959e+02\n",
      "   7.1643048e+02  1.9213019e+02]\n",
      " ...\n",
      " [-1.5555363e+00  1.0000000e+00  1.1071768e+00 ... -9.2072601e+00\n",
      "  -1.5298898e+02 -5.3421535e+01]\n",
      " [-1.5558929e+00  1.0000000e+00  1.1066456e+00 ...  1.8329044e+01\n",
      "   3.9310352e+01 -1.4563313e+01]\n",
      " [-1.5558825e+00  1.0000000e+00  1.1066608e+00 ... -3.2290384e+02\n",
      "   4.9146475e+02  2.6804075e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.2       0.\n",
      " 0.        0.        0.        0.4       0.        0.        0.9000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8217402e+00  1.0000000e+00  5.7196426e-01 ... -7.4671150e+01\n",
      "  -4.7730399e+02  1.9343147e+03]\n",
      " [-1.8216667e+00  1.0000000e+00  5.7220268e-01 ... -3.4122126e+03\n",
      "   2.2804387e+03 -2.9116611e+03]\n",
      " [-1.8216000e+00  1.0000000e+00  5.7257658e-01 ... -1.3503248e+01\n",
      "  -2.4324208e+02 -5.8729527e+01]\n",
      " ...\n",
      " [-1.8216457e+00  1.0000000e+00  5.7240295e-01 ... -1.9650665e+03\n",
      "  -4.9886847e+02 -9.7817902e+02]\n",
      " [-1.8217926e+00  1.0000000e+00  5.7177544e-01 ... -1.1708762e+01\n",
      "   3.8020862e+02  1.5424518e+02]\n",
      " [-1.8218079e+00  1.0000000e+00  5.7179260e-01 ...  6.5096027e+02\n",
      "  -8.6546577e+01  8.9812180e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9094219e+00  1.0000000e+00 -1.9407272e-02 ... -1.7983725e+02\n",
      "   1.4663605e+02 -3.2142857e+01]\n",
      " [-1.9094172e+00  1.0000000e+00 -1.9185066e-02 ...  7.7623608e+03\n",
      "   6.2382227e+03  4.1936348e+03]\n",
      " [-1.9094639e+00  1.0000000e+00 -1.8776270e-02 ... -1.0226750e+02\n",
      "   3.4686261e+02  2.2708351e+02]\n",
      " ...\n",
      " [-1.9094601e+00  1.0000000e+00 -1.8973351e-02 ...  1.6909055e+02\n",
      "   2.9006050e+03  5.9926418e+01]\n",
      " [-1.9093933e+00  1.0000000e+00 -1.9599915e-02 ...  2.8340204e+02\n",
      "  -3.3301706e+02 -4.4347565e+01]\n",
      " [-1.9094238e+00  1.0000000e+00 -1.9582748e-02 ...  2.5530350e+01\n",
      "   3.4679745e+01  1.2624703e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.3       0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8100920e+00  1.0000000e+00 -6.0839844e-01 ...  5.0007805e+01\n",
      "   6.0505424e+01  2.2673204e+02]\n",
      " [-1.8101606e+00  1.0000000e+00 -6.0813332e-01 ... -1.8311591e+03\n",
      "  -5.0545074e+02  1.3692963e+02]\n",
      " [-1.8103409e+00  1.0000000e+00 -6.0775191e-01 ... -4.7187757e+00\n",
      "  -5.2680979e+00  1.3627163e+01]\n",
      " ...\n",
      " [-1.8102722e+00  1.0000000e+00 -6.0793209e-01 ...  1.0773606e+03\n",
      "   1.4684670e+03 -9.2730951e+02]\n",
      " [-1.8099957e+00  1.0000000e+00 -6.0858154e-01 ... -6.8735712e+02\n",
      "  -4.6990488e+02 -9.3628186e+02]\n",
      " [-1.8100252e+00  1.0000000e+00 -6.0856438e-01 ...  1.7059669e+01\n",
      "   1.3465475e+02  9.4497665e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.5       0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5334539e+00  1.0000000e+00 -1.1380978e+00 ...  7.0206287e+02\n",
      "  -1.1581302e+03 -4.1043509e+02]\n",
      " [-1.5335903e+00  1.0000000e+00 -1.1378536e+00 ...  7.0628058e+02\n",
      "   2.3062742e+03 -4.1580563e+02]\n",
      " [-1.5338860e+00  1.0000000e+00 -1.1375344e+00 ... -2.6645135e+02\n",
      "   6.7985344e+01 -1.7251765e+02]\n",
      " ...\n",
      " [-1.5337505e+00  1.0000000e+00 -1.1376820e+00 ... -4.4976151e+02\n",
      "  -5.1276648e+02 -6.9407922e+02]\n",
      " [-1.5333195e+00  1.0000000e+00 -1.1382523e+00 ...  1.3378964e+02\n",
      "  -2.3481784e+00  2.2230671e+01]\n",
      " [-1.5333204e+00  1.0000000e+00 -1.1382370e+00 ... -8.0696098e+01\n",
      "   1.0655218e+02  1.8037720e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.5       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1063604    1.          -1.5564327 ...  185.0951     118.38784\n",
      "   -12.465595 ]\n",
      " [  -1.1065502    1.          -1.5562305 ... -177.68263   -616.91187\n",
      "   112.789085 ]\n",
      " [  -1.1069489    1.          -1.5560119 ...  -14.169051    56.43836\n",
      "    -2.2013512]\n",
      " ...\n",
      " [  -1.10676      1.          -1.5561161 ...    7.967997    81.151596\n",
      "    93.81179  ]\n",
      " [  -1.1061993    1.          -1.5565491 ...  333.02637    -27.431555\n",
      "   -48.57518  ]\n",
      " [  -1.106185     1.          -1.5565376 ... -116.98979    239.32875\n",
      "   260.36356  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.7142925e-01  1.0000000e+00 -1.8221436e+00 ... -5.5015460e+02\n",
      "  -4.9240073e+02 -2.5992004e+02]\n",
      " [-5.7165051e-01  1.0000000e+00 -1.8220072e+00 ... -4.1236346e+02\n",
      "  -2.0106552e+02 -1.1936392e+02]\n",
      " [-5.7210541e-01  1.0000000e+00 -1.8219039e+00 ... -1.0928311e+03\n",
      "   2.6682983e+02  6.5860614e+02]\n",
      " ...\n",
      " [-5.7189751e-01  1.0000000e+00 -1.8219519e+00 ... -1.1310512e+02\n",
      "   5.6351624e+01 -3.6916248e+01]\n",
      " [-5.7124138e-01  1.0000000e+00 -1.8222103e+00 ... -7.2496674e+01\n",
      "   2.8315863e+02  7.5693170e+02]\n",
      " [-5.7123089e-01  1.0000000e+00 -1.8222008e+00 ...  1.5852331e+01\n",
      "  -9.2644661e+01 -2.2948416e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9662857e-02  1.0000000e+00 -1.9095917e+00 ... -1.1102174e+01\n",
      "  -6.3601173e+01  3.9248032e+01]\n",
      " [ 1.9427299e-02  1.0000000e+00 -1.9095345e+00 ...  4.2665833e+01\n",
      "   2.7060971e+02 -4.3047977e+02]\n",
      " [ 1.8945694e-02  1.0000000e+00 -1.9095621e+00 ...  2.4886256e+02\n",
      "  -1.0361790e+03  5.0877866e+02]\n",
      " ...\n",
      " [ 1.9163132e-02  1.0000000e+00 -1.9095469e+00 ... -6.8696442e+01\n",
      "  -1.0002446e+01 -3.6548359e+01]\n",
      " [ 1.9845963e-02  1.0000000e+00 -1.9096031e+00 ... -1.0275520e+02\n",
      "  -6.1133270e+01 -9.5115784e+01]\n",
      " [ 1.9866943e-02  1.0000000e+00 -1.9095955e+00 ... -6.7079218e+02\n",
      "  -9.6255287e+01 -1.9215442e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.6091118     1.           -1.8099747  ... -264.47043\n",
      "   170.85048     213.3643    ]\n",
      " [   0.60888386    1.           -1.8099833  ...   81.64289\n",
      "  -113.02214     172.54808   ]\n",
      " [   0.6084099     1.           -1.8101516  ...   18.702465\n",
      "   279.31653      87.736244  ]\n",
      " ...\n",
      " [   0.6086197     1.           -1.8100548  ...  -20.347317\n",
      "   -18.461174    -21.344288  ]\n",
      " [   0.60927963    1.           -1.8099232  ...   69.80099\n",
      "   150.41965     -94.495674  ]\n",
      " [   0.60930634    1.           -1.8099174  ...  336.72556\n",
      "    -6.3392034   196.08138   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1386166    1.          -1.532959  ...  127.783844   224.43529\n",
      "  -355.832    ]\n",
      " [   1.1384277    1.          -1.53302   ...   -8.275952    77.033646\n",
      "    16.424181 ]\n",
      " [   1.1379871    1.          -1.5333241 ... -133.16829    -32.305557\n",
      "   -70.44327  ]\n",
      " ...\n",
      " [   1.1381683    1.          -1.5331697 ...   -1.635315    -6.142032\n",
      "   -10.336598 ]\n",
      " [   1.138752     1.          -1.5328522 ...   56.65308    199.72305\n",
      "   -68.729126 ]\n",
      " [   1.1387835    1.          -1.5328465 ... -297.06433   -153.83672\n",
      "   146.2578   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5563278e+00  1.0000000e+00 -1.1061306e+00 ... -2.5127331e+01\n",
      "   3.9323765e+01  1.0307691e+02]\n",
      " [ 1.5561867e+00  1.0000000e+00 -1.1062593e+00 ... -8.7021532e+00\n",
      "  -3.8989632e+01  2.8959906e+01]\n",
      " [ 1.5559044e+00  1.0000000e+00 -1.1066619e+00 ... -4.0673604e+00\n",
      "   1.9331881e+01  2.7810726e+01]\n",
      " ...\n",
      " [ 1.5560398e+00  1.0000000e+00 -1.1064692e+00 ...  3.2196345e+00\n",
      "  -6.7888260e-02  2.5533283e+00]\n",
      " [ 1.5564327e+00  1.0000000e+00 -1.1059799e+00 ...  1.0934181e+01\n",
      "   8.4764526e+01  2.0299057e+01]\n",
      " [ 1.5564613e+00  1.0000000e+00 -1.1059723e+00 ...  6.6130478e+01\n",
      "  -6.7438339e+01 -2.1567479e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8221588e+00  1.0000000e+00 -5.7064629e-01 ...  4.6955376e+01\n",
      "  -6.0498455e+01  7.6972481e+01]\n",
      " [ 1.8220882e+00  1.0000000e+00 -5.7081223e-01 ...  6.6280762e+01\n",
      "  -8.4415222e+01  2.8725899e+01]\n",
      " [ 1.8219090e+00  1.0000000e+00 -5.7129335e-01 ...  1.6245708e+00\n",
      "  -3.0871296e-01  2.0797391e+00]\n",
      " ...\n",
      " [ 1.8219948e+00  1.0000000e+00 -5.7106304e-01 ...  7.5822735e-01\n",
      "  -7.9471397e-01  5.7530737e-01]\n",
      " [ 1.8221912e+00  1.0000000e+00 -5.7047653e-01 ...  8.8117615e+02\n",
      "  -7.2171643e+02 -2.4719895e+03]\n",
      " [ 1.8222408e+00  1.0000000e+00 -5.7046700e-01 ... -1.8241948e+02\n",
      "  -6.1417732e+01 -8.1683235e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.9000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9093962e+00  1.0000000e+00  2.0484924e-02 ...  1.2848079e+02\n",
      "  -8.3841492e+02  7.0282391e+02]\n",
      " [ 1.9093838e+00  1.0000000e+00  2.0294189e-02 ...  4.8898019e+02\n",
      "   6.6011761e+02  2.1994512e+03]\n",
      " [ 1.9093399e+00  1.0000000e+00  1.9792054e-02 ...  2.0792313e+00\n",
      "   9.6301670e+00 -1.3794017e-01]\n",
      " ...\n",
      " [ 1.9093590e+00  1.0000000e+00  2.0034790e-02 ... -2.2496371e+00\n",
      "   1.0651503e+00 -1.2858665e-01]\n",
      " [ 1.9093647e+00  1.0000000e+00  2.0654678e-02 ...  1.4825943e+02\n",
      "  -1.9644095e+02  2.9289307e+02]\n",
      " [ 1.9094172e+00  1.0000000e+00  2.0664215e-02 ... -6.4270729e+01\n",
      "  -3.0847562e+02  3.3381340e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8096466e+00  1.0000000e+00  6.0957527e-01 ... -8.4242401e+02\n",
      "  -2.0344928e+03  6.5022321e+02]\n",
      " [ 1.8096848e+00  1.0000000e+00  6.0938072e-01 ...  1.8556687e+03\n",
      "   3.7954055e+03  5.3518134e+02]\n",
      " [ 1.8098087e+00  1.0000000e+00  6.0891122e-01 ...  2.8571157e+00\n",
      "  -7.6985836e-02 -3.2056904e-01]\n",
      " ...\n",
      " [ 1.8097687e+00  1.0000000e+00  6.0913277e-01 ...  4.4749379e-01\n",
      "   3.2891798e+00 -1.3922584e-01]\n",
      " [ 1.8095665e+00  1.0000000e+00  6.0973740e-01 ...  9.1004494e+01\n",
      "   1.3737497e+02 -1.0443539e+02]\n",
      " [ 1.8096094e+00  1.0000000e+00  6.0974693e-01 ...  6.2321335e+01\n",
      "   4.0080011e+02  1.4304858e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5328312e+00  1.0000000e+00  1.1389065e+00 ... -2.5728497e+02\n",
      "  -2.4228973e+01  8.7594971e+02]\n",
      " [ 1.5329285e+00  1.0000000e+00  1.1387138e+00 ... -7.5188397e+02\n",
      "  -1.3185378e+03 -3.0914465e+03]\n",
      " [ 1.5332222e+00  1.0000000e+00  1.1383213e+00 ... -6.0432386e-01\n",
      "  -3.5993361e+00 -1.1034521e+00]\n",
      " ...\n",
      " [ 1.5331249e+00  1.0000000e+00  1.1385002e+00 ...  1.2583780e-01\n",
      "   5.3052998e-01 -1.3454640e+00]\n",
      " [ 1.5327358e+00  1.0000000e+00  1.1390266e+00 ... -1.9893873e+00\n",
      "  -1.1430890e+02  2.8948898e+02]\n",
      " [ 1.5327234e+00  1.0000000e+00  1.1390343e+00 ... -2.5766190e+02\n",
      "  -1.0572401e+02 -2.1241335e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1053610e+00  1.0000000e+00  1.5569859e+00 ... -1.9180977e+02\n",
      "  -1.0421116e+03 -4.8124481e+01]\n",
      " [ 1.1055040e+00  1.0000000e+00  1.5568657e+00 ... -1.3243425e+03\n",
      "  -7.6735339e+02 -9.5324280e+02]\n",
      " [ 1.1059189e+00  1.0000000e+00  1.5565646e+00 ...  7.6665819e-01\n",
      "  -5.2391739e+00  1.6577905e+00]\n",
      " ...\n",
      " [ 1.1057663e+00  1.0000000e+00  1.5566988e+00 ...  4.0887356e-02\n",
      "   1.4471545e+00 -1.5621883e+00]\n",
      " [ 1.1052303e+00  1.0000000e+00  1.5570717e+00 ...  2.9016948e+00\n",
      "   1.9089455e+02  1.4459969e+02]\n",
      " [ 1.1052151e+00  1.0000000e+00  1.5570774e+00 ...  1.9356273e+02\n",
      "  -1.6857796e+01  2.4887387e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.7023811e-01  1.0000000e+00  1.8221245e+00 ...  1.1627633e+03\n",
      "   4.1516528e+02 -1.0338361e+03]\n",
      " [ 5.7041931e-01  1.0000000e+00  1.8220921e+00 ... -2.0861223e+02\n",
      "  -2.5116643e+02 -1.4886870e+02]\n",
      " [ 5.7087135e-01  1.0000000e+00  1.8219184e+00 ... -6.2360182e+00\n",
      "   6.8948383e+00  9.9711195e-02]\n",
      " ...\n",
      " [ 5.7069016e-01  1.0000000e+00  1.8219919e+00 ...  1.3581135e+00\n",
      "   1.2243977e+00 -7.0934725e-01]\n",
      " [ 5.7007408e-01  1.0000000e+00  1.8221836e+00 ... -5.7576092e+01\n",
      "  -1.3335413e+02  1.0176506e+02]\n",
      " [ 5.7006741e-01  1.0000000e+00  1.8221798e+00 ... -5.7276859e+01\n",
      "   3.7214977e+01  8.8946861e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.1195412e-02  1.0000000e+00  1.9092522e+00 ... -9.3599869e+01\n",
      "   1.6652552e+01 -5.9077324e+01]\n",
      " [-2.1005630e-02  1.0000000e+00  1.9092569e+00 ... -3.1525693e+02\n",
      "  -5.7334293e+01  3.0880380e+02]\n",
      " [-2.0568848e-02  1.0000000e+00  1.9092383e+00 ... -5.1437817e+00\n",
      "   5.6501775e+00  1.1351666e+00]\n",
      " ...\n",
      " [-2.0751953e-02  1.0000000e+00  1.9092340e+00 ...  4.0791349e+00\n",
      "  -1.5276718e-01 -1.2160099e+00]\n",
      " [-2.1396637e-02  1.0000000e+00  1.9092522e+00 ...  6.7401512e+01\n",
      "  -7.0350586e+01 -4.8755600e+01]\n",
      " [-2.1372795e-02  1.0000000e+00  1.9092445e+00 ...  5.6938522e+01\n",
      "   1.4003922e+02  5.7084984e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.1027050e-01  1.0000000e+00  1.8093128e+00 ...  3.3630362e+02\n",
      "  -1.1896729e+00  6.5946924e+02]\n",
      " [-6.1009884e-01  1.0000000e+00  1.8093510e+00 ... -5.2933643e+02\n",
      "  -4.0931400e+02 -4.2876422e+02]\n",
      " [-6.0966110e-01  1.0000000e+00  1.8094879e+00 ...  7.6837397e-01\n",
      "   7.7404881e-01 -1.5177286e+00]\n",
      " ...\n",
      " [-6.0983658e-01  1.0000000e+00  1.8094034e+00 ...  5.1327410e+00\n",
      "   9.2985749e-01 -2.3569391e+00]\n",
      " [-6.1044121e-01  1.0000000e+00  1.8092422e+00 ... -2.9707706e+01\n",
      "   6.1895359e+01 -3.1875044e+01]\n",
      " [-6.1044025e-01  1.0000000e+00  1.8092327e+00 ...  4.7057501e+02\n",
      "  -3.4532721e+02 -6.3065497e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.13975239e+00  1.00000000e+00  1.53201294e+00 ... -3.53251228e+01\n",
      "   1.47635742e+02  9.60734711e+01]\n",
      " [-1.13961315e+00  1.00000000e+00  1.53217602e+00 ...  6.21441727e+01\n",
      "  -2.65218658e+02 -1.74134476e+02]\n",
      " [-1.13921738e+00  1.00000000e+00  1.53242147e+00 ...  6.94405289e+01\n",
      "  -1.23437935e+02 -6.32048950e+01]\n",
      " ...\n",
      " [-1.13936043e+00  1.00000000e+00  1.53230095e+00 ...  1.76680565e-01\n",
      "   2.19398499e+00 -6.72229409e-01]\n",
      " [-1.13988686e+00  1.00000000e+00  1.53189659e+00 ...  1.70413864e+02\n",
      "   1.71869644e+02  4.33342323e+01]\n",
      " [-1.13989067e+00  1.00000000e+00  1.53188515e+00 ... -2.12473999e+02\n",
      "  -3.03533983e+00 -1.06024010e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5574484e+00  1.0000000e+00  1.1048775e+00 ... -1.6942958e+02\n",
      "   2.9969089e+02 -1.0877858e+02]\n",
      " [-1.5573502e+00  1.0000000e+00  1.1051054e+00 ...  4.4537086e+01\n",
      "  -1.1659483e+02 -8.8584755e+01]\n",
      " [-1.5570869e+00  1.0000000e+00  1.1054652e+00 ...  1.3088251e+02\n",
      "  -7.8602303e+01  2.7265637e+02]\n",
      " ...\n",
      " [-1.5571938e+00  1.0000000e+00  1.1052856e+00 ... -3.7976573e+00\n",
      "   2.0135877e+00  2.9542565e-02]\n",
      " [-1.5575466e+00  1.0000000e+00  1.1047287e+00 ...  2.5883826e+02\n",
      "   3.4108917e+01 -2.5857836e+01]\n",
      " [-1.5575552e+00  1.0000000e+00  1.1047173e+00 ... -1.0760247e+02\n",
      "   1.6254221e+02 -1.5592453e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82276249e+00  1.00000000e+00  5.69412231e-01 ... -1.23390755e+02\n",
      "   6.65397930e+00 -1.87800476e+02]\n",
      " [-1.82271099e+00  1.00000000e+00  5.69643021e-01 ... -3.37184906e-01\n",
      "  -1.26321098e+02 -8.56020813e+01]\n",
      " [-1.82257462e+00  1.00000000e+00  5.70068419e-01 ... -5.20378235e+02\n",
      "  -1.46128381e+03  2.11071826e+03]\n",
      " ...\n",
      " [-1.82263565e+00  1.00000000e+00  5.69857597e-01 ...  9.84119511e+00\n",
      "  -6.51618242e+00 -4.70051229e-01]\n",
      " [-1.82279015e+00  1.00000000e+00  5.69240570e-01 ... -5.22685623e+01\n",
      "  -2.96840858e+01  8.59319763e+01]\n",
      " [-1.82281399e+00  1.00000000e+00  5.69223404e-01 ... -9.96055679e+01\n",
      "  -1.20201859e+02  3.60581665e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9095669e+00  1.0000000e+00 -2.1921158e-02 ...  7.4281342e+02\n",
      "  -1.4407433e+03  1.0533601e+03]\n",
      " [-1.9095850e+00  1.0000000e+00 -2.1691322e-02 ...  3.4385199e+02\n",
      "   4.2963690e+02  1.4878996e+02]\n",
      " [-1.9095840e+00  1.0000000e+00 -2.1230128e-02 ... -4.5525092e+02\n",
      "   7.4905695e+02 -3.7395435e+02]\n",
      " ...\n",
      " [-1.9095764e+00  1.0000000e+00 -2.1462440e-02 ...  7.7915134e+00\n",
      "  -5.5627642e+00 -2.5062582e+00]\n",
      " [-1.9095325e+00  1.0000000e+00 -2.2098541e-02 ... -1.3109111e+02\n",
      "  -5.8190079e+00  4.1936680e+01]\n",
      " [-1.9095535e+00  1.0000000e+00 -2.2115707e-02 ... -8.0142310e+02\n",
      "   3.7579160e+03  5.2137422e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.80928421e+00  1.00000000e+00 -6.10666275e-01 ...  2.23404633e+02\n",
      "  -2.25618393e+02 -2.23410416e+02]\n",
      " [-1.80936432e+00  1.00000000e+00 -6.10453606e-01 ...  1.55119568e+02\n",
      "  -1.19172107e+03 -9.64621094e+02]\n",
      " [-1.80949593e+00  1.00000000e+00 -6.10012352e-01 ... -2.47792343e+02\n",
      "  -4.54954132e+02 -1.00601814e+02]\n",
      " ...\n",
      " [-1.80941963e+00  1.00000000e+00 -6.10238075e-01 ... -1.50981832e+00\n",
      "   8.45780373e-01 -4.84371364e-01]\n",
      " [-1.80919838e+00  1.00000000e+00 -6.10828400e-01 ...  1.35883286e+02\n",
      "  -9.50291252e+00  9.88341904e+01]\n",
      " [-1.80919838e+00  1.00000000e+00 -6.10845566e-01 ...  2.03483715e+01\n",
      "   6.57079926e+01 -2.12437347e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5319395e+00  1.0000000e+00 -1.1401272e+00 ... -3.3229572e+02\n",
      "  -2.4523250e+02  5.3274603e+00]\n",
      " [-1.5320663e+00  1.0000000e+00 -1.1399155e+00 ...  1.0922488e+03\n",
      "  -1.8543051e+02  6.2434578e+01]\n",
      " [-1.5323086e+00  1.0000000e+00 -1.1395515e+00 ...  6.9298058e+01\n",
      "   1.1261869e+01  8.0057549e+01]\n",
      " ...\n",
      " [-1.5321674e+00  1.0000000e+00 -1.1397390e+00 ...  1.5583262e+00\n",
      "  -6.2993526e-02 -1.2532834e+00]\n",
      " [-1.5317783e+00  1.0000000e+00 -1.1402588e+00 ...  1.2294793e+01\n",
      "  -1.6440262e+01  3.9351448e+01]\n",
      " [-1.5317955e+00  1.0000000e+00 -1.1402760e+00 ...  2.0130280e+02\n",
      "   4.9136452e+01  1.8565729e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.10438347e+00  1.00000000e+00 -1.55778122e+00 ... -4.09964172e+02\n",
      "  -1.81717758e+02  3.48288361e+02]\n",
      " [-1.10456467e+00  1.00000000e+00 -1.55763531e+00 ... -1.97821823e+02\n",
      "   8.30332870e+01 -1.06682014e+02]\n",
      " [-1.10489273e+00  1.00000000e+00 -1.55736315e+00 ...  5.81835403e+01\n",
      "   6.83226852e+01  7.42282410e+01]\n",
      " ...\n",
      " [-1.10471153e+00  1.00000000e+00 -1.55750656e+00 ... -6.41037226e-02\n",
      "   2.23612928e+00  2.36705661e-01]\n",
      " [-1.10418129e+00  1.00000000e+00 -1.55786514e+00 ... -2.30360103e+00\n",
      "  -4.08932877e+00 -1.81212950e+00]\n",
      " [-1.10419941e+00  1.00000000e+00 -1.55787849e+00 ... -7.67902588e+02\n",
      "   1.73377243e+02 -4.86768433e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.6883621e-01  1.0000000e+00 -1.8228474e+00 ... -7.5430962e+01\n",
      "  -9.8390846e+01 -1.2980952e+02]\n",
      " [-5.6904888e-01  1.0000000e+00 -1.8227806e+00 ...  2.7705338e+02\n",
      "   2.9742825e+02 -2.8172476e+02]\n",
      " [-5.6941032e-01  1.0000000e+00 -1.8226303e+00 ... -2.7728893e+01\n",
      "  -1.5995473e+01 -7.4032394e+01]\n",
      " ...\n",
      " [-5.6919861e-01  1.0000000e+00 -1.8227291e+00 ...  1.2375164e-01\n",
      "   7.4717999e-01 -4.7840905e-01]\n",
      " [-5.6856728e-01  1.0000000e+00 -1.8228683e+00 ...  1.1668052e+03\n",
      "  -1.0569055e+03  3.7468906e+02]\n",
      " [-5.6863117e-01  1.0000000e+00 -1.8228817e+00 ...  5.1158703e+01\n",
      "  -1.0362723e+02  3.8018337e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 2.22148895e-02  1.00000000e+00 -1.90946960e+00 ... -3.29371429e+02\n",
      "   2.74987274e+02 -2.40751999e+02]\n",
      " [ 2.19945908e-02  1.00000000e+00 -1.90944386e+00 ... -1.35580856e+02\n",
      "  -4.17511940e+01 -1.04354448e+01]\n",
      " [ 2.15702057e-02  1.00000000e+00 -1.90943801e+00 ...  3.07155975e+02\n",
      "  -3.48130989e+01  1.20574646e+02]\n",
      " ...\n",
      " [ 2.17914581e-02  1.00000000e+00 -1.90946674e+00 ... -2.26281643e-01\n",
      "   1.23346758e+00  3.64355087e-01]\n",
      " [ 2.24552155e-02  1.00000000e+00 -1.90942001e+00 ...  3.04891479e+02\n",
      "   1.58197314e+03 -7.55204712e+02]\n",
      " [ 2.24275589e-02  1.00000000e+00 -1.90943527e+00 ...  3.93782776e+02\n",
      "   2.50408646e+02 -1.24381668e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.11080170e-01  1.00000000e+00 -1.80903244e+00 ...  3.09322433e+01\n",
      "   4.79103508e+01  5.94822216e+00]\n",
      " [ 6.10869408e-01  1.00000000e+00 -1.80905437e+00 ...  8.99122620e+01\n",
      "  -1.49180618e+02  3.56977921e+01]\n",
      " [ 6.10443115e-01  1.00000000e+00 -1.80919230e+00 ...  1.41659103e+02\n",
      "   2.61913757e+01 -5.83211937e+01]\n",
      " ...\n",
      " [ 6.10654831e-01  1.00000000e+00 -1.80914307e+00 ... -3.41367841e-01\n",
      "   1.59280014e+00  3.07572365e-01]\n",
      " [ 6.11301422e-01  1.00000000e+00 -1.80891800e+00 ... -3.79628735e+03\n",
      "  -4.42604492e+03  3.38395972e+03]\n",
      " [ 6.11281395e-01  1.00000000e+00 -1.80893517e+00 ... -3.31177948e+02\n",
      "   1.04946884e+02  1.85854584e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.3\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1404772e+00  1.0000000e+00 -1.5315018e+00 ... -1.6011236e+01\n",
      "   4.0854012e+01  2.2135559e+01]\n",
      " [ 1.1403027e+00  1.0000000e+00 -1.5315714e+00 ... -5.0471753e+02\n",
      "  -6.3667719e+02 -4.2549716e+02]\n",
      " [ 1.1399422e+00  1.0000000e+00 -1.5318316e+00 ... -8.3706497e+01\n",
      "   4.7364491e+01  9.6159584e+01]\n",
      " ...\n",
      " [ 1.1401234e+00  1.0000000e+00 -1.5317183e+00 ... -6.0327482e-01\n",
      "   1.7390399e+00 -7.8696370e-02]\n",
      " [ 1.1406956e+00  1.0000000e+00 -1.5313301e+00 ... -2.8653488e+02\n",
      "   8.4063916e+02 -8.8470215e+02]\n",
      " [ 1.1406517e+00  1.0000000e+00 -1.5313473e+00 ... -1.9027802e+01\n",
      "  -1.4630165e+02 -1.4815622e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.1       0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.9000001\n",
      " 0.        0.        0.        0.9000001 0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5579557    1.          -1.1041584 ...  -61.944435  -215.12183\n",
      "    72.092285 ]\n",
      " [   1.5578432    1.          -1.1042576 ... -303.87305     78.09085\n",
      "   136.24341  ]\n",
      " [   1.5575905    1.          -1.1046075 ...  -31.190624   139.97585\n",
      "   212.58934  ]\n",
      " ...\n",
      " [   1.5577145    1.          -1.1044617 ...   -0.5430063    6.3575006\n",
      "     0.4698214]\n",
      " [   1.5581474    1.          -1.1039295 ...   87.734566   -74.970345\n",
      "  -421.39502  ]\n",
      " [   1.5580807    1.          -1.1039467 ...   66.919044    56.565228\n",
      "   131.1098   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.4       0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8229837e+00  1.0000000e+00 -5.6849861e-01 ... -6.5837714e+02\n",
      "   1.2245410e+03  2.1847490e+03]\n",
      " [ 1.8229265e+00  1.0000000e+00 -5.6862736e-01 ... -6.0569355e+01\n",
      "  -8.9615929e+01 -1.2066506e+02]\n",
      " [ 1.8228054e+00  1.0000000e+00 -5.6904012e-01 ...  7.4297699e+01\n",
      "  -4.2842365e+01 -2.5895041e+02]\n",
      " ...\n",
      " [ 1.8228741e+00  1.0000000e+00 -5.6886196e-01 ...  1.4793749e+00\n",
      "  -5.7358232e+00  8.9452815e-01]\n",
      " [ 1.8231144e+00  1.0000000e+00 -5.6823540e-01 ... -7.2712439e+02\n",
      "   1.6155054e+03 -6.4123322e+02]\n",
      " [ 1.8230486e+00  1.0000000e+00 -5.6825829e-01 ...  8.2381241e+01\n",
      "   1.3507664e+03 -7.0779150e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9092731e+00  1.0000000e+00  2.2947311e-02 ...  1.2909761e+02\n",
      "  -4.2140768e+02 -1.1605258e+03]\n",
      " [ 1.9092760e+00  1.0000000e+00  2.2799492e-02 ... -1.9973713e+01\n",
      "  -6.4257874e+02 -7.1216309e+02]\n",
      " [ 1.9092789e+00  1.0000000e+00  2.2361889e-02 ...  1.7837251e+02\n",
      "   4.7140213e+01 -1.5155081e+02]\n",
      " ...\n",
      " [ 1.9092884e+00  1.0000000e+00  2.2552490e-02 ...  1.5967846e-01\n",
      "  -2.4631720e+00  2.8407574e-04]\n",
      " [ 1.9092999e+00  1.0000000e+00  2.3227692e-02 ...  5.2126752e+02\n",
      "   9.1700067e+02 -2.3182594e+02]\n",
      " [ 1.9092751e+00  1.0000000e+00  2.3204803e-02 ...  2.5002223e+02\n",
      "   2.2020970e+02  3.6207330e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8087072e+00  1.0000000e+00  6.1201477e-01 ... -1.3550358e+03\n",
      "   1.1593810e+03  1.6429923e+03]\n",
      " [ 1.8087854e+00  1.0000000e+00  6.1185741e-01 ...  1.2527014e+02\n",
      "   4.8252914e+01  2.4822950e+00]\n",
      " [ 1.8088970e+00  1.0000000e+00  6.1145002e-01 ... -7.2111786e+01\n",
      "   3.4844524e+01 -1.8240492e+01]\n",
      " ...\n",
      " [ 1.8088531e+00  1.0000000e+00  6.1162853e-01 ... -3.5774469e-02\n",
      "  -4.2656064e+00 -3.2055107e-01]\n",
      " [ 1.8086662e+00  1.0000000e+00  6.1228752e-01 ... -8.5418079e+02\n",
      "  -8.9328503e+02 -1.0425969e+03]\n",
      " [ 1.8086491e+00  1.0000000e+00  6.1226654e-01 ... -3.2693845e+02\n",
      "  -3.9080896e+02  3.2550488e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5310564e+00  1.0000000e+00  1.1410122e+00 ...  3.4303027e+02\n",
      "   1.8539365e+02  6.4194305e+02]\n",
      " [ 1.5311880e+00  1.0000000e+00  1.1408873e+00 ...  1.7008737e+02\n",
      "  -9.6758652e+01  8.5854309e+01]\n",
      " [ 1.5314465e+00  1.0000000e+00  1.1405389e+00 ...  5.5087113e+00\n",
      "  -1.6593489e+01  9.6179649e+01]\n",
      " ...\n",
      " [ 1.5313435e+00  1.0000000e+00  1.1406879e+00 ...  6.5341806e-01\n",
      "  -1.0424755e+01 -8.0765295e-01]\n",
      " [ 1.5309582e+00  1.0000000e+00  1.1412392e+00 ...  9.9602203e+01\n",
      "   4.0852989e+01 -7.5379425e+02]\n",
      " [ 1.5309401e+00  1.0000000e+00  1.1412201e+00 ... -4.6888996e+01\n",
      "   5.5845093e+01  5.9631695e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1036253e+00  1.0000000e+00  1.5583324e+00 ...  4.8394566e+01\n",
      "   2.6794809e+02 -2.3289056e+02]\n",
      " [ 1.1037788e+00  1.0000000e+00  1.5582409e+00 ... -2.7948345e+01\n",
      "  -1.7516614e+02  3.8380196e+01]\n",
      " [ 1.1041412e+00  1.0000000e+00  1.5579808e+00 ... -6.0136555e+01\n",
      "  -3.8259650e+02  2.7940079e+02]\n",
      " ...\n",
      " [ 1.1040077e+00  1.0000000e+00  1.5580807e+00 ...  8.4986711e-01\n",
      "   1.8577809e+00 -2.5806189e-01]\n",
      " [ 1.1034508e+00  1.0000000e+00  1.5584984e+00 ...  9.4498840e+01\n",
      "   9.6503494e+01  1.0961875e+02]\n",
      " [ 1.1034584e+00  1.0000000e+00  1.5584831e+00 ...  4.6090378e+01\n",
      "  -4.2762672e+01 -7.4259636e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5679493     1.            1.823206   ...   29.419043\n",
      "     7.9441547   102.458     ]\n",
      " [   0.56812763    1.            1.8231449  ...    1.1834149\n",
      "     0.6565409     0.2037816 ]\n",
      " [   0.56855583    1.            1.8230101  ...   87.751015\n",
      "   -24.199316     54.64137   ]\n",
      " ...\n",
      " [   0.5683975     1.            1.8230572  ...    0.42997837\n",
      "     7.5880237    -0.23131186]\n",
      " [   0.5677357     1.            1.8232994  ...  -98.75395\n",
      "  -102.04561     178.05904   ]\n",
      " [   0.56773853    1.            1.823288   ...  -16.170639\n",
      "    31.656116    -68.84627   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.3565292e-02  1.0000000e+00  1.9094982e+00 ...  1.9900806e+01\n",
      "  -2.7328279e+00  6.9458473e+01]\n",
      " [-2.3383141e-02  1.0000000e+00  1.9095144e+00 ...  3.8413322e+01\n",
      "  -8.9968548e+00  2.7523447e+01]\n",
      " [-2.2909164e-02  1.0000000e+00  1.9095120e+00 ...  4.2678906e+02\n",
      "   4.4519318e+02 -1.8756441e+01]\n",
      " ...\n",
      " [-2.3071289e-02  1.0000000e+00  1.9094992e+00 ... -2.8334477e+00\n",
      "  -5.8452058e+00  1.9637620e-01]\n",
      " [-2.3761749e-02  1.0000000e+00  1.9094944e+00 ... -1.1526716e+01\n",
      "   5.2312527e+00 -3.3634686e+01]\n",
      " [-2.3784637e-02  1.0000000e+00  1.9094887e+00 ...  1.5857639e+02\n",
      "   2.7182352e+02 -6.6423393e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.12361908e-01  1.00000000e+00  1.80868912e+00 ...  2.01326431e+02\n",
      "  -8.13049698e+00  5.74807739e+01]\n",
      " [-6.12187386e-01  1.00000000e+00  1.80877304e+00 ... -2.43633530e+02\n",
      "   2.96459412e+02  2.70271088e+02]\n",
      " [-6.11780167e-01  1.00000000e+00  1.80889761e+00 ... -2.23311371e+02\n",
      "   1.49281311e+02  1.36259293e+02]\n",
      " ...\n",
      " [-6.11927032e-01  1.00000000e+00  1.80882740e+00 ... -3.42080617e+00\n",
      "  -1.11784401e+01  8.32603812e-01]\n",
      " [-6.12565994e-01  1.00000000e+00  1.80861664e+00 ... -1.38734455e+01\n",
      "  -4.42758293e+01 -9.76954346e+01]\n",
      " [-6.12566948e-01  1.00000000e+00  1.80861664e+00 ... -1.09626457e+02\n",
      "  -3.77090302e+02 -6.96059082e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1416616e+00  1.0000000e+00  1.5307217e+00 ...  5.8633018e+00\n",
      "  -2.4927698e+01  4.4136414e+01]\n",
      " [-1.1415186e+00  1.0000000e+00  1.5308638e+00 ... -1.1042417e+02\n",
      "   3.5124344e+01  2.0804227e+02]\n",
      " [-1.1411438e+00  1.0000000e+00  1.5311061e+00 ...  3.4938731e+00\n",
      "  -4.0444865e+00 -1.6367903e+00]\n",
      " ...\n",
      " [-1.1412621e+00  1.0000000e+00  1.5309887e+00 ...  3.2524772e+00\n",
      "   7.6300597e+00  3.8611600e-01]\n",
      " [-1.1417961e+00  1.0000000e+00  1.5305824e+00 ...  8.5604324e+01\n",
      "   2.2271082e+01  1.8935738e+01]\n",
      " [-1.1418352e+00  1.0000000e+00  1.5305843e+00 ... -2.5979221e+02\n",
      "   2.7097622e+01  4.6107281e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5587282e+00  1.0000000e+00  1.1031761e+00 ... -3.3531833e+02\n",
      "   3.0301047e+02  1.1918872e+02]\n",
      " [-1.5586185e+00  1.0000000e+00  1.1033764e+00 ...  1.0198403e+03\n",
      "  -2.5818933e+02 -2.3029552e+02]\n",
      " [-1.5583553e+00  1.0000000e+00  1.1037235e+00 ... -1.2706546e+02\n",
      "  -6.6860924e+01 -2.8869922e+02]\n",
      " ...\n",
      " [-1.5584164e+00  1.0000000e+00  1.1035681e+00 ... -3.0308719e+00\n",
      "  -6.8124804e+00 -2.1331292e-01]\n",
      " [-1.5587978e+00  1.0000000e+00  1.1029949e+00 ...  2.2468668e+02\n",
      "   2.5292300e+02  2.7154037e+02]\n",
      " [-1.5588560e+00  1.0000000e+00  1.1029968e+00 ...  8.0522888e+02\n",
      "   1.2010173e+03 -1.3898535e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.5       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82325363e+00  1.00000000e+00  5.67096710e-01 ... -1.53445374e+02\n",
      "  -5.99775887e+01 -5.77208557e+02]\n",
      " [-1.82319355e+00  1.00000000e+00  5.67316055e-01 ... -3.88836395e+02\n",
      "   1.56062927e+01 -3.60160217e+02]\n",
      " [-1.82305717e+00  1.00000000e+00  5.67719877e-01 ... -1.16984039e+02\n",
      "  -1.94759979e+02  1.05589935e+02]\n",
      " ...\n",
      " [-1.82306862e+00  1.00000000e+00  5.67538261e-01 ... -4.66901493e+00\n",
      "  -1.19042158e+01 -7.34158516e-01]\n",
      " [-1.82323456e+00  1.00000000e+00  5.66883087e-01 ... -5.52496071e+01\n",
      "   3.26129074e+01 -2.00571404e+01]\n",
      " [-1.82331944e+00  1.00000000e+00  5.66883087e-01 ...  1.73895416e+01\n",
      "   5.16468658e+01 -7.48903427e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.6       0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9091549e+00  1.0000000e+00 -2.4236679e-02 ... -8.9113640e+01\n",
      "   1.0010834e+01 -1.3394425e+01]\n",
      " [-1.9091597e+00  1.0000000e+00 -2.4022102e-02 ... -3.7057034e+02\n",
      "  -2.5354951e+02  4.5795932e+02]\n",
      " [-1.9091663e+00  1.0000000e+00 -2.3601227e-02 ... -8.1289009e+01\n",
      "   1.8955859e+02  2.5480576e+01]\n",
      " ...\n",
      " [-1.9091225e+00  1.0000000e+00 -2.3789406e-02 ... -3.6874244e+00\n",
      "  -8.0825834e+00  8.1833827e-01]\n",
      " [-1.9090672e+00  1.0000000e+00 -2.4457932e-02 ...  1.3415804e+02\n",
      "   5.5089645e+01  3.4452591e+01]\n",
      " [-1.9091406e+00  1.0000000e+00 -2.4457932e-02 ... -1.6811700e+02\n",
      "   1.5501903e+02 -1.1857846e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8082504e+00  1.0000000e+00 -6.1320305e-01 ... -2.3520079e+01\n",
      "  -1.5017789e+02  3.9680389e+02]\n",
      " [-1.8083220e+00  1.0000000e+00 -6.1298943e-01 ... -2.7630737e+02\n",
      "   8.3200073e+01 -2.4326343e+02]\n",
      " [-1.8084393e+00  1.0000000e+00 -6.1259013e-01 ... -3.9072784e+01\n",
      "   1.3692047e+02 -2.1564357e+02]\n",
      " ...\n",
      " [-1.8083305e+00  1.0000000e+00 -6.1276436e-01 ... -2.2728038e-01\n",
      "  -3.0103297e+00  6.8475163e-01]\n",
      " [-1.8080692e+00  1.0000000e+00 -6.1341476e-01 ...  3.2246922e+01\n",
      "  -1.0619053e+03 -1.0356724e+02]\n",
      " [-1.8081598e+00  1.0000000e+00 -6.1341286e-01 ... -9.7142038e+00\n",
      "   2.3592297e+01 -8.5522758e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5304108e+00  1.0000000e+00 -1.1418915e+00 ...  1.5436299e+02\n",
      "   2.3043832e+02  2.9583661e+02]\n",
      " [-1.5305405e+00  1.0000000e+00 -1.1417160e+00 ...  4.3606699e+03\n",
      "   1.9610186e+03 -2.3809236e+03]\n",
      " [-1.5307884e+00  1.0000000e+00 -1.1413772e+00 ...  6.5596710e+01\n",
      "   1.0511422e+00 -7.6770691e+01]\n",
      " ...\n",
      " [-1.5306339e+00  1.0000000e+00 -1.1415291e+00 ... -9.8461032e-01\n",
      "  -9.5331306e+00  1.5008993e+00]\n",
      " [-1.5301533e+00  1.0000000e+00 -1.1420517e+00 ... -4.4917285e+02\n",
      "   2.1439153e+02  9.0109875e+02]\n",
      " [-1.5302515e+00  1.0000000e+00 -1.1420498e+00 ...  3.5630966e+01\n",
      "   2.5280655e+01 -4.5711349e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.10227203e+00  1.00000000e+00 -1.55906296e+00 ...  1.01454172e+05\n",
      "   6.78675078e+04 -3.80649707e+03]\n",
      " [-1.10243988e+00  1.00000000e+00 -1.55892563e+00 ...  2.96295898e+03\n",
      "   1.00339966e+02 -8.32813797e+01]\n",
      " [-1.10280037e+00  1.00000000e+00 -1.55868328e+00 ... -1.03852493e+02\n",
      "   1.05793129e+02  1.32985163e+01]\n",
      " ...\n",
      " [-1.10260391e+00  1.00000000e+00 -1.55879211e+00 ...  4.73168314e-01\n",
      "   4.33775902e-01 -3.15011144e-02]\n",
      " [-1.10198784e+00  1.00000000e+00 -1.55916595e+00 ... -1.17761560e+03\n",
      "  -4.56238330e+03 -6.64559766e+03]\n",
      " [-1.10206699e+00  1.00000000e+00 -1.55916405e+00 ... -7.44192352e+01\n",
      "   3.41783867e+01  4.16611786e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.6663799e-01  1.0000000e+00 -1.8234482e+00 ... -2.0876201e+04\n",
      "  -1.8975229e+04  3.9255965e+04]\n",
      " [-5.6682777e-01  1.0000000e+00 -1.8233662e+00 ... -1.3572067e+03\n",
      "  -2.2011235e+02  3.2324940e+01]\n",
      " [-5.6725121e-01  1.0000000e+00 -1.8232409e+00 ... -6.1304573e+01\n",
      "   1.4225250e+02 -2.2663245e+02]\n",
      " ...\n",
      " [-5.6702423e-01  1.0000000e+00 -1.8232975e+00 ... -5.0280094e-01\n",
      "  -1.6149731e+00  4.9100840e-01]\n",
      " [-5.6632805e-01  1.0000000e+00 -1.8234882e+00 ... -2.7817188e+03\n",
      "  -2.6505161e+03 -1.4021171e+03]\n",
      " [-5.6639385e-01  1.0000000e+00 -1.8234863e+00 ...  1.4752500e+03\n",
      "  -5.3367263e-01  9.7082843e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 2.4713516e-02  1.0000000e+00 -1.9093781e+00 ...  9.9048594e+03\n",
      "  -6.9436226e+03  3.5699139e+02]\n",
      " [ 2.4508476e-02  1.0000000e+00 -1.9093676e+00 ...  5.8530347e+02\n",
      "   4.6856693e+01  1.9774038e+01]\n",
      " [ 2.4038315e-02  1.0000000e+00 -1.9093696e+00 ... -4.4371021e+02\n",
      "   4.3906488e+02  8.9804987e+02]\n",
      " ...\n",
      " [ 2.4271011e-02  1.0000000e+00 -1.9093580e+00 ...  2.4689442e-01\n",
      "  -1.1244473e+00  1.7190099e-01]\n",
      " [ 2.4997711e-02  1.0000000e+00 -1.9093189e+00 ...  2.4574875e+03\n",
      "   1.6734789e+03  2.8771128e+03]\n",
      " [ 2.4968147e-02  1.0000000e+00 -1.9093304e+00 ...  1.7790586e+03\n",
      "   2.4136530e+02 -1.3913186e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.1396217e-01  1.0000000e+00 -1.8082218e+00 ... -2.2121704e+03\n",
      "   3.5632388e+03 -2.3438079e+03]\n",
      " [ 6.1377525e-01  1.0000000e+00 -1.8082829e+00 ...  4.4755017e+02\n",
      "  -3.6024274e+02  3.1723071e+02]\n",
      " [ 6.1329460e-01  1.0000000e+00 -1.8084114e+00 ... -3.0607925e+01\n",
      "  -2.5856461e+02 -1.7214861e+02]\n",
      " ...\n",
      " [ 6.1351776e-01  1.0000000e+00 -1.8083296e+00 ... -1.5265262e+00\n",
      "   1.0751341e+01 -8.6326754e-01]\n",
      " [ 6.1420059e-01  1.0000000e+00 -1.8080864e+00 ... -3.5150873e+02\n",
      "   5.0456088e+02  1.3536804e+02]\n",
      " [ 6.1420727e-01  1.0000000e+00 -1.8081055e+00 ...  2.2337729e+02\n",
      "   2.3688196e+02 -2.8440994e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.14255619e+00  1.00000000e+00 -1.53013802e+00 ... -1.16714134e+02\n",
      "  -6.74774551e+01 -2.00640274e+02]\n",
      " [ 1.14240170e+00  1.00000000e+00 -1.53028965e+00 ... -1.10151138e+02\n",
      "  -2.19266037e+02  3.06237518e+02]\n",
      " [ 1.14199448e+00  1.00000000e+00 -1.53053868e+00 ... -1.04687660e+02\n",
      "   1.32979574e+01  2.35303783e+01]\n",
      " ...\n",
      " [ 1.14219093e+00  1.00000000e+00 -1.53039360e+00 ...  2.82152486e+00\n",
      "  -5.80163908e+00 -4.54291910e-01]\n",
      " [ 1.14275551e+00  1.00000000e+00 -1.52992821e+00 ... -9.38118267e+00\n",
      "  -9.62561646e+01  2.83617493e+02]\n",
      " [ 1.14276695e+00  1.00000000e+00 -1.52995872e+00 ... -8.62957397e+02\n",
      "   3.78491058e+02  7.77706055e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5596304e+00  1.0000000e+00 -1.1020908e+00 ...  2.1805826e+02\n",
      "  -2.8729688e+02 -3.7549164e+01]\n",
      " [ 1.5595121e+00  1.0000000e+00 -1.1022730e+00 ...  4.7764244e+01\n",
      "  -3.5604321e+02  2.9595987e+02]\n",
      " [ 1.5592060e+00  1.0000000e+00 -1.1026055e+00 ... -3.0655598e+01\n",
      "   1.1333429e+02 -1.5626411e+02]\n",
      " ...\n",
      " [ 1.5593605e+00  1.0000000e+00 -1.1024256e+00 ...  1.5950301e+00\n",
      "  -5.1575894e+00 -8.0972958e-01]\n",
      " [ 1.5597591e+00  1.0000000e+00 -1.1018219e+00 ...  3.8142494e+01\n",
      "   2.0081204e+02  1.9613844e+02]\n",
      " [ 1.5597906e+00  1.0000000e+00 -1.1018600e+00 ... -1.7522792e+03\n",
      "   1.2272656e+03  4.5780502e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8237562e+00  1.0000000e+00 -5.6619835e-01 ... -4.8440750e+02\n",
      "  -1.4209852e+03  3.9602039e+02]\n",
      " [ 1.8236904e+00  1.0000000e+00 -5.6637764e-01 ...  2.1499031e+02\n",
      "   2.5258112e+02  4.6285355e+02]\n",
      " [ 1.8234997e+00  1.0000000e+00 -5.6676841e-01 ...  1.1517114e+03\n",
      "  -4.6082565e+02  2.7415604e+02]\n",
      " ...\n",
      " [ 1.8235912e+00  1.0000000e+00 -5.6655788e-01 ... -8.1610441e-01\n",
      "  -2.9065609e-01  7.6134771e-02]\n",
      " [ 1.8237953e+00  1.0000000e+00 -5.6590080e-01 ... -5.8048588e+01\n",
      "   5.1181992e+01  1.2607358e+02]\n",
      " [ 1.8238373e+00  1.0000000e+00 -5.6594086e-01 ...  5.6661578e+02\n",
      "  -9.0351540e+01 -4.8222229e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.70000005 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90947533e+00  1.00000000e+00  2.51770020e-02 ... -6.74018616e+02\n",
      "   6.24291382e+02 -6.11565186e+02]\n",
      " [ 1.90947342e+00  1.00000000e+00  2.49814987e-02 ...  1.20071495e+02\n",
      "  -5.56387062e+01 -1.37186604e+01]\n",
      " [ 1.90944290e+00  1.00000000e+00  2.45852340e-02 ...  4.76622833e+02\n",
      "  -2.60939484e+02  1.09519913e+02]\n",
      " ...\n",
      " [ 1.90946007e+00  1.00000000e+00  2.47898102e-02 ...  1.71410799e-01\n",
      "   2.10418224e-01 -4.49100807e-02]\n",
      " [ 1.90944481e+00  1.00000000e+00  2.54840851e-02 ... -1.16807098e+01\n",
      "  -7.91987534e+01 -5.19239922e+01]\n",
      " [ 1.90947151e+00  1.00000000e+00  2.54421234e-02 ...  5.13973541e+01\n",
      "   7.02327209e+02 -3.26026855e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8081188e+00  1.0000000e+00  6.1439133e-01 ...  7.4039893e+02\n",
      "   1.2109762e+03  1.5934538e+02]\n",
      " [ 1.8081789e+00  1.0000000e+00  6.1421680e-01 ... -2.1472589e+02\n",
      "   3.1658414e+02 -1.5279897e+02]\n",
      " [ 1.8083134e+00  1.0000000e+00  6.1383688e-01 ... -5.2051090e+01\n",
      "   4.6533173e+01 -8.3780190e+01]\n",
      " ...\n",
      " [ 1.8082676e+00  1.0000000e+00  6.1403561e-01 ...  6.1087275e-01\n",
      "  -1.6381898e+00 -1.1184913e-01]\n",
      " [ 1.8080158e+00  1.0000000e+00  6.1467934e-01 ... -2.8073834e+02\n",
      "  -7.5345184e+01  3.9929156e+00]\n",
      " [ 1.8080492e+00  1.0000000e+00  6.1464119e-01 ...  1.3846552e+02\n",
      "  -6.7454193e+01 -1.2400714e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5299568e+00  1.0000000e+00  1.1428032e+00 ...  6.3942645e+02\n",
      "   2.6679297e+02  1.1224384e+02]\n",
      " [ 1.5300751e+00  1.0000000e+00  1.1426620e+00 ...  1.0810395e+02\n",
      "   1.4438019e+02 -4.7840969e+01]\n",
      " [ 1.5303631e+00  1.0000000e+00  1.1423558e+00 ...  2.1606216e+02\n",
      "   1.8953343e+02 -5.6280865e+01]\n",
      " ...\n",
      " [ 1.5302620e+00  1.0000000e+00  1.1425104e+00 ... -3.2973003e-01\n",
      "  -1.9660721e+00 -4.2324823e-01]\n",
      " [ 1.5297623e+00  1.0000000e+00  1.1430550e+00 ...  1.2191453e+02\n",
      "   1.5031097e+02 -2.9713303e+02]\n",
      " [ 1.5298138e+00  1.0000000e+00  1.1430225e+00 ... -1.5590630e+02\n",
      "  -4.7483231e+02 -3.1273506e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1017294e+00  1.0000000e+00  1.5597839e+00 ...  8.8570328e+01\n",
      "   1.1282992e+01 -1.6532561e+02]\n",
      " [ 1.1018867e+00  1.0000000e+00  1.5596809e+00 ... -3.6740994e+00\n",
      "   8.1678596e+01 -3.7674873e+01]\n",
      " [ 1.1022949e+00  1.0000000e+00  1.5594562e+00 ... -9.7583107e+01\n",
      "  -7.8837708e+01 -1.9617132e+01]\n",
      " ...\n",
      " [ 1.1021442e+00  1.0000000e+00  1.5595694e+00 ... -1.4325416e-01\n",
      "  -3.3567281e+00 -4.7343373e-01]\n",
      " [ 1.1014709e+00  1.0000000e+00  1.5599632e+00 ... -4.4893131e+01\n",
      "  -2.8373308e+01 -7.2760910e+01]\n",
      " [ 1.1015358e+00  1.0000000e+00  1.5599403e+00 ...  3.4514041e+02\n",
      "  -4.0334909e+02 -2.1971878e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5657778     1.            1.8237171  ...  -65.26413\n",
      "    98.78197     101.84739   ]\n",
      " [   0.5659609     1.            1.8236628  ...  -52.163654\n",
      "   -20.51759      12.361847  ]\n",
      " [   0.5664196     1.            1.8235638  ... -547.61127\n",
      "   -77.56922     439.50674   ]\n",
      " ...\n",
      " [   0.5662365     1.            1.8236341  ...    0.60366535\n",
      "   -11.027007      0.709231  ]\n",
      " [   0.56547165    1.            1.8237991  ... -234.13089\n",
      "  -267.2244       93.40247   ]\n",
      " [   0.5655556     1.            1.8237839  ...   73.966286\n",
      "    51.258877    -23.301477  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.5877953e-02  1.0000000e+00  1.9090290e+00 ...  4.0487804e+00\n",
      "   2.9958366e+01 -1.2620978e+01]\n",
      " [-2.5688171e-02  1.0000000e+00  1.9090519e+00 ... -2.1659375e+02\n",
      "   3.2736069e+01 -2.5193568e+02]\n",
      " [-2.5226593e-02  1.0000000e+00  1.9090954e+00 ...  1.6591103e+02\n",
      "   1.7877686e+02  2.8471002e+02]\n",
      " ...\n",
      " [-2.5419235e-02  1.0000000e+00  1.9090948e+00 ...  5.8688337e-01\n",
      "   1.9756804e+00  3.7019402e-02]\n",
      " [-2.6212692e-02  1.0000000e+00  1.9090195e+00 ...  1.0753718e+02\n",
      "   1.3292941e+02 -4.4322895e+01]\n",
      " [-2.6109695e-02  1.0000000e+00  1.9090214e+00 ... -6.1293915e+02\n",
      "  -5.6482074e+02 -2.2389943e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.14525795e-01  1.00000000e+00  1.80745316e+00 ...  1.08631262e+03\n",
      "   3.68092896e+02  3.48074646e+02]\n",
      " [-6.14345551e-01  1.00000000e+00  1.80755329e+00 ... -8.93739548e+01\n",
      "  -1.38022442e+01 -4.63617859e+01]\n",
      " [-6.13920212e-01  1.00000000e+00  1.80771768e+00 ... -1.00316414e+02\n",
      "  -8.38196869e+01 -2.54862785e+01]\n",
      " ...\n",
      " [-6.14109039e-01  1.00000000e+00  1.80767155e+00 ...  1.01838720e+00\n",
      "   2.31564665e+00 -4.78549004e-01]\n",
      " [-6.14852905e-01  1.00000000e+00  1.80735588e+00 ...  1.97330078e+02\n",
      "  -2.24272476e+02 -2.17950150e+02]\n",
      " [-6.14742279e-01  1.00000000e+00  1.80738640e+00 ... -4.12449646e+01\n",
      "  -2.60869331e+01 -8.06918945e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.14349079e+00  1.00000000e+00  1.52882957e+00 ... -4.14358940e+01\n",
      "  -9.12151306e+02  8.08931503e+01]\n",
      " [-1.14333630e+00  1.00000000e+00  1.52902508e+00 ...  3.49220610e+00\n",
      "  -4.87654877e+01 -6.31425400e+01]\n",
      " [-1.14296150e+00  1.00000000e+00  1.52928770e+00 ...  4.85671768e+01\n",
      "   1.67330856e+02  3.00938751e+02]\n",
      " ...\n",
      " [-1.14312363e+00  1.00000000e+00  1.52920532e+00 ... -1.04189157e-01\n",
      "  -1.02215195e+00 -5.90532422e-01]\n",
      " [-1.14374161e+00  1.00000000e+00  1.52865410e+00 ...  1.15637825e+02\n",
      "  -5.98962402e+01 -7.90825882e+01]\n",
      " [-1.14367390e+00  1.00000000e+00  1.52870369e+00 ...  1.00397781e+02\n",
      "   5.52798576e+01 -1.85925125e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5601788e+00  1.0000000e+00  1.1005230e+00 ... -5.6497440e+01\n",
      "  -9.4393723e+01  7.8773274e+00]\n",
      " [-1.5600691e+00  1.0000000e+00  1.1007786e+00 ...  1.2142754e+02\n",
      "   8.8565302e-01 -1.4095490e+02]\n",
      " [-1.5598164e+00  1.0000000e+00  1.1011360e+00 ... -4.8114749e+02\n",
      "  -3.7419501e+02  4.4823074e+02]\n",
      " ...\n",
      " [-1.5599365e+00  1.0000000e+00  1.1010132e+00 ... -2.1853716e+00\n",
      "  -4.5626206e+00 -6.5163219e-01]\n",
      " [-1.5603771e+00  1.0000000e+00  1.1002560e+00 ... -8.4301154e+02\n",
      "   4.3719986e+02 -6.4780876e+01]\n",
      " [-1.5603151e+00  1.0000000e+00  1.1003246e+00 ... -1.6793202e+02\n",
      "  -6.3841187e+02 -7.7075142e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82389164e+00  1.00000000e+00  5.64332962e-01 ... -1.24143097e+02\n",
      "   1.30318741e+02  5.16295128e+01]\n",
      " [-1.82385063e+00  1.00000000e+00  5.64635277e-01 ...  3.40070686e+01\n",
      "   8.76509552e+01  4.06330780e+02]\n",
      " [-1.82370758e+00  1.00000000e+00  5.65052271e-01 ...  1.30952515e+02\n",
      "  -3.15658173e+02  1.91149445e+01]\n",
      " ...\n",
      " [-1.82378197e+00  1.00000000e+00  5.64899445e-01 ... -2.91261578e+00\n",
      "  -8.93009567e+00 -4.01761770e-01]\n",
      " [-1.82401466e+00  1.00000000e+00  5.64006805e-01 ... -5.68337669e+01\n",
      "   4.25357437e+01 -9.76026077e+01]\n",
      " [-1.82395744e+00  1.00000000e+00  5.64090729e-01 ...  2.50093948e+02\n",
      "   1.13674225e+02 -2.23049210e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.1       1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9089193e+00  1.0000000e+00 -2.7086258e-02 ... -1.2384190e+02\n",
      "  -1.9944540e+01  1.4587135e+02]\n",
      " [-1.9089518e+00  1.0000000e+00 -2.6787758e-02 ... -1.4295959e+00\n",
      "  -1.0678826e+02 -5.0719952e+01]\n",
      " [-1.9089336e+00  1.0000000e+00 -2.6345618e-02 ... -6.1562085e+00\n",
      "  -2.5802609e+02  2.5870758e+02]\n",
      " ...\n",
      " [-1.9089642e+00  1.0000000e+00 -2.6515961e-02 ...  1.9734371e+00\n",
      "   6.0068455e+00 -8.2335162e-01]\n",
      " [-1.9089336e+00  1.0000000e+00 -2.7429581e-02 ...  3.2258466e+02\n",
      "   4.0682660e+02  9.3247986e+01]\n",
      " [-1.9089146e+00  1.0000000e+00 -2.7339935e-02 ...  2.8542096e+02\n",
      "   1.6123085e+02  2.8366965e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8072214e+00  1.0000000e+00 -6.1592865e-01 ... -5.3121734e+00\n",
      "   7.7468638e+00 -1.0453570e+01]\n",
      " [-1.8073244e+00  1.0000000e+00 -6.1565399e-01 ... -1.5666650e+02\n",
      "   1.1785769e+02 -4.5233334e+01]\n",
      " [-1.8074284e+00  1.0000000e+00 -6.1522859e-01 ... -1.8717049e+02\n",
      "   6.8535568e+01  6.7062355e+01]\n",
      " ...\n",
      " [-1.8073959e+00  1.0000000e+00 -6.1539268e-01 ...  2.5155783e+00\n",
      "   8.4220858e+00  1.5303487e-01]\n",
      " [-1.8071098e+00  1.0000000e+00 -6.1626053e-01 ...  1.7999606e+00\n",
      "  -1.4276824e+00  4.4449844e+00]\n",
      " [-1.8071375e+00  1.0000000e+00 -6.1617279e-01 ...  1.2519766e+01\n",
      "  -3.5482758e+01  6.8951538e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.1       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.52842808e+00  1.00000000e+00 -1.14451981e+00 ... -2.53232697e+02\n",
      "  -8.64558228e+02  5.82722168e+02]\n",
      " [-1.52861214e+00  1.00000000e+00 -1.14426422e+00 ...  1.15188515e+02\n",
      "  -1.87035965e+02  1.67857773e+02]\n",
      " [-1.52882385e+00  1.00000000e+00 -1.14390087e+00 ...  2.46055832e+01\n",
      "   1.04837875e+02 -5.10614443e+00]\n",
      " ...\n",
      " [-1.52874374e+00  1.00000000e+00 -1.14403725e+00 ...  2.47620058e+00\n",
      "   7.12082720e+00  3.99092942e-01]\n",
      " [-1.52823639e+00  1.00000000e+00 -1.14480019e+00 ...  9.31029439e-01\n",
      "   6.85453949e+01  4.56144714e+01]\n",
      " [-1.52827835e+00  1.00000000e+00 -1.14472580e+00 ...  3.06488171e+01\n",
      "   1.52255783e+01 -5.83583794e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.099803     1.          -1.5610905 ... -206.45386    177.1804\n",
      "   -80.1044   ]\n",
      " [  -1.1000395    1.          -1.5609102 ...  135.09308   -523.3545\n",
      "    -8.051549 ]\n",
      " [  -1.1003456    1.          -1.5606384 ...  130.77548     59.52243\n",
      "   113.50876  ]\n",
      " ...\n",
      " [  -1.1002216    1.          -1.5607424 ...    0.9064217    7.1650295\n",
      "     2.2152977]\n",
      " [  -1.0995255    1.          -1.5613022 ...   27.630524   -16.365002\n",
      "    11.591366 ]\n",
      " [  -1.099596     1.          -1.5612469 ... -387.23605     -7.789449\n",
      "   296.83405  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.563818      1.           -1.8245869  ... -141.1986\n",
      "  -260.2824      -44.09094   ]\n",
      " [  -0.5640936     1.           -1.8244963  ... -556.2572\n",
      "   -41.815277   -335.15387   ]\n",
      " [  -0.5644283     1.           -1.8243449  ...   13.174603\n",
      "    -4.137324    278.2638    ]\n",
      " ...\n",
      " [  -0.5642834     1.           -1.8244028  ...   -0.7833446\n",
      "     3.638678      0.69882256]\n",
      " [  -0.56347275    1.           -1.8247299  ...  114.58402\n",
      "    30.1258     -126.86849   ]\n",
      " [  -0.5635786     1.           -1.8246937  ...  176.42827\n",
      "    52.151745   -121.798225  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 2.7924538e-02  1.0000000e+00 -1.9095879e+00 ...  5.1083122e+01\n",
      "  -4.6171823e+00 -1.5964998e+01]\n",
      " [ 2.7635574e-02  1.0000000e+00 -1.9095984e+00 ...  2.2085294e+02\n",
      "   3.3079193e+02 -3.3393765e+02]\n",
      " [ 2.7299881e-02  1.0000000e+00 -1.9095656e+00 ... -2.6694990e+01\n",
      "   8.1167809e+01 -5.4115887e+01]\n",
      " ...\n",
      " [ 2.7450562e-02  1.0000000e+00 -1.9095840e+00 ... -4.8199085e+01\n",
      "   3.4764280e+00 -4.8580032e+01]\n",
      " [ 2.8301239e-02  1.0000000e+00 -1.9096260e+00 ... -6.4443771e+01\n",
      "  -1.8467006e+02  7.6252449e+01]\n",
      " [ 2.8174400e-02  1.0000000e+00 -1.9096241e+00 ... -4.8186249e+01\n",
      "  -1.3592021e+02  4.0208454e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.1638927e-01  1.0000000e+00 -1.8076172e+00 ...  7.1548782e+01\n",
      "   5.9886742e-01  2.7077576e+01]\n",
      " [ 6.1612034e-01  1.0000000e+00 -1.8077126e+00 ... -1.9034192e+02\n",
      "  -8.5176041e+01 -5.5565151e+01]\n",
      " [ 6.1580276e-01  1.0000000e+00 -1.8078084e+00 ... -4.4758884e+02\n",
      "  -1.4782626e+02  7.9328772e+02]\n",
      " ...\n",
      " [ 6.1594391e-01  1.0000000e+00 -1.8077641e+00 ...  6.9748459e+01\n",
      "  -4.0211349e+01  4.5674065e+01]\n",
      " [ 6.1674309e-01  1.0000000e+00 -1.8075390e+00 ... -3.9882519e+01\n",
      "   7.4540009e+01  6.5369141e+01]\n",
      " [ 6.1662674e-01  1.0000000e+00 -1.8075733e+00 ...  1.6580647e+02\n",
      "  -3.8352747e+02  1.5341841e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1448994    1.          -1.5286083 ...   56.936962   -44.781895\n",
      "    20.298103 ]\n",
      " [   1.1446657    1.          -1.5288229 ...   -9.711387  -311.48553\n",
      "  -156.92741  ]\n",
      " [   1.1444111    1.          -1.5290095 ...  -30.244307    16.632711\n",
      "  -142.74445  ]\n",
      " ...\n",
      " [   1.1445217    1.          -1.5289335 ...  222.1696     312.59024\n",
      "  -183.72269  ]\n",
      " [   1.1452026    1.          -1.5284481 ...  -63.95653     21.531586\n",
      "    26.621641 ]\n",
      " [   1.1450996    1.          -1.5285072 ...  106.76073    292.2354\n",
      "   257.1071   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5613127    1.          -1.0998745 ... -101.04666      6.955109\n",
      "    65.52043  ]\n",
      " [   1.561142     1.          -1.1001759 ...  196.3633      96.2271\n",
      "    65.92588  ]\n",
      " [   1.5609608    1.          -1.1004236 ... -388.59204    474.90744\n",
      "  -476.48877  ]\n",
      " ...\n",
      " [   1.5610485    1.          -1.1003361 ...  156.32393     73.267395\n",
      "     6.587856 ]\n",
      " [   1.5615177    1.          -1.099638  ...  -13.812427    22.974556\n",
      "    92.0155   ]\n",
      " [   1.5614729    1.          -1.09972   ...   96.42527   -854.24646\n",
      "   469.42413  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8245859     1.           -0.563488   ...   31.108725\n",
      "   -68.7521       38.883274  ]\n",
      " [   1.8245125     1.           -0.56380844 ...  -20.61008\n",
      "   -61.35506       9.509447  ]\n",
      " [   1.8244457     1.           -0.564111   ...  -18.146566\n",
      "    15.308024   -178.44865   ]\n",
      " ...\n",
      " [   1.8244915     1.           -0.56399155 ... -298.36337\n",
      "  -158.91605    -349.97327   ]\n",
      " [   1.8247147     1.           -0.56319046 ...  -58.682053\n",
      "  -106.69366     -28.53891   ]\n",
      " [   1.8246784     1.           -0.56328964 ...  -34.432354\n",
      "    46.69057     -87.82469   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9093952e+00  1.0000000e+00  2.8091431e-02 ... -2.5465468e+02\n",
      "  -1.6685593e+02  4.5454279e+02]\n",
      " [ 1.9094172e+00  1.0000000e+00  2.7767181e-02 ...  1.1787842e+02\n",
      "  -1.5524985e+02  6.7238441e+01]\n",
      " [ 1.9095116e+00  1.0000000e+00  2.7435392e-02 ...  9.2095078e+01\n",
      "  -1.1500211e+02  5.9731827e+00]\n",
      " ...\n",
      " [ 1.9095097e+00  1.0000000e+00  2.7576447e-02 ...  2.6605399e+02\n",
      "   1.0674138e+02  1.4494885e+02]\n",
      " [ 1.9094620e+00  1.0000000e+00  2.8411865e-02 ...  6.1021145e+01\n",
      "  -8.0718506e+01  2.0340907e+02]\n",
      " [ 1.9094095e+00  1.0000000e+00  2.8301239e-02 ... -2.4237575e+02\n",
      "   3.9263522e+02  8.1588959e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8073301     1.            0.61652565 ...  -12.741814\n",
      "   -99.66933      20.13992   ]\n",
      " [   1.8074532     1.            0.61618996 ...  263.95294\n",
      "   242.036       145.63635   ]\n",
      " [   1.8076324     1.            0.6158755  ...    6.0579257\n",
      "   -53.870842     33.558422  ]\n",
      " ...\n",
      " [   1.807581      1.            0.6160116  ...   73.903465\n",
      "   102.09943     -12.534727  ]\n",
      " [   1.80727       1.            0.616827   ...  -36.3132\n",
      "   -20.703308    -82.45149   ]\n",
      " [   1.8072786     1.            0.616724   ...  -66.894844\n",
      "  -124.21934    -132.86803   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5280943e+00  1.0000000e+00  1.1451855e+00 ...  9.6164902e+01\n",
      "  -5.3941753e+01 -1.6491127e+01]\n",
      " [ 1.5283051e+00  1.0000000e+00  1.1448917e+00 ...  9.3785156e+01\n",
      "   4.0355949e+01  3.5717163e+01]\n",
      " [ 1.5286522e+00  1.0000000e+00  1.1446260e+00 ... -8.0726509e+00\n",
      "   1.3320096e+02 -1.2723371e+01]\n",
      " ...\n",
      " [ 1.5285606e+00  1.0000000e+00  1.1447315e+00 ...  9.2781998e+01\n",
      "  -1.3219131e+02  1.5047523e+02]\n",
      " [ 1.5280056e+00  1.0000000e+00  1.1454468e+00 ...  7.6523950e+03\n",
      "  -1.0065388e+04  1.2953062e+04]\n",
      " [ 1.5279799e+00  1.0000000e+00  1.1453514e+00 ...  9.1744965e+01\n",
      "  -2.7382906e+01  1.7909081e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         1.0000001\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.09949589e+00  1.00000000e+00  1.56117821e+00 ... -6.38428650e+02\n",
      "   1.76848175e+02 -4.36383362e+02]\n",
      " [ 1.09978104e+00  1.00000000e+00  1.56099892e+00 ...  7.45493393e+01\n",
      "  -2.82071777e+02  5.99235153e+01]\n",
      " [ 1.10024261e+00  1.00000000e+00  1.56081545e+00 ...  2.45893192e+01\n",
      "   1.01135731e-01  1.09314438e+02]\n",
      " ...\n",
      " [ 1.10012245e+00  1.00000000e+00  1.56089115e+00 ... -3.79616302e+02\n",
      "  -1.17759766e+02  1.23042310e+03]\n",
      " [ 1.09935570e+00  1.00000000e+00  1.56135559e+00 ...  3.62655566e+03\n",
      "  -8.52968262e+03  8.96834375e+03]\n",
      " [ 1.09934139e+00  1.00000000e+00  1.56128120e+00 ... -9.03384781e+01\n",
      "   4.45676498e+01 -9.78177071e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.6317234e-01  1.0000000e+00  1.8245430e+00 ...  2.7468262e+01\n",
      "  -2.0722691e+01  9.4312469e+01]\n",
      " [ 5.6350517e-01  1.0000000e+00  1.8244495e+00 ...  1.9766061e+02\n",
      "  -7.8201271e+01  2.3292101e+02]\n",
      " [ 5.6401062e-01  1.0000000e+00  1.8243926e+00 ...  1.7337714e+02\n",
      "   2.6098526e+02 -5.4213342e+02]\n",
      " ...\n",
      " [ 5.6386566e-01  1.0000000e+00  1.8244076e+00 ...  1.2080872e+03\n",
      "   1.8871478e+02 -4.6603725e+01]\n",
      " [ 5.6298447e-01  1.0000000e+00  1.8246326e+00 ... -6.9247791e+02\n",
      "  -5.0186035e+01  1.4490870e+03]\n",
      " [ 5.6298733e-01  1.0000000e+00  1.8245888e+00 ... -6.4829140e+01\n",
      "  -1.4784448e+02 -3.2965827e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-2.83393860e-02  1.00000000e+00  1.90927315e+00 ... -8.71853733e+00\n",
      "   1.01064148e+01 -2.23043976e+01]\n",
      " [-2.79951096e-02  1.00000000e+00  1.90928936e+00 ... -1.23057434e+02\n",
      "   2.55398224e+02 -2.52534485e+02]\n",
      " [-2.74372101e-02  1.00000000e+00  1.90936148e+00 ...  1.20306870e+02\n",
      "  -1.32537827e+02  1.78529816e+02]\n",
      " ...\n",
      " [-2.75936127e-02  1.00000000e+00  1.90932846e+00 ...  6.48248596e+02\n",
      "  -1.65714020e+02 -2.30502670e+02]\n",
      " [-2.85167694e-02  1.00000000e+00  1.90929985e+00 ... -2.56368805e+02\n",
      "   5.29240036e+01 -5.48504333e+02]\n",
      " [-2.85282135e-02  1.00000000e+00  1.90928078e+00 ...  4.47021942e+02\n",
      "   1.44886307e+02 -7.20945129e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:4, Score:2.30, Best Score:2.33, Average Score:1.70, Best Avg Score:1.72\n",
      "Episode number: 5\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57d2d4310>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1456785    1.           1.527628  ...  -80.435455  -115.7735\n",
      "    54.415382 ]\n",
      " [  -1.1454105    1.           1.5278397 ...  -22.377428   125.16624\n",
      "    98.27567  ]\n",
      " [  -1.144947     1.           1.5281632 ...  193.48384    632.0928\n",
      "   106.91556  ]\n",
      " ...\n",
      " [  -1.1450729    1.           1.5280457 ...   -5.869069    45.90578\n",
      "   -17.596281 ]\n",
      " [  -1.1458073    1.           1.5275078 ... -150.29906     11.345025\n",
      "   162.7383   ]\n",
      " [  -1.1458387    1.           1.527525  ... -489.55807   -215.1239\n",
      "    13.454558 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.56154346e+00  1.00000000e+00  1.09901810e+00 ... -5.82058430e-02\n",
      "  -7.43010044e+00 -1.10431004e+01]\n",
      " [-1.56135845e+00  1.00000000e+00  1.09929085e+00 ...  2.56829987e+02\n",
      "  -2.55704865e+02  4.39809036e+01]\n",
      " [-1.56097984e+00  1.00000000e+00  1.09973252e+00 ... -2.74174866e+02\n",
      "  -5.68676025e+02 -3.10822540e+02]\n",
      " ...\n",
      " [-1.56107140e+00  1.00000000e+00  1.09957409e+00 ...  3.68980072e+02\n",
      "   1.80814610e+01 -3.91933655e+02]\n",
      " [-1.56162834e+00  1.00000000e+00  1.09884262e+00 ... -4.76859703e+01\n",
      "  -3.43140984e+00  8.53922882e+01]\n",
      " [-1.56166172e+00  1.00000000e+00  1.09886742e+00 ... -8.51680145e+01\n",
      "   2.22816086e+02  1.05004974e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82488155e+00  1.00000000e+00  5.62368393e-01 ...  7.87562637e+01\n",
      "  -3.85330887e+01  3.66349030e+02]\n",
      " [-1.82478428e+00  1.00000000e+00  5.62694550e-01 ...  1.23167224e+03\n",
      "   6.16855957e+02  1.08755432e+02]\n",
      " [-1.82455635e+00  1.00000000e+00  5.63210070e-01 ... -1.68483142e+03\n",
      "   7.22214966e+02 -4.24583801e+02]\n",
      " ...\n",
      " [-1.82460594e+00  1.00000000e+00  5.63027382e-01 ...  2.09086594e+02\n",
      "   1.23890083e+02 -2.45734768e+01]\n",
      " [-1.82488060e+00  1.00000000e+00  5.62162399e-01 ...  1.98890518e+02\n",
      "   1.34982346e+02 -1.14306595e+02]\n",
      " [-1.82494736e+00  1.00000000e+00  5.62191010e-01 ...  1.29009048e+02\n",
      "  -1.10567314e+02 -1.09071625e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.1 0.5 0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9093838e+00  1.0000000e+00 -2.9262543e-02 ... -5.4343036e+02\n",
      "  -5.0914371e+02 -8.4073871e+02]\n",
      " [-1.9093838e+00  1.0000000e+00 -2.8875351e-02 ... -4.9213648e+00\n",
      "   3.6585735e+01 -3.8259960e+01]\n",
      " [-1.9092960e+00  1.0000000e+00 -2.8347189e-02 ...  1.5133519e+01\n",
      "  -1.1877611e+02  1.1773116e+02]\n",
      " ...\n",
      " [-1.9093132e+00  1.0000000e+00 -2.8532028e-02 ... -9.8111961e+01\n",
      "   7.0649261e+00 -2.1116966e+01]\n",
      " [-1.9092999e+00  1.0000000e+00 -2.9476166e-02 ...  9.9442368e+01\n",
      "  -2.5414877e+01 -2.3101440e+01]\n",
      " [-1.9093771e+00  1.0000000e+00 -2.9447556e-02 ... -1.7954391e+02\n",
      "   4.8149509e+00  4.3710461e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.806963      1.           -0.61777115 ...   -8.354065\n",
      "   -60.653343      5.9311323 ]\n",
      " [  -1.807066      1.           -0.617424   ...   32.733536\n",
      "    72.54121      31.67516   ]\n",
      " [  -1.8071404     1.           -0.6169062  ...   -2.5944643\n",
      "   -15.922745     10.066882  ]\n",
      " ...\n",
      " [  -1.8071136     1.           -0.61709595 ... -473.11316\n",
      "   567.22797     -59.519222  ]\n",
      " [  -1.8068123     1.           -0.6179714  ... -138.03589\n",
      "    19.180206     28.0095    ]\n",
      " [  -1.8068991     1.           -0.6179466  ...  -24.842102\n",
      "    36.196083     67.10136   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.1       0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5275908    1.          -1.1460209 ...  -53.921875   -23.74315\n",
      "   -26.88478  ]\n",
      " [  -1.5277767    1.          -1.1457253 ...  -45.725746    48.755516\n",
      "   -86.86718  ]\n",
      " [  -1.5280094    1.          -1.1452873 ...   73.47675   -253.77982\n",
      "  -255.26093  ]\n",
      " ...\n",
      " [  -1.5279198    1.          -1.1454353 ...  319.37463    143.24751\n",
      "  -125.62694  ]\n",
      " [  -1.5273495    1.          -1.1461887 ...  303.19623    546.15564\n",
      "  -558.362    ]\n",
      " [  -1.5274715    1.          -1.1461678 ...   -9.584834    -2.0006535\n",
      "    -7.298641 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0985756e+00  1.0000000e+00 -1.5620689e+00 ...  2.6655949e+01\n",
      "   3.4507965e+01 -1.0180037e+01]\n",
      " [-1.0988541e+00  1.0000000e+00 -1.5618401e+00 ...  1.7373932e+01\n",
      "   1.8527496e+02  6.7342728e+01]\n",
      " [-1.0991745e+00  1.0000000e+00 -1.5615062e+00 ... -1.4060582e+02\n",
      "   2.1410500e+02  2.2626547e+02]\n",
      " ...\n",
      " [-1.0990486e+00  1.0000000e+00 -1.5616131e+00 ... -6.7799133e+01\n",
      "  -1.3910467e+01  2.3703356e+01]\n",
      " [-1.0982819e+00  1.0000000e+00 -1.5621872e+00 ...  9.5142065e+02\n",
      "   5.5827667e+01 -1.0349800e+03]\n",
      " [-1.0984125e+00  1.0000000e+00 -1.5621758e+00 ... -2.3498585e+01\n",
      "   4.7062977e+01  7.5878746e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.8000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.5\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.61820984e-01  1.00000000e+00 -1.82495689e+00 ... -1.21854385e+02\n",
      "   2.25567627e+00 -4.44901352e+01]\n",
      " [-5.62150955e-01  1.00000000e+00 -1.82487011e+00 ...  3.47367554e+01\n",
      "   2.51757614e+02 -6.64813004e+01]\n",
      " [-5.62526703e-01  1.00000000e+00 -1.82466090e+00 ... -2.98091240e+01\n",
      "   4.13682289e+01  2.48701363e+01]\n",
      " ...\n",
      " [-5.62379837e-01  1.00000000e+00 -1.82473564e+00 ... -5.98232422e+01\n",
      "   1.43448648e+01 -1.02079926e+02]\n",
      " [-5.61477661e-01  1.00000000e+00 -1.82503891e+00 ... -9.63174683e+02\n",
      "   8.81785706e+02  1.52340027e+02]\n",
      " [-5.61628342e-01  1.00000000e+00 -1.82503891e+00 ... -2.73535767e+01\n",
      "   5.67512207e+01  1.17003632e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 2.9735565e-02  1.0000000e+00 -1.9089851e+00 ...  3.1598090e+02\n",
      "   1.4883559e+02  5.2581894e+01]\n",
      " [ 2.9388428e-02  1.0000000e+00 -1.9090309e+00 ...  2.6357401e+01\n",
      "  -7.2048459e+00 -3.2208853e+00]\n",
      " [ 2.8982162e-02  1.0000000e+00 -1.9089799e+00 ... -4.8127754e+01\n",
      "   9.6289864e+01 -9.1090202e+00]\n",
      " ...\n",
      " [ 2.9136658e-02  1.0000000e+00 -1.9089975e+00 ...  9.5313971e+02\n",
      "  -1.4434342e+03 -2.4183442e+03]\n",
      " [ 3.0084610e-02  1.0000000e+00 -1.9090118e+00 ...  1.2462640e+02\n",
      "  -1.8924582e+02  2.8301197e+01]\n",
      " [ 2.9931068e-02  1.0000000e+00 -1.9090328e+00 ...  1.1231024e+01\n",
      "  -1.3640433e+02 -4.0371099e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.18071556e-01  1.00000000e+00 -1.80626869e+00 ...  3.25821960e+02\n",
      "   1.54805505e+03 -1.85978564e+03]\n",
      " [ 6.17747307e-01  1.00000000e+00 -1.80644798e+00 ... -8.31138077e+01\n",
      "  -4.14877586e+01 -5.58065262e+01]\n",
      " [ 6.17326736e-01  1.00000000e+00 -1.80653584e+00 ...  9.38472168e+02\n",
      "   6.76235596e+02 -1.29156281e+02]\n",
      " ...\n",
      " [ 6.17473602e-01  1.00000000e+00 -1.80650806e+00 ... -8.36616394e+02\n",
      "  -1.08152374e+02  6.39205055e+01]\n",
      " [ 6.18375778e-01  1.00000000e+00 -1.80619431e+00 ...  2.85787872e+02\n",
      "   6.46627808e+01 -1.74073620e+01]\n",
      " [ 6.18257523e-01  1.00000000e+00 -1.80624962e+00 ... -3.67807312e+01\n",
      "   1.79433716e+02  7.24089127e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1464996e+00  1.0000000e+00 -1.5265808e+00 ... -2.8501425e+02\n",
      "   3.5113074e+02  4.6227020e+02]\n",
      " [ 1.1462297e+00  1.0000000e+00 -1.5268583e+00 ... -4.1028482e+02\n",
      "   3.5762157e+01  2.1680513e+02]\n",
      " [ 1.1458740e+00  1.0000000e+00 -1.5270725e+00 ... -9.3678003e+02\n",
      "   6.6091101e+02 -7.4215381e+02]\n",
      " ...\n",
      " [ 1.1459999e+00  1.0000000e+00 -1.5269957e+00 ... -1.0992765e+03\n",
      "  -7.3243396e+02 -1.4538483e+03]\n",
      " [ 1.1467419e+00  1.0000000e+00 -1.5264378e+00 ...  1.1560122e+01\n",
      "  -3.1205799e+01  5.3693047e+01]\n",
      " [ 1.1466541e+00  1.0000000e+00 -1.5265141e+00 ... -1.3459633e+02\n",
      "  -4.2478981e+01  2.9757299e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5623703e+00  1.0000000e+00 -1.0973930e+00 ... -4.3450790e+02\n",
      "   6.7177452e+01  7.5680847e+02]\n",
      " [ 1.5621910e+00  1.0000000e+00 -1.0977678e+00 ... -1.2224643e+02\n",
      "   1.2744268e+02  1.3967413e+02]\n",
      " [ 1.5619469e+00  1.0000000e+00 -1.0980823e+00 ... -4.7974100e+00\n",
      "  -3.8351190e+02 -7.4013550e+01]\n",
      " ...\n",
      " [ 1.5620327e+00  1.0000000e+00 -1.0979662e+00 ... -6.2343384e+02\n",
      "   3.7087444e+01  1.3995208e+03]\n",
      " [ 1.5625534e+00  1.0000000e+00 -1.0971909e+00 ...  2.0143732e+01\n",
      "   6.5653641e+01  1.6180553e+02]\n",
      " [ 1.5624866e+00  1.0000000e+00 -1.0972919e+00 ...  4.8923242e+02\n",
      "  -1.4086784e+02 -2.6769041e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.5\n",
      " 0.         0.         0.         0.         0.3        0.\n",
      " 0.         0.70000005 0.4        0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8251076e+00  1.0000000e+00 -5.6112671e-01 ... -9.1831779e+01\n",
      "   1.8158655e+00 -1.1745127e+02]\n",
      " [ 1.8250256e+00  1.0000000e+00 -5.6153393e-01 ...  1.3490306e+02\n",
      "   5.8925885e+02 -2.1622241e+02]\n",
      " [ 1.8248940e+00  1.0000000e+00 -5.6190896e-01 ...  9.3079395e+02\n",
      "   4.3384392e+01  7.1471704e+02]\n",
      " ...\n",
      " [ 1.8249302e+00  1.0000000e+00 -5.6176281e-01 ...  6.7088020e+01\n",
      "  -9.3447334e+01 -3.3579745e+02]\n",
      " [ 1.8251915e+00  1.0000000e+00 -5.6089783e-01 ... -1.0362086e+00\n",
      "  -9.4469460e+01  1.0222970e+02]\n",
      " [ 1.8251858e+00  1.0000000e+00 -5.6100655e-01 ...  5.1725361e+01\n",
      "   5.6505329e+01 -1.9832214e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.1       0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9093208e+00  1.0000000e+00  3.0416489e-02 ... -5.1721642e+01\n",
      "   5.2897099e+01  5.4507263e+01]\n",
      " [ 1.9093390e+00  1.0000000e+00  3.0018806e-02 ... -3.7834644e+01\n",
      "  -4.9037895e+01 -5.6396347e+01]\n",
      " [ 1.9093552e+00  1.0000000e+00  2.9621508e-02 ...  9.4902214e+01\n",
      "   7.3088455e+01  8.7901947e+01]\n",
      " ...\n",
      " [ 1.9093342e+00  1.0000000e+00  2.9775620e-02 ... -8.4224457e+02\n",
      "   4.2973022e+02 -1.9260788e+01]\n",
      " [ 1.9092884e+00  1.0000000e+00  3.0658722e-02 ... -1.3067911e+02\n",
      "   9.4708725e+01 -2.7573453e+02]\n",
      " [ 1.9093533e+00  1.0000000e+00  3.0546188e-02 ... -3.0735455e+02\n",
      "  -3.9462378e+02  2.4418509e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1       0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.8064537    1.           0.6192589  ... 526.1912     216.23418\n",
      "  172.35683   ]\n",
      " [  1.8065939    1.           0.61889935 ... -24.948029   -82.81477\n",
      "   86.10888   ]\n",
      " [  1.8067589    1.           0.61850864 ... -99.99852    -24.856073\n",
      "   49.062172  ]\n",
      " ...\n",
      " [  1.806694     1.           0.61866856 ...   1.126153    57.949932\n",
      "  138.558     ]\n",
      " [  1.8063698    1.           0.6194744  ...  43.32332     11.33695\n",
      "  -42.983196  ]\n",
      " [  1.8064413    1.           0.6193714  ...  40.254547    51.92828\n",
      "  -67.73263   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5267696    1.           1.1471748 ... -676.2573    -433.26978\n",
      "   324.5715   ]\n",
      " [   1.52703      1.           1.1468744 ...  370.1304     587.81274\n",
      "  -149.03079  ]\n",
      " [   1.5272865    1.           1.1465409 ...  334.84827    181.35283\n",
      "    43.06979  ]\n",
      " ...\n",
      " [   1.5271759    1.           1.1466789 ...   -5.6598873  -33.48305\n",
      "   -23.104172 ]\n",
      " [   1.5266037    1.           1.1473484 ...  327.66266    113.10627\n",
      "   199.20877  ]\n",
      " [   1.5267191    1.           1.1472588 ... -100.6245      96.124016\n",
      "   -89.321365 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0972233    1.           1.563076  ... -288.17734    -75.00854\n",
      "  -193.3543   ]\n",
      " [   1.0975714    1.           1.5628519 ...  -30.013372   111.69536\n",
      "    31.424004 ]\n",
      " [   1.0979347    1.           1.5626161 ...  -84.29964     35.05519\n",
      "    -7.8547573]\n",
      " ...\n",
      " [   1.0977898    1.           1.5627155 ...  369.74847   -154.39256\n",
      "   174.99495  ]\n",
      " [   1.0970116    1.           1.5631962 ... -121.25584    -31.334976\n",
      "  -289.57266  ]\n",
      " [   1.0971489    1.           1.563139  ...  120.801926   -88.935326\n",
      "  -317.48303  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.6087017e-01  1.0000000e+00  1.8255329e+00 ... -2.8137164e+02\n",
      "  -6.9550140e+01 -2.3939388e+02]\n",
      " [ 5.6127071e-01  1.0000000e+00  1.8254547e+00 ... -9.8041893e+01\n",
      "   2.0749372e+01  1.2374647e+01]\n",
      " [ 5.6170273e-01  1.0000000e+00  1.8253244e+00 ... -6.7262520e+01\n",
      "  -5.9633793e+01  5.0881802e+01]\n",
      " ...\n",
      " [ 5.6153488e-01  1.0000000e+00  1.8253899e+00 ... -1.5995100e+02\n",
      "  -7.8098785e+01 -6.9291742e+02]\n",
      " [ 5.6063461e-01  1.0000000e+00  1.8255882e+00 ... -1.1487990e+02\n",
      "  -6.3717182e+01 -4.6092434e+01]\n",
      " [ 5.6076908e-01  1.0000000e+00  1.8255692e+00 ...  4.0234047e+01\n",
      "  -5.0262024e+01 -6.3772556e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.1127930e-02  1.0000000e+00  1.9094944e+00 ... -3.1460076e+03\n",
      "   5.2215542e+03  4.3346558e+03]\n",
      " [-3.0714989e-02  1.0000000e+00  1.9095440e+00 ... -3.2201151e+02\n",
      "   2.7533281e+01  3.8421014e+02]\n",
      " [-3.0282974e-02  1.0000000e+00  1.9095489e+00 ... -2.2878174e+03\n",
      "  -1.1977861e+03 -7.4209192e+02]\n",
      " ...\n",
      " [-3.0462265e-02  1.0000000e+00  1.9095669e+00 ...  8.0203552e+00\n",
      "   4.7563232e+01 -2.5410307e+01]\n",
      " [-3.1394958e-02  1.0000000e+00  1.9094563e+00 ...  2.3043318e+02\n",
      "   2.3436430e+01  1.5891354e+01]\n",
      " [-3.1232834e-02  1.0000000e+00  1.9094791e+00 ... -4.7193539e+01\n",
      "   9.1664124e+01 -1.2967838e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.1969185e-01  1.0000000e+00  1.8063431e+00 ...  5.0420737e+02\n",
      "   3.2274655e+02 -3.7603355e+01]\n",
      " [-6.1929607e-01  1.0000000e+00  1.8065262e+00 ...  2.0876901e+02\n",
      "  -1.1767888e+02 -1.7812483e+01]\n",
      " [-6.1888695e-01  1.0000000e+00  1.8066763e+00 ...  4.1033896e+02\n",
      "   1.3505717e+02  2.6740286e+02]\n",
      " ...\n",
      " [-6.1905861e-01  1.0000000e+00  1.8066416e+00 ...  8.1002472e+02\n",
      "   4.4952307e+02  2.1930006e+01]\n",
      " [-6.1993408e-01  1.0000000e+00  1.8062458e+00 ...  2.6398952e+01\n",
      "  -2.8112255e+01  2.0240135e+01]\n",
      " [-6.1979294e-01  1.0000000e+00  1.8062897e+00 ... -1.8057430e+01\n",
      "  -8.2395563e+00  1.9954449e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1476135    1.           1.5263577 ... -970.1314     -65.22456\n",
      "   298.6043   ]\n",
      " [  -1.1472712    1.           1.5266733 ...  -13.263733  -292.6308\n",
      "   -79.535934 ]\n",
      " [  -1.146944     1.           1.5269375 ... -184.50894    188.43289\n",
      "    67.49756  ]\n",
      " ...\n",
      " [  -1.147089     1.           1.5268602 ...   40.694084  -217.80814\n",
      "  -141.37402  ]\n",
      " [  -1.147829     1.           1.5261898 ... -124.42566    590.9968\n",
      "  -616.25446  ]\n",
      " [  -1.1476984    1.           1.5262642 ... -149.26428    319.54227\n",
      "  -215.7064   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5630207    1.           1.0971203 ... -109.404076  -307.11053\n",
      "  -443.3306   ]\n",
      " [  -1.5627651    1.           1.0975513 ...  149.05348     97.56246\n",
      "    -5.368092 ]\n",
      " [  -1.5625248    1.           1.0979024 ...  -40.34666     38.57374\n",
      "   -99.14045  ]\n",
      " ...\n",
      " [  -1.5626354    1.           1.0978079 ...  -58.86518   -158.10365\n",
      "  -106.19339  ]\n",
      " [  -1.5631886    1.           1.0969009 ... -167.45747   -155.99283\n",
      "    36.586605 ]\n",
      " [  -1.5630751    1.           1.0970058 ... -733.2104    -736.2711\n",
      "   885.2372   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8255005    1.           0.560524  ...   99.71177    -74.28179\n",
      "    81.91632  ]\n",
      " [  -1.8253756    1.           0.5610361 ...  -83.33106     67.03137\n",
      "   -28.901041 ]\n",
      " [  -1.8252697    1.           0.5614406 ...  -46.31347    189.98213\n",
      "    -9.543088 ]\n",
      " ...\n",
      " [  -1.8253193    1.           0.5613289 ...  100.871956   -37.795586\n",
      "   161.21709  ]\n",
      " [  -1.8255959    1.           0.5602722 ...   33.373455     0.5531001\n",
      "   -47.299934 ]\n",
      " [  -1.8255129    1.           0.5603924 ...  -78.55768   -519.0115\n",
      "   275.0027   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9092093e+00  1.0000000e+00 -3.1345367e-02 ... -5.8815899e+01\n",
      "   1.9178770e+02  5.4297565e+02]\n",
      " [-1.9092112e+00  1.0000000e+00 -3.0761719e-02 ... -6.9006355e+01\n",
      "  -3.3134789e+01 -3.3819572e+01]\n",
      " [-1.9092236e+00  1.0000000e+00 -3.0341096e-02 ... -9.3554376e+02\n",
      "   1.3339504e+03 -3.2854932e+02]\n",
      " ...\n",
      " [-1.9092274e+00  1.0000000e+00 -3.0458450e-02 ...  7.2992813e+01\n",
      "   7.1420876e+01 -2.6734497e+01]\n",
      " [-1.9091759e+00  1.0000000e+00 -3.1604767e-02 ... -1.8570900e+00\n",
      "  -8.1904821e+00 -2.5765999e+01]\n",
      " [-1.9091740e+00  1.0000000e+00 -3.1480789e-02 ... -2.1721611e+02\n",
      "  -3.3322079e+01 -2.8227039e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.80608273e+00  1.00000000e+00 -6.19825363e-01 ... -1.46784973e+03\n",
      "  -1.01473700e+03 -8.30097107e+02]\n",
      " [-1.80622387e+00  1.00000000e+00 -6.19262695e-01 ...  6.82452087e+02\n",
      "   1.13349170e+03  7.80179565e+02]\n",
      " [-1.80639267e+00  1.00000000e+00 -6.18879259e-01 ... -4.49260529e+02\n",
      "   1.06694214e+02 -3.31689423e+02]\n",
      " ...\n",
      " [-1.80635834e+00  1.00000000e+00 -6.18971825e-01 ... -4.41601372e+01\n",
      "   1.91109943e+01  1.10723793e+02]\n",
      " [-1.80598259e+00  1.00000000e+00 -6.20069504e-01 ... -6.24233665e+01\n",
      "  -2.67720871e+01 -2.54308653e+00]\n",
      " [-1.80601406e+00  1.00000000e+00 -6.19949341e-01 ...  1.26591820e+02\n",
      "   5.10429718e+02  5.19492615e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5260925    1.          -1.147726  ...  121.571365  -149.47774\n",
      "   288.5426   ]\n",
      " [  -1.526371     1.          -1.1472263 ...   99.96646   -172.13992\n",
      "   139.35776  ]\n",
      " [  -1.5266476    1.          -1.1469153 ...  -12.968697  -213.02074\n",
      "   119.76585  ]\n",
      " ...\n",
      " [  -1.5265751    1.          -1.146985  ... -103.07617     70.059845\n",
      "    16.943863 ]\n",
      " [  -1.5259132    1.          -1.1479359 ...   36.040752    18.10497\n",
      "    -4.5505013]\n",
      " [  -1.5259867    1.          -1.1478271 ...   10.119865    59.86118\n",
      "    21.218857 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0965109e+00  1.0000000e+00 -1.5633335e+00 ... -3.4277560e+02\n",
      "  -1.2340283e+00  3.0863257e+02]\n",
      " [-1.0969000e+00  1.0000000e+00 -1.5629396e+00 ...  9.7836819e+00\n",
      "   6.0238525e+01 -1.8510869e+02]\n",
      " [-1.0972672e+00  1.0000000e+00 -1.5627223e+00 ... -7.2729874e+01\n",
      "  -1.7781261e+01  1.9566629e+02]\n",
      " ...\n",
      " [-1.0971622e+00  1.0000000e+00 -1.5627575e+00 ... -2.6583607e+01\n",
      "   2.4771635e+01 -7.7772489e+00]\n",
      " [-1.0962543e+00  1.0000000e+00 -1.5634861e+00 ...  3.2728905e+01\n",
      "   2.3569605e+01  4.2936230e+01]\n",
      " [-1.0963840e+00  1.0000000e+00 -1.5634041e+00 ...  3.1918447e+03\n",
      "  -2.3031631e+03 -3.3641577e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.59727669e-01  1.00000000e+00 -1.82566452e+00 ...  2.15020233e+02\n",
      "  -8.40345993e+01 -1.27508335e+01]\n",
      " [-5.60180664e-01  1.00000000e+00 -1.82547379e+00 ... -1.28724308e+01\n",
      "   1.08882599e+01 -2.02107601e+01]\n",
      " [-5.60621262e-01  1.00000000e+00 -1.82536185e+00 ...  5.92997055e+01\n",
      "  -4.49525948e+01  5.71564255e+01]\n",
      " ...\n",
      " [-5.60491562e-01  1.00000000e+00 -1.82537174e+00 ...  1.47423370e+02\n",
      "  -3.20278107e+02 -1.11870308e+02]\n",
      " [-5.59396744e-01  1.00000000e+00 -1.82576180e+00 ... -1.27793709e+02\n",
      "   1.84042473e+01  5.46443901e+01]\n",
      " [-5.59592247e-01  1.00000000e+00 -1.82569504e+00 ... -6.94081604e+02\n",
      "  -2.03334007e+01 -1.37834192e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.2049179e-02  1.0000000e+00 -1.9092922e+00 ...  1.0640008e+02\n",
      "   2.3218033e+01 -8.5152298e+01]\n",
      " [ 3.1577110e-02  1.0000000e+00 -1.9092455e+00 ... -4.5696936e+00\n",
      "  -2.7358692e+01 -2.5490839e+01]\n",
      " [ 3.1099319e-02  1.0000000e+00 -1.9092811e+00 ...  1.9454102e+02\n",
      "  -1.7614859e+02 -4.2485527e+01]\n",
      " ...\n",
      " [ 3.1232834e-02  1.0000000e+00 -1.9092426e+00 ...  5.1110569e+01\n",
      "  -1.0185322e+02 -2.2208530e+02]\n",
      " [ 3.2379150e-02  1.0000000e+00 -1.9092903e+00 ...  8.2319878e+01\n",
      "  -9.3844597e+01  6.4515564e+01]\n",
      " [ 3.2190323e-02  1.0000000e+00 -1.9092731e+00 ...  5.1078033e+02\n",
      "  -4.0385220e+02  2.1215761e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.2053680e-01  1.0000000e+00 -1.8059387e+00 ...  1.0671718e+02\n",
      "   1.9518282e+01 -8.1583679e+01]\n",
      " [ 6.2009907e-01  1.0000000e+00 -1.8060217e+00 ... -5.9607586e+01\n",
      "  -7.5647621e+01  2.0279194e+01]\n",
      " [ 6.1967659e-01  1.0000000e+00 -1.8061877e+00 ... -3.2895941e+02\n",
      "   3.9214029e+02 -9.7949623e+01]\n",
      " ...\n",
      " [ 6.1980820e-01  1.0000000e+00 -1.8061047e+00 ...  1.9555480e+04\n",
      "  -3.3059717e+03 -1.9600730e+04]\n",
      " [ 6.2088585e-01  1.0000000e+00 -1.8058434e+00 ...  4.3689075e+00\n",
      "   3.5722647e+00 -3.1538582e+01]\n",
      " [ 6.2066841e-01  1.0000000e+00 -1.8058681e+00 ... -1.1359755e+03\n",
      "  -3.3639212e+02 -3.3098567e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.14823532e+00  1.00000000e+00 -1.52577400e+00 ... -3.15127319e+02\n",
      "   1.00506294e+02 -1.26772636e+02]\n",
      " [ 1.14786053e+00  1.00000000e+00 -1.52603436e+00 ... -4.84197540e+01\n",
      "   2.59836693e+01  3.83334160e+01]\n",
      " [ 1.14747238e+00  1.00000000e+00 -1.52632022e+00 ... -7.68802063e+02\n",
      "   6.23224072e+03 -7.51836816e+03]\n",
      " ...\n",
      " [ 1.14758682e+00  1.00000000e+00 -1.52620697e+00 ... -3.16248535e+02\n",
      "  -1.24474335e+02  3.56873169e+02]\n",
      " [ 1.14851570e+00  1.00000000e+00 -1.52559853e+00 ...  2.17181885e+02\n",
      "   1.39767029e+02  1.70871597e+02]\n",
      " [ 1.14834309e+00  1.00000000e+00 -1.52566338e+00 ... -1.18229507e+02\n",
      "  -9.46361618e+01  4.10219955e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5637369    1.          -1.0960464 ...  -34.68137      7.31161\n",
      "   -17.299637 ]\n",
      " [   1.5634651    1.          -1.096385  ...    1.7693921  100.72907\n",
      "   -75.42557  ]\n",
      " [   1.5631866    1.          -1.0967741 ...  340.6808     176.42499\n",
      "    88.27845  ]\n",
      " ...\n",
      " [   1.5632763    1.          -1.0966158 ... -245.8        162.10863\n",
      "   125.56128  ]\n",
      " [   1.56394      1.          -1.0957985 ...   85.82811    -76.27108\n",
      "  -204.54704  ]\n",
      " [   1.5638046    1.          -1.0959034 ... -281.0867      59.598824\n",
      "  -184.42635  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8259392e+00  1.0000000e+00 -5.5912209e-01 ... -8.9771988e+01\n",
      "   1.1315363e+02  1.5950653e+02]\n",
      " [ 1.8257933e+00  1.0000000e+00 -5.5951500e-01 ...  6.5811699e+01\n",
      "   6.9370170e+00 -2.3487894e+01]\n",
      " [ 1.8256683e+00  1.0000000e+00 -5.5997539e-01 ...  9.8762375e+01\n",
      "   1.7938078e+02 -3.0331082e+02]\n",
      " ...\n",
      " [ 1.8257027e+00  1.0000000e+00 -5.5978680e-01 ...  3.5013263e+02\n",
      "  -9.2503290e+00  7.3184222e+02]\n",
      " [ 1.8260670e+00  1.0000000e+00 -5.5883217e-01 ...  7.0225506e+00\n",
      "  -8.7635681e+01  2.1477367e+01]\n",
      " [ 1.8259592e+00  1.0000000e+00 -5.5896187e-01 ... -1.1676846e+03\n",
      "  -8.9144910e+02  4.6631018e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9093752e+00  1.0000000e+00  3.2678604e-02 ... -9.2191772e+00\n",
      "   6.3017416e+00 -1.8235931e+02]\n",
      " [ 1.9093637e+00  1.0000000e+00  3.2249451e-02 ... -2.0170992e+02\n",
      "   1.1742194e+03 -6.2189001e+02]\n",
      " [ 1.9094124e+00  1.0000000e+00  3.1760182e-02 ... -2.3063217e+02\n",
      "  -2.2186062e+03 -5.8445664e+03]\n",
      " ...\n",
      " [ 1.9093952e+00  1.0000000e+00  3.1966209e-02 ...  1.5635378e+04\n",
      "  -1.7635484e+04  8.6436602e+03]\n",
      " [ 1.9094028e+00  1.0000000e+00  3.2979965e-02 ...  2.9994711e+02\n",
      "  -3.6775549e+02  4.6027609e+02]\n",
      " [ 1.9093447e+00  1.0000000e+00  3.2844543e-02 ...  2.7273898e+02\n",
      "   9.3356903e+01 -5.6262561e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.80584240e+00  1.00000000e+00  6.21068954e-01 ...  4.94466057e+01\n",
      "   1.08202457e+01 -4.38420105e+01]\n",
      " [ 1.80596352e+00  1.00000000e+00  6.20668411e-01 ...  7.23444977e+01\n",
      "   7.18426285e+01  1.10411331e+02]\n",
      " [ 1.80616570e+00  1.00000000e+00  6.20212495e-01 ...  1.30890649e+03\n",
      "  -2.43299512e+03  3.16976587e+03]\n",
      " ...\n",
      " [ 1.80610085e+00  1.00000000e+00  6.20404243e-01 ...  7.82563843e+02\n",
      "  -1.76967499e+02 -3.92959180e+03]\n",
      " [ 1.80577087e+00  1.00000000e+00  6.21334076e-01 ... -2.57582764e+02\n",
      "   1.19987915e+02 -1.14172498e+03]\n",
      " [ 1.80576038e+00  1.00000000e+00  6.21212006e-01 ...  4.32321094e+03\n",
      "   3.62818054e+02 -3.26268945e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5256891e+00  1.0000000e+00  1.1485348e+00 ...  9.6905312e+01\n",
      "   1.3016841e+02 -2.0565359e+01]\n",
      " [ 1.5259142e+00  1.0000000e+00  1.1482248e+00 ...  2.0730434e+02\n",
      "  -5.0706345e+02 -2.3750226e+02]\n",
      " [ 1.5262508e+00  1.0000000e+00  1.1478184e+00 ... -8.4194971e+02\n",
      "   6.1496588e+02  5.2626196e+02]\n",
      " ...\n",
      " [ 1.5261459e+00  1.0000000e+00  1.1480026e+00 ... -6.7922226e+01\n",
      "   3.2452213e+02 -1.7834679e+02]\n",
      " [ 1.5255356e+00  1.0000000e+00  1.1487560e+00 ...  1.5331555e+02\n",
      "  -2.4396529e+02  4.5432253e+00]\n",
      " [ 1.5255604e+00  1.0000000e+00  1.1486530e+00 ... -2.4964841e+03\n",
      "  -1.0321204e+02 -1.6616710e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0959587    1.           1.5639191 ...  -46.506744    48.03579\n",
      "    25.821035 ]\n",
      " [   1.096283     1.           1.5636654 ...  428.23972     56.005783\n",
      "  -216.31383  ]\n",
      " [   1.0967102    1.           1.5633714 ...   26.01908     39.036522\n",
      "  -307.6064   ]\n",
      " ...\n",
      " [   1.096571     1.           1.5635185 ...   92.16351    345.7495\n",
      "   516.29865  ]\n",
      " [   1.0957451    1.           1.5640717 ...   18.327374  -124.0238\n",
      "   165.64543  ]\n",
      " [   1.0957966    1.           1.5640011 ...  386.2644     109.375496\n",
      "   500.92548  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5590763     1.            1.8259716  ... -264.2177\n",
      "  -105.824234    107.586365  ]\n",
      " [   0.5594654     1.            1.8258905  ...   38.341503\n",
      "     4.4641333    -2.9950223 ]\n",
      " [   0.5599556     1.            1.8257113  ...  -91.462234\n",
      "   -14.145636     35.27114   ]\n",
      " ...\n",
      " [   0.5597801     1.            1.8258171  ...   55.501984\n",
      "  -307.15192      86.459076  ]\n",
      " [   0.558815      1.            1.8260307  ... -131.30086\n",
      "   -48.002663   -157.72604   ]\n",
      " [   0.55889416    1.            1.8259983  ...  -78.804054\n",
      "   120.40358     246.41357   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.3027649e-02  1.0000000e+00  1.9091511e+00 ... -2.0129673e+01\n",
      "   1.0235998e+02 -9.1860603e+01]\n",
      " [-3.2623291e-02  1.0000000e+00  1.9091930e+00 ... -3.1895912e+01\n",
      "   2.7027521e+02  4.1978943e+01]\n",
      " [-3.2110214e-02  1.0000000e+00  1.9091712e+00 ... -6.6861099e+01\n",
      "   1.1635920e+01  4.9746754e+01]\n",
      " ...\n",
      " [-3.2295227e-02  1.0000000e+00  1.9092274e+00 ... -3.7132300e+02\n",
      "   8.9337387e+01  3.6415329e+02]\n",
      " [-3.3288956e-02  1.0000000e+00  1.9091434e+00 ... -1.7660144e+02\n",
      "   1.1192751e+01 -1.5578375e+01]\n",
      " [-3.3217430e-02  1.0000000e+00  1.9091320e+00 ...  4.9796841e+01\n",
      "   3.3948845e+01  7.3304451e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.21335030e-01  1.00000000e+00  1.80554008e+00 ... -6.24695663e+01\n",
      "  -4.72531464e+02  2.64355249e+03]\n",
      " [-6.20954514e-01  1.00000000e+00  1.80569935e+00 ... -1.08649933e+02\n",
      "   1.10046692e+02  1.43408401e+02]\n",
      " [-6.20475769e-01  1.00000000e+00  1.80583549e+00 ... -1.03743736e+02\n",
      "   6.78293533e+01  1.54087112e+02]\n",
      " ...\n",
      " [-6.20653152e-01  1.00000000e+00  1.80582905e+00 ...  8.41520264e+02\n",
      "   2.09308057e+03 -8.91509888e+02]\n",
      " [-6.21593475e-01  1.00000000e+00  1.80545998e+00 ...  1.24853577e+02\n",
      "  -1.39189331e+02  1.07238102e+01]\n",
      " [-6.21513367e-01  1.00000000e+00  1.80547333e+00 ...  7.29130249e+02\n",
      "   1.01249084e+03 -2.12726273e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1491890e+00  1.0000000e+00  1.5249538e+00 ... -6.8596826e+03\n",
      "   4.1365864e+03  3.4228411e+03]\n",
      " [-1.1488667e+00  1.0000000e+00  1.5252037e+00 ...  2.3447383e+02\n",
      "  -5.7957390e+01 -9.4880829e+00]\n",
      " [-1.1484718e+00  1.0000000e+00  1.5254778e+00 ...  9.4615883e+01\n",
      "   1.8326964e+02 -8.9261185e+01]\n",
      " ...\n",
      " [-1.1486187e+00  1.0000000e+00  1.5254240e+00 ... -1.5285638e+02\n",
      "  -1.8766193e+01  3.8355995e+01]\n",
      " [-1.1494350e+00  1.0000000e+00  1.5247822e+00 ...  9.1280909e+00\n",
      "  -3.9268811e+02 -8.8843941e+01]\n",
      " [-1.1493378e+00  1.0000000e+00  1.5248432e+00 ... -1.9187064e+02\n",
      "  -1.5041293e+02  1.0049210e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5640764e+00  1.0000000e+00  1.0953522e+00 ... -9.4572583e+02\n",
      "  -2.3669524e+02 -1.3268151e+03]\n",
      " [-1.5638447e+00  1.0000000e+00  1.0956850e+00 ... -2.3075879e+04\n",
      "   9.2790752e+03 -1.8178410e+04]\n",
      " [-1.5635529e+00  1.0000000e+00  1.0960699e+00 ... -1.8087881e+02\n",
      "  -5.2708087e+02  2.0288312e+02]\n",
      " ...\n",
      " [-1.5636692e+00  1.0000000e+00  1.0959625e+00 ... -6.1019272e+01\n",
      "   2.3878946e+02 -3.8994537e+01]\n",
      " [-1.5642681e+00  1.0000000e+00  1.0951233e+00 ...  4.4765244e+01\n",
      "   2.8490219e+02  2.4003607e+02]\n",
      " [-1.5641890e+00  1.0000000e+00  1.0952034e+00 ...  3.7554146e+01\n",
      "  -1.6131392e+01  7.1813904e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8260813e+00  1.0000000e+00  5.5821419e-01 ... -3.1709501e+01\n",
      "  -3.6081644e+02 -8.7000328e+01]\n",
      " [-1.8259735e+00  1.0000000e+00  5.5861473e-01 ... -9.8149414e+01\n",
      "   3.2073810e+02 -8.7517932e+02]\n",
      " [-1.8258133e+00  1.0000000e+00  5.5906266e-01 ... -1.1184009e+01\n",
      "  -8.8784266e-01  2.5042551e+01]\n",
      " ...\n",
      " [-1.8258953e+00  1.0000000e+00  5.5892944e-01 ... -2.1316313e+02\n",
      "  -2.7791250e+01  7.6582024e+01]\n",
      " [-1.8262100e+00  1.0000000e+00  5.5794525e-01 ... -7.6469440e+02\n",
      "   8.9750549e+02  1.7329255e+03]\n",
      " [-1.8261499e+00  1.0000000e+00  5.5804253e-01 ...  6.3510937e+01\n",
      "   4.0386627e+01  1.3182154e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9092331e+00  1.0000000e+00 -3.3580780e-02 ...  2.8816266e+02\n",
      "   1.6274666e+01  3.2703610e+02]\n",
      " [-1.9092588e+00  1.0000000e+00 -3.3143997e-02 ...  2.4903027e+03\n",
      "  -9.2489343e+02 -2.6018062e+03]\n",
      " [-1.9092312e+00  1.0000000e+00 -3.2674763e-02 ... -5.2035297e+01\n",
      "  -4.8755283e+01 -2.3700119e+01]\n",
      " ...\n",
      " [-1.9092808e+00  1.0000000e+00 -3.2814026e-02 ... -1.9717491e+01\n",
      "  -8.2500214e+01 -1.6984019e+00]\n",
      " [-1.9093208e+00  1.0000000e+00 -3.3866882e-02 ...  1.9783909e+01\n",
      "  -3.4154533e+01 -2.6384216e+01]\n",
      " [-1.9092541e+00  1.0000000e+00 -3.3765793e-02 ... -2.3040716e+01\n",
      "   8.2613991e+01 -1.1189171e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8054085e+00  1.0000000e+00 -6.2207794e-01 ...  2.6038211e+02\n",
      "  -1.3656070e+02 -1.2283677e+03]\n",
      " [-1.8055735e+00  1.0000000e+00 -6.2163544e-01 ...  1.6684740e+03\n",
      "   6.4116370e+02  2.0894727e+03]\n",
      " [-1.8056889e+00  1.0000000e+00 -6.2120891e-01 ...  8.6249039e+01\n",
      "  -8.7073578e+01 -1.1228976e+01]\n",
      " ...\n",
      " [-1.8056927e+00  1.0000000e+00 -6.2132454e-01 ...  3.0986087e+02\n",
      "   6.2103473e+02  3.2288425e+02]\n",
      " [-1.8054237e+00  1.0000000e+00 -6.2234306e-01 ...  1.2948558e+01\n",
      "  -1.1212319e+00  2.1299414e+01]\n",
      " [-1.8053751e+00  1.0000000e+00 -6.2225723e-01 ... -4.3590897e+01\n",
      "  -2.5434998e+01 -1.1684117e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5245743e+00  1.0000000e+00 -1.1495495e+00 ... -2.6867361e+02\n",
      "   6.1924634e+02  2.8293130e+02]\n",
      " [-1.5248566e+00  1.0000000e+00 -1.1492023e+00 ... -2.4887518e+02\n",
      "   1.0005283e+02  1.0426415e+02]\n",
      " [-1.5250397e+00  1.0000000e+00 -1.1488383e+00 ...  8.1073471e+01\n",
      "  -2.0564856e+01  9.0069862e+01]\n",
      " ...\n",
      " [-1.5249958e+00  1.0000000e+00 -1.1489363e+00 ... -5.9605055e+00\n",
      "   6.0250259e+01 -2.9641470e+01]\n",
      " [-1.5244560e+00  1.0000000e+00 -1.1497746e+00 ... -3.6375183e+03\n",
      "   2.5005532e+03  1.7425441e+03]\n",
      " [-1.5244923e+00  1.0000000e+00 -1.1497059e+00 ...  2.2098793e+02\n",
      "   4.3317365e+02  7.8819145e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.09456062e+00  1.00000000e+00 -1.56434441e+00 ... -3.98985519e+01\n",
      "  -5.02332726e+01 -7.68172913e+01]\n",
      " [-1.09494019e+00  1.00000000e+00 -1.56410503e+00 ...  1.01292896e+03\n",
      "  -1.84092197e+01  6.97251038e+02]\n",
      " [-1.09521484e+00  1.00000000e+00 -1.56385076e+00 ...  7.25724564e+01\n",
      "  -7.23219681e+01 -1.60792526e+02]\n",
      " ...\n",
      " [-1.09514999e+00  1.00000000e+00 -1.56392288e+00 ...  6.57230301e+01\n",
      "  -8.66781769e+01  1.46758118e+02]\n",
      " [-1.09442329e+00  1.00000000e+00 -1.56452370e+00 ...  9.03469788e+02\n",
      "  -2.07080615e+03  4.43499023e+02]\n",
      " [-1.09443665e+00  1.00000000e+00 -1.56447411e+00 ... -1.04883862e+03\n",
      "  -8.78408630e+02 -2.61000879e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.5745792e-01  1.0000000e+00 -1.8261356e+00 ...  2.2895592e+02\n",
      "   1.1526418e+02  6.8372787e+01]\n",
      " [-5.5789948e-01  1.0000000e+00 -1.8259983e+00 ...  3.7368848e+02\n",
      "  -1.1315995e+01  1.0470282e+01]\n",
      " [-5.5820084e-01  1.0000000e+00 -1.8258740e+00 ...  6.3941402e+00\n",
      "  -4.7433235e+01 -2.2899536e+02]\n",
      " ...\n",
      " [-5.5810928e-01  1.0000000e+00 -1.8259096e+00 ... -1.5636774e+02\n",
      "  -7.2310448e+01  1.0657479e+02]\n",
      " [-5.5725288e-01  1.0000000e+00 -1.8262444e+00 ...  7.7353264e+01\n",
      "   1.4271085e+02  8.0012970e+01]\n",
      " [-5.5730915e-01  1.0000000e+00 -1.8262157e+00 ... -8.9539252e+02\n",
      "  -6.4890610e+01 -4.8707660e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.4542084e-02  1.0000000e+00 -1.9091148e+00 ...  8.6589806e+01\n",
      "   3.0962418e+01 -3.6649757e+01]\n",
      " [ 3.4081459e-02  1.0000000e+00 -1.9090958e+00 ...  7.3847914e-01\n",
      "  -3.8682754e+00  3.1274078e+00]\n",
      " [ 3.3735275e-02  1.0000000e+00 -1.9091088e+00 ... -1.3320033e+02\n",
      "   1.1979984e+02  4.1607300e+01]\n",
      " ...\n",
      " [ 3.3838272e-02  1.0000000e+00 -1.9091072e+00 ... -2.2097023e+01\n",
      "  -3.1835815e+01  1.3361882e+02]\n",
      " [ 3.4734726e-02  1.0000000e+00 -1.9091473e+00 ...  9.0112892e+01\n",
      "  -5.7353859e+01 -2.5610003e+01]\n",
      " [ 3.4700394e-02  1.0000000e+00 -1.9091492e+00 ...  3.6110025e+02\n",
      "  -7.2705865e+02  2.3387299e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.22895241e-01  1.00000000e+00 -1.80506516e+00 ... -5.92170067e+01\n",
      "  -6.62699280e+01  5.92758636e+01]\n",
      " [ 6.22464180e-01  1.00000000e+00 -1.80517769e+00 ... -6.90474472e+01\n",
      "   2.84331779e+01  3.50076675e+01]\n",
      " [ 6.22154236e-01  1.00000000e+00 -1.80531788e+00 ...  3.74358246e+02\n",
      "   1.03052032e+02 -1.25155846e+02]\n",
      " ...\n",
      " [ 6.22249603e-01  1.00000000e+00 -1.80527115e+00 ...  6.19083595e+00\n",
      "  -2.47541332e+01 -2.94888744e+01]\n",
      " [ 6.23096466e-01  1.00000000e+00 -1.80500984e+00 ... -6.32915535e+01\n",
      "  -1.53353073e+02 -3.84817078e+02]\n",
      " [ 6.23043060e-01  1.00000000e+00 -1.80503082e+00 ... -3.46389526e+02\n",
      "   1.07327344e+03  4.48493744e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1502419    1.          -1.5242672 ...   -9.160301    45.771\n",
      "   -32.499435 ]\n",
      " [   1.1498728    1.          -1.5245056 ...  -44.67062     85.19455\n",
      "    22.993763 ]\n",
      " [   1.149641     1.          -1.5247469 ...  291.22946    -52.36192\n",
      "   451.29922  ]\n",
      " ...\n",
      " [   1.1497097    1.          -1.5246763 ...   66.46518    341.49466\n",
      "    19.533197 ]\n",
      " [   1.1504173    1.          -1.5241413 ... -430.7888    -744.7253\n",
      "    23.336044 ]\n",
      " [   1.1503687    1.          -1.5241909 ...  -67.06346     90.49575\n",
      "   104.25971  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5649529e+00  1.0000000e+00 -1.0938702e+00 ...  1.2431451e+01\n",
      "  -6.4567192e+01 -7.3771294e+01]\n",
      " [ 1.5646667e+00  1.0000000e+00 -1.0941591e+00 ... -1.3496376e+02\n",
      "  -4.7968468e+01 -7.3930046e+01]\n",
      " [ 1.5645237e+00  1.0000000e+00 -1.0945009e+00 ...  1.9585507e+02\n",
      "   2.0600092e+02 -2.8322208e+02]\n",
      " ...\n",
      " [ 1.5645638e+00  1.0000000e+00 -1.0943956e+00 ... -1.2418033e+02\n",
      "  -4.5525983e+02  2.5392032e+02]\n",
      " [ 1.5650635e+00  1.0000000e+00 -1.0936832e+00 ...  3.9091921e-01\n",
      "  -1.7306676e+00 -4.7836475e+01]\n",
      " [ 1.5650368e+00  1.0000000e+00 -1.0937634e+00 ... -3.4623665e+01\n",
      "  -1.0232530e+01  4.8914003e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8265266     1.           -0.5565891  ...  -98.83948\n",
      "    86.243744     34.670326  ]\n",
      " [   1.8263512     1.           -0.5569401  ...  180.05031\n",
      "  -156.49437    -221.9032    ]\n",
      " [   1.826294      1.           -0.5573349  ... -170.94218\n",
      "   -95.26723     247.16711   ]\n",
      " ...\n",
      " [   1.8262978     1.           -0.55720615 ...   57.788517\n",
      "   508.3559      458.24008   ]\n",
      " [   1.8265572     1.           -0.5563755  ...   37.7593\n",
      "   -39.182964     17.504547  ]\n",
      " [   1.82656       1.           -0.55646515 ...   -2.6310172\n",
      "   -17.066103      9.663795  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9091959e+00  1.0000000e+00  3.5051346e-02 ... -1.9819033e+00\n",
      "  -8.3221893e+01 -2.6370039e+01]\n",
      " [ 1.9091692e+00  1.0000000e+00  3.4695625e-02 ...  2.3126165e+01\n",
      "  -4.3483585e+01  1.2047207e+02]\n",
      " [ 1.9092293e+00  1.0000000e+00  3.4277081e-02 ...  6.6501297e+01\n",
      "  -4.3518032e+01 -3.6961544e+01]\n",
      " ...\n",
      " [ 1.9091949e+00  1.0000000e+00  3.4420013e-02 ...  1.6474306e+02\n",
      "  -1.0944249e+01  2.3224892e+02]\n",
      " [ 1.9091682e+00  1.0000000e+00  3.5268784e-02 ...  1.7069685e+02\n",
      "   5.2275928e+02 -2.5668604e+02]\n",
      " [ 1.9091816e+00  1.0000000e+00  3.5179138e-02 ...  5.1578967e+02\n",
      "  -1.1023133e+02  5.8953833e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8049269     1.            0.6235218  ...  -13.847809\n",
      "   -11.056826    110.06098   ]\n",
      " [   1.8050346     1.            0.62319183 ...  -81.942566\n",
      "  -252.51796     -23.408684  ]\n",
      " [   1.8052216     1.            0.6227938  ...  190.06467\n",
      "   -52.68759     -21.281496  ]\n",
      " ...\n",
      " [   1.8051567     1.            0.6229296  ...  111.5845\n",
      "  -181.48291      43.723946  ]\n",
      " [   1.8048401     1.            0.62373924 ...  226.227\n",
      "  -373.3384       63.945625  ]\n",
      " [   1.8048658     1.            0.6236496  ...  361.6624\n",
      "    44.164547    -40.670002  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.524128     1.           1.150568  ...   -4.9942846  146.84601\n",
      "    99.94118  ]\n",
      " [   1.5243511    1.           1.150281  ... -190.97058     73.98997\n",
      "   100.78793  ]\n",
      " [   1.5246048    1.           1.1499459 ... -208.4703      58.36326\n",
      "   114.709366 ]\n",
      " ...\n",
      " [   1.5245113    1.           1.1500559 ...  -88.76664    256.98846\n",
      "   392.8646   ]\n",
      " [   1.5239601    1.           1.1507416 ... -116.20744    297.5207\n",
      "    -5.016163 ]\n",
      " [   1.5240307    1.           1.150671  ... -167.96916      1.8208848\n",
      "   156.1784   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0936308    1.           1.5655041 ...   43.89548     66.10871\n",
      "   175.08736  ]\n",
      " [   1.0939331    1.           1.5653362 ...  -11.074144   -31.718214\n",
      "   -64.65459  ]\n",
      " [   1.094305     1.           1.5650972 ...   61.95637     52.72646\n",
      "   -23.510302 ]\n",
      " ...\n",
      " [   1.0941887    1.           1.565177  ...   36.057236   -17.096066\n",
      "   -43.98009  ]\n",
      " [   1.0934277    1.           1.5656204 ...   20.240978   -96.70117\n",
      "   180.26291  ]\n",
      " [   1.0934925    1.           1.5655689 ... -299.74908     24.221073\n",
      "  -300.0609   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.5627251e-01  1.0000000e+00  1.8268852e+00 ... -2.6937363e+01\n",
      "  -3.6701868e+02  1.5726636e+03]\n",
      " [ 5.5661964e-01  1.0000000e+00  1.8268270e+00 ...  3.1345630e+02\n",
      "  -2.1575043e+02  7.1862022e+01]\n",
      " [ 5.5705643e-01  1.0000000e+00  1.8266870e+00 ...  2.4249702e+01\n",
      "   1.1403789e+02 -2.1415469e+02]\n",
      " ...\n",
      " [ 5.5692101e-01  1.0000000e+00  1.8267384e+00 ... -4.5005264e+01\n",
      "  -4.0356606e+01 -3.4045029e+01]\n",
      " [ 5.5604362e-01  1.0000000e+00  1.8269501e+00 ...  8.6522705e+01\n",
      "   1.5168158e+01  1.8604481e+01]\n",
      " [ 5.5611610e-01  1.0000000e+00  1.8269138e+00 ...  3.5970104e+01\n",
      "  -2.2756372e+01  1.6498997e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.5176277e-02  1.0000000e+00  1.9093246e+00 ... -2.8985987e+01\n",
      "   3.4778049e+01  5.7169292e+01]\n",
      " [-3.4805298e-02  1.0000000e+00  1.9094143e+00 ...  1.9752925e+03\n",
      "   3.9700916e+03 -1.6750277e+03]\n",
      " [-3.4383774e-02  1.0000000e+00  1.9094036e+00 ...  2.3122959e+01\n",
      "  -3.8408081e+01 -6.3802940e+01]\n",
      " ...\n",
      " [-3.4524918e-02  1.0000000e+00  1.9094028e+00 ... -4.5750720e+02\n",
      "  -8.3826801e+02  3.5977166e+03]\n",
      " [-3.5449982e-02  1.0000000e+00  1.9093170e+00 ... -3.2758932e+02\n",
      "  -2.8940075e+01  1.3476894e+02]\n",
      " [-3.5338402e-02  1.0000000e+00  1.9093075e+00 ...  1.3728757e+02\n",
      "   6.8554054e+01 -3.0985920e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.2375546e-01  1.0000000e+00  1.8049202e+00 ...  2.6554651e+01\n",
      "   4.1869652e+01 -7.0691404e+00]\n",
      " [-6.2341022e-01  1.0000000e+00  1.8051195e+00 ... -2.5442417e+03\n",
      "  -1.0498547e+03 -1.7428073e+03]\n",
      " [-6.2301445e-01  1.0000000e+00  1.8052336e+00 ... -1.1820778e+02\n",
      "  -1.2528606e+01 -1.1718982e+02]\n",
      " ...\n",
      " [-6.2315178e-01  1.0000000e+00  1.8051891e+00 ... -3.6392398e+02\n",
      "   2.2082195e+02  2.3580672e+02]\n",
      " [-6.2401009e-01  1.0000000e+00  1.8048153e+00 ... -1.8567699e+02\n",
      "  -6.1483387e+01  7.2882103e+01]\n",
      " [-6.2390614e-01  1.0000000e+00  1.8048496e+00 ...  3.4535103e+01\n",
      "  -1.6987410e+02  3.4530548e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1510954e+00  1.0000000e+00  1.5237598e+00 ...  7.6362185e+00\n",
      "   5.1018814e+01  1.0788580e+02]\n",
      " [-1.1507921e+00  1.0000000e+00  1.5240707e+00 ...  7.4385718e+02\n",
      "   1.6063805e+03 -3.4054426e+03]\n",
      " [-1.1504707e+00  1.0000000e+00  1.5242952e+00 ... -1.2401791e+03\n",
      "   2.3885251e+02 -5.3670508e+02]\n",
      " ...\n",
      " [-1.1505699e+00  1.0000000e+00  1.5242224e+00 ...  6.9815125e+01\n",
      "  -1.3594973e+01  9.0493488e+00]\n",
      " [-1.1513042e+00  1.0000000e+00  1.5236073e+00 ... -1.5205353e+03\n",
      "  -1.7240500e+03  4.6979546e+03]\n",
      " [-1.1512213e+00  1.0000000e+00  1.5236626e+00 ...  8.2788277e+01\n",
      "  -3.7712658e+01 -1.2776526e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5657425    1.           1.093277  ...   -2.2446055   86.446526\n",
      "   -20.216425 ]\n",
      " [  -1.565525     1.           1.0936775 ...   -2.1336923    1.3995543\n",
      "   -53.52084  ]\n",
      " [  -1.5652771    1.           1.0940022 ...  159.4657    -135.69112\n",
      "  -346.1406   ]\n",
      " ...\n",
      " [  -1.5653553    1.           1.0938978 ...   -7.600362    99.583885\n",
      "  -195.74547  ]\n",
      " [  -1.5658913    1.           1.0930634 ... -122.60958     51.352802\n",
      "   -87.18536  ]\n",
      " [  -1.5658274    1.           1.0931416 ...   40.933918   -37.860207\n",
      "   -45.422623 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82696724e+00  1.00000000e+00  5.55900574e-01 ... -1.15420049e+04\n",
      "  -5.71346045e+03 -1.41381475e+04]\n",
      " [-1.82685471e+00  1.00000000e+00  5.56340218e-01 ...  8.67890549e+01\n",
      "   1.51567108e+02 -5.56890602e+01]\n",
      " [-1.82672691e+00  1.00000000e+00  5.56723058e-01 ...  8.09264587e+02\n",
      "   7.91796875e+01 -1.24487076e+02]\n",
      " ...\n",
      " [-1.82677841e+00  1.00000000e+00  5.56606293e-01 ... -1.69470940e+01\n",
      "   2.72388725e+01 -1.11487865e+01]\n",
      " [-1.82705116e+00  1.00000000e+00  5.55658340e-01 ... -2.78024254e+01\n",
      "  -1.14251335e+02  4.93437119e+01]\n",
      " [-1.82701683e+00  1.00000000e+00  5.55744171e-01 ...  2.69458942e+01\n",
      "  -1.78442093e+02 -7.51963959e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9093513e+00  1.0000000e+00 -3.5932541e-02 ...  2.5399831e+02\n",
      "   9.7158704e+02  3.3667795e+03]\n",
      " [-1.9093523e+00  1.0000000e+00 -3.5475731e-02 ...  1.3619000e+02\n",
      "  -1.3258240e+02 -3.0114807e+02]\n",
      " [-1.9093590e+00  1.0000000e+00 -3.5088129e-02 ...  1.0458336e+03\n",
      "   5.2389282e+03  1.7241593e+03]\n",
      " ...\n",
      " [-1.9093494e+00  1.0000000e+00 -3.5197258e-02 ...  9.6739716e+01\n",
      "   6.9168015e+01  3.1569483e+01]\n",
      " [-1.9093513e+00  1.0000000e+00 -3.6184311e-02 ... -2.6016092e+02\n",
      "   8.6802521e+01 -3.9537766e+01]\n",
      " [-1.9093542e+00  1.0000000e+00 -3.6090851e-02 ...  5.8902594e+02\n",
      "   1.7044627e+02  4.9648014e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8047533e+00  1.0000000e+00 -6.2422562e-01 ...  1.3652227e+01\n",
      "  -3.5588470e+02 -1.4565285e+02]\n",
      " [-1.8048868e+00  1.0000000e+00 -6.2378693e-01 ...  2.5157187e+01\n",
      "  -9.5609489e+01  8.3163811e+01]\n",
      " [-1.8050003e+00  1.0000000e+00 -6.2342864e-01 ...  4.7963538e+02\n",
      "   6.2559875e+02  8.6767664e+00]\n",
      " ...\n",
      " [-1.8049450e+00  1.0000000e+00 -6.2352753e-01 ... -4.1166393e+01\n",
      "  -2.5041162e+01  4.4615025e+01]\n",
      " [-1.8046741e+00  1.0000000e+00 -6.2446213e-01 ...  8.8110901e+01\n",
      "   4.5664886e+02  3.7701389e+02]\n",
      " [-1.8047085e+00  1.0000000e+00 -6.2437630e-01 ...  2.1132545e+01\n",
      "   5.2676933e+01  1.2638112e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5234699    1.          -1.1514301 ...  157.18658   -368.26303\n",
      "   149.01509  ]\n",
      " [  -1.5237131    1.          -1.151042  ...  448.97827    359.6995\n",
      "   102.816734 ]\n",
      " [  -1.5239334    1.          -1.1507708 ...   44.606476  -687.72485\n",
      "    15.079135 ]\n",
      " ...\n",
      " [  -1.5238552    1.          -1.1508408 ...  -93.975975    89.508934\n",
      "   -38.212925 ]\n",
      " [  -1.5232868    1.          -1.1516285 ... -774.60693   -380.40884\n",
      "   278.9188   ]\n",
      " [  -1.5233879    1.          -1.1515598 ...  118.640526  -148.21396\n",
      "  -112.41309  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0930166e+00  1.0000000e+00 -1.5658283e+00 ... -2.1944984e+01\n",
      "  -1.9265862e+01  2.0464558e+01]\n",
      " [-1.0933628e+00  1.0000000e+00 -1.5655613e+00 ...  5.7042301e+01\n",
      "   2.6845071e+02  1.0559890e+02]\n",
      " [-1.0936451e+00  1.0000000e+00 -1.5653775e+00 ...  6.0757797e+01\n",
      "  -2.0882947e+03  1.3615115e+03]\n",
      " ...\n",
      " [-1.0935364e+00  1.0000000e+00 -1.5654259e+00 ...  9.6997238e+01\n",
      "  -8.9059479e+01 -9.7031158e+01]\n",
      " [-1.0928001e+00  1.0000000e+00 -1.5659771e+00 ...  3.2506848e+03\n",
      "  -3.5078608e+03 -1.1838009e+03]\n",
      " [-1.0928974e+00  1.0000000e+00 -1.5659256e+00 ... -5.7028744e+01\n",
      "   6.7919220e+01 -6.8551765e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.5540276e-01  1.0000000e+00 -1.8267651e+00 ...  1.2116964e+02\n",
      "  -2.2433731e+02 -1.2257603e+01]\n",
      " [-5.5581665e-01  1.0000000e+00 -1.8266096e+00 ... -1.0798388e+02\n",
      "   1.6036971e+02  7.9219864e+01]\n",
      " [-5.5612564e-01  1.0000000e+00 -1.8265377e+00 ... -2.6065570e+02\n",
      "   9.5620105e+02  5.5053625e+02]\n",
      " ...\n",
      " [-5.5598831e-01  1.0000000e+00 -1.8265524e+00 ... -1.9561020e+01\n",
      "   2.0942266e+01  5.7387871e+01]\n",
      " [-5.5511475e-01  1.0000000e+00 -1.8268585e+00 ...  4.7237471e+03\n",
      "   5.1711079e+03  5.7857549e+03]\n",
      " [-5.5526638e-01  1.0000000e+00 -1.8268223e+00 ... -5.4154282e+00\n",
      "   1.9590953e+02 -1.1496627e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.60460281e-02  1.00000000e+00 -1.90903091e+00 ...  1.88394623e+01\n",
      "  -2.57789955e+01 -1.68297539e+01]\n",
      " [ 3.56149673e-02  1.00000000e+00 -1.90895462e+00 ... -7.96888685e+00\n",
      "  -1.13146774e+02 -4.10080070e+01]\n",
      " [ 3.53012085e-02  1.00000000e+00 -1.90900552e+00 ...  2.96408356e+02\n",
      "  -4.00257294e+02  1.19984989e+01]\n",
      " ...\n",
      " [ 3.54385376e-02  1.00000000e+00 -1.90897751e+00 ...  7.87775755e+00\n",
      "  -6.52834854e+01 -1.02152390e+02]\n",
      " [ 3.63483429e-02  1.00000000e+00 -1.90902328e+00 ...  3.21416064e+03\n",
      "   6.25666138e+02  3.20547144e+03]\n",
      " [ 3.61852646e-02  1.00000000e+00 -1.90903091e+00 ...  3.23025635e+02\n",
      "  -6.37749138e+01 -9.99585190e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.2475872e-01  1.0000000e+00 -1.8042774e+00 ...  1.5654810e+02\n",
      "   2.4395177e+02  1.3393112e+02]\n",
      " [ 6.2435532e-01  1.0000000e+00 -1.8043394e+00 ...  8.0440605e+01\n",
      "   1.9386139e+01  2.2210386e+01]\n",
      " [ 6.2404060e-01  1.0000000e+00 -1.8045011e+00 ... -2.5607727e+01\n",
      "  -6.9351585e+01  2.9131354e+02]\n",
      " ...\n",
      " [ 6.2417603e-01  1.0000000e+00 -1.8044329e+00 ... -3.7982601e+01\n",
      "  -9.8608704e+00 -1.9500284e+01]\n",
      " [ 6.2504578e-01  1.0000000e+00 -1.8041878e+00 ...  8.1900543e+01\n",
      "  -1.3244505e+02  9.5156921e+02]\n",
      " [ 6.2488842e-01  1.0000000e+00 -1.8042202e+00 ... -6.4771028e+00\n",
      "   1.1413404e+02  2.0655365e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.151659     1.          -1.5230503 ... -672.3746    -162.25256\n",
      "   878.71704  ]\n",
      " [   1.1513138    1.          -1.5232277 ... -136.65695    239.02092\n",
      "    55.708534 ]\n",
      " [   1.1510506    1.          -1.5234832 ... -722.9853     423.16275\n",
      "  -401.84506  ]\n",
      " ...\n",
      " [   1.1511688    1.          -1.5233898 ...  -17.339048    10.703334\n",
      "     7.425576 ]\n",
      " [   1.1518955    1.          -1.5228882 ...   45.24282     28.517378\n",
      "   -38.77866  ]\n",
      " [   1.1517639    1.          -1.522953  ...  -85.02      -120.27327\n",
      "   104.253365 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5662031    1.          -1.0922756 ... -135.89986     45.425415\n",
      "   -11.161259 ]\n",
      " [   1.5659428    1.          -1.0925303 ... -286.13528    -77.19269\n",
      "  -144.11888  ]\n",
      " [   1.5657845    1.          -1.0928779 ...    4.552407    29.02158\n",
      "   -34.660217 ]\n",
      " ...\n",
      " [   1.5658684    1.          -1.09274   ...  126.548386    -7.9905705\n",
      "  -278.04346  ]\n",
      " [   1.5663948    1.          -1.0920715 ... -233.12146     55.572933\n",
      "   223.94754  ]\n",
      " [   1.5662661    1.          -1.0921555 ...   37.01319    457.86472\n",
      "   -48.795124 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8270588     1.           -0.55496407 ...   -0.42945194\n",
      "   -74.59298     -37.361385  ]\n",
      " [   1.8269091     1.           -0.55530643 ... -112.15677\n",
      "   100.4986     -168.41762   ]\n",
      " [   1.8268566     1.           -0.5556976  ...   36.893993\n",
      "   -33.509045     -5.585225  ]\n",
      " ...\n",
      " [   1.8268929     1.           -0.5555525  ...  -22.063795\n",
      "    71.14662      58.695343  ]\n",
      " [   1.8271675     1.           -0.5547352  ...    4.67931\n",
      "  -107.09682      85.641106  ]\n",
      " [   1.8270845     1.           -0.5548248  ... -300.13245\n",
      "    29.284641    199.50887   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90920258e+00  1.00000000e+00  3.71532440e-02 ...  1.24133423e+02\n",
      "   2.65911682e+02 -8.65642242e+01]\n",
      " [ 1.90917683e+00  1.00000000e+00  3.67946625e-02 ... -9.10436859e+01\n",
      "  -1.93214371e+02 -5.97252319e+02]\n",
      " [ 1.90923882e+00  1.00000000e+00  3.63977887e-02 ...  7.52857132e+01\n",
      "  -1.01351555e+02 -8.33264542e+01]\n",
      " ...\n",
      " [ 1.90924072e+00  1.00000000e+00  3.65390778e-02 ... -5.59147339e+02\n",
      "  -7.25309265e+02  7.02163239e+01]\n",
      " [ 1.90922928e+00  1.00000000e+00  3.73992920e-02 ...  9.42348877e+02\n",
      "   1.71317253e+01 -1.64079956e+03]\n",
      " [ 1.90918732e+00  1.00000000e+00  3.73020172e-02 ...  1.11808266e+02\n",
      "   1.76348362e+01  2.83618965e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.80426788e+00  1.00000000e+00  6.25457764e-01 ...  8.19298633e+03\n",
      "   1.02138633e+04 -2.32549785e+04]\n",
      " [ 1.80435467e+00  1.00000000e+00  6.25126839e-01 ... -7.86713638e+01\n",
      "  -4.24618912e+01  5.88013916e+01]\n",
      " [ 1.80452538e+00  1.00000000e+00  6.24756813e-01 ... -6.82248592e+00\n",
      "   6.45081863e+01 -8.56573639e+01]\n",
      " ...\n",
      " [ 1.80449295e+00  1.00000000e+00  6.24883652e-01 ... -5.89390869e+02\n",
      "   7.54955597e+01 -1.58951538e+02]\n",
      " [ 1.80420303e+00  1.00000000e+00  6.25686646e-01 ...  4.76772217e+03\n",
      "   1.09996179e+03 -2.24217627e+03]\n",
      " [ 1.80421066e+00  1.00000000e+00  6.25598907e-01 ... -1.85611877e+01\n",
      "   1.14451256e+02  1.40330944e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5229263e+00  1.0000000e+00  1.1521149e+00 ... -2.3978119e+04\n",
      "   4.5544930e+04 -1.6232063e+04]\n",
      " [ 1.5231037e+00  1.0000000e+00  1.1518507e+00 ...  2.7659036e+02\n",
      "   4.5410977e+02  1.9088042e+02]\n",
      " [ 1.5234165e+00  1.0000000e+00  1.1515433e+00 ... -2.4519200e+02\n",
      "  -2.1184102e+02 -8.3189178e+01]\n",
      " ...\n",
      " [ 1.5233459e+00  1.0000000e+00  1.1516495e+00 ... -1.9720938e+02\n",
      "  -9.8564423e+01  8.8772957e+01]\n",
      " [ 1.5227928e+00  1.0000000e+00  1.1523018e+00 ... -1.2427975e+02\n",
      "  -7.1749115e+01  2.6257996e+02]\n",
      " [ 1.5228300e+00  1.0000000e+00  1.1522293e+00 ... -1.1757061e+03\n",
      "   6.4674542e+02  6.5674597e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0919905e+00  1.0000000e+00  1.5666218e+00 ...  2.3711387e+03\n",
      "  -5.9796207e+04 -1.3330852e+04]\n",
      " [ 1.0922298e+00  1.0000000e+00  1.5664492e+00 ...  3.3236379e+03\n",
      "   3.7911346e+01  5.0134341e+03]\n",
      " [ 1.0926590e+00  1.0000000e+00  1.5662351e+00 ...  9.7014137e+01\n",
      "  -2.1017311e+02  2.2154813e+02]\n",
      " ...\n",
      " [ 1.0925503e+00  1.0000000e+00  1.5662947e+00 ...  2.8409796e+02\n",
      "  -2.9804031e+02 -3.1534317e+01]\n",
      " [ 1.0917854e+00  1.0000000e+00  1.5667629e+00 ...  1.3971002e+02\n",
      "   6.2424829e+02 -6.5558173e+02]\n",
      " [ 1.0918627e+00  1.0000000e+00  1.5667095e+00 ...  1.2474508e+02\n",
      "  -3.0422540e+02  5.0147644e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.5431080e-01  1.0000000e+00  1.8274326e+00 ...  5.4319750e+04\n",
      "   2.5164680e+04  4.5271753e+03]\n",
      " [ 5.5461407e-01  1.0000000e+00  1.8273268e+00 ... -1.4641620e+02\n",
      "   1.8286974e+01 -2.7344821e+02]\n",
      " [ 5.5511093e-01  1.0000000e+00  1.8272407e+00 ... -9.8733147e+01\n",
      "  -1.8424220e+02  2.0675248e-01]\n",
      " ...\n",
      " [ 5.5498695e-01  1.0000000e+00  1.8272476e+00 ...  4.2142879e+01\n",
      "  -1.2370403e+02  5.7196552e+01]\n",
      " [ 5.5409813e-01  1.0000000e+00  1.8274803e+00 ...  1.3275298e+02\n",
      "   8.3369255e+01  2.2568132e+01]\n",
      " [ 5.5416870e-01  1.0000000e+00  1.8274670e+00 ... -1.1780436e+01\n",
      "  -8.5115578e+01  2.5268179e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.7540436e-02  1.0000000e+00  1.9092731e+00 ...  1.2210376e+04\n",
      "   5.2722852e+03  1.1584050e+04]\n",
      " [-3.7222862e-02  1.0000000e+00  1.9092569e+00 ...  4.0712189e+01\n",
      "   1.6208603e+02 -8.4145836e+01]\n",
      " [-3.6684036e-02  1.0000000e+00  1.9092989e+00 ...  3.3794455e+02\n",
      "   6.7166161e+01 -1.3055354e+02]\n",
      " ...\n",
      " [-3.6811829e-02  1.0000000e+00  1.9092531e+00 ... -7.5515358e+01\n",
      "   2.5041337e+00  1.5900857e+01]\n",
      " [-3.7734985e-02  1.0000000e+00  1.9092407e+00 ...  3.9054092e+02\n",
      "   4.1359282e+00  4.2496235e+01]\n",
      " [-3.7690163e-02  1.0000000e+00  1.9092693e+00 ...  2.0074833e+01\n",
      "   1.4987408e+02  3.0359628e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.2582874e-01  1.0000000e+00  1.8041515e+00 ... -4.8618687e+03\n",
      "  -4.6266577e+03  2.2871719e+03]\n",
      " [-6.2552834e-01  1.0000000e+00  1.8042374e+00 ...  1.4965175e+01\n",
      "  -6.6719994e+01 -1.4965441e+01]\n",
      " [-6.2499046e-01  1.0000000e+00  1.8044140e+00 ... -8.9968094e+01\n",
      "   3.7251962e+02  4.7697291e+00]\n",
      " ...\n",
      " [-6.2511444e-01  1.0000000e+00  1.8043251e+00 ... -1.3104999e+02\n",
      "  -3.2032025e+02  1.0402443e+02]\n",
      " [-6.2598801e-01  1.0000000e+00  1.8040333e+00 ...  3.9754131e+01\n",
      "  -7.8381004e+01 -3.5067226e+01]\n",
      " [-6.2596893e-01  1.0000000e+00  1.8041039e+00 ...  1.5910506e+01\n",
      "   1.7502618e+02  1.7216975e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1528101e+00  1.0000000e+00  1.5223942e+00 ...  1.1264133e+02\n",
      "   1.8740757e+03 -8.1214661e+01]\n",
      " [-1.1525717e+00  1.0000000e+00  1.5226116e+00 ... -1.0028757e+02\n",
      "  -6.5680229e+01  1.4914389e+01]\n",
      " [-1.1521225e+00  1.0000000e+00  1.5228910e+00 ... -3.7539603e+02\n",
      "   6.5226923e+02  1.1408852e+01]\n",
      " ...\n",
      " [-1.1522198e+00  1.0000000e+00  1.5227947e+00 ... -2.8961737e+02\n",
      "   6.7345474e+01 -1.0077598e+02]\n",
      " [-1.1529598e+00  1.0000000e+00  1.5222149e+00 ...  7.5900589e+01\n",
      "  -2.1321356e+01  5.9417316e+01]\n",
      " [-1.1529284e+00  1.0000000e+00  1.5223083e+00 ...  6.5205902e+01\n",
      "  -1.6495981e+02  1.5457874e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5667868    1.           1.091711  ... -823.5384     153.4573\n",
      "   301.8705   ]\n",
      " [  -1.5666237    1.           1.0920124 ... -141.42441    -60.885056\n",
      "   -28.605385 ]\n",
      " [  -1.5663013    1.           1.0923928 ...  587.8919       9.751167\n",
      "   216.70383  ]\n",
      " ...\n",
      " [  -1.5663795    1.           1.0922737 ...  -11.038254    75.566956\n",
      "    48.276165 ]\n",
      " [  -1.5669079    1.           1.0914955 ...  382.6661      35.874157\n",
      "  -158.8031   ]\n",
      " [  -1.5668793    1.           1.0916004 ...  -72.952194   -78.81005\n",
      "   -18.3521   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8274736e+00  1.0000000e+00  5.5392075e-01 ...  4.1140848e+02\n",
      "   3.7581732e+02 -4.5993045e+02]\n",
      " [-1.8274088e+00  1.0000000e+00  5.5423260e-01 ...  3.9302391e+01\n",
      "  -3.1898518e+01 -1.6839167e+01]\n",
      " [-1.8272190e+00  1.0000000e+00  5.5469745e-01 ...  1.8891611e+02\n",
      "   3.5941437e+02  1.3323683e+03]\n",
      " ...\n",
      " [-1.8272572e+00  1.0000000e+00  5.5455875e-01 ...  1.5995152e+02\n",
      "   1.1957993e+02  5.2010212e+01]\n",
      " [-1.8275204e+00  1.0000000e+00  5.5367279e-01 ... -6.0550365e+01\n",
      "   3.2531036e+02  1.9931656e+02]\n",
      " [-1.8275261e+00  1.0000000e+00  5.5379295e-01 ...  1.7425121e+02\n",
      "   2.0241277e+02  3.3134304e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9091797e+00  1.0000000e+00 -3.7765503e-02 ...  3.0019356e+01\n",
      "  -6.4674579e+02 -1.9859114e+02]\n",
      " [-1.9092112e+00  1.0000000e+00 -3.7446022e-02 ... -4.2064224e+01\n",
      "  -1.8291803e+01  1.7711699e+01]\n",
      " [-1.9091721e+00  1.0000000e+00 -3.6967054e-02 ...  2.9047760e+02\n",
      "  -3.5227411e+02  4.2095435e+02]\n",
      " ...\n",
      " [-1.9091587e+00  1.0000000e+00 -3.7102699e-02 ...  9.4431335e+01\n",
      "   3.3669392e+01  6.1989789e+00]\n",
      " [-1.9091682e+00  1.0000000e+00 -3.8022995e-02 ...  5.6563633e+03\n",
      "   5.2751544e+02 -1.3563883e+04]\n",
      " [-1.9091883e+00  1.0000000e+00 -3.7899017e-02 ...  5.7852002e+02\n",
      "   5.7607105e+01  2.7083676e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8039999e+00  1.0000000e+00 -6.2607765e-01 ...  1.5922581e+02\n",
      "   1.3767708e+02 -1.1049019e+03]\n",
      " [-1.8041248e+00  1.0000000e+00 -6.2574291e-01 ... -4.1831303e+01\n",
      "  -3.8130684e+01  5.4249710e+01]\n",
      " [-1.8042240e+00  1.0000000e+00 -6.2530231e-01 ... -2.8702522e+01\n",
      "   9.5265341e+00 -3.0702065e+01]\n",
      " ...\n",
      " [-1.8041649e+00  1.0000000e+00 -6.2541866e-01 ...  5.5823307e+01\n",
      "  -1.1435888e+02 -9.0768089e+00]\n",
      " [-1.8039150e+00  1.0000000e+00 -6.2631989e-01 ... -2.4970305e+04\n",
      "  -7.6404448e+03 -1.8674127e+04]\n",
      " [-1.8039560e+00  1.0000000e+00 -6.2620544e-01 ... -4.0590326e+02\n",
      "  -5.1259186e+02 -1.1802079e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5222359e+00  1.0000000e+00 -1.1529579e+00 ...  1.3346788e+02\n",
      "  -1.7425931e+02  2.6292987e+02]\n",
      " [-1.5224533e+00  1.0000000e+00 -1.1526546e+00 ...  7.5842148e+01\n",
      "  -3.1615148e+00 -3.4180260e+01]\n",
      " [-1.5227127e+00  1.0000000e+00 -1.1522830e+00 ...  7.8961611e+00\n",
      "   3.5429832e+01  4.0981209e+01]\n",
      " ...\n",
      " [-1.5226135e+00  1.0000000e+00 -1.1523733e+00 ... -1.9919041e+01\n",
      "   1.6925211e+01  3.6869617e+02]\n",
      " [-1.5221062e+00  1.0000000e+00 -1.1531601e+00 ... -7.3171680e+03\n",
      "   1.5323250e+03 -7.2815645e+03]\n",
      " [-1.5221653e+00  1.0000000e+00 -1.1530609e+00 ...  4.1518536e+01\n",
      "   4.1494343e+01  1.0580734e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0911713e+00  1.0000000e+00 -1.5671101e+00 ... -4.7617655e+00\n",
      "   6.8636230e+01  2.1182751e+01]\n",
      " [-1.0914736e+00  1.0000000e+00 -1.5668688e+00 ...  2.5460197e+02\n",
      "   2.4081213e+02  1.5853233e+01]\n",
      " [-1.0917873e+00  1.0000000e+00 -1.5666057e+00 ... -4.5407448e+01\n",
      "  -8.1823387e+00  1.5003759e+01]\n",
      " ...\n",
      " [-1.0916595e+00  1.0000000e+00 -1.5666571e+00 ...  7.8833237e+01\n",
      "  -4.5217850e+02 -1.2896738e+02]\n",
      " [-1.0909786e+00  1.0000000e+00 -1.5672531e+00 ...  8.8953632e+02\n",
      "  -1.2206510e+03  2.8604132e+02]\n",
      " [-1.0910587e+00  1.0000000e+00 -1.5671844e+00 ... -6.0173717e+01\n",
      "  -7.0739050e+00 -9.0549746e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.5534859     1.           -1.827591   ...   61.136494\n",
      "    69.81696      32.303215  ]\n",
      " [  -0.55383587    1.           -1.8274784  ...  -57.379322\n",
      "    71.63079      54.753143  ]\n",
      " [  -0.5541897     1.           -1.827329   ...   24.852125\n",
      "    42.083508    -20.3182    ]\n",
      " ...\n",
      " [  -0.5540447     1.           -1.8273487  ...  122.49628\n",
      "   144.96034     395.75848   ]\n",
      " [  -0.55327606    1.           -1.8276787  ...  -10.809164\n",
      "   -26.670483    -22.206722  ]\n",
      " [  -0.5533495     1.           -1.827631   ...  -19.757229\n",
      "   124.97787    -196.06827   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 3.8239479e-02  1.0000000e+00 -1.9091320e+00 ...  3.9039505e+01\n",
      "  -1.9057055e+01  9.1950203e+01]\n",
      " [ 3.7872314e-02  1.0000000e+00 -1.9091787e+00 ... -1.0345833e+01\n",
      "   2.6700555e+02  1.2748303e+02]\n",
      " [ 3.7487030e-02  1.0000000e+00 -1.9091527e+00 ...  1.6831477e+02\n",
      "   1.2454598e+02 -1.7607774e+02]\n",
      " ...\n",
      " [ 3.7633896e-02  1.0000000e+00 -1.9091415e+00 ...  3.8554115e+00\n",
      "  -1.9183069e+02  2.9120325e+02]\n",
      " [ 3.8440704e-02  1.0000000e+00 -1.9091434e+00 ... -1.8749987e+01\n",
      "  -1.3135231e+02  8.1526024e+01]\n",
      " [ 3.8383484e-02  1.0000000e+00 -1.9091339e+00 ...  7.3094202e+02\n",
      "  -2.6954351e+01  2.2931789e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.62640285    1.           -1.8038349  ... -132.79413\n",
      "   -15.777        42.824688  ]\n",
      " [   0.6260519     1.           -1.803998   ...   94.93359\n",
      "     9.488187      7.5078125 ]\n",
      " [   0.6256714     1.           -1.8041075  ...  -65.90168\n",
      "   320.54913     104.435005  ]\n",
      " ...\n",
      " [   0.6258087     1.           -1.8040619  ...   31.00072\n",
      "   -98.52063     128.94353   ]\n",
      " [   0.62657166    1.           -1.8037586  ...  -28.480991\n",
      "    -5.126503     50.362022  ]\n",
      " [   0.6265421     1.           -1.8037968  ... -336.5171\n",
      "   -70.432335     84.42777   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1534519e+00  1.0000000e+00 -1.5217705e+00 ... -4.6353888e+02\n",
      "  -1.9356105e+02  6.5975378e+02]\n",
      " [ 1.1531506e+00  1.0000000e+00 -1.5220299e+00 ... -3.7957142e+01\n",
      "  -1.6805166e+01 -3.5698250e+01]\n",
      " [ 1.1528244e+00  1.0000000e+00 -1.5222656e+00 ...  2.6862576e+01\n",
      "  -6.1147160e+01 -1.8207878e+01]\n",
      " ...\n",
      " [ 1.1529446e+00  1.0000000e+00 -1.5221901e+00 ...  1.6250136e+02\n",
      "   1.9686670e+02 -8.3253212e+01]\n",
      " [ 1.1536007e+00  1.0000000e+00 -1.5216293e+00 ... -3.6573471e+02\n",
      "  -8.1674324e+01  2.0243313e+01]\n",
      " [ 1.1535749e+00  1.0000000e+00 -1.5216866e+00 ...  1.2420733e+02\n",
      "  -1.4165914e+03  2.3793530e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5673685    1.          -1.0906963 ...  193.77238    -35.866405\n",
      "   -99.76282  ]\n",
      " [   1.5671473    1.          -1.0910435 ...   58.94169    -54.51332\n",
      "    59.855724 ]\n",
      " [   1.5669155    1.          -1.0913723 ...   20.883396   -10.990266\n",
      "    23.24085  ]\n",
      " ...\n",
      " [   1.567009     1.          -1.091258  ...   30.162584    34.989532\n",
      "   -92.59941  ]\n",
      " [   1.5674706    1.          -1.090517  ...   17.621073   287.1833\n",
      "   473.17084  ]\n",
      " [   1.5674591    1.          -1.0905895 ...  205.28183    -85.510765\n",
      "  -517.7264   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.827712      1.           -0.55309105 ...  -37.975677\n",
      "  -109.02422    -115.955605  ]\n",
      " [   1.8276052     1.           -0.55348873 ...   15.290942\n",
      "  -133.22438      -4.5733805 ]\n",
      " [   1.827465      1.           -0.5538544  ...   36.925583\n",
      "   118.77825     120.48737   ]\n",
      " ...\n",
      " [   1.8274994     1.           -0.55373955 ... -132.69733\n",
      "   -36.47391      62.71633   ]\n",
      " [   1.8277149     1.           -0.5528774  ...   55.881626\n",
      "   104.694275     29.439806  ]\n",
      " [   1.8277721     1.           -0.5529556  ...  -19.706656\n",
      "   215.09392     -50.203808  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90920925e+00  1.00000000e+00  3.88183594e-02 ... -4.30881691e+01\n",
      "   6.94137268e+01  1.78273582e+01]\n",
      " [ 1.90921211e+00  1.00000000e+00  3.83520126e-02 ...  1.20759649e+01\n",
      "  -1.12217720e+02  1.27638695e+02]\n",
      " [ 1.90918922e+00  1.00000000e+00  3.79875675e-02 ...  1.29688873e+02\n",
      "  -3.75291672e+01  4.69407158e+01]\n",
      " ...\n",
      " [ 1.90917778e+00  1.00000000e+00  3.80907059e-02 ...  1.23052528e+02\n",
      "   7.02240906e+02 -5.16217834e+02]\n",
      " [ 1.90914536e+00  1.00000000e+00  3.90377045e-02 ... -5.22084389e+01\n",
      "  -6.39409943e+01 -3.22388458e+00]\n",
      " [ 1.90922070e+00  1.00000000e+00  3.89556885e-02 ...  7.16572876e+01\n",
      "  -2.27486694e+02 -4.17730621e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.1       0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.80344486e+00  1.00000000e+00  6.27332687e-01 ... -5.01391296e+01\n",
      "   4.57928429e+01 -3.03181915e+01]\n",
      " [ 1.80357170e+00  1.00000000e+00  6.26911163e-01 ... -5.46732373e+03\n",
      "   1.52728137e+03 -1.14253564e+03]\n",
      " [ 1.80365181e+00  1.00000000e+00  6.26564264e-01 ...  4.03755249e+02\n",
      "  -1.68407761e+02  9.94501190e+01]\n",
      " ...\n",
      " [ 1.80360031e+00  1.00000000e+00  6.26666069e-01 ... -2.51199799e+02\n",
      "   2.50032074e+02 -2.92184265e+02]\n",
      " [ 1.80330276e+00  1.00000000e+00  6.27532959e-01 ...  1.25480797e+02\n",
      "   1.86114731e+02  1.55061249e+02]\n",
      " [ 1.80341148e+00  1.00000000e+00  6.27454758e-01 ... -1.18702240e+02\n",
      "   1.29452763e+01  1.00691505e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5215206    1.           1.153738  ...  117.77048   -103.91683\n",
      "   162.90031  ]\n",
      " [   1.5217457    1.           1.1533928 ...  436.69388    123.71018\n",
      "   414.6551   ]\n",
      " [   1.5219345    1.           1.1531063 ... -144.46436    332.92422\n",
      "  -178.0488   ]\n",
      " ...\n",
      " [   1.5218487    1.           1.1531811 ...  103.10506     58.844044\n",
      "   249.23973  ]\n",
      " [   1.521307     1.           1.1539135 ...  -69.95461     73.798584\n",
      "  -232.90425  ]\n",
      " [   1.5214529    1.           1.1538525 ...  -31.847473    -7.301564\n",
      "    11.308495 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0904398e+00  1.0000000e+00  1.5675888e+00 ...  1.0164503e+02\n",
      "  -6.0349913e+00 -2.8790302e+01]\n",
      " [ 1.0907421e+00  1.0000000e+00  1.5672884e+00 ... -3.4515042e+02\n",
      "  -4.9316544e+02  4.9138214e+01]\n",
      " [ 1.0910282e+00  1.0000000e+00  1.5671055e+00 ...  2.7136669e+01\n",
      "   1.4228011e+02  1.7631570e+02]\n",
      " ...\n",
      " [ 1.0909004e+00  1.0000000e+00  1.5671453e+00 ...  4.1127213e+01\n",
      "  -6.8461380e+01  1.4617944e+02]\n",
      " [ 1.0901699e+00  1.0000000e+00  1.5677166e+00 ...  8.7535594e+04\n",
      "   8.6856578e+04 -9.2588898e+04]\n",
      " [ 1.0903416e+00  1.0000000e+00  1.5676785e+00 ... -3.8490433e+01\n",
      "  -5.4057526e+01 -2.5893118e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.52211761e-01  1.00000000e+00  1.82802391e+00 ... -6.07593346e+01\n",
      "  -3.08860531e+01 -1.58286057e+01]\n",
      " [ 5.52567482e-01  1.00000000e+00  1.82781792e+00 ... -5.13054504e+01\n",
      "   3.33513165e+00 -8.70807571e+01]\n",
      " [ 5.52902222e-01  1.00000000e+00  1.82774472e+00 ... -3.63629189e+01\n",
      "   1.10535145e+01  2.26777840e+00]\n",
      " ...\n",
      " [ 5.52753448e-01  1.00000000e+00  1.82774925e+00 ...  2.06627930e+02\n",
      "   5.60051346e+01  3.28130865e+00]\n",
      " [ 5.51904678e-01  1.00000000e+00  1.82808304e+00 ... -1.83502051e+04\n",
      "  -3.56689990e+03  1.36114209e+04]\n",
      " [ 5.52088737e-01  1.00000000e+00  1.82807159e+00 ...  5.11876984e+02\n",
      "  -6.33636658e+02 -5.58235931e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-3.94639969e-02  1.00000000e+00  1.90917969e+00 ... -7.91460514e+00\n",
      "   1.28449707e+01  4.25360146e+01]\n",
      " [-3.90892029e-02  1.00000000e+00  1.90913010e+00 ...  3.00502899e+02\n",
      "   1.48811890e+02  5.46064880e+02]\n",
      " [-3.86962891e-02  1.00000000e+00  1.90916073e+00 ...  1.92708911e+03\n",
      "  -1.78866943e+03 -5.50156189e+02]\n",
      " ...\n",
      " [-3.88526917e-02  1.00000000e+00  1.90914154e+00 ... -3.65905322e+03\n",
      "  -1.59341875e+04 -7.93690820e+03]\n",
      " [-3.97491455e-02  1.00000000e+00  1.90917969e+00 ...  1.24283545e+04\n",
      "   6.47903613e+03 -3.70205298e+03]\n",
      " [-3.95870209e-02  1.00000000e+00  1.90918922e+00 ...  6.51402588e+01\n",
      "   5.37246590e+01 -1.67417328e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:5, Score:1.54, Best Score:2.33, Average Score:1.67, Best Avg Score:1.72\n",
      "Episode number: 6\n",
      "Hand Exit\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c3e85b0>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1542244e+00  1.0000000e+00  1.5212784e+00 ...  3.8300903e+01\n",
      "  -9.8823952e+01 -7.9774408e+00]\n",
      " [-1.1539259e+00  1.0000000e+00  1.5214748e+00 ...  8.4101578e+01\n",
      "  -6.6934288e+01 -1.9416103e+02]\n",
      " [-1.1536465e+00  1.0000000e+00  1.5217303e+00 ...  4.4544964e+01\n",
      "   4.4123615e+02 -1.6490813e+02]\n",
      " ...\n",
      " [-1.1537819e+00  1.0000000e+00  1.5216379e+00 ... -8.7332578e+03\n",
      "   5.1087036e+03  1.6664622e+03]\n",
      " [-1.1545105e+00  1.0000000e+00  1.5211506e+00 ... -1.0830487e+03\n",
      "  -2.4316992e+03  4.4899973e+02]\n",
      " [-1.1543112e+00  1.0000000e+00  1.5212364e+00 ...  3.5878365e+01\n",
      "   6.5217857e+01 -1.4529025e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5678978e+00  1.0000000e+00  1.0897865e+00 ... -1.7709976e+01\n",
      "   2.8204142e+01 -2.7459848e+01]\n",
      " [-1.5676899e+00  1.0000000e+00  1.0900669e+00 ...  1.1001659e+02\n",
      "  -1.7563359e+02  1.3905644e+02]\n",
      " [-1.5675240e+00  1.0000000e+00  1.0904098e+00 ... -8.2457443e+01\n",
      "  -5.7673119e+01 -3.7512058e+01]\n",
      " ...\n",
      " [-1.5676174e+00  1.0000000e+00  1.0902939e+00 ...  6.5613885e+02\n",
      "  -3.4063799e+03  3.4810962e+03]\n",
      " [-1.5681438e+00  1.0000000e+00  1.0895920e+00 ...  6.7973126e+02\n",
      "  -1.6916747e+03  3.3406741e+02]\n",
      " [-1.5679579e+00  1.0000000e+00  1.0897121e+00 ... -7.5139099e+01\n",
      "   1.1560344e+01  5.7875624e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8279781e+00  1.0000000e+00  5.5183411e-01 ... -5.2759083e+01\n",
      "  -9.1434105e+01 -4.6271416e+01]\n",
      " [-1.8278885e+00  1.0000000e+00  5.5217648e-01 ... -2.9730350e+01\n",
      "   2.2713350e+02  2.0560710e+02]\n",
      " [-1.8278198e+00  1.0000000e+00  5.5257171e-01 ... -2.1413298e+01\n",
      "  -7.7560249e+01  3.8423910e+00]\n",
      " ...\n",
      " [-1.8278656e+00  1.0000000e+00  5.5243397e-01 ... -6.1491412e+02\n",
      "   4.7382745e+02  2.3582766e+03]\n",
      " [-1.8281345e+00  1.0000000e+00  5.5161858e-01 ...  3.2078711e+02\n",
      "  -7.1905876e+01 -3.2455978e+02]\n",
      " [-1.8280125e+00  1.0000000e+00  5.5174637e-01 ... -3.3231247e+01\n",
      "  -8.0572662e+01 -5.1559685e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9090853e+00  1.0000000e+00 -4.0134430e-02 ... -1.8857800e+01\n",
      "   3.3263052e+00  9.1398514e+01]\n",
      " [-1.9091177e+00  1.0000000e+00 -3.9780617e-02 ... -2.0999118e+02\n",
      "  -2.0582533e+02  7.2295990e+02]\n",
      " [-1.9091587e+00  1.0000000e+00 -3.9365571e-02 ...  3.7478752e+01\n",
      "  -9.9175453e+01  3.2088159e+02]\n",
      " ...\n",
      " [-1.9091454e+00  1.0000000e+00 -3.9515495e-02 ...  1.3716276e+03\n",
      "  -5.2579120e+02 -3.1040680e+02]\n",
      " [-1.9091721e+00  1.0000000e+00 -4.0357590e-02 ... -2.7816229e+02\n",
      "  -3.6344724e+02 -3.5779715e+02]\n",
      " [-1.9090891e+00  1.0000000e+00 -4.0222168e-02 ... -1.5544569e+02\n",
      "  -8.8144714e+01  1.8501355e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8032227e+00  1.0000000e+00 -6.2831116e-01 ...  6.1305428e+01\n",
      "  -6.5013786e+01  2.5702972e+02]\n",
      " [-1.8033867e+00  1.0000000e+00 -6.2795734e-01 ...  1.0080124e+02\n",
      "  -1.7007703e+02  3.3915913e+01]\n",
      " [-1.8035469e+00  1.0000000e+00 -6.2756306e-01 ...  9.1122307e+01\n",
      "  -6.8814758e+01 -2.3861635e+01]\n",
      " ...\n",
      " [-1.8035088e+00  1.0000000e+00 -6.2770176e-01 ... -7.4685693e+02\n",
      "  -4.2670999e+02 -7.9846283e+01]\n",
      " [-1.8032398e+00  1.0000000e+00 -6.2852478e-01 ...  2.5238787e+01\n",
      "   2.5468239e+01 -8.9455002e+01]\n",
      " [-1.8031836e+00  1.0000000e+00 -6.2839508e-01 ... -5.9246517e+01\n",
      "  -4.1473442e+01 -9.5698662e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5208282e+00  1.0000000e+00 -1.1549015e+00 ...  2.2303780e+02\n",
      "  -4.4955376e+01  5.2437103e+01]\n",
      " [-1.5210781e+00  1.0000000e+00 -1.1545782e+00 ... -2.5568003e+01\n",
      "  -1.4042614e+01  3.2861035e+02]\n",
      " [-1.5213585e+00  1.0000000e+00 -1.1542469e+00 ... -5.2387146e+02\n",
      "   4.7364197e+02 -4.6769220e+02]\n",
      " ...\n",
      " [-1.5212784e+00  1.0000000e+00 -1.1543655e+00 ... -6.3387335e+02\n",
      "  -1.4015345e+03  1.0826642e+03]\n",
      " [-1.5207577e+00  1.0000000e+00 -1.1550713e+00 ...  2.3745369e+01\n",
      "  -1.1416092e+02 -3.0635387e+01]\n",
      " [-1.5207634e+00  1.0000000e+00 -1.1549664e+00 ... -2.6838403e+02\n",
      "   1.4097951e+01  5.1782288e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0895672    1.          -1.5683079 ...   49.704205  -147.86919\n",
      "    11.696729 ]\n",
      " [  -1.0899038    1.          -1.5680637 ...   60.249264     2.0296636\n",
      "    41.78236  ]\n",
      " [  -1.0903053    1.          -1.5678414 ...  -53.165     -110.02653\n",
      "   -22.041636 ]\n",
      " ...\n",
      " [  -1.0901852    1.          -1.5679264 ...  -75.39271     95.06856\n",
      "   351.78992  ]\n",
      " [  -1.0894489    1.          -1.5684319 ...  409.30206   -119.67178\n",
      "   470.7949   ]\n",
      " [  -1.0894852    1.          -1.5683517 ...  214.86858   -174.90369\n",
      "   141.16669  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.5136585e-01  1.0000000e+00 -1.8282833e+00 ... -1.5206612e+02\n",
      "   4.5566399e+01  2.4265896e+01]\n",
      " [-5.5176258e-01  1.0000000e+00 -1.8281727e+00 ... -3.8639927e+01\n",
      "  -2.2661002e+02 -5.8965924e+02]\n",
      " [-5.5217934e-01  1.0000000e+00 -1.8280556e+00 ... -3.7975391e+01\n",
      "  -1.5590057e+02  1.3093915e+02]\n",
      " ...\n",
      " [-5.5203438e-01  1.0000000e+00 -1.8281031e+00 ... -2.1255029e+02\n",
      "  -6.3718683e+02 -7.4596759e+02]\n",
      " [-5.5119133e-01  1.0000000e+00 -1.8283463e+00 ... -6.8361839e+01\n",
      "   9.9819878e+01 -4.4099464e+01]\n",
      " [-5.5127144e-01  1.0000000e+00 -1.8282948e+00 ...  8.3882538e+02\n",
      "   4.2157928e+02  4.7067990e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.04787064e-02  1.00000000e+00 -1.90917206e+00 ...  5.16452866e+01\n",
      "  -2.41922836e+02 -1.17040306e+02]\n",
      " [ 4.00667191e-02  1.00000000e+00 -1.90919209e+00 ...  1.09523666e+02\n",
      "  -1.77380390e+01  1.21207169e+02]\n",
      " [ 3.96404266e-02  1.00000000e+00 -1.90920150e+00 ...  6.20103149e+01\n",
      "  -8.98435020e+00  2.92706184e+01]\n",
      " ...\n",
      " [ 3.97911072e-02  1.00000000e+00 -1.90919781e+00 ... -3.81046844e+02\n",
      "  -2.04382675e+02 -3.47511383e+02]\n",
      " [ 4.06646729e-02  1.00000000e+00 -1.90917587e+00 ... -1.23785957e+02\n",
      "  -6.73642151e+02  5.62891724e+02]\n",
      " [ 4.05769348e-02  1.00000000e+00 -1.90915680e+00 ...  3.52589111e+01\n",
      "  -4.41857376e+01 -1.86306114e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.28528595e-01  1.00000000e+00 -1.80317116e+00 ... -2.85533447e+02\n",
      "   5.66223572e+02 -4.63205475e+02]\n",
      " [ 6.28136635e-01  1.00000000e+00 -1.80333424e+00 ... -1.06150932e+02\n",
      "  -6.04776344e+01 -1.00571945e+02]\n",
      " [ 6.27763748e-01  1.00000000e+00 -1.80347478e+00 ... -1.86235218e+01\n",
      "   1.42253906e+02  9.38906174e+01]\n",
      " ...\n",
      " [ 6.27908707e-01  1.00000000e+00 -1.80342293e+00 ... -7.77215652e+01\n",
      "  -4.00013466e+01 -7.17201004e+01]\n",
      " [ 6.28732681e-01  1.00000000e+00 -1.80308914e+00 ... -5.30170471e+02\n",
      "  -2.56421143e+02  6.79206299e+02]\n",
      " [ 6.28623009e-01  1.00000000e+00 -1.80310631e+00 ...  5.15143585e+01\n",
      "  -4.31598625e+01 -1.57449684e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1551304    1.          -1.5205173 ... -372.5216     157.22983\n",
      "   -87.5938   ]\n",
      " [   1.1547976    1.          -1.5208015 ...  -30.790455   -86.90503\n",
      "     5.507783 ]\n",
      " [   1.1545029    1.          -1.5210506 ...  -90.91295    -19.902973\n",
      "    -3.5265493]\n",
      " ...\n",
      " [   1.154623     1.          -1.5209646 ...  -63.488632   -49.946297\n",
      "    70.885445 ]\n",
      " [   1.1553059    1.          -1.5203857 ...  -17.11666      1.6194636\n",
      "   -16.500534 ]\n",
      " [   1.1552076    1.          -1.5204391 ...  160.15887   -215.14833\n",
      "    21.616655 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5686836    1.          -1.0888596 ...  -26.060637  -490.40375\n",
      "    51.54472  ]\n",
      " [   1.568429     1.          -1.0892563 ...  -21.616606   -13.7225895\n",
      "    24.319725 ]\n",
      " [   1.5682144    1.          -1.0895816 ...  343.21182   -115.14564\n",
      "   -84.80115  ]\n",
      " ...\n",
      " [   1.5683079    1.          -1.089467  ...  123.96195     -1.2567389\n",
      "    65.95083  ]\n",
      " [   1.5687885    1.          -1.0886822 ...  900.4886     374.61826\n",
      "   108.80473  ]\n",
      " [   1.5687399    1.          -1.0887604 ...   21.030142  -213.97032\n",
      "    54.922066 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8284225e+00  1.0000000e+00 -5.5080795e-01 ... -2.4760782e+02\n",
      "   1.2369718e+02 -1.7184320e+02]\n",
      " [ 1.8282824e+00  1.0000000e+00 -5.5125713e-01 ...  1.3786731e+02\n",
      "  -8.2594467e+01  2.4338844e+01]\n",
      " [ 1.8282318e+00  1.0000000e+00 -5.5162907e-01 ... -5.5010952e+03\n",
      "  -4.1452258e+02  4.1342573e+03]\n",
      " ...\n",
      " [ 1.8282928e+00  1.0000000e+00 -5.5149460e-01 ... -1.8814335e+01\n",
      "  -2.0722832e+01  1.7873852e+01]\n",
      " [ 1.8284836e+00  1.0000000e+00 -5.5060196e-01 ...  3.0854266e+02\n",
      "  -5.4344403e+02 -1.9293098e+03]\n",
      " [ 1.8284378e+00  1.0000000e+00 -5.5069733e-01 ...  1.1877600e+01\n",
      "  -2.4580656e+01 -5.3182831e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9091625e+00  1.0000000e+00  4.1070938e-02 ... -7.4612961e+01\n",
      "   7.9599251e+01  8.7156609e+01]\n",
      " [ 1.9091587e+00  1.0000000e+00  4.0576935e-02 ... -2.3013083e+01\n",
      "  -1.9541046e+01 -3.8961067e+01]\n",
      " [ 1.9092197e+00  1.0000000e+00  4.0184371e-02 ... -4.2373477e+02\n",
      "  -2.3602766e+02  8.3639893e+01]\n",
      " ...\n",
      " [ 1.9092274e+00  1.0000000e+00  4.0328979e-02 ...  4.7836887e+01\n",
      "   2.0426231e+02  9.8248505e+02]\n",
      " [ 1.9091225e+00  1.0000000e+00  4.1286469e-02 ...  5.9425677e+02\n",
      "  -6.4686023e+02  7.4265973e+02]\n",
      " [ 1.9091415e+00  1.0000000e+00  4.1185379e-02 ...  5.8600327e+01\n",
      "  -4.3217312e+01  1.6492963e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8029165    1.           0.6294594 ...  -98.995316   -14.489424\n",
      "    12.706958 ]\n",
      " [   1.8030348    1.           0.6290016 ... -158.03453     67.06133\n",
      "   -10.09389  ]\n",
      " [   1.8032246    1.           0.6286327 ...   10.043947   -26.268177\n",
      "    -8.454709 ]\n",
      " ...\n",
      " [   1.8032036    1.           0.6287632 ...  305.3047     -39.752975\n",
      "   -66.81272  ]\n",
      " [   1.802803     1.           0.6296654 ...   34.0976      92.60161\n",
      "    23.025211 ]\n",
      " [   1.8028584    1.           0.62957   ...  133.70729     55.846523\n",
      "  -240.91235  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.52037239e+00  1.00000000e+00  1.15548515e+00 ...  1.29574738e+02\n",
      "   1.09968147e+02  1.19632164e+02]\n",
      " [ 1.52062130e+00  1.00000000e+00  1.15513706e+00 ...  2.64031128e+03\n",
      "  -8.29929016e+02  3.57014099e+02]\n",
      " [ 1.52091408e+00  1.00000000e+00  1.15481150e+00 ...  1.28211288e+02\n",
      "  -1.94896889e+01  1.01999107e+02]\n",
      " ...\n",
      " [ 1.52086067e+00  1.00000000e+00  1.15494442e+00 ...  4.69677429e+01\n",
      "  -1.12119141e+02  6.13405609e+01]\n",
      " [ 1.52022171e+00  1.00000000e+00  1.15566254e+00 ...  2.88025604e+02\n",
      "   1.57577930e+03  1.04255408e+03]\n",
      " [ 1.52027798e+00  1.00000000e+00  1.15558052e+00 ... -1.14180771e+02\n",
      "  -9.42093353e+01 -1.81717682e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.08857250e+00  1.00000000e+00  1.56870461e+00 ... -5.18386475e+02\n",
      "   4.18930847e+02 -5.26878296e+02]\n",
      " [ 1.08891582e+00  1.00000000e+00  1.56844139e+00 ...  5.32411423e+01\n",
      "   1.70091064e+02 -6.62262192e+01]\n",
      " [ 1.08934975e+00  1.00000000e+00  1.56821430e+00 ...  7.52933979e+00\n",
      "  -1.33561239e+01  7.51705093e+01]\n",
      " ...\n",
      " [ 1.08925438e+00  1.00000000e+00  1.56830597e+00 ... -1.50677820e+03\n",
      "   1.27762794e+02 -1.55500366e+02]\n",
      " [ 1.08841324e+00  1.00000000e+00  1.56881332e+00 ... -7.95153046e+01\n",
      "  -8.90812073e+01 -1.07235527e+02]\n",
      " [ 1.08845329e+00  1.00000000e+00  1.56877136e+00 ... -1.78871933e+02\n",
      "   4.74054993e+02 -8.24729492e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.5031395e-01  1.0000000e+00  1.8284206e+00 ...  1.4087802e+02\n",
      "   3.1790222e+02 -3.6858322e+02]\n",
      " [ 5.5070782e-01  1.0000000e+00  1.8282785e+00 ...  5.5069672e+01\n",
      "   1.7352438e+02  1.2294489e+01]\n",
      " [ 5.5122757e-01  1.0000000e+00  1.8281718e+00 ...  7.2302155e+01\n",
      "  -8.4800091e+00  4.9956665e+01]\n",
      " ...\n",
      " [ 5.5111313e-01  1.0000000e+00  1.8282347e+00 ... -4.7845254e+03\n",
      "  -1.2839936e+04 -1.8634475e+04]\n",
      " [ 5.5013847e-01  1.0000000e+00  1.8284569e+00 ... -2.2275699e+02\n",
      "   4.9199130e+02  5.7679581e+01]\n",
      " [ 5.5017757e-01  1.0000000e+00  1.8284550e+00 ...  7.0869133e+01\n",
      "  -8.5111580e+01  1.9253284e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.1610718e-02  1.0000000e+00  1.9090157e+00 ... -8.6702843e+01\n",
      "   2.8382867e+02 -7.5975995e+00]\n",
      " [-4.1199684e-02  1.0000000e+00  1.9089918e+00 ...  1.5331418e+03\n",
      "   8.4662036e+02  5.6059863e+02]\n",
      " [-4.0681839e-02  1.0000000e+00  1.9090240e+00 ...  6.2695065e+01\n",
      "   6.8132332e+01  4.2549683e+01]\n",
      " ...\n",
      " [-4.0798187e-02  1.0000000e+00  1.9090490e+00 ... -3.0226431e+03\n",
      "  -1.9702546e+03 -5.3192218e+02]\n",
      " [-4.1799545e-02  1.0000000e+00  1.9089851e+00 ... -1.0189407e+02\n",
      "  -5.6288052e+01 -3.3221603e+01]\n",
      " [-4.1752815e-02  1.0000000e+00  1.9090157e+00 ...  1.4992935e+01\n",
      "  -8.3540024e+01  1.5996662e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.2960052e-01  1.0000000e+00  1.8027115e+00 ...  8.7325996e+01\n",
      "   2.3053509e+01 -2.4906895e+01]\n",
      " [-6.2921238e-01  1.0000000e+00  1.8028498e+00 ... -2.7698404e+02\n",
      "  -1.5528664e+02  1.9601538e+02]\n",
      " [-6.2872124e-01  1.0000000e+00  1.8030150e+00 ... -1.7874773e+02\n",
      "  -2.0827669e+02  3.7672863e+01]\n",
      " ...\n",
      " [-6.2882614e-01  1.0000000e+00  1.8030062e+00 ...  3.5246250e+03\n",
      "   1.7517058e+03 -1.0940745e+03]\n",
      " [-6.2977028e-01  1.0000000e+00  1.8026180e+00 ...  6.4345715e+02\n",
      "  -2.2672432e+02 -1.5971474e+02]\n",
      " [-6.2973404e-01  1.0000000e+00  1.8026714e+00 ...  3.6878273e+01\n",
      "  -8.3013138e+01 -1.9370848e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1562176e+00  1.0000000e+00  1.5195255e+00 ... -2.2566365e+02\n",
      "   1.0892899e+02  1.0116964e+01]\n",
      " [-1.1558933e+00  1.0000000e+00  1.5197372e+00 ...  1.2078104e+02\n",
      "   2.6612469e+01 -2.7207237e+01]\n",
      " [-1.1554947e+00  1.0000000e+00  1.5200506e+00 ... -1.7924107e+02\n",
      "  -1.2272910e+00 -1.1736557e+02]\n",
      " ...\n",
      " [-1.1555862e+00  1.0000000e+00  1.5199909e+00 ...  2.1035544e+03\n",
      "   6.1738861e+02  7.4829590e+02]\n",
      " [-1.1563530e+00  1.0000000e+00  1.5193634e+00 ... -2.2994513e+02\n",
      "  -8.7266772e+02  2.4355142e+03]\n",
      " [-1.1563396e+00  1.0000000e+00  1.5194283e+00 ... -3.2537827e+03\n",
      "   1.2292683e+03  8.7495654e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.56923389e+00  1.00000000e+00  1.08790970e+00 ...  2.16645554e+02\n",
      "   1.43994949e+02 -7.61399155e+01]\n",
      " [-1.56901646e+00  1.00000000e+00  1.08816433e+00 ...  1.77207169e+02\n",
      "  -2.85763763e+02 -1.88289474e+02]\n",
      " [-1.56873131e+00  1.00000000e+00  1.08859885e+00 ... -1.10863716e+02\n",
      "   8.07053375e+01 -1.49574814e+02]\n",
      " ...\n",
      " [-1.56878662e+00  1.00000000e+00  1.08849430e+00 ... -3.12283478e+02\n",
      "   4.02761139e+02  7.54770447e+02]\n",
      " [-1.56933403e+00  1.00000000e+00  1.08768654e+00 ...  1.45764014e+03\n",
      "  -5.46787170e+02  2.34678882e+03]\n",
      " [-1.56933117e+00  1.00000000e+00  1.08777237e+00 ... -5.63922005e+01\n",
      "   2.82956360e+02 -7.52484818e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8285217e+00  1.0000000e+00  5.4959297e-01 ...  7.5712822e+01\n",
      "   3.9973074e+02  1.7931522e+02]\n",
      " [-1.8284159e+00  1.0000000e+00  5.4991722e-01 ... -4.7337605e+04\n",
      "  -3.7366011e+03  4.1421840e+04]\n",
      " [-1.8282604e+00  1.0000000e+00  5.5042535e-01 ... -8.8812971e-01\n",
      "  -8.4932510e+01  6.9413667e+00]\n",
      " ...\n",
      " [-1.8282700e+00  1.0000000e+00  5.5029869e-01 ... -1.3095505e+02\n",
      "   1.3489851e+02  1.1623636e+02]\n",
      " [-1.8285580e+00  1.0000000e+00  5.4933357e-01 ... -3.6587515e+02\n",
      "   4.4739743e+02  9.6191321e+02]\n",
      " [-1.8285780e+00  1.0000000e+00  5.4942513e-01 ...  6.5862183e+02\n",
      "  -8.0218127e+02  7.0614679e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9089375e+00  1.0000000e+00 -4.2654037e-02 ... -1.9863724e+02\n",
      "   8.8618523e+01  1.1993494e+02]\n",
      " [-1.9089546e+00  1.0000000e+00 -4.2283058e-02 ...  7.3010547e+02\n",
      "  -1.0907598e+03 -1.3583303e+03]\n",
      " [-1.9089241e+00  1.0000000e+00 -4.1772936e-02 ...  1.3575203e+02\n",
      "  -1.0763690e+02 -2.6352921e+00]\n",
      " ...\n",
      " [-1.9089031e+00  1.0000000e+00 -4.1889191e-02 ...  1.1350144e+03\n",
      "   9.2861127e+02  7.7572617e+01]\n",
      " [-1.9089146e+00  1.0000000e+00 -4.2924881e-02 ...  5.7585797e+02\n",
      "   7.1393982e+01  1.9230379e+02]\n",
      " [-1.9089470e+00  1.0000000e+00 -4.2831421e-02 ... -7.2570862e+02\n",
      "   7.9638342e+02 -8.3466022e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.80221558e+00  1.00000000e+00 -6.30477905e-01 ...  4.82544327e+01\n",
      "  -2.10884399e+01  1.66158875e+02]\n",
      " [-1.80234718e+00  1.00000000e+00 -6.30152702e-01 ... -1.50541992e+03\n",
      "   7.70194824e+02  5.78846130e+02]\n",
      " [-1.80250359e+00  1.00000000e+00 -6.29662275e-01 ...  3.43586235e+01\n",
      "  -1.74229782e+02  1.53766220e+02]\n",
      " ...\n",
      " [-1.80245018e+00  1.00000000e+00 -6.29780769e-01 ...  1.10054924e+02\n",
      "  -2.18708786e+02  3.77402458e+01]\n",
      " [-1.80212975e+00  1.00000000e+00 -6.30727768e-01 ... -1.07331696e+02\n",
      "  -4.23544235e+01 -1.48285538e+02]\n",
      " [-1.80216980e+00  1.00000000e+00 -6.30641937e-01 ...  1.59134354e+02\n",
      "   9.35627823e+01 -5.25489014e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5190945e+00  1.0000000e+00 -1.1567822e+00 ... -3.1695270e+02\n",
      "  -2.2025013e+01  3.6819998e+02]\n",
      " [-1.5193443e+00  1.0000000e+00 -1.1565361e+00 ... -2.8447556e+02\n",
      "   8.0174451e+02  7.5867633e+02]\n",
      " [-1.5196590e+00  1.0000000e+00 -1.1561161e+00 ...  1.9776465e+02\n",
      "   1.2088255e+02 -1.9575746e+02]\n",
      " ...\n",
      " [-1.5195713e+00  1.0000000e+00 -1.1562252e+00 ... -1.0612990e+02\n",
      "   1.4690892e+02  3.5225639e+01]\n",
      " [-1.5189724e+00  1.0000000e+00 -1.1570110e+00 ...  3.5598702e+00\n",
      "  -3.9378128e+02  3.0052521e+02]\n",
      " [-1.5190153e+00  1.0000000e+00 -1.1569309e+00 ...  3.1423444e+02\n",
      "   7.6063164e+01  1.1070161e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0872259    1.          -1.5697212 ...   13.029403    76.71106\n",
      "   -80.09155  ]\n",
      " [  -1.0875607    1.          -1.56954   ... -195.82996     94.36225\n",
      "   -87.81057  ]\n",
      " [  -1.0879707    1.          -1.5692452 ...  -62.185196   -27.907455\n",
      "   -66.81428  ]\n",
      " ...\n",
      " [  -1.0878601    1.          -1.5693111 ...   23.501768    -5.582752\n",
      "    17.166437 ]\n",
      " [  -1.0870743    1.          -1.5698986 ... -207.24231     86.635155\n",
      "   119.48834  ]\n",
      " [  -1.0871058    1.          -1.56983   ...    6.62517    -32.819202\n",
      "  -115.26465  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.4876900e-01  1.0000000e+00 -1.8290234e+00 ...  2.7415176e+01\n",
      "   7.5357330e+01  5.8541574e+00]\n",
      " [-5.4916000e-01  1.0000000e+00 -1.8289194e+00 ...  4.9129422e+02\n",
      "   1.3549850e+03 -1.1205330e+02]\n",
      " [-5.4963875e-01  1.0000000e+00 -1.8287805e+00 ... -8.6178917e+01\n",
      "   1.1669287e+02  5.4096176e+01]\n",
      " ...\n",
      " [-5.4951096e-01  1.0000000e+00 -1.8288021e+00 ... -1.7826160e+01\n",
      "   8.1674652e+01  1.5851588e+02]\n",
      " [-5.4861259e-01  1.0000000e+00 -1.8291340e+00 ...  4.1951254e+02\n",
      "   1.6511598e+02 -5.4268829e+01]\n",
      " [-5.4862595e-01  1.0000000e+00 -1.8290863e+00 ...  5.2137512e+02\n",
      "   2.3218201e+01  4.3269989e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.3416977e-02  1.0000000e+00 -1.9091454e+00 ...  1.7402302e+01\n",
      "   2.1860889e+01 -1.9197182e+01]\n",
      " [ 4.3014526e-02  1.0000000e+00 -1.9091568e+00 ... -3.5836148e+02\n",
      "  -5.0937933e+02  4.6520819e+02]\n",
      " [ 4.2501450e-02  1.0000000e+00 -1.9091809e+00 ...  9.1555954e+01\n",
      "  -1.2713039e+02  1.1122176e+02]\n",
      " ...\n",
      " [ 4.2631149e-02  1.0000000e+00 -1.9091663e+00 ...  1.3236491e+02\n",
      "   1.3509560e-01  2.0642028e+00]\n",
      " [ 4.3577194e-02  1.0000000e+00 -1.9092045e+00 ...  3.7779995e+01\n",
      "   2.5808679e+02  2.5303969e+02]\n",
      " [ 4.3575287e-02  1.0000000e+00 -1.9091778e+00 ... -3.1542482e+00\n",
      "   1.1806989e+02  3.2103177e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  0.6311302    1.          -1.8024044  ... -53.259644    11.835068\n",
      "  -52.51576   ]\n",
      " [  0.63075066   1.          -1.8025427  ... 183.24237     60.84365\n",
      "  -60.329487  ]\n",
      " [  0.63030434   1.          -1.8027053  ... -46.94803     60.630108\n",
      "  -21.286366  ]\n",
      " ...\n",
      " [  0.6304188    1.          -1.8026562  ... 116.91452     21.509434\n",
      "   24.84821   ]\n",
      " [  0.6313076    1.          -1.8023968  ...  27.815048    56.109676\n",
      "  -73.99886   ]\n",
      " [  0.6312828    1.          -1.802391   ...  85.92025    -35.849052\n",
      "  -56.74634   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1573067    1.          -1.5191631 ...   76.760735   -16.779663\n",
      "   -19.168991 ]\n",
      " [   1.156971     1.          -1.5193872 ... -108.20555    130.92468\n",
      "   -39.073364 ]\n",
      " [   1.1566277    1.          -1.5196948 ...  133.9329      -7.9802413\n",
      "   -84.57041  ]\n",
      " ...\n",
      " [   1.1567173    1.          -1.5196018 ...  187.18591   -199.99886\n",
      "   501.3812   ]\n",
      " [   1.1574554    1.          -1.519083  ...   -5.9398317  172.31998\n",
      "    50.941975 ]\n",
      " [   1.1574354    1.          -1.5190983 ...   10.948364   195.29019\n",
      "  -487.97882  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5702896    1.          -1.0869293 ... -262.98416     94.1034\n",
      "   186.19055  ]\n",
      " [   1.5700397    1.          -1.0872536 ...  -71.66008    164.38531\n",
      "   317.08145  ]\n",
      " [   1.5698204    1.          -1.087667  ...  -74.99493     37.57033\n",
      "    16.702332 ]\n",
      " ...\n",
      " [   1.5698681    1.          -1.0875416 ...   19.662552    16.54066\n",
      "     4.1086445]\n",
      " [   1.5703926    1.          -1.0868149 ... -444.0541      76.83413\n",
      "   202.2345   ]\n",
      " [   1.5703831    1.          -1.086834  ...   88.42739    -57.13846\n",
      "    51.276382 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8292284e+00  1.0000000e+00 -5.4885483e-01 ...  2.2385542e+01\n",
      "  -3.7167311e+00 -3.0753040e+00]\n",
      " [ 1.8290863e+00  1.0000000e+00 -5.4920673e-01 ...  3.5383008e+02\n",
      "  -2.8316257e+01  5.7999091e+02]\n",
      " [ 1.8290081e+00  1.0000000e+00 -5.4968894e-01 ...  5.3283839e+00\n",
      "  -9.8183449e+01  1.3468039e+02]\n",
      " ...\n",
      " [ 1.8290157e+00  1.0000000e+00 -5.4954529e-01 ...  9.4988867e+02\n",
      "   3.5305991e+03  4.7061221e+03]\n",
      " [ 1.8293152e+00  1.0000000e+00 -5.4871559e-01 ... -3.7311530e+02\n",
      "   5.0093208e+01  1.4435445e+02]\n",
      " [ 1.8292780e+00  1.0000000e+00 -5.4873848e-01 ...  5.9754429e+00\n",
      "   5.2105095e+01 -3.6180855e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9091063e+00  1.0000000e+00  4.3619156e-02 ... -1.9763000e+01\n",
      "   4.4334931e+00 -5.5143051e+00]\n",
      " [ 1.9090929e+00  1.0000000e+00  4.3272972e-02 ...  1.5555548e+01\n",
      "  -2.4713882e+02  1.1610084e+02]\n",
      " [ 1.9091549e+00  1.0000000e+00  4.2758100e-02 ...  7.9032127e+01\n",
      "   3.8211951e+02  8.2485168e+01]\n",
      " ...\n",
      " [ 1.9091206e+00  1.0000000e+00  4.2919159e-02 ... -2.3231853e+03\n",
      "  -6.4892395e+02  7.4994395e+03]\n",
      " [ 1.9091282e+00  1.0000000e+00  4.3769836e-02 ... -1.2637316e+02\n",
      "   6.6217506e+01 -5.5611507e+01]\n",
      " [ 1.9091005e+00  1.0000000e+00  4.3746948e-02 ... -8.8492554e+01\n",
      "  -9.0070770e+01 -3.2688599e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8022633e+00  1.0000000e+00  6.3125229e-01 ...  2.9965096e+01\n",
      "  -1.0843800e+02 -2.2191795e+02]\n",
      " [ 1.8023558e+00  1.0000000e+00  6.3089180e-01 ...  3.8483749e+02\n",
      "   3.1060120e+02  1.9827132e+02]\n",
      " [ 1.8025494e+00  1.0000000e+00  6.3042098e-01 ... -1.3276222e+01\n",
      "   8.0734138e+00 -2.4654715e+01]\n",
      " ...\n",
      " [ 1.8024769e+00  1.0000000e+00  6.3056564e-01 ...  1.3981089e+03\n",
      "   3.9357092e+02 -2.3070341e+02]\n",
      " [ 1.8022289e+00  1.0000000e+00  6.3139153e-01 ...  2.1570292e+00\n",
      "   2.2779694e+01  2.9812825e+00]\n",
      " [ 1.8022070e+00  1.0000000e+00  6.3136864e-01 ...  3.6432320e+01\n",
      "   1.5240524e+02 -4.3496479e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5187778e+00  1.0000000e+00  1.1575756e+00 ... -4.0731696e+02\n",
      "   7.0257886e+02 -5.8889392e+02]\n",
      " [ 1.5189772e+00  1.0000000e+00  1.1572828e+00 ...  1.6436377e+02\n",
      "  -1.3242139e+02  6.4578918e+01]\n",
      " [ 1.5192966e+00  1.0000000e+00  1.1568767e+00 ... -2.1001675e+01\n",
      "  -4.6968174e+00 -1.8083902e+01]\n",
      " ...\n",
      " [ 1.5191860e+00  1.0000000e+00  1.1570044e+00 ... -9.1187218e+01\n",
      "   1.3284746e+03  1.2764603e+03]\n",
      " [ 1.5186768e+00  1.0000000e+00  1.1576900e+00 ...  4.7469693e+02\n",
      "   1.1511141e+02 -1.4941499e+02]\n",
      " [ 1.5186806e+00  1.0000000e+00  1.1576729e+00 ...  3.9623844e+01\n",
      "  -2.7745796e+01 -6.1400356e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.08671856e+00  1.00000000e+00  1.57022858e+00 ... -3.78793831e+01\n",
      "  -2.00260944e+01  1.29240055e+01]\n",
      " [ 1.08700848e+00  1.00000000e+00  1.57002640e+00 ... -1.53376917e+03\n",
      "   1.48566742e+02  5.96572876e+02]\n",
      " [ 1.08740616e+00  1.00000000e+00  1.56973302e+00 ...  2.27433136e+02\n",
      "   1.06863266e+02  8.71277390e+01]\n",
      " ...\n",
      " [ 1.08725739e+00  1.00000000e+00  1.56982899e+00 ... -2.96125488e+02\n",
      "  -1.08342964e+02 -3.99040108e+01]\n",
      " [ 1.08655548e+00  1.00000000e+00  1.57030106e+00 ... -5.17829514e+00\n",
      "  -2.88737231e+03  1.06394189e+03]\n",
      " [ 1.08659649e+00  1.00000000e+00  1.57029343e+00 ...  1.66571411e+02\n",
      "  -3.87320862e+02  1.08524121e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.48047066e-01  1.00000000e+00  1.82926941e+00 ...  7.16613998e+01\n",
      "  -8.06435089e+01 -1.22794296e+02]\n",
      " [ 5.48386574e-01  1.00000000e+00  1.82915878e+00 ... -5.34964355e+02\n",
      "  -3.06641449e+02  8.54077835e+01]\n",
      " [ 5.48862457e-01  1.00000000e+00  1.82900846e+00 ... -2.06241608e+01\n",
      "   2.31928673e+01  1.10677214e+01]\n",
      " ...\n",
      " [ 5.48690796e-01  1.00000000e+00  1.82906818e+00 ... -1.08812637e+01\n",
      "   2.36364105e+02  6.02378357e+02]\n",
      " [ 5.47887802e-01  1.00000000e+00  1.82929993e+00 ...  1.77024643e+02\n",
      "   3.97847473e+02  3.53895294e+02]\n",
      " [ 5.47906876e-01  1.00000000e+00  1.82929993e+00 ...  8.38005371e+01\n",
      "   1.48760567e+01  1.01323296e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.41541672e-02  1.00000000e+00  1.90911674e+00 ... -3.56669922e+01\n",
      "  -2.48313141e+01  9.95154190e+01]\n",
      " [-4.38032150e-02  1.00000000e+00  1.90907764e+00 ... -2.72419128e+01\n",
      "   2.40734695e+02  1.37091034e+02]\n",
      " [-4.32662964e-02  1.00000000e+00  1.90908742e+00 ... -1.56140320e+02\n",
      "   1.16229701e+00  8.90871716e+00]\n",
      " ...\n",
      " [-4.34589386e-02  1.00000000e+00  1.90907574e+00 ...  2.76368622e+02\n",
      "  -1.28156815e+02 -2.37883240e+02]\n",
      " [-4.43096161e-02  1.00000000e+00  1.90905762e+00 ... -6.47134552e+01\n",
      "  -1.01907814e+02 -6.02166595e+01]\n",
      " [-4.42991257e-02  1.00000000e+00  1.90907097e+00 ... -1.74500031e+02\n",
      "   3.73310204e+01  1.29157074e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.3199139e-01  1.0000000e+00  1.8019619e+00 ...  2.4035475e+01\n",
      "   4.2473679e+01  4.4953184e+00]\n",
      " [-6.3166332e-01  1.0000000e+00  1.8020420e+00 ... -2.8400986e+01\n",
      "   2.7845364e+01  5.0288506e+01]\n",
      " [-6.3114166e-01  1.0000000e+00  1.8022145e+00 ...  6.1045547e+01\n",
      "   3.3112007e+01 -1.1830905e+02]\n",
      " ...\n",
      " [-6.3132095e-01  1.0000000e+00  1.8021574e+00 ... -1.0760837e+02\n",
      "  -1.8822958e+02 -1.4433382e+02]\n",
      " [-6.3212395e-01  1.0000000e+00  1.8018494e+00 ... -1.2663104e+03\n",
      "  -5.1131610e+02  6.1093573e+02]\n",
      " [-6.3212681e-01  1.0000000e+00  1.8018684e+00 ...  2.2674319e+02\n",
      "   4.2504242e+01 -3.8695183e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1578341e+00  1.0000000e+00  1.5182858e+00 ...  2.1354021e+02\n",
      "  -3.3472189e+02 -1.6433755e+02]\n",
      " [-1.1575594e+00  1.0000000e+00  1.5184860e+00 ...  3.4260545e+00\n",
      "  -4.8649731e+00  5.1740279e+00]\n",
      " [-1.1571236e+00  1.0000000e+00  1.5188036e+00 ... -1.7377496e+02\n",
      "  -2.1737036e+02 -9.8653419e+01]\n",
      " ...\n",
      " [-1.1572876e+00  1.0000000e+00  1.5187073e+00 ...  3.3519356e+01\n",
      "   2.2693529e+01 -9.2385651e+01]\n",
      " [-1.1579685e+00  1.0000000e+00  1.5181408e+00 ...  1.8279922e+02\n",
      "   4.5202216e+02 -1.7915965e+02]\n",
      " [-1.1579418e+00  1.0000000e+00  1.5181637e+00 ... -1.5639688e+02\n",
      "  -1.2428745e+03 -3.1924780e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.570344     1.           1.085886  ...   42.17327     51.36768\n",
      "   107.932625 ]\n",
      " [  -1.5701618    1.           1.0861969 ...  -52.061207   122.89517\n",
      "    10.542456 ]\n",
      " [  -1.5698204    1.           1.0866159 ...  -23.847626    19.256845\n",
      "   -48.573196 ]\n",
      " ...\n",
      " [  -1.5699425    1.           1.0864973 ...   81.08024   -138.94771\n",
      "     3.285628 ]\n",
      " [  -1.5704193    1.           1.0857124 ... -274.35577   -114.745895\n",
      "   752.0421   ]\n",
      " [  -1.5704317    1.           1.085741  ...  -22.913868     4.8684216\n",
      "    80.3053   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.82910442e+00  1.00000000e+00  5.47536850e-01 ...  1.84862350e+02\n",
      "  -1.31500198e+02 -1.03123276e+02]\n",
      " [-1.82900620e+00  1.00000000e+00  5.47904015e-01 ... -6.89627552e+00\n",
      "  -1.65206089e+01 -6.78870239e+01]\n",
      " [-1.82879829e+00  1.00000000e+00  5.48390627e-01 ...  1.54561319e+01\n",
      "  -1.00277826e+03  2.35864166e+02]\n",
      " ...\n",
      " [-1.82889366e+00  1.00000000e+00  5.48253059e-01 ...  4.81072197e+01\n",
      "   8.18830490e+00  1.62246826e+02]\n",
      " [-1.82911301e+00  1.00000000e+00  5.47348022e-01 ... -6.16409569e+01\n",
      "   2.45636139e+02  4.55361671e+01]\n",
      " [-1.82915688e+00  1.00000000e+00  5.47382355e-01 ...  2.28860046e+02\n",
      "   2.10452362e+02  2.55746002e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9088564e+00  1.0000000e+00 -4.4839859e-02 ... -6.2959957e+00\n",
      "  -3.8696918e+01 -2.9595613e+01]\n",
      " [-1.9088678e+00  1.0000000e+00 -4.4457436e-02 ...  7.0175407e+01\n",
      "  -3.8557916e+02  8.8933441e+01]\n",
      " [-1.9088573e+00  1.0000000e+00 -4.3949440e-02 ...  3.5456119e+02\n",
      "  -2.7471387e+02 -2.7376932e+02]\n",
      " ...\n",
      " [-1.9089050e+00  1.0000000e+00 -4.4099808e-02 ...  1.1617949e+03\n",
      "   1.5481476e+02 -1.4438293e+01]\n",
      " [-1.9088440e+00  1.0000000e+00 -4.5032501e-02 ...  1.1954536e+01\n",
      "  -3.0708960e+01  3.5047703e+01]\n",
      " [-1.9088669e+00  1.0000000e+00 -4.4998169e-02 ... -3.2078339e+01\n",
      "   5.2318562e+01 -5.4943180e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.80153751e+00  1.00000000e+00 -6.32871628e-01 ...  3.81969818e+02\n",
      "  -1.14383896e+02 -2.38652512e+02]\n",
      " [-1.80165577e+00  1.00000000e+00 -6.32535934e-01 ... -1.89098854e+01\n",
      "   4.44171600e+01 -2.03020153e+01]\n",
      " [-1.80182648e+00  1.00000000e+00 -6.32052422e-01 ...  1.33148331e+02\n",
      "  -7.27469406e+01 -9.13931885e+02]\n",
      " ...\n",
      " [-1.80182266e+00  1.00000000e+00 -6.32196426e-01 ... -1.31566162e+02\n",
      "  -8.81208344e+01 -1.85070679e+02]\n",
      " [-1.80149078e+00  1.00000000e+00 -6.33052826e-01 ... -1.59095483e+03\n",
      "  -7.09219788e+02  7.44448853e+02]\n",
      " [-1.80150795e+00  1.00000000e+00 -6.33020401e-01 ... -1.46350204e+02\n",
      "  -1.97119293e+02  1.64312103e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5176668e+00  1.0000000e+00 -1.1585102e+00 ...  2.5135679e+01\n",
      "   6.0547123e+01  5.8992523e+02]\n",
      " [-1.5178804e+00  1.0000000e+00 -1.1582413e+00 ... -6.4708420e+01\n",
      "  -1.7968640e+02  1.2459814e+02]\n",
      " [-1.5181999e+00  1.0000000e+00 -1.1578163e+00 ...  4.7187680e+02\n",
      "  -7.2128961e+02  2.1705762e+03]\n",
      " ...\n",
      " [-1.5181561e+00  1.0000000e+00 -1.1579561e+00 ...  1.5112590e+01\n",
      "  -2.3231365e+01 -3.0482840e+01]\n",
      " [-1.5175686e+00  1.0000000e+00 -1.1586609e+00 ... -4.3331696e+02\n",
      "   1.4629839e+02  5.9759662e+02]\n",
      " [-1.5175943e+00  1.0000000e+00 -1.1586304e+00 ...  2.0711060e+02\n",
      "  -1.5577657e+02 -5.9545998e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.08520985e+00  1.00000000e+00 -1.57103157e+00 ... -1.49068817e+02\n",
      "  -3.94996289e+03 -3.67570526e+02]\n",
      " [-1.08550930e+00  1.00000000e+00 -1.57085037e+00 ... -4.55274353e+01\n",
      "   1.82694454e+01 -1.52171183e+01]\n",
      " [-1.08599854e+00  1.00000000e+00 -1.57053566e+00 ...  3.46898041e+02\n",
      "   2.61909313e+01 -4.37476929e+02]\n",
      " ...\n",
      " [-1.08591080e+00  1.00000000e+00 -1.57064819e+00 ...  6.48283768e+01\n",
      "   1.02237465e+02  2.55004272e+01]\n",
      " [-1.08513641e+00  1.00000000e+00 -1.57114029e+00 ...  1.21503532e+02\n",
      "   1.56645172e+02  4.66830444e+01]\n",
      " [-1.08510494e+00  1.00000000e+00 -1.57111359e+00 ...  2.96562561e+02\n",
      "   6.43338989e+02  2.69438362e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.4645443e-01  1.0000000e+00 -1.8296566e+00 ...  4.6015533e+02\n",
      "   1.4364504e+03 -2.3557376e+02]\n",
      " [-5.4679775e-01  1.0000000e+00 -1.8295612e+00 ...  7.9546133e+03\n",
      "   3.5088386e+03 -2.5382395e+03]\n",
      " [-5.4732895e-01  1.0000000e+00 -1.8293965e+00 ...  6.3605457e+01\n",
      "   3.6031281e+01 -3.4221893e+01]\n",
      " ...\n",
      " [-5.4722023e-01  1.0000000e+00 -1.8294487e+00 ...  1.2858630e+02\n",
      "   2.0703714e+02 -8.1709282e+01]\n",
      " [-5.4633904e-01  1.0000000e+00 -1.8297024e+00 ...  5.6827903e-01\n",
      "   1.9977682e+00  1.3551813e+01]\n",
      " [-5.4633141e-01  1.0000000e+00 -1.8296871e+00 ... -1.4702074e+02\n",
      "  -3.9354059e+02  3.1853485e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.5864105e-02  1.0000000e+00 -1.9088535e+00 ... -1.7647841e+02\n",
      "  -1.4230228e+02  7.4824127e+01]\n",
      " [ 4.5501709e-02  1.0000000e+00 -1.9088449e+00 ...  2.8482883e+02\n",
      "   4.1362302e+02 -5.8056427e+01]\n",
      " [ 4.4954300e-02  1.0000000e+00 -1.9088593e+00 ...  1.4706288e+02\n",
      "   1.3670927e+02 -1.6274306e+02]\n",
      " ...\n",
      " [ 4.5066833e-02  1.0000000e+00 -1.9088564e+00 ...  2.3501854e+01\n",
      "  -3.2188181e+02 -4.5674355e+01]\n",
      " [ 4.5989990e-02  1.0000000e+00 -1.9088383e+00 ...  1.4039551e+02\n",
      "   9.1781281e+02 -1.5028134e+02]\n",
      " [ 4.5989037e-02  1.0000000e+00 -1.9088306e+00 ...  5.5165314e+01\n",
      "   2.6215303e+01 -3.9374716e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.3337040e-01  1.0000000e+00 -1.8011513e+00 ...  3.8053173e+01\n",
      "  -4.8552905e+02  5.7169983e+02]\n",
      " [ 6.3302803e-01  1.0000000e+00 -1.8012600e+00 ... -5.1427722e+02\n",
      "  -8.4436958e+01 -2.2483698e+02]\n",
      " [ 6.3246727e-01  1.0000000e+00 -1.8014432e+00 ... -1.3637996e+01\n",
      "   4.9937458e+01  1.5074375e+01]\n",
      " ...\n",
      " [ 6.3256836e-01  1.0000000e+00 -1.8014030e+00 ... -5.4247997e+01\n",
      "   9.1467720e+01 -1.1416075e+00]\n",
      " [ 6.3344193e-01  1.0000000e+00 -1.8010902e+00 ...  8.7080780e+02\n",
      "   1.8198638e+03 -3.5110150e+02]\n",
      " [ 6.3348770e-01  1.0000000e+00 -1.8010845e+00 ... -8.7969209e+03\n",
      "   1.5975125e+03  1.0084383e+04]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1590738    1.          -1.5173607 ... -262.01047   -104.075745\n",
      "    73.54425  ]\n",
      " [   1.1587763    1.          -1.5175629 ...  179.35495    546.51965\n",
      "   163.29956  ]\n",
      " [   1.1582623    1.          -1.5178845 ...   35.53324   -132.23668\n",
      "  -132.73729  ]\n",
      " ...\n",
      " [   1.1583424    1.          -1.5178118 ...  -63.11321     50.360928\n",
      "   129.5518   ]\n",
      " [   1.15909      1.          -1.5172577 ...  201.1138     383.21155\n",
      "    31.08646  ]\n",
      " [   1.159174     1.          -1.5172539 ... -585.8238     341.6529\n",
      "   453.92822  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5716343e+00  1.0000000e+00 -1.0845757e+00 ... -1.5847569e+02\n",
      "  -2.7745062e+02 -2.3102045e+02]\n",
      " [ 1.5714197e+00  1.0000000e+00 -1.0848579e+00 ...  1.0075264e+02\n",
      "   7.9975189e+01 -2.5158401e+01]\n",
      " [ 1.5710297e+00  1.0000000e+00 -1.0852807e+00 ...  1.0546943e+02\n",
      "   1.6466700e+01  4.6463219e+01]\n",
      " ...\n",
      " [ 1.5710888e+00  1.0000000e+00 -1.0851898e+00 ... -6.4584839e+01\n",
      "  -2.1671766e+01 -2.9373459e+01]\n",
      " [ 1.5716000e+00  1.0000000e+00 -1.0844402e+00 ...  1.5535677e+02\n",
      "  -8.7746265e+02 -1.3760184e+02]\n",
      " [ 1.5717058e+00  1.0000000e+00 -1.0844364e+00 ...  1.4114244e+03\n",
      "   3.7244197e+03  7.4082776e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.82989693e+00  1.00000000e+00 -5.46075821e-01 ... -5.27501793e+01\n",
      "  -1.97584106e+02  4.47421539e+02]\n",
      " [ 1.82979202e+00  1.00000000e+00 -5.46418190e-01 ... -1.82449860e+02\n",
      "  -7.69152222e+01 -9.66157990e+01]\n",
      " [ 1.82954979e+00  1.00000000e+00 -5.46907067e-01 ...  1.89585510e+02\n",
      "  -1.97982635e+02  1.07934021e+02]\n",
      " ...\n",
      " [ 1.82958794e+00  1.00000000e+00 -5.46808243e-01 ...  7.16006279e+00\n",
      "  -1.10674217e+02  2.52089279e+02]\n",
      " [ 1.82981873e+00  1.00000000e+00 -5.45923233e-01 ...  8.87324448e+01\n",
      "  -3.31972137e+02 -4.99645874e+02]\n",
      " [ 1.82993793e+00  1.00000000e+00 -5.45917511e-01 ...  2.53887177e+02\n",
      "  -9.16778198e+02 -1.20787766e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90917683e+00  1.00000000e+00  4.62074280e-02 ... -3.72971313e+02\n",
      "   5.98978516e+02 -2.71179382e+02]\n",
      " [ 1.90917015e+00  1.00000000e+00  4.58421707e-02 ... -5.24884720e+01\n",
      "  -8.47415237e+01  1.51774597e+02]\n",
      " [ 1.90908623e+00  1.00000000e+00  4.53366786e-02 ... -1.30448090e+02\n",
      "  -1.50626205e+02 -3.66609070e+02]\n",
      " ...\n",
      " [ 1.90909004e+00  1.00000000e+00  4.54320908e-02 ... -2.15798676e+02\n",
      "   8.15928574e+01 -1.40773287e+01]\n",
      " [ 1.90903664e+00  1.00000000e+00  4.63638306e-02 ... -3.02001801e+01\n",
      "   1.01158638e+02  1.00654434e+02]\n",
      " [ 1.90916348e+00  1.00000000e+00  4.63733673e-02 ...  9.20515442e+01\n",
      "  -8.17838058e+01 -1.01819801e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8014259e+00  1.0000000e+00  6.3407326e-01 ... -1.6082272e+02\n",
      "  -5.1164938e+02  3.9183524e+02]\n",
      " [ 1.8015375e+00  1.0000000e+00  6.3374043e-01 ...  2.8082012e+01\n",
      "   5.9141688e+00 -5.9979202e+01]\n",
      " [ 1.8016357e+00  1.0000000e+00  6.3326883e-01 ...  1.1967308e+02\n",
      "  -9.1556215e+00 -6.0994152e+01]\n",
      " ...\n",
      " [ 1.8016071e+00  1.0000000e+00  6.3335323e-01 ... -2.6034678e+03\n",
      "  -6.1591571e+02 -7.3648779e+02]\n",
      " [ 1.8012753e+00  1.0000000e+00  6.3422775e-01 ... -2.0139743e+02\n",
      "   1.5581404e+02  3.5081680e+01]\n",
      " [ 1.8013725e+00  1.0000000e+00  6.3423538e-01 ... -3.8277878e+02\n",
      "  -1.2394237e+01 -1.4062141e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5174646    1.           1.1595898 ...   36.33099     65.86285\n",
      "   152.67743  ]\n",
      " [   1.5176811    1.           1.1593246 ... -800.6782     813.6623\n",
      "    18.70333  ]\n",
      " [   1.5179062    1.           1.1589268 ...   -2.6162634    5.3042564\n",
      "    -1.1602061]\n",
      " ...\n",
      " [   1.5178356    1.           1.1589966 ...  115.036156   -44.753532\n",
      "   111.71367  ]\n",
      " [   1.5172462    1.           1.1597233 ...   17.803614    43.513393\n",
      "  -129.34381  ]\n",
      " [   1.5173588    1.           1.1597309 ... -191.886       89.207\n",
      "  -378.762    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0845261e+00  1.0000000e+00  1.5720062e+00 ...  8.4869545e+01\n",
      "  -2.7014566e+01  2.4147390e+01]\n",
      " [ 1.0848036e+00  1.0000000e+00  1.5718002e+00 ... -1.0998889e+03\n",
      "   8.0914124e+02 -1.3173120e+03]\n",
      " [ 1.0851612e+00  1.0000000e+00  1.5715268e+00 ... -1.4148637e+01\n",
      "   3.0359733e+00 -7.2311382e+00]\n",
      " ...\n",
      " [ 1.0850773e+00  1.0000000e+00  1.5715723e+00 ...  1.5075215e+02\n",
      "  -4.5382837e+02  4.0472931e+02]\n",
      " [ 1.0842896e+00  1.0000000e+00  1.5720997e+00 ...  1.8625026e+02\n",
      "   5.3592371e+02 -3.9337592e+02]\n",
      " [ 1.0843973e+00  1.0000000e+00  1.5721092e+00 ... -4.3530716e+01\n",
      "  -4.5345097e+01  9.1188980e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.45978546e-01  1.00000000e+00  1.83007622e+00 ...  6.03114510e+01\n",
      "   1.12603065e+02  6.69510345e+01]\n",
      " [ 5.46307564e-01  1.00000000e+00  1.82997990e+00 ...  1.26806669e+01\n",
      "   7.02506714e+02  6.93700317e+02]\n",
      " [ 5.46699524e-01  1.00000000e+00  1.82984686e+00 ... -3.07768488e+00\n",
      "   3.02041698e+00 -5.45310688e+00]\n",
      " ...\n",
      " [ 5.46613693e-01  1.00000000e+00  1.82987213e+00 ...  1.03771118e+02\n",
      "  -4.39292946e+01 -4.50331573e+02]\n",
      " [ 5.45705795e-01  1.00000000e+00  1.83010483e+00 ... -5.68136139e+01\n",
      "  -3.43160515e+01 -1.13598000e+02]\n",
      " [ 5.45825958e-01  1.00000000e+00  1.83012390e+00 ...  1.10887466e+02\n",
      "   4.60932350e+00  1.42614288e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.64782715e-02  1.00000000e+00  1.90901375e+00 ...  6.19688377e+01\n",
      "  -3.16695923e+02  1.02056537e+03]\n",
      " [-4.61311340e-02  1.00000000e+00  1.90902424e+00 ...  5.58280897e+00\n",
      "  -1.96641159e+01  8.53774414e+01]\n",
      " [-4.57324982e-02  1.00000000e+00  1.90903735e+00 ...  5.16422844e+00\n",
      "  -2.67000341e+00  3.88590038e-01]\n",
      " ...\n",
      " [-4.58221436e-02  1.00000000e+00  1.90902138e+00 ...  5.11070480e+01\n",
      "   1.16471367e+01 -2.57019135e+02]\n",
      " [-4.67720032e-02  1.00000000e+00  1.90899086e+00 ...  7.71014252e+01\n",
      "   2.18794098e+02  4.99855728e+01]\n",
      " [-4.66346741e-02  1.00000000e+00  1.90902710e+00 ...  1.00581383e+02\n",
      "   5.60397625e+00  8.79699097e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.3433456e-01  1.0000000e+00  1.8008595e+00 ...  1.3565691e+01\n",
      "  -1.2391179e+01 -3.7501751e+01]\n",
      " [-6.3401031e-01  1.0000000e+00  1.8009949e+00 ... -3.0342340e+02\n",
      "   1.3378947e+02  1.4798253e+02]\n",
      " [-6.3365936e-01  1.0000000e+00  1.8011523e+00 ...  1.4294247e+01\n",
      "  -4.0724693e+01  9.4776253e+01]\n",
      " ...\n",
      " [-6.3374329e-01  1.0000000e+00  1.8011055e+00 ... -1.7511624e+02\n",
      "  -2.8484216e+02  4.0588966e+01]\n",
      " [-6.3463020e-01  1.0000000e+00  1.8007755e+00 ...  9.6464716e+02\n",
      "  -6.9991943e+02  8.7278458e+01]\n",
      " [-6.3448238e-01  1.0000000e+00  1.8008289e+00 ... -4.4822414e+01\n",
      "  -2.8147228e+01  8.7504128e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1600285e+00  1.0000000e+00  1.5165176e+00 ... -8.0979853e+00\n",
      "   1.8851862e+01 -1.1145346e+01]\n",
      " [-1.1597528e+00  1.0000000e+00  1.5167589e+00 ... -2.0514502e+01\n",
      "   9.6277392e-01 -8.9387749e+01]\n",
      " [-1.1594601e+00  1.0000000e+00  1.5170445e+00 ... -3.6297327e+02\n",
      "  -3.6139099e+02  1.4994772e+03]\n",
      " ...\n",
      " [-1.1595364e+00  1.0000000e+00  1.5169687e+00 ... -2.8420920e+02\n",
      "   8.0294995e+02 -1.1818848e+02]\n",
      " [-1.1602764e+00  1.0000000e+00  1.5163727e+00 ... -4.7429767e+02\n",
      "   1.5809027e+02 -1.5436698e+03]\n",
      " [-1.1601591e+00  1.0000000e+00  1.5164452e+00 ... -3.8233505e+01\n",
      "  -1.9159763e+02 -8.3198845e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.57186699e+00  1.00000000e+00  1.08366585e+00 ... -7.39194397e+02\n",
      "  -6.37088440e+02 -5.63767395e+02]\n",
      " [-1.57164764e+00  1.00000000e+00  1.08395100e+00 ...  1.25783707e+02\n",
      "   1.31003475e+01  1.49805002e+01]\n",
      " [-1.57147598e+00  1.00000000e+00  1.08434117e+00 ... -3.44782982e+01\n",
      "   6.43112869e+01  7.62692871e+01]\n",
      " ...\n",
      " [-1.57153130e+00  1.00000000e+00  1.08424854e+00 ... -1.24267601e+02\n",
      "   1.13176155e+02  1.61683853e+02]\n",
      " [-1.57208061e+00  1.00000000e+00  1.08347511e+00 ... -1.51167145e+01\n",
      "  -1.03103302e+02  3.67969275e+00]\n",
      " [-1.57196426e+00  1.00000000e+00  1.08356476e+00 ... -6.81884613e+01\n",
      "   6.71146973e+02  1.09013318e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8297949e+00  1.0000000e+00  5.4462051e-01 ...  3.8642938e+02\n",
      "  -5.6974017e+02 -3.6445496e+02]\n",
      " [-1.8296556e+00  1.0000000e+00  5.4494190e-01 ... -1.8420750e+01\n",
      "  -2.1834378e+02  2.5868677e+02]\n",
      " [-1.8295918e+00  1.0000000e+00  5.4540122e-01 ... -8.3754759e+00\n",
      "   5.7000313e+01  1.5776440e+01]\n",
      " ...\n",
      " [-1.8296070e+00  1.0000000e+00  5.4528999e-01 ... -5.8095008e-01\n",
      "   2.6856146e+01  6.2806139e+00]\n",
      " [-1.8298950e+00  1.0000000e+00  5.4440117e-01 ... -3.7373590e+02\n",
      "  -1.8189099e+02  5.5804163e+02]\n",
      " [-1.8298512e+00  1.0000000e+00  5.4449654e-01 ... -1.5011981e+02\n",
      "  -1.8905842e+01 -1.5463668e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9086466e+00  1.0000000e+00 -4.7779083e-02 ... -1.3691506e+03\n",
      "  -7.6289117e+02 -9.8503351e+02]\n",
      " [-1.9086065e+00  1.0000000e+00 -4.7424316e-02 ...  9.3789848e+01\n",
      "  -2.7209496e+01 -1.6920376e+01]\n",
      " [-1.9086971e+00  1.0000000e+00 -4.6959884e-02 ...  1.0970192e+02\n",
      "   4.1254345e+01 -4.7708679e+01]\n",
      " ...\n",
      " [-1.9086685e+00  1.0000000e+00 -4.7064781e-02 ... -4.9729263e+01\n",
      "  -2.0067192e+02 -1.2954030e+02]\n",
      " [-1.9086704e+00  1.0000000e+00 -4.8011780e-02 ... -2.0821039e+02\n",
      "   1.8182598e+01  8.4834270e+00]\n",
      " [-1.9086571e+00  1.0000000e+00 -4.7910690e-02 ...  3.7792477e+02\n",
      "  -3.4141147e+02 -9.0014488e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.80047798e+00  1.00000000e+00 -6.35152817e-01 ...  6.87564209e+02\n",
      "   3.55155548e+02 -3.39956741e+01]\n",
      " [-1.80056000e+00  1.00000000e+00 -6.34809494e-01 ... -1.24992096e+02\n",
      "   7.98964739e+00 -5.57857475e+01]\n",
      " [-1.80073738e+00  1.00000000e+00 -6.34369135e-01 ... -2.02891483e+01\n",
      "   2.75127869e+01  1.05094986e+01]\n",
      " ...\n",
      " [-1.80068016e+00  1.00000000e+00 -6.34470940e-01 ...  8.83403206e+00\n",
      "  -5.05235176e+01 -1.77110352e+02]\n",
      " [-1.80044174e+00  1.00000000e+00 -6.35370255e-01 ... -1.48056519e+02\n",
      "   5.97424316e+01 -1.30042744e+01]\n",
      " [-1.80043411e+00  1.00000000e+00 -6.35274887e-01 ... -1.11009773e+02\n",
      "   3.58936005e+01 -9.38035507e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.51602840e+00  1.00000000e+00 -1.16073036e+00 ... -5.57447083e+02\n",
      "  -1.08926538e+03  5.94872864e+02]\n",
      " [-1.51620293e+00  1.00000000e+00 -1.16044617e+00 ... -1.89237473e+02\n",
      "  -6.59392929e+01  2.96560383e+01]\n",
      " [-1.51652908e+00  1.00000000e+00 -1.16007650e+00 ...  1.35128662e+02\n",
      "  -7.89947510e+01 -1.81029434e+02]\n",
      " ...\n",
      " [-1.51642418e+00  1.00000000e+00 -1.16016388e+00 ...  1.21004745e+02\n",
      "   1.45046326e+02 -8.79166107e+01]\n",
      " [-1.51592064e+00  1.00000000e+00 -1.16092491e+00 ...  3.59839592e+01\n",
      "  -8.25937012e+02 -2.85822632e+02]\n",
      " [-1.51595116e+00  1.00000000e+00 -1.16084099e+00 ... -1.07146545e+02\n",
      "  -2.40201935e+02  7.86046677e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.08301544e+00  1.00000000e+00 -1.57263374e+00 ...  2.25860327e+03\n",
      "   1.17011658e+03  2.32628418e+03]\n",
      " [-1.08325672e+00  1.00000000e+00 -1.57243824e+00 ...  2.92372360e+01\n",
      "   1.14858864e+02  2.59405651e+01]\n",
      " [-1.08370209e+00  1.00000000e+00 -1.57218158e+00 ...  1.60590515e+02\n",
      "  -1.13514572e+02 -1.34936462e+02]\n",
      " ...\n",
      " [-1.08357811e+00  1.00000000e+00 -1.57223129e+00 ...  5.48058434e+01\n",
      "   2.33279907e+02 -9.32852783e+01]\n",
      " [-1.08289909e+00  1.00000000e+00 -1.57276154e+00 ... -1.42424911e+02\n",
      "   1.79249298e+02 -2.77870045e+01]\n",
      " [-1.08291817e+00  1.00000000e+00 -1.57270813e+00 ... -1.17918350e+02\n",
      "  -3.98267975e+02  1.20513977e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.43917656e-01  1.00000000e+00 -1.83050537e+00 ... -2.62768036e+02\n",
      "   4.66518860e+02 -7.25696777e+02]\n",
      " [-5.44204712e-01  1.00000000e+00 -1.83039570e+00 ... -3.52006287e+02\n",
      "   2.74893890e+02 -7.90542664e+02]\n",
      " [-5.44719696e-01  1.00000000e+00 -1.83027518e+00 ...  7.95527878e+01\n",
      "  -4.63109360e+01 -8.99373474e+01]\n",
      " ...\n",
      " [-5.44586182e-01  1.00000000e+00 -1.83028889e+00 ... -1.31834545e+01\n",
      "   1.27809620e+01 -1.26506615e+01]\n",
      " [-5.43783188e-01  1.00000000e+00 -1.83057022e+00 ... -2.02505280e+02\n",
      "  -2.31903114e+01  6.90580566e+02]\n",
      " [-5.43804169e-01  1.00000000e+00 -1.83054161e+00 ... -1.25559063e+01\n",
      "  -2.03059196e-01 -6.35062866e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.3]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.8310280e-02  1.0000000e+00 -1.9090862e+00 ... -1.8790592e+01\n",
      "  -1.8048834e+02  4.5206262e+02]\n",
      " [ 4.8009872e-02  1.0000000e+00 -1.9090738e+00 ...  1.0602937e+02\n",
      "   2.2141457e+02 -2.4593763e+01]\n",
      " [ 4.7458649e-02  1.0000000e+00 -1.9090889e+00 ...  2.0086647e+01\n",
      "  -1.4703941e-01 -9.1856270e+00]\n",
      " ...\n",
      " [ 4.7603607e-02  1.0000000e+00 -1.9090662e+00 ... -1.9441858e+02\n",
      "  -1.2871947e+02 -1.5108537e+02]\n",
      " [ 4.8452377e-02  1.0000000e+00 -1.9090862e+00 ...  4.2474556e+01\n",
      "   2.6153309e+01  7.2619415e+01]\n",
      " [ 4.8425674e-02  1.0000000e+00 -1.9090824e+00 ... -7.9064102e+00\n",
      "  -1.4954972e+01  6.8389187e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.36064529e-01  1.00000000e+00 -1.80074310e+00 ...  1.12124219e+03\n",
      "  -9.96211365e+02  6.46011780e+02]\n",
      " [ 6.35785103e-01  1.00000000e+00 -1.80081081e+00 ... -5.49765259e+02\n",
      "   3.04389404e+02  8.81809631e+02]\n",
      " [ 6.35255814e-01  1.00000000e+00 -1.80098069e+00 ... -5.24667931e+01\n",
      "  -6.59622116e+01  3.85996933e+01]\n",
      " ...\n",
      " [ 6.35391235e-01  1.00000000e+00 -1.80090904e+00 ...  2.46142319e+02\n",
      "   1.21311755e+03  1.27146576e+02]\n",
      " [ 6.36194229e-01  1.00000000e+00 -1.80068398e+00 ... -2.74035797e+01\n",
      "   2.09556618e+01  1.34366653e+02]\n",
      " [ 6.36171341e-01  1.00000000e+00 -1.80069733e+00 ... -2.10575123e+01\n",
      "  -4.73717384e+01 -3.44025612e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1614218e+00  1.0000000e+00 -1.5160961e+00 ...  4.6692402e+01\n",
      "   8.7329445e+01 -3.0039846e+01]\n",
      " [ 1.1611834e+00  1.0000000e+00 -1.5162182e+00 ... -2.7114754e+01\n",
      "  -3.9666367e+02  1.9058951e+03]\n",
      " [ 1.1607494e+00  1.0000000e+00 -1.5165399e+00 ... -3.8887402e+01\n",
      "  -4.8866066e+01 -6.7383385e+01]\n",
      " ...\n",
      " [ 1.1608677e+00  1.0000000e+00 -1.5164318e+00 ...  1.1684178e+03\n",
      "   1.9180082e+03  2.0981148e+02]\n",
      " [ 1.1615200e+00  1.0000000e+00 -1.5159988e+00 ... -5.5061050e+01\n",
      "  -2.0668300e+02 -4.0375607e+01]\n",
      " [ 1.1615095e+00  1.0000000e+00 -1.5160198e+00 ... -1.3189148e+01\n",
      "  -1.6053850e+01 -6.3767281e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.57311726e+00  1.00000000e+00 -1.08294678e+00 ... -2.36995117e+03\n",
      "  -7.69639551e+03  1.67401794e+03]\n",
      " [ 1.57295704e+00  1.00000000e+00 -1.08314610e+00 ... -8.67646301e+02\n",
      "  -1.65671436e+03  2.95398956e+02]\n",
      " [ 1.57262039e+00  1.00000000e+00 -1.08357728e+00 ...  1.80839195e+01\n",
      "   3.04020824e+01  4.24911690e+01]\n",
      " ...\n",
      " [ 1.57271385e+00  1.00000000e+00 -1.08343601e+00 ...  1.55719495e+03\n",
      "  -1.19191101e+03  7.23436127e+01]\n",
      " [ 1.57314682e+00  1.00000000e+00 -1.08281136e+00 ... -1.19634026e+02\n",
      "  -2.16772308e+02  1.44264908e+02]\n",
      " [ 1.57317543e+00  1.00000000e+00 -1.08284187e+00 ... -3.94265366e+01\n",
      "  -8.40815735e+00  2.51885738e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8308039e+00  1.0000000e+00 -5.4375458e-01 ...  2.7864690e+03\n",
      "   1.3427917e+03 -1.4581449e+03]\n",
      " [ 1.8307228e+00  1.0000000e+00 -5.4398346e-01 ...  1.1953949e+04\n",
      "  -2.1571111e+03 -7.5662073e+02]\n",
      " [ 1.8305511e+00  1.0000000e+00 -5.4449272e-01 ...  6.5351433e+01\n",
      "  -4.9066753e+00  1.2818380e+01]\n",
      " ...\n",
      " [ 1.8306103e+00  1.0000000e+00 -5.4433727e-01 ...  3.6151965e+02\n",
      "   1.9240886e+02  6.5095081e+02]\n",
      " [ 1.8307991e+00  1.0000000e+00 -5.4358673e-01 ... -1.1893822e+02\n",
      "  -1.3840572e+01  1.0471003e+02]\n",
      " [ 1.8308287e+00  1.0000000e+00 -5.4362106e-01 ...  2.5307356e+01\n",
      "  -2.5663245e+00 -1.0461955e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90919971e+00  1.00000000e+00  4.82807159e-02 ... -4.64795807e+02\n",
      "   4.72284546e+02  1.18943352e+02]\n",
      " [ 1.90920639e+00  1.00000000e+00  4.80203629e-02 ... -1.34092285e+04\n",
      "   7.73993311e+03 -7.13161865e+03]\n",
      " [ 1.90920639e+00  1.00000000e+00  4.74906787e-02 ... -1.92674362e+02\n",
      "  -1.57651978e+02  2.23880188e+02]\n",
      " ...\n",
      " [ 1.90923691e+00  1.00000000e+00  4.76522446e-02 ...  1.46358795e+02\n",
      "   1.48119675e+02  3.15083191e+02]\n",
      " [ 1.90914154e+00  1.00000000e+00  4.84523773e-02 ... -1.94114288e+02\n",
      "   3.25243561e+02 -5.41153183e+01]\n",
      " [ 1.90918732e+00  1.00000000e+00  4.84180450e-02 ... -4.54152802e+02\n",
      "   2.44827789e+02 -1.19390114e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8006830e+00  1.0000000e+00  6.3621521e-01 ... -1.0476034e+02\n",
      "  -1.0347406e+03  2.9333188e+02]\n",
      " [ 1.8007822e+00  1.0000000e+00  6.3594532e-01 ... -6.2173594e+03\n",
      "   2.3048204e+02 -3.6927217e+03]\n",
      " [ 1.8009338e+00  1.0000000e+00  6.3544649e-01 ...  1.7853157e+01\n",
      "  -7.1381235e+00  4.5242596e-01]\n",
      " ...\n",
      " [ 1.8009357e+00  1.0000000e+00  6.3559818e-01 ...  1.5523926e+02\n",
      "   5.1768593e+01 -2.2816114e+00]\n",
      " [ 1.8005829e+00  1.0000000e+00  6.3637733e-01 ...  1.5449515e+02\n",
      "   2.5851630e+02 -2.9080206e+02]\n",
      " [ 1.8006468e+00  1.0000000e+00  6.3634872e-01 ...  4.3095477e+02\n",
      "   2.8869424e+02 -5.1266321e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5158682e+00  1.0000000e+00  1.1615887e+00 ... -1.7725470e+00\n",
      "   2.7176365e+01  1.3228294e+02]\n",
      " [ 1.5160494e+00  1.0000000e+00  1.1613703e+00 ...  1.8338203e+03\n",
      "  -8.2838501e+02  4.0981807e+03]\n",
      " [ 1.5163097e+00  1.0000000e+00  1.1609448e+00 ... -4.0404959e+02\n",
      "  -2.5064523e+02 -2.3905278e+02]\n",
      " ...\n",
      " [ 1.5162811e+00  1.0000000e+00  1.1610699e+00 ... -1.2998905e+01\n",
      "  -4.5842108e+02  2.9785494e+01]\n",
      " [ 1.5157223e+00  1.0000000e+00  1.1617222e+00 ...  2.4591618e+01\n",
      "   1.2629702e+01 -3.6952999e+01]\n",
      " [ 1.5157900e+00  1.0000000e+00  1.1617050e+00 ...  2.2676514e+03\n",
      "  -1.9498269e+03 -1.1967505e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0826530e+00  1.0000000e+00  1.5731812e+00 ...  3.1882143e-01\n",
      "   7.0596405e+01  5.1806259e+01]\n",
      " [ 1.0828953e+00  1.0000000e+00  1.5729847e+00 ... -1.0400808e+03\n",
      "  -1.0952009e+03  4.5939169e+02]\n",
      " [ 1.0832443e+00  1.0000000e+00  1.5726911e+00 ...  1.2096639e+02\n",
      "  -2.7861273e+02 -2.5902499e+02]\n",
      " ...\n",
      " [ 1.0831928e+00  1.0000000e+00  1.5727663e+00 ...  5.7625574e+02\n",
      "  -3.9637491e+02 -2.3766802e+03]\n",
      " [ 1.0824661e+00  1.0000000e+00  1.5732784e+00 ... -8.8392670e+01\n",
      "  -1.0148957e+02  1.6948637e+02]\n",
      " [ 1.0825453e+00  1.0000000e+00  1.5732708e+00 ...  1.1662075e+01\n",
      "   9.6350189e+01 -9.8766716e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.4362488e-01  1.0000000e+00  1.8305225e+00 ...  1.6737389e+01\n",
      "   6.6612189e+02  5.6035828e+02]\n",
      " [ 5.4391289e-01  1.0000000e+00  1.8304262e+00 ...  9.6561932e+02\n",
      "   1.8828401e+02 -7.7451764e+02]\n",
      " [ 5.4430389e-01  1.0000000e+00  1.8302623e+00 ...  1.4803235e+02\n",
      "   3.0907831e+02  8.6674683e+01]\n",
      " ...\n",
      " [ 5.4423904e-01  1.0000000e+00  1.8302879e+00 ... -7.2733636e+02\n",
      "   3.3701166e+02 -3.4640393e+02]\n",
      " [ 5.4343796e-01  1.0000000e+00  1.8305759e+00 ...  1.9125356e+02\n",
      "   3.1482700e+02 -8.1243912e+01]\n",
      " [ 5.4349232e-01  1.0000000e+00  1.8305798e+00 ...  1.3108905e+02\n",
      "   1.2169551e+02 -1.4843710e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.8922539e-02  1.0000000e+00  1.9088802e+00 ...  3.5636242e+01\n",
      "  -2.0956747e+02  2.0530643e+01]\n",
      " [-4.8621178e-02  1.0000000e+00  1.9088764e+00 ...  1.0757512e+02\n",
      "   3.4045517e+02  1.5918369e+02]\n",
      " [-4.8213959e-02  1.0000000e+00  1.9088464e+00 ...  5.1685431e+02\n",
      "   3.0916042e+03 -2.7770935e+03]\n",
      " ...\n",
      " [-4.8290253e-02  1.0000000e+00  1.9088316e+00 ...  9.3868721e+01\n",
      "   3.2294836e+02 -1.3570242e+02]\n",
      " [-4.9131393e-02  1.0000000e+00  1.9088688e+00 ...  2.3169254e+02\n",
      "   2.9077829e+02  1.4032733e+02]\n",
      " [-4.9064636e-02  1.0000000e+00  1.9088840e+00 ... -5.2806659e+02\n",
      "   1.4159237e+03  6.7260002e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.3625813e-01  1.0000000e+00  1.8001747e+00 ... -1.4290138e+02\n",
      "   2.6255180e+01 -1.4990132e+02]\n",
      " [-6.3598156e-01  1.0000000e+00  1.8002558e+00 ... -1.7422955e+01\n",
      "   1.4559807e+02  1.3945067e+02]\n",
      " [-6.3564110e-01  1.0000000e+00  1.8003688e+00 ... -4.0800308e+02\n",
      "   5.5090808e+02  8.3661232e+01]\n",
      " ...\n",
      " [-6.3570976e-01  1.0000000e+00  1.8003273e+00 ... -7.2758949e+01\n",
      "   3.7140354e+01  2.6372552e-01]\n",
      " [-6.3649559e-01  1.0000000e+00  1.8000908e+00 ... -1.7325479e+02\n",
      "   4.1281445e+01  3.4548285e+02]\n",
      " [-6.3639545e-01  1.0000000e+00  1.8001308e+00 ... -2.6975212e+01\n",
      "   1.9721733e+02 -8.6867744e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1615629e+00  1.0000000e+00  1.5153103e+00 ...  1.3333406e+01\n",
      "   8.3151535e+01 -3.2168385e+01]\n",
      " [-1.1613493e+00  1.0000000e+00  1.5154772e+00 ...  1.7118604e+02\n",
      "   2.0152272e+02 -8.8520363e+01]\n",
      " [-1.1610622e+00  1.0000000e+00  1.5157222e+00 ...  1.0892227e+02\n",
      "   1.8654968e+01  4.7135750e+01]\n",
      " ...\n",
      " [-1.1611195e+00  1.0000000e+00  1.5156384e+00 ...  1.3570554e+02\n",
      "   1.2017419e+03 -8.8276465e+02]\n",
      " [-1.1618176e+00  1.0000000e+00  1.5151615e+00 ...  3.0126362e+01\n",
      "  -5.6617575e+00 -3.8593124e+02]\n",
      " [-1.1616745e+00  1.0000000e+00  1.5152149e+00 ... -1.6683766e+02\n",
      "  -1.6228587e+02 -1.6005374e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.57318306e+00  1.00000000e+00  1.08195877e+00 ...  1.29470306e+02\n",
      "   1.14778847e+02  2.02037628e+02]\n",
      " [-1.57303429e+00  1.00000000e+00  1.08217049e+00 ...  2.14623688e+02\n",
      "  -6.62930450e+01  2.27633247e+01]\n",
      " [-1.57283401e+00  1.00000000e+00  1.08251166e+00 ...  3.16124268e+01\n",
      "   4.67984734e+01 -2.61415367e+01]\n",
      " ...\n",
      " [-1.57285118e+00  1.00000000e+00  1.08240128e+00 ...  2.40163986e+02\n",
      "   1.22115686e+03 -4.65642242e+02]\n",
      " [-1.57336998e+00  1.00000000e+00  1.08175468e+00 ...  1.63648804e+03\n",
      "   1.41970142e+03  1.32088293e+03]\n",
      " [-1.57326031e+00  1.00000000e+00  1.08182526e+00 ...  9.78609848e+01\n",
      "   6.48919373e+01 -1.16512695e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.83056355e+00  1.00000000e+00  5.42928696e-01 ...  1.18141251e+02\n",
      "  -5.01653023e+01 -1.24531456e+02]\n",
      " [-1.83049774e+00  1.00000000e+00  5.43152809e-01 ... -4.66222763e-01\n",
      "  -1.14683466e+01 -1.21319351e+01]\n",
      " [-1.83042717e+00  1.00000000e+00  5.43574154e-01 ...  1.38992508e+02\n",
      "  -1.05901268e+02  1.39920776e+02]\n",
      " ...\n",
      " [-1.83041954e+00  1.00000000e+00  5.43436050e-01 ... -1.67487442e+02\n",
      "   1.15544556e+02 -1.73171177e+01]\n",
      " [-1.83071709e+00  1.00000000e+00  5.42707443e-01 ... -2.23292297e+02\n",
      "   1.57182281e+02  3.14612518e+02]\n",
      " [-1.83059883e+00  1.00000000e+00  5.42779922e-01 ...  6.56155212e+02\n",
      "  -8.43982601e+00 -4.77197510e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.9000001 0.1       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9087610e+00  1.0000000e+00 -4.9703598e-02 ... -1.9642567e+01\n",
      "   1.0693308e+02  2.3276170e+02]\n",
      " [-1.9087944e+00  1.0000000e+00 -4.9468040e-02 ...  3.0108574e+01\n",
      "  -3.0775705e+01  3.4242088e+01]\n",
      " [-1.9088554e+00  1.0000000e+00 -4.9028296e-02 ... -6.0325160e+00\n",
      "  -1.1368346e+02 -5.9301037e+01]\n",
      " ...\n",
      " [-1.9088154e+00  1.0000000e+00 -4.9165726e-02 ...  4.9564468e+02\n",
      "   2.3334193e+02 -1.7104720e+02]\n",
      " [-1.9088364e+00  1.0000000e+00 -4.9936295e-02 ...  4.1212256e+02\n",
      "  -5.7896710e+02 -1.8643028e+02]\n",
      " [-1.9087543e+00  1.0000000e+00 -4.9858093e-02 ... -2.0585480e+01\n",
      "   1.3010512e+02 -7.1798677e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.7999649     1.           -0.6373787  ... -250.49437\n",
      "  -111.095894   -421.23648   ]\n",
      " [  -1.8000898     1.           -0.6371784  ...  109.47302\n",
      "   169.55898      82.00488   ]\n",
      " [  -1.8002758     1.           -0.6367409  ...   55.603935\n",
      "  -122.54516     -77.59657   ]\n",
      " ...\n",
      " [  -1.8002148     1.           -0.63689137 ...  274.69876\n",
      "   -51.388714   -232.61331   ]\n",
      " [  -1.7999821     1.           -0.63759995 ...  117.36028\n",
      "   107.16518    -161.86444   ]\n",
      " [  -1.7999115     1.           -0.63752556 ...  -43.369934\n",
      "  -209.58067      27.781694  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5149136    1.          -1.1625423 ...   18.387768   101.70973\n",
      "   -28.518486 ]\n",
      " [  -1.5151167    1.          -1.1623764 ...    1.441431    65.57354\n",
      "    92.52917  ]\n",
      " [  -1.51544      1.          -1.1619916 ...  -11.02038     28.708052\n",
      "  -151.18878  ]\n",
      " ...\n",
      " [  -1.5153503    1.          -1.1621246 ...   47.330055  -137.29709\n",
      "    33.15662  ]\n",
      " [  -1.5148754    1.          -1.1627312 ...   61.939095   219.7889\n",
      "    26.303675 ]\n",
      " [  -1.5148125    1.          -1.1626682 ... -361.09726    120.791336\n",
      "   -50.136116 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.0817032   1.         -1.5737057 ...  -6.799117   23.370562\n",
      "    8.137899 ]\n",
      " [ -1.081974    1.         -1.5735998 ... -37.646317   53.322964\n",
      "   38.64994  ]\n",
      " [ -1.0823727   1.         -1.5733213 ... -79.407845  123.41917\n",
      "  165.20439  ]\n",
      " ...\n",
      " [ -1.0822582   1.         -1.5734272 ... -38.011055  -81.32597\n",
      "   31.86649  ]\n",
      " [ -1.0816288   1.         -1.5738411 ...  12.003871  -16.8469\n",
      "   74.677864 ]\n",
      " [ -1.0815735   1.         -1.5737915 ...   5.5568595  80.267426\n",
      "  -53.653286 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.42432785e-01  1.00000000e+00 -1.83101654e+00 ... -3.20721405e+02\n",
      "   2.99649719e+02 -8.08542053e+02]\n",
      " [-5.42744637e-01  1.00000000e+00 -1.83099365e+00 ... -6.03312622e+02\n",
      "  -4.19059662e+02  5.08945709e+02]\n",
      " [-5.43174744e-01  1.00000000e+00 -1.83083534e+00 ...  7.81988297e+01\n",
      "   1.49997721e+01  8.74802551e+01]\n",
      " ...\n",
      " [-5.43039322e-01  1.00000000e+00 -1.83091545e+00 ...  2.56244019e+02\n",
      "   1.35798965e+02  1.18942314e+02]\n",
      " [-5.42324066e-01  1.00000000e+00 -1.83111572e+00 ... -4.24469471e-01\n",
      "   3.45759087e+01 -5.74153290e+01]\n",
      " [-5.42280197e-01  1.00000000e+00 -1.83107185e+00 ...  4.04803352e+01\n",
      "  -3.95292330e+00  3.73905029e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0123215e-02  1.0000000e+00 -1.9090557e+00 ...  1.4694095e+02\n",
      "   2.3571991e+02  4.6123676e+02]\n",
      " [ 4.9798965e-02  1.0000000e+00 -1.9090986e+00 ...  4.5953320e+01\n",
      "  -2.5564653e+01 -1.2170475e+02]\n",
      " [ 4.9325943e-02  1.0000000e+00 -1.9090967e+00 ...  1.1039820e+02\n",
      "  -7.9885551e+01 -1.9744661e+02]\n",
      " ...\n",
      " [ 4.9468994e-02  1.0000000e+00 -1.9091253e+00 ... -9.1965523e+00\n",
      "   6.4500748e+01  4.3130543e+01]\n",
      " [ 5.0218582e-02  1.0000000e+00 -1.9091015e+00 ...  7.7231560e+00\n",
      "  -9.1754494e+00 -2.0537376e-03]\n",
      " [ 5.0281525e-02  1.0000000e+00 -1.9090652e+00 ...  2.4085007e+03\n",
      "  -4.3839434e+03 -2.2500178e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.6375084     1.           -1.8001671  ...  -29.13073\n",
      "   -13.847954     19.381336  ]\n",
      " [   0.6372032     1.           -1.8003006  ...  -27.401833\n",
      "   175.6496      -32.06713   ]\n",
      " [   0.63673973    1.           -1.8004367  ...  -71.26072\n",
      "    80.68469     -17.950272  ]\n",
      " ...\n",
      " [   0.6368809     1.           -1.800416   ...  -50.119404\n",
      "    86.96587     133.07948   ]\n",
      " [   0.6375923     1.           -1.8001556  ...   59.71066\n",
      "    36.764435    -11.1677685 ]\n",
      " [   0.6376591     1.           -1.8001213  ...  -91.90251\n",
      "  -290.23047    -123.74053   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1627827     1.           -1.5149002  ...  286.80792\n",
      "    43.418976     -0.35702872]\n",
      " [   1.1625099     1.           -1.5151539  ... -180.75148\n",
      "  -214.46262    -356.5929    ]\n",
      " [   1.1621284     1.           -1.5154155  ...  137.3405\n",
      "    55.15936       0.6383237 ]\n",
      " ...\n",
      " [   1.1622562     1.           -1.5153608  ...    4.550853\n",
      "   -11.312642   -319.40695   ]\n",
      " [   1.1628475     1.           -1.5148525  ...   69.178\n",
      "   300.07587      81.33624   ]\n",
      " [   1.1629057     1.           -1.5148125  ... -274.79895\n",
      "  -181.93715    -340.2738    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5740910e+00  1.0000000e+00 -1.0812874e+00 ...  2.9185367e+02\n",
      "  -6.0305823e+02  4.9310416e+02]\n",
      " [ 1.5738821e+00  1.0000000e+00 -1.0816145e+00 ...  1.2769090e+04\n",
      "   4.1738618e+03  2.6882410e+03]\n",
      " [ 1.5736313e+00  1.0000000e+00 -1.0819718e+00 ... -2.1310838e+02\n",
      "   3.0252026e+02 -2.1055450e+02]\n",
      " ...\n",
      " [ 1.5737305e+00  1.0000000e+00 -1.0818853e+00 ... -7.0411491e+01\n",
      "   1.2756108e+01  1.9212830e+02]\n",
      " [ 1.5741615e+00  1.0000000e+00 -1.0812073e+00 ...  7.7849480e+01\n",
      "  -3.4599823e+01  2.3851535e+02]\n",
      " [ 1.5741682e+00  1.0000000e+00 -1.0811615e+00 ...  3.0199583e+03\n",
      "   2.7790740e+03 -2.6527173e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8312111e+00  1.0000000e+00 -5.4176712e-01 ... -2.3663998e+01\n",
      "  -6.2545311e+01  1.2955032e+02]\n",
      " [ 1.8310995e+00  1.0000000e+00 -5.4217815e-01 ... -1.1993340e+00\n",
      "   8.9386237e+02 -5.9261554e+02]\n",
      " [ 1.8310089e+00  1.0000000e+00 -5.4258549e-01 ... -2.3520471e+02\n",
      "  -6.0415820e+02  3.5143912e+02]\n",
      " ...\n",
      " [ 1.8310623e+00  1.0000000e+00 -5.4249763e-01 ...  2.8573553e+02\n",
      "   1.2591502e+02 -3.2818149e+02]\n",
      " [ 1.8313084e+00  1.0000000e+00 -5.4167747e-01 ... -3.0551434e+02\n",
      "  -2.5041968e+02 -1.4984206e+02]\n",
      " [ 1.8312321e+00  1.0000000e+00 -5.4163170e-01 ...  1.9255040e+02\n",
      "   2.8061459e+02  4.4090576e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9090166e+00  1.0000000e+00  5.0546646e-02 ... -2.3215193e+01\n",
      "   3.1909084e+00  2.2992462e+01]\n",
      " [ 1.9090023e+00  1.0000000e+00  5.0114632e-02 ...  1.1080731e+03\n",
      "  -1.6663362e+02  3.6921466e+02]\n",
      " [ 1.9090805e+00  1.0000000e+00  4.9695134e-02 ...  6.6028125e+02\n",
      "   6.4944086e+02  8.5009148e+01]\n",
      " ...\n",
      " [ 1.9090939e+00  1.0000000e+00  4.9784660e-02 ... -6.0668179e+01\n",
      "   3.3457639e+02  6.0564568e+01]\n",
      " [ 1.9090576e+00  1.0000000e+00  5.0638199e-02 ... -3.4764935e+01\n",
      "   1.2393436e+01 -4.3096508e+01]\n",
      " [ 1.9089794e+00  1.0000000e+00  5.0685883e-02 ... -3.7273479e+01\n",
      "  -3.1295155e+01  3.6332569e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7999697e+00  1.0000000e+00  6.3798141e-01 ... -7.4922188e+01\n",
      "  -2.2651150e+01 -8.4902748e+01]\n",
      " [ 1.8000536e+00  1.0000000e+00  6.3755989e-01 ...  8.1358185e+01\n",
      "  -2.6319043e+02  6.8432904e+02]\n",
      " [ 1.8002892e+00  1.0000000e+00  6.3715851e-01 ...  1.1209969e+02\n",
      "   8.3769934e+02 -9.4533531e+01]\n",
      " ...\n",
      " [ 1.8002605e+00  1.0000000e+00  6.3724136e-01 ... -5.2502518e+01\n",
      "  -1.2234532e+02 -2.1358068e+01]\n",
      " [ 1.7999496e+00  1.0000000e+00  6.3806534e-01 ... -3.8170991e+00\n",
      "   6.6610060e+00  5.2616070e+01]\n",
      " [ 1.7998838e+00  1.0000000e+00  6.3811302e-01 ...  5.5265121e+01\n",
      "   5.7328087e+01  9.6809082e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5145636e+00  1.0000000e+00  1.1631966e+00 ... -1.1270395e+02\n",
      "   6.6184158e+01 -3.9298105e+00]\n",
      " [ 1.5147505e+00  1.0000000e+00  1.1628103e+00 ... -9.6468567e+02\n",
      "  -1.3024619e+03  6.1033746e+02]\n",
      " [ 1.5151329e+00  1.0000000e+00  1.1624799e+00 ...  6.0888135e+02\n",
      "   2.0922285e+03 -2.0744608e+02]\n",
      " ...\n",
      " [ 1.5150814e+00  1.0000000e+00  1.1625433e+00 ...  1.8620115e+01\n",
      "   1.2630522e+01  8.8369398e+00]\n",
      " [ 1.5145588e+00  1.0000000e+00  1.1632500e+00 ...  6.9064789e+01\n",
      "  -7.3797653e+01  6.2749249e+01]\n",
      " [ 1.5144377e+00  1.0000000e+00  1.1632919e+00 ... -1.4999693e+01\n",
      "  -7.2417618e+01 -1.0947641e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0811520e+00  1.0000000e+00  1.5741615e+00 ... -1.7176730e+02\n",
      "  -2.8068082e+02  2.8400507e+02]\n",
      " [ 1.0814304e+00  1.0000000e+00  1.5738583e+00 ... -2.2738300e+01\n",
      "  -9.2029199e+02 -2.7480980e+01]\n",
      " [ 1.0819340e+00  1.0000000e+00  1.5736307e+00 ... -1.1693680e+03\n",
      "  -4.6014824e+03 -7.1765924e+02]\n",
      " ...\n",
      " [ 1.0818577e+00  1.0000000e+00  1.5736704e+00 ... -4.6279105e+02\n",
      "   1.3889985e+03 -1.3468857e+03]\n",
      " [ 1.0811768e+00  1.0000000e+00  1.5741787e+00 ... -4.3601421e+01\n",
      "   1.7490421e+02 -9.5267883e+01]\n",
      " [ 1.0809917e+00  1.0000000e+00  1.5742111e+00 ...  1.0191265e+01\n",
      "   4.0283482e+01  1.3399031e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.4171562e-01  1.0000000e+00  1.8311710e+00 ... -3.8752483e+01\n",
      "   1.1756746e+02  1.5019376e+02]\n",
      " [ 5.4203510e-01  1.0000000e+00  1.8310032e+00 ...  3.5735378e+03\n",
      "  -2.2705449e+03 -3.8579980e+02]\n",
      " [ 5.4260635e-01  1.0000000e+00  1.8309029e+00 ... -6.2856938e+03\n",
      "  -8.6053027e+03 -4.4897568e+03]\n",
      " ...\n",
      " [ 5.4251099e-01  1.0000000e+00  1.8309050e+00 ... -4.2183588e+02\n",
      "   3.3666855e+02 -2.7483929e+02]\n",
      " [ 5.4172325e-01  1.0000000e+00  1.8311443e+00 ...  1.1840092e+02\n",
      "   4.7856323e+01  5.0565193e+01]\n",
      " [ 5.4153442e-01  1.0000000e+00  1.8311539e+00 ...  1.8963966e+02\n",
      "   5.0039772e+01  1.3480440e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0972939e-02  1.0000000e+00  1.9089012e+00 ... -3.2161804e+02\n",
      "   1.9397820e+02 -3.9139034e+01]\n",
      " [-5.0638199e-02  1.0000000e+00  1.9088593e+00 ... -4.2375970e+02\n",
      "   1.3237819e+02  3.3096030e+02]\n",
      " [-5.0054550e-02  1.0000000e+00  1.9089240e+00 ...  1.0268846e+02\n",
      "  -7.0223495e+01  5.7377850e+02]\n",
      " ...\n",
      " [-5.0151825e-02  1.0000000e+00  1.9088888e+00 ... -8.7996681e+01\n",
      "   1.3255240e+02 -2.7761771e+02]\n",
      " [-5.0985336e-02  1.0000000e+00  1.9088631e+00 ...  2.4063380e+02\n",
      "   1.8007274e+02 -8.2665854e+00]\n",
      " [-5.1163673e-02  1.0000000e+00  1.9088459e+00 ... -2.7516745e+02\n",
      "   7.2443932e+01 -1.4969133e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:6, Score:0.69, Best Score:2.33, Average Score:1.50, Best Avg Score:1.72\n",
      "Episode number: 7\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57d2e7340>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1631937e+00  1.0000000e+00  1.5142403e+00 ... -2.4175905e+02\n",
      "   5.3755219e+02  4.0955704e+02]\n",
      " [-1.1629419e+00  1.0000000e+00  1.5144176e+00 ... -1.7784227e+01\n",
      "  -2.1528084e+02 -2.1810814e+01]\n",
      " [-1.1624756e+00  1.0000000e+00  1.5148122e+00 ...  4.2940891e+02\n",
      "  -2.4589214e+02 -1.5959026e+02]\n",
      " ...\n",
      " [-1.1625500e+00  1.0000000e+00  1.5147133e+00 ...  3.9016586e+01\n",
      "   5.2215080e+01  1.0608183e+02]\n",
      " [-1.1632004e+00  1.0000000e+00  1.5141773e+00 ... -1.0514660e+02\n",
      "   1.7003260e+01  6.2701038e+01]\n",
      " [-1.1633587e+00  1.0000000e+00  1.5140629e+00 ...  1.4660316e+02\n",
      "   1.5650337e+03  3.4671633e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5742931e+00  1.0000000e+00  1.0802174e+00 ...  4.9467859e+02\n",
      "   5.9193138e+01 -2.7129767e+02]\n",
      " [-1.5741072e+00  1.0000000e+00  1.0805101e+00 ...  8.1625928e+02\n",
      "   1.4656694e+03 -9.5050378e+02]\n",
      " [-1.5738220e+00  1.0000000e+00  1.0810164e+00 ... -1.2694058e+02\n",
      "  -1.7096719e+02  1.5336624e+02]\n",
      " ...\n",
      " [-1.5738697e+00  1.0000000e+00  1.0808973e+00 ...  9.4208168e+01\n",
      "  -3.4158167e+02 -2.2401520e+02]\n",
      " [-1.5743160e+00  1.0000000e+00  1.0801449e+00 ... -2.2853533e+03\n",
      "  -1.6970516e+03  2.6568838e+03]\n",
      " [-1.5744114e+00  1.0000000e+00  1.0800056e+00 ... -2.9089470e+00\n",
      "  -1.5322430e+02  2.8267841e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.83122444e+00  1.00000000e+00  5.40599823e-01 ...  3.26928650e+02\n",
      "  -2.54710770e+01  2.37611130e+02]\n",
      " [-1.83110809e+00  1.00000000e+00  5.40912628e-01 ...  3.22461456e+02\n",
      "  -9.79006348e+01 -1.38631477e+01]\n",
      " [-1.83101845e+00  1.00000000e+00  5.41496217e-01 ... -6.88168701e+02\n",
      "  -1.14043225e+03  3.25166602e+03]\n",
      " ...\n",
      " [-1.83103943e+00  1.00000000e+00  5.41376114e-01 ... -1.24085419e+02\n",
      "  -1.03617912e+02  1.15663086e+02]\n",
      " [-1.83121300e+00  1.00000000e+00  5.40521622e-01 ... -4.45448181e+02\n",
      "  -5.08174805e+02  5.51550674e+01]\n",
      " [-1.83129597e+00  1.00000000e+00  5.40365219e-01 ...  1.18022156e+02\n",
      "   2.00340744e+02 -1.24899340e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9087238e+00  1.0000000e+00 -5.1502228e-02 ... -6.5293040e+00\n",
      "   5.8174133e+02  2.8140509e+02]\n",
      " [-1.9087076e+00  1.0000000e+00 -5.1191330e-02 ...  4.3544898e+00\n",
      "  -6.5674828e+01  2.9217047e+02]\n",
      " [-1.9087849e+00  1.0000000e+00 -5.0591692e-02 ... -1.5958088e+03\n",
      "  -4.1061221e+03  4.0836145e+03]\n",
      " ...\n",
      " [-1.9087772e+00  1.0000000e+00 -5.0707817e-02 ...  1.6422391e+02\n",
      "  -6.5639267e+01  1.2625033e+02]\n",
      " [-1.9087048e+00  1.0000000e+00 -5.1582336e-02 ... -1.4304250e+01\n",
      "  -1.1132009e+02  4.8879387e+01]\n",
      " [-1.9087381e+00  1.0000000e+00 -5.1746368e-02 ...  1.2526074e+02\n",
      "   2.9984886e+01 -1.4074370e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.3       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79940701e+00  1.00000000e+00 -6.38959885e-01 ...  2.20429173e+01\n",
      "  -6.44313717e+00  2.36995697e+00]\n",
      " [-1.79949379e+00  1.00000000e+00 -6.38664246e-01 ... -1.00156975e+01\n",
      "  -3.06662659e+02 -6.82608032e+01]\n",
      " [-1.79974174e+00  1.00000000e+00 -6.38107598e-01 ...  7.07110535e+02\n",
      "   1.02543567e+03  7.05638977e+02]\n",
      " ...\n",
      " [-1.79970932e+00  1.00000000e+00 -6.38212204e-01 ...  1.40992212e+01\n",
      "  -1.39347519e+02  5.74494705e+01]\n",
      " [-1.79939651e+00  1.00000000e+00 -6.39038086e-01 ... -2.00325966e+01\n",
      "  -3.27440491e+01  3.06740665e+01]\n",
      " [-1.79934216e+00  1.00000000e+00 -6.39200211e-01 ... -2.12396979e+00\n",
      "   2.07634155e+02 -3.20025330e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.2       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5138912    1.          -1.1638489 ... -213.50833    544.0599\n",
      "   264.37567  ]\n",
      " [  -1.5140667    1.          -1.1635885 ...   21.025848    39.868607\n",
      "    -6.033967 ]\n",
      " [  -1.5144768    1.          -1.1631324 ... -465.19388   -109.326225\n",
      "   310.68118  ]\n",
      " ...\n",
      " [  -1.5144081    1.          -1.1632004 ... -189.14299    322.44287\n",
      "   -90.034164 ]\n",
      " [  -1.513874     1.          -1.1639175 ...  -37.195114   -42.156822\n",
      "    23.024097 ]\n",
      " [  -1.5137768    1.          -1.1640682 ...   -4.557872  -496.12445\n",
      "   -86.85167  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0798416    1.          -1.5749874 ...  227.69763     -5.361169\n",
      "    42.57951  ]\n",
      " [  -1.08008      1.          -1.5747843 ... -104.80928    150.34651\n",
      "  -292.38257  ]\n",
      " [  -1.0805969    1.          -1.5744674 ...  457.39163    522.69666\n",
      "  -474.21362  ]\n",
      " ...\n",
      " [  -1.0805092    1.          -1.5745039 ...  -82.01621   -165.4494\n",
      "   -98.75363  ]\n",
      " [  -1.0798016    1.          -1.5750389 ...  299.52972   -309.15332\n",
      "   485.4141   ]\n",
      " [  -1.0796757    1.          -1.5751534 ...  -29.787636  -169.15208\n",
      "   -61.02082  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.4051113e-01  1.0000000e+00 -1.8314819e+00 ...  3.6101199e+02\n",
      "  -9.3462604e+02  1.5852787e+03]\n",
      " [-5.4079247e-01  1.0000000e+00 -1.8313732e+00 ...  2.6483356e+01\n",
      "  -1.5040842e+02  2.4736830e+02]\n",
      " [-5.4138756e-01  1.0000000e+00 -1.8312334e+00 ...  2.9196082e+02\n",
      "  -3.1519727e+02 -4.5407608e+02]\n",
      " ...\n",
      " [-5.4128265e-01  1.0000000e+00 -1.8312407e+00 ...  1.1349169e+02\n",
      "   1.3669926e+02 -1.2178622e+02]\n",
      " [-5.4046631e-01  1.0000000e+00 -1.8315163e+00 ... -3.5312006e+02\n",
      "   2.7246716e+01  1.2635889e+02]\n",
      " [-5.4031944e-01  1.0000000e+00 -1.8315887e+00 ... -5.2934580e+00\n",
      "  -1.1981126e+01  8.4071228e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.2332878e-02  1.0000000e+00 -1.9088821e+00 ...  7.9165573e+01\n",
      "  -5.2400242e+01  4.0197586e+01]\n",
      " [ 5.2042961e-02  1.0000000e+00 -1.9088869e+00 ...  2.0878363e+02\n",
      "   8.4020370e+01 -1.5476079e+02]\n",
      " [ 5.1372528e-02  1.0000000e+00 -1.9089273e+00 ... -3.2049490e+02\n",
      "   1.3121487e+02  4.9173352e+02]\n",
      " ...\n",
      " [ 5.1479340e-02  1.0000000e+00 -1.9089136e+00 ...  3.0435255e+01\n",
      "  -1.1083631e+02  9.4563293e-01]\n",
      " [ 5.2341461e-02  1.0000000e+00 -1.9089012e+00 ... -5.6053949e+02\n",
      "  -4.4767551e+01  3.9391727e+02]\n",
      " [ 5.2528381e-02  1.0000000e+00 -1.9089317e+00 ...  3.9374149e+01\n",
      "   1.1570455e+02  3.2007839e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.39800072e-01  1.00000000e+00 -1.79924393e+00 ...  1.28959160e+01\n",
      "   1.09572182e+01  2.00076714e+01]\n",
      " [ 6.39531136e-01  1.00000000e+00 -1.79933929e+00 ... -7.05332373e+03\n",
      "   8.12500793e+02 -2.89791138e+03]\n",
      " [ 6.38902664e-01  1.00000000e+00 -1.79957187e+00 ...  3.24199402e+02\n",
      "  -6.01340599e+01  3.36046783e+02]\n",
      " ...\n",
      " [ 6.38999939e-01  1.00000000e+00 -1.79952621e+00 ... -8.58165359e+01\n",
      "   7.37410583e+01 -2.86180664e+02]\n",
      " [ 6.39829636e-01  1.00000000e+00 -1.79923058e+00 ... -8.40697021e+01\n",
      "  -1.03818474e+02 -1.91838913e+02]\n",
      " [ 6.39980316e-01  1.00000000e+00 -1.79921341e+00 ...  1.05377548e+02\n",
      "  -1.90674026e+02 -6.57787857e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1642513e+00  1.0000000e+00 -1.5137196e+00 ... -1.4245674e+01\n",
      "  -3.3624306e+01  2.0333792e+02]\n",
      " [ 1.1640329e+00  1.0000000e+00 -1.5138836e+00 ... -1.3004974e+02\n",
      "   1.1215877e+02 -3.6872387e+01]\n",
      " [ 1.1634903e+00  1.0000000e+00 -1.5142816e+00 ... -4.6001944e+02\n",
      "   6.9113686e+01  5.6862085e+02]\n",
      " ...\n",
      " [ 1.1635685e+00  1.0000000e+00 -1.5141907e+00 ... -8.5721184e+01\n",
      "   4.8836281e+01  1.6989326e+02]\n",
      " [ 1.1642647e+00  1.0000000e+00 -1.5136890e+00 ... -1.4899823e+03\n",
      "   1.3840496e+03  2.2444041e+02]\n",
      " [ 1.1643972e+00  1.0000000e+00 -1.5136089e+00 ...  5.3471643e+02\n",
      "   4.1713568e+02  1.7295189e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.9000001 0.        0.        0.\n",
      " 0.        0.        0.        0.1       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5751114    1.          -1.0797596 ...  167.53769    -93.26903\n",
      "   -67.812485 ]\n",
      " [   1.5749674    1.          -1.079977  ...   -8.425402    38.670418\n",
      "   109.97911  ]\n",
      " [   1.5745945    1.          -1.0805234 ...  100.46421   -296.46243\n",
      "  -837.2869   ]\n",
      " ...\n",
      " [   1.5746593    1.          -1.0804024 ...   17.834583   -20.499224\n",
      "    35.93283  ]\n",
      " [   1.5751534    1.          -1.0797195 ... -235.19638    -69.70153\n",
      "  -371.02515  ]\n",
      " [   1.5752201    1.          -1.0796108 ...  -14.231762   253.7075\n",
      "     2.137527 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.8000001 0.        0.        0.\n",
      " 0.3       0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.83172321e+00  1.00000000e+00 -5.40016174e-01 ...  5.75646973e+02\n",
      "  -1.03554459e+02  1.14784393e+02]\n",
      " [ 1.83166504e+00  1.00000000e+00 -5.40294647e-01 ...  4.61854591e+01\n",
      "   1.07926788e+02  1.56461639e+02]\n",
      " [ 1.83145905e+00  1.00000000e+00 -5.40926516e-01 ...  8.19458866e+00\n",
      "   7.50552893e+00 -3.61491871e+00]\n",
      " ...\n",
      " [ 1.83149338e+00  1.00000000e+00 -5.40795326e-01 ... -7.35756760e+01\n",
      "   1.59002213e+02 -1.89225845e+02]\n",
      " [ 1.83172035e+00  1.00000000e+00 -5.39968491e-01 ...  1.18358894e+02\n",
      "  -1.12400597e+02  1.57689789e+02]\n",
      " [ 1.83177853e+00  1.00000000e+00 -5.39838791e-01 ... -4.30835876e+02\n",
      "   9.37987183e+02  1.76027725e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9089575e+00  1.0000000e+00  5.2501678e-02 ... -8.9759987e+01\n",
      "   1.5372780e+02  2.5988330e+02]\n",
      " [ 1.9090023e+00  1.0000000e+00  5.2213669e-02 ... -1.1403763e+01\n",
      "   8.5832306e+01  5.9145575e+00]\n",
      " [ 1.9089947e+00  1.0000000e+00  5.1553044e-02 ...  3.8756908e+01\n",
      "  -6.8240204e+00 -2.1484249e+01]\n",
      " ...\n",
      " [ 1.9089870e+00  1.0000000e+00  5.1693916e-02 ... -2.3668909e+01\n",
      "   2.8472276e+00 -9.7390076e+01]\n",
      " [ 1.9089527e+00  1.0000000e+00  5.2549362e-02 ... -3.7098264e+02\n",
      "  -4.6858823e+02  8.9611671e+01]\n",
      " [ 1.9089375e+00  1.0000000e+00  5.2686691e-02 ... -4.7965130e+01\n",
      "  -4.4601303e+01  2.5558472e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1        0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  1.7992611    1.           0.6399498  ...   6.3362865    2.2753599\n",
      "  -78.01882   ]\n",
      " [  1.7993908    1.           0.6396885  ... -35.94405     36.04745\n",
      "   11.495341  ]\n",
      " [  1.799572     1.           0.6390637  ...  -4.3054647    1.1332903\n",
      "  -12.361065  ]\n",
      " ...\n",
      " [  1.7995033    1.           0.63919353 ...  80.13322    -91.69564\n",
      "   13.629959  ]\n",
      " [  1.7992172    1.           0.6399956  ...   6.0050483  -19.526955\n",
      "   40.119892  ]\n",
      " [  1.7991772    1.           0.640131   ... -23.751854    72.778206\n",
      "   42.845894  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5131016    1.           1.1647816 ...  -39.760025   180.81912\n",
      "    33.250916 ]\n",
      " [   1.5133066    1.           1.1645565 ... -260.96173    143.30754\n",
      "  -129.66046  ]\n",
      " [   1.513649     1.           1.1640319 ...    9.178997    14.825483\n",
      "    71.91697  ]\n",
      " ...\n",
      " [   1.5135498    1.           1.1641254 ...   -2.7053409   20.242283\n",
      "   -11.408806 ]\n",
      " [   1.5130215    1.           1.1648178 ...   -4.2962923   -9.924448\n",
      "    -4.5927496]\n",
      " [   1.512948     1.           1.1649284 ...   66.33618     -6.9158826\n",
      "   -17.91366  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.07896709e+00  1.00000000e+00  1.57523155e+00 ...  6.17485275e+01\n",
      "  -2.43388634e+01 -1.23301521e+02]\n",
      " [ 1.07923222e+00  1.00000000e+00  1.57505798e+00 ... -2.95051918e+01\n",
      "  -9.40801514e+02 -4.78126831e+02]\n",
      " [ 1.07968521e+00  1.00000000e+00  1.57469428e+00 ...  2.30025757e+02\n",
      "   1.13139862e+02 -3.59999466e+01]\n",
      " ...\n",
      " [ 1.07955551e+00  1.00000000e+00  1.57475185e+00 ...  5.51947571e+02\n",
      "  -4.08824524e+02  1.29931628e+03]\n",
      " [ 1.07886505e+00  1.00000000e+00  1.57525253e+00 ... -4.66688576e+01\n",
      "   5.11959743e+00  4.06023502e+00]\n",
      " [ 1.07876873e+00  1.00000000e+00  1.57532120e+00 ...  1.88295090e+02\n",
      "  -1.00486916e+02  7.92966156e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.3939438e-01  1.0000000e+00  1.8315887e+00 ... -2.8682224e+02\n",
      "   3.4305295e+02 -7.8796036e+01]\n",
      " [ 5.3970242e-01  1.0000000e+00  1.8314524e+00 ... -2.8699598e+01\n",
      "   2.4837129e+02 -1.9688389e+01]\n",
      " [ 5.4018593e-01  1.0000000e+00  1.8312817e+00 ...  2.4435776e+02\n",
      "  -5.1633501e+03  1.5005519e+03]\n",
      " ...\n",
      " [ 5.4003334e-01  1.0000000e+00  1.8313007e+00 ... -7.3198999e+02\n",
      "  -2.2144094e+03  5.4311987e+02]\n",
      " [ 5.3923416e-01  1.0000000e+00  1.8315811e+00 ... -8.5230273e+02\n",
      "   1.7769579e+02  2.6554895e+02]\n",
      " [ 5.3916836e-01  1.0000000e+00  1.8316059e+00 ... -1.8739796e+02\n",
      "  -3.8923992e+01 -5.6738434e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.34954071e-02  1.00000000e+00  1.90864754e+00 ... -6.75638275e+01\n",
      "   3.39280586e+01  5.38222046e+01]\n",
      " [-5.31778336e-02  1.00000000e+00  1.90861320e+00 ...  3.15130482e+01\n",
      "  -1.12375801e+02 -3.40570740e+02]\n",
      " [-5.26580811e-02  1.00000000e+00  1.90863347e+00 ...  4.10475250e+02\n",
      "  -2.72594818e+02  2.11522797e+02]\n",
      " ...\n",
      " [-5.28144836e-02  1.00000000e+00  1.90862083e+00 ... -4.82308960e+02\n",
      "  -2.62346130e+02 -1.01332684e+03]\n",
      " [-5.36518097e-02  1.00000000e+00  1.90861320e+00 ...  2.28695114e+02\n",
      "   1.39154831e+02 -2.83770447e+02]\n",
      " [-5.37309647e-02  1.00000000e+00  1.90859604e+00 ...  3.21595345e+01\n",
      "   1.34160706e+02  8.74850922e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.40529633e-01  1.00000000e+00  1.79883766e+00 ... -2.17884560e+01\n",
      "  -1.20914070e+02  1.41213074e+02]\n",
      " [-6.40227318e-01  1.00000000e+00  1.79888344e+00 ...  5.20206184e+01\n",
      "  -1.22354675e+02  8.86194458e+01]\n",
      " [-6.39770508e-01  1.00000000e+00  1.79908097e+00 ... -1.23121680e+03\n",
      "  -6.08445068e+02  3.06367157e+02]\n",
      " ...\n",
      " [-6.39919281e-01  1.00000000e+00  1.79902077e+00 ...  4.45806396e+02\n",
      "   3.85027275e+01 -5.31213623e+02]\n",
      " [-6.40712738e-01  1.00000000e+00  1.79877090e+00 ... -2.36807739e+02\n",
      "   1.12866418e+03 -2.40642014e+02]\n",
      " [-6.40753746e-01  1.00000000e+00  1.79870796e+00 ...  4.76285019e+01\n",
      "   7.88919067e+01  7.95999374e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.16527367e+00  1.00000000e+00  1.51274872e+00 ... -1.06242325e+02\n",
      "  -1.76159077e+01  2.10098591e+01]\n",
      " [-1.16501808e+00  1.00000000e+00  1.51289940e+00 ... -1.86837433e+02\n",
      "  -2.52838306e+01 -9.60116882e+01]\n",
      " [-1.16460037e+00  1.00000000e+00  1.51324272e+00 ...  1.56835098e+02\n",
      "  -2.18712906e+02  1.83006317e+02]\n",
      " ...\n",
      " [-1.16470718e+00  1.00000000e+00  1.51312733e+00 ... -1.82269192e+01\n",
      "  -4.68703522e+02  3.99018188e+02]\n",
      " [-1.16538429e+00  1.00000000e+00  1.51265335e+00 ...  1.33090356e+03\n",
      "  -5.59960632e+02 -3.77814514e+02]\n",
      " [-1.16546822e+00  1.00000000e+00  1.51257133e+00 ... -1.52463684e+02\n",
      "  -4.04920288e+03 -1.52200708e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.575778     1.           1.0786686 ...  119.93183    -89.766205\n",
      "   -37.346367 ]\n",
      " [  -1.5755901    1.           1.0788746 ...   20.753185   -50.540363\n",
      "   -18.637411 ]\n",
      " [  -1.5752487    1.           1.0793427 ...   95.85793   -147.54362\n",
      "    29.087572 ]\n",
      " ...\n",
      " [  -1.575325     1.           1.0791874 ...   46.565067  -293.54034\n",
      "   492.3245   ]\n",
      " [  -1.5758362    1.           1.0785275 ...   19.241129   -23.808111\n",
      "   127.47268  ]\n",
      " [  -1.5759115    1.           1.0784264 ...  512.5805    -440.34885\n",
      "    12.044508 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8320589     1.            0.53873634 ...  237.51204\n",
      "    77.14842      41.933964  ]\n",
      " [  -1.831975      1.            0.5389643  ...  -32.292267\n",
      "   194.09276    -267.0208    ]\n",
      " [  -1.8317242     1.            0.53951365 ...   -3.4825191\n",
      "  -191.94136      15.115153  ]\n",
      " ...\n",
      " [  -1.8317719     1.            0.53932667 ...  -35.789307\n",
      "    46.906414    -31.03118   ]\n",
      " [  -1.8320827     1.            0.5385761  ... -202.4647\n",
      "   123.75952      61.023453  ]\n",
      " [  -1.8321142     1.            0.53845024 ...  -32.90573\n",
      "    21.560194    -48.05684   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90888119e+00  1.00000000e+00 -5.39512634e-02 ... -1.31083130e+02\n",
      "  -3.48764267e+01 -2.41868027e+02]\n",
      " [-1.90887833e+00  1.00000000e+00 -5.36851883e-02 ... -4.47083702e+01\n",
      "  -1.09927297e+00  8.61894417e+00]\n",
      " [-1.90886116e+00  1.00000000e+00 -5.31268343e-02 ... -4.33400536e+01\n",
      "  -4.50042725e+01  2.65391598e+01]\n",
      " ...\n",
      " [-1.90885735e+00  1.00000000e+00 -5.33056259e-02 ... -3.89644836e+02\n",
      "   1.68137054e+02  6.47970352e+01]\n",
      " [-1.90890312e+00  1.00000000e+00 -5.41152954e-02 ... -1.08628006e+02\n",
      "  -5.50382538e+01  8.81923904e+01]\n",
      " [-1.90886974e+00  1.00000000e+00 -5.42449951e-02 ... -7.88217087e+01\n",
      "  -1.75490913e+01  1.46305038e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7988863e+00  1.0000000e+00 -6.4098549e-01 ...  5.1639423e+01\n",
      "   3.7710983e+01  8.8785965e+01]\n",
      " [-1.7989616e+00  1.0000000e+00 -6.4073753e-01 ... -3.3183606e+02\n",
      "  -1.5805458e+02  6.6224127e+02]\n",
      " [-1.7991772e+00  1.0000000e+00 -6.4020139e-01 ...  5.2930988e+02\n",
      "  -1.4635738e+01 -5.0528625e+02]\n",
      " ...\n",
      " [-1.7991314e+00  1.0000000e+00 -6.4037800e-01 ...  7.0594673e+01\n",
      "  -1.2040542e+02  2.3075224e+02]\n",
      " [-1.7988834e+00  1.0000000e+00 -6.4113808e-01 ...  7.6808144e+01\n",
      "  -4.0065033e+01  1.4992378e+02]\n",
      " [-1.7988052e+00  1.0000000e+00 -6.4126205e-01 ...  3.1128590e+01\n",
      "   3.6220730e+01 -1.3942601e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5127373e+00  1.0000000e+00 -1.1655426e+00 ...  6.5668945e+01\n",
      "   8.1807503e+01  3.3369621e+01]\n",
      " [-1.5129080e+00  1.0000000e+00 -1.1653900e+00 ...  1.4427144e+03\n",
      "   3.1463544e+02 -3.7663876e+02]\n",
      " [-1.5132408e+00  1.0000000e+00 -1.1649303e+00 ... -3.8492441e+00\n",
      "  -2.0184312e+02  8.6151056e+02]\n",
      " ...\n",
      " [-1.5131588e+00  1.0000000e+00 -1.1650972e+00 ...  5.4140864e+03\n",
      "   5.2887236e+03  8.3613672e+03]\n",
      " [-1.5126915e+00  1.0000000e+00 -1.1656685e+00 ... -2.6654245e+01\n",
      "   2.0301954e+01  1.8178665e+01]\n",
      " [-1.5125895e+00  1.0000000e+00 -1.1657715e+00 ...  3.1795142e+02\n",
      "   7.6607379e+02  1.1079105e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0782433e+00  1.0000000e+00 -1.5759888e+00 ...  1.1289878e+03\n",
      "  -3.8388779e+02  2.2957709e+02]\n",
      " [-1.0784607e+00  1.0000000e+00 -1.5758791e+00 ...  7.3205975e+02\n",
      "   1.8133989e+02 -4.3137550e+01]\n",
      " [-1.0789318e+00  1.0000000e+00 -1.5755508e+00 ... -3.4658542e+02\n",
      "   2.0277369e+01  1.9832790e+02]\n",
      " ...\n",
      " [-1.0788136e+00  1.0000000e+00 -1.5756712e+00 ... -5.2865771e+02\n",
      "   2.0912527e+03  2.7201067e+03]\n",
      " [-1.0781555e+00  1.0000000e+00 -1.5760765e+00 ...  1.3514163e+02\n",
      "  -6.7642227e+01  4.0863916e+02]\n",
      " [-1.0780334e+00  1.0000000e+00 -1.5761642e+00 ... -2.2393797e+02\n",
      "  -1.2348779e+02 -1.9022150e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.3851795e-01  1.0000000e+00 -1.8320694e+00 ...  2.7476303e+02\n",
      "  -2.4141238e+02 -2.3483066e+02]\n",
      " [-5.3876400e-01  1.0000000e+00 -1.8320189e+00 ... -2.2813362e+02\n",
      "  -1.9985596e+02  1.3676784e+02]\n",
      " [-5.3928185e-01  1.0000000e+00 -1.8318548e+00 ...  2.9760963e+01\n",
      "  -3.0834806e+01  1.4207611e+01]\n",
      " ...\n",
      " [-5.3914833e-01  1.0000000e+00 -1.8319197e+00 ...  1.8641285e+04\n",
      "  -3.9951833e+03  7.2372036e+03]\n",
      " [-5.3836823e-01  1.0000000e+00 -1.8321152e+00 ... -1.9566360e+01\n",
      "   1.6697478e-01  5.7446480e+01]\n",
      " [-5.3826904e-01  1.0000000e+00 -1.8321705e+00 ...  2.1257462e+02\n",
      "   3.4995184e+00  2.7877571e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.4190636e-02  1.0000000e+00 -1.9088001e+00 ...  1.5674553e+00\n",
      "   1.2904994e+02 -6.7929494e-01]\n",
      " [ 5.3932190e-02  1.0000000e+00 -1.9088335e+00 ...  3.4849207e+02\n",
      "   3.3029422e+02  5.5745599e+02]\n",
      " [ 5.3398132e-02  1.0000000e+00 -1.9088389e+00 ...  7.8796402e+01\n",
      "   3.4650229e+02 -3.5375461e+02]\n",
      " ...\n",
      " [ 5.3535461e-02  1.0000000e+00 -1.9088459e+00 ...  6.1740747e+03\n",
      "  -1.0913365e+04  6.1744141e+02]\n",
      " [ 5.4338455e-02  1.0000000e+00 -1.9088135e+00 ... -3.7094830e+01\n",
      "  -4.0680380e+00  5.0517879e+01]\n",
      " [ 5.4449081e-02  1.0000000e+00 -1.9088383e+00 ... -1.1394264e+01\n",
      "   1.3032827e+01  3.7357907e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.4148808e-01  1.0000000e+00 -1.7986259e+00 ...  1.9333029e+01\n",
      "   8.3611746e+00 -3.2710518e+01]\n",
      " [ 6.4124680e-01  1.0000000e+00 -1.7987156e+00 ... -3.8647031e+02\n",
      "   8.8209625e+02  5.6060242e+02]\n",
      " [ 6.4072037e-01  1.0000000e+00 -1.7988971e+00 ... -1.1761669e+02\n",
      "   1.3600673e+03  7.9285591e+01]\n",
      " ...\n",
      " [ 6.4084435e-01  1.0000000e+00 -1.7988644e+00 ...  6.2160608e+02\n",
      "  -1.1267341e+03  2.4373008e+03]\n",
      " [ 6.4159393e-01  1.0000000e+00 -1.7986012e+00 ... -2.3398802e+01\n",
      "   3.5076382e+01 -1.0551833e+01]\n",
      " [ 6.4173317e-01  1.0000000e+00 -1.7985764e+00 ... -1.5983225e+01\n",
      "  -5.4271652e+01  1.7300770e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1661053e+00  1.0000000e+00 -1.5122414e+00 ... -3.0259790e+01\n",
      "   8.6385864e+01  3.9782787e+01]\n",
      " [ 1.1659060e+00  1.0000000e+00 -1.5123806e+00 ... -1.4893421e+01\n",
      "  -1.7660294e+02 -3.4485049e+02]\n",
      " [ 1.1654549e+00  1.0000000e+00 -1.5127084e+00 ... -1.0235864e+03\n",
      "  -5.6914307e+02 -1.3843365e+02]\n",
      " ...\n",
      " [ 1.1655502e+00  1.0000000e+00 -1.5126400e+00 ... -2.7722839e+02\n",
      "   2.6644876e+02 -5.0170584e+02]\n",
      " [ 1.1661854e+00  1.0000000e+00 -1.5121765e+00 ...  5.8683090e+01\n",
      "   7.4412964e+01 -9.4690228e+00]\n",
      " [ 1.1663198e+00  1.0000000e+00 -1.5121212e+00 ... -2.2308838e+01\n",
      "   7.0860779e+01 -4.2898434e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5764256e+00  1.0000000e+00 -1.0777283e+00 ...  3.3414703e+02\n",
      "   4.1307285e+02 -1.1397263e+01]\n",
      " [ 1.5762749e+00  1.0000000e+00 -1.0779076e+00 ...  3.5250942e+03\n",
      "  -6.1340387e+02  3.1369009e+03]\n",
      " [ 1.5759621e+00  1.0000000e+00 -1.0783672e+00 ...  4.0928134e+02\n",
      "   2.3865916e+03  6.0579181e+02]\n",
      " ...\n",
      " [ 1.5760250e+00  1.0000000e+00 -1.0782614e+00 ...  7.8063654e+02\n",
      "   4.4122446e+02  5.1462811e+02]\n",
      " [ 1.5764599e+00  1.0000000e+00 -1.0776424e+00 ... -5.6658478e+02\n",
      "   3.5367023e+02 -3.6367676e+01]\n",
      " [ 1.5765762e+00  1.0000000e+00 -1.0775433e+00 ...  1.7771342e+03\n",
      "   2.6571955e+04 -1.0191149e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8323555e+00  1.0000000e+00 -5.3767776e-01 ... -2.9026245e+02\n",
      "  -6.4494757e+02 -6.9728455e+01]\n",
      " [ 1.8322887e+00  1.0000000e+00 -5.3791618e-01 ...  1.4211197e+04\n",
      "  -1.2376062e+03 -8.1465210e+03]\n",
      " [ 1.8321228e+00  1.0000000e+00 -5.3843093e-01 ...  9.3754555e+01\n",
      "   1.8594087e+02 -1.8635202e+02]\n",
      " ...\n",
      " [ 1.8321476e+00  1.0000000e+00 -5.3832626e-01 ... -1.0758767e+04\n",
      "   5.7877603e+03 -6.3054106e+03]\n",
      " [ 1.8323784e+00  1.0000000e+00 -5.3758049e-01 ... -1.8466760e+02\n",
      "   2.3526544e+02 -1.6730614e+02]\n",
      " [ 1.8324223e+00  1.0000000e+00 -5.3746223e-01 ... -5.7006720e+02\n",
      "  -5.2628149e+03  3.6998047e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9088354e+00  1.0000000e+00  5.4733276e-02 ...  1.0800719e+02\n",
      "   7.8938026e+01  2.0365314e+01]\n",
      " [ 1.9088478e+00  1.0000000e+00  5.4482460e-02 ... -2.9668063e+02\n",
      "   3.1590750e+03  1.3429744e+03]\n",
      " [ 1.9088440e+00  1.0000000e+00  5.3944122e-02 ... -1.6297282e+02\n",
      "   3.0482359e+00  3.8733232e+00]\n",
      " ...\n",
      " [ 1.9088230e+00  1.0000000e+00  5.4056168e-02 ... -1.7429844e+03\n",
      "   3.4710854e+03 -5.3948398e+03]\n",
      " [ 1.9088306e+00  1.0000000e+00  5.4838181e-02 ...  1.3263809e+02\n",
      "   7.5313469e+01  6.5761208e+01]\n",
      " [ 1.9088211e+00  1.0000000e+00  5.4964066e-02 ...  3.6302304e+02\n",
      "  -6.0674709e+01 -2.8458142e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.79840946e+00  1.00000000e+00  6.42131805e-01 ... -1.78492813e+02\n",
      "  -3.67432098e+02  9.66944408e+00]\n",
      " [ 1.79850388e+00  1.00000000e+00  6.41905785e-01 ... -5.49549561e+01\n",
      "   4.06687817e+03 -4.23378174e+03]\n",
      " [ 1.79868317e+00  1.00000000e+00  6.41392708e-01 ...  2.87621857e+02\n",
      "   2.25226273e+02 -2.07122925e+02]\n",
      " ...\n",
      " [ 1.79862022e+00  1.00000000e+00  6.41500473e-01 ...  1.07761772e+02\n",
      "  -3.90431671e+01 -1.46385818e+02]\n",
      " [ 1.79843903e+00  1.00000000e+00  6.42230988e-01 ... -1.19231033e+01\n",
      "  -1.10924126e+02  7.25351028e+01]\n",
      " [ 1.79831982e+00  1.00000000e+00  6.42349243e-01 ... -3.34083954e+02\n",
      "  -4.91802139e+01  2.68233826e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.51185989e+00  1.00000000e+00  1.16657829e+00 ...  4.15055298e+02\n",
      "   7.95491104e+01 -1.44265076e+02]\n",
      " [ 1.51203251e+00  1.00000000e+00  1.16640282e+00 ... -1.40943268e+02\n",
      "   1.72348779e+03 -1.36237280e+03]\n",
      " [ 1.51234627e+00  1.00000000e+00  1.16595244e+00 ... -1.04816895e+02\n",
      "  -3.91602516e+00  6.90103626e+00]\n",
      " ...\n",
      " [ 1.51223564e+00  1.00000000e+00  1.16606140e+00 ... -1.58159729e+02\n",
      "   3.49252052e+01  2.96561680e+01]\n",
      " [ 1.51186752e+00  1.00000000e+00  1.16666222e+00 ... -7.31771517e+00\n",
      "   1.80233932e+02  1.44994171e+02]\n",
      " [ 1.51170921e+00  1.00000000e+00  1.16675186e+00 ... -1.61024948e+02\n",
      "  -3.76185669e+02  1.13757774e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.1       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0775719e+00  1.0000000e+00  1.5765381e+00 ... -5.4206486e+01\n",
      "   1.0406477e+01  8.3366661e+01]\n",
      " [ 1.0777969e+00  1.0000000e+00  1.5764074e+00 ...  4.8947339e+02\n",
      "   3.9670498e+01 -5.3303497e+02]\n",
      " [ 1.0782528e+00  1.0000000e+00  1.5760943e+00 ... -6.6195648e+01\n",
      "  -3.7620087e+01 -1.2073026e+02]\n",
      " ...\n",
      " [ 1.0781231e+00  1.0000000e+00  1.5761595e+00 ... -1.8045032e+02\n",
      "   8.7926703e+02  9.4774338e+01]\n",
      " [ 1.0775948e+00  1.0000000e+00  1.5766106e+00 ... -1.3788368e+02\n",
      "   2.5079515e+01  4.2897453e+01]\n",
      " [ 1.0773668e+00  1.0000000e+00  1.5766582e+00 ... -2.7015232e+03\n",
      "  -1.5211650e+04  6.1746763e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.1       0.        0.        0.\n",
      " 0.2       0.        0.        0.9000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.3734207e-01  1.0000000e+00  1.8324242e+00 ... -1.2427430e+02\n",
      "  -8.7649948e+01 -2.0892468e+02]\n",
      " [ 5.3759193e-01  1.0000000e+00  1.8323460e+00 ... -7.8134018e+01\n",
      "  -5.9488808e+01 -2.1552325e+02]\n",
      " [ 5.3816032e-01  1.0000000e+00  1.8321961e+00 ... -4.1357307e+01\n",
      "  -1.3614790e+02  2.2795209e+02]\n",
      " ...\n",
      " [ 5.3801155e-01  1.0000000e+00  1.8322201e+00 ...  6.6734663e+03\n",
      "  -1.1029684e+03  1.3856633e+04]\n",
      " [ 5.3737450e-01  1.0000000e+00  1.8324757e+00 ... -2.1952591e+02\n",
      "   3.7796543e+01  2.5548827e+02]\n",
      " [ 5.3710461e-01  1.0000000e+00  1.8324795e+00 ... -9.9019739e+02\n",
      "   7.7767578e+02 -1.3125854e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.4996490e-02  1.0000000e+00  1.9088135e+00 ... -1.3185757e+01\n",
      "   4.6862049e+01 -2.4418467e+01]\n",
      " [-5.4735184e-02  1.0000000e+00  1.9087849e+00 ...  3.3505872e+02\n",
      "  -5.9003369e+02 -1.8190913e+02]\n",
      " [-5.4164886e-02  1.0000000e+00  1.9088070e+00 ...  6.0559076e+02\n",
      "  -4.1314908e+02  4.0812619e+02]\n",
      " ...\n",
      " [-5.4317474e-02  1.0000000e+00  1.9088001e+00 ...  4.4855881e+02\n",
      "  -6.0120593e+02 -7.6684363e+02]\n",
      " [-5.4985046e-02  1.0000000e+00  1.9088478e+00 ... -4.3173073e+01\n",
      "   6.4353889e+01 -7.9330193e+01]\n",
      " [-5.5248260e-02  1.0000000e+00  1.9088039e+00 ...  6.8243262e+02\n",
      "   4.7670006e+01 -2.4257228e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.4233971e-01  1.0000000e+00  1.7983341e+00 ...  7.1191353e+01\n",
      "  -1.3780162e+02  5.0288315e+01]\n",
      " [-6.4209652e-01  1.0000000e+00  1.7983885e+00 ... -8.3266312e+01\n",
      "  -5.3760529e+01  6.2142059e+01]\n",
      " [-6.4158249e-01  1.0000000e+00  1.7985795e+00 ... -3.0782055e+01\n",
      "  -9.6336422e+00 -1.8126839e+02]\n",
      " ...\n",
      " [-6.4172363e-01  1.0000000e+00  1.7985268e+00 ... -6.5780994e+02\n",
      "   4.2978031e+01  3.4926956e+02]\n",
      " [-6.4233208e-01  1.0000000e+00  1.7983608e+00 ...  3.3616846e+02\n",
      "  -2.6805533e+02  1.9332716e+02]\n",
      " [-6.4258003e-01  1.0000000e+00  1.7982521e+00 ... -8.5529991e+01\n",
      "  -1.7269695e+01 -4.5275049e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.166606     1.           1.5118237 ...  -61.30799     21.712898\n",
      "   -46.581734 ]\n",
      " [  -1.1664028    1.           1.5119839 ...  218.46819     84.591415\n",
      "    63.15506  ]\n",
      " [  -1.1659527    1.           1.5123203 ...   19.890213    57.912796\n",
      "   -14.346056 ]\n",
      " ...\n",
      " [  -1.1660805    1.           1.5122318 ... -771.24       710.0284\n",
      "   826.54193  ]\n",
      " [  -1.1665955    1.           1.5118427 ...   25.398182   -96.83581\n",
      "    36.820614 ]\n",
      " [  -1.1668081    1.           1.5116844 ...  -35.96734    -48.249386\n",
      "  -110.15436  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5768051e+00  1.0000000e+00  1.0768242e+00 ... -6.4595993e+01\n",
      "  -2.7552679e+02 -9.8260231e+00]\n",
      " [-1.5766554e+00  1.0000000e+00  1.0770254e+00 ...  4.7438934e+01\n",
      "   8.2097305e+01 -3.0175972e-01]\n",
      " [-1.5763187e+00  1.0000000e+00  1.0774688e+00 ...  1.1766563e+02\n",
      "  -2.3987282e+01 -2.1533058e+02]\n",
      " ...\n",
      " [-1.5764141e+00  1.0000000e+00  1.0773602e+00 ... -6.6655212e+01\n",
      "  -7.4891586e+01 -1.2871571e+02]\n",
      " [-1.5767593e+00  1.0000000e+00  1.0768394e+00 ... -4.0858572e+03\n",
      "   1.3871382e+04 -8.4731221e+03]\n",
      " [-1.5769463e+00  1.0000000e+00  1.0766449e+00 ...  3.6261285e+02\n",
      "  -2.2767786e+01  2.6361664e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8323975e+00  1.0000000e+00  5.3665924e-01 ... -2.6587408e+01\n",
      "   9.0280899e+01  5.9728134e+01]\n",
      " [-1.8323412e+00  1.0000000e+00  5.3686047e-01 ... -2.1732811e+02\n",
      "  -1.2991675e+02 -4.4538266e+02]\n",
      " [-1.8321419e+00  1.0000000e+00  5.3737932e-01 ...  2.7849530e+02\n",
      "  -2.0111618e+02  4.7060593e+02]\n",
      " ...\n",
      " [-1.8321819e+00  1.0000000e+00  5.3724289e-01 ...  1.5015023e+01\n",
      "  -3.3307257e+02  2.7571936e+02]\n",
      " [-1.8323460e+00  1.0000000e+00  5.3667068e-01 ...  5.2998004e+02\n",
      "   5.5787254e+01  2.3565056e+03]\n",
      " [-1.8324738e+00  1.0000000e+00  5.3644943e-01 ...  5.5358530e+03\n",
      "   1.1775931e+03  4.4879514e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.1       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90858650e+00  1.00000000e+00 -5.59692383e-02 ...  1.46474657e+01\n",
      "  -1.73618790e+02 -5.55230522e+01]\n",
      " [-1.90860748e+00  1.00000000e+00 -5.57403564e-02 ... -3.21971558e+03\n",
      "   1.36293060e+02 -2.27997583e+03]\n",
      " [-1.90860367e+00  1.00000000e+00 -5.52058071e-02 ... -4.10664177e+01\n",
      "   3.32485657e+02  2.06354663e+03]\n",
      " ...\n",
      " [-1.90860367e+00  1.00000000e+00 -5.53379059e-02 ...  6.19748718e+02\n",
      "  -3.83489044e+02  1.29914398e+02]\n",
      " [-1.90856171e+00  1.00000000e+00 -5.59577942e-02 ...  1.31981262e+03\n",
      "  -4.43157568e+03 -1.26348526e+02]\n",
      " [-1.90859032e+00  1.00000000e+00 -5.61943054e-02 ...  4.97910742e+03\n",
      "  -8.82453247e+02  1.47455969e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79792404e+00  1.00000000e+00 -6.43039703e-01 ...  1.18641022e+02\n",
      "  -1.08852455e+02  1.49171799e+02]\n",
      " [-1.79801369e+00  1.00000000e+00 -6.42821312e-01 ...  4.35046158e+01\n",
      "  -4.07179070e+01  5.24398613e+01]\n",
      " [-1.79819298e+00  1.00000000e+00 -6.42324209e-01 ... -4.33991280e+01\n",
      "  -4.39529724e+01 -2.47749634e+01]\n",
      " ...\n",
      " [-1.79815292e+00  1.00000000e+00 -6.42446518e-01 ... -1.01582892e+03\n",
      "  -2.96404358e+02 -5.98173279e+02]\n",
      " [-1.79793549e+00  1.00000000e+00 -6.43026352e-01 ... -2.03225998e+02\n",
      "   5.53037659e+02  4.78326599e+02]\n",
      " [-1.79785538e+00  1.00000000e+00 -6.43249512e-01 ... -5.45988846e+00\n",
      "  -3.92803070e+02 -2.44021271e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5111074e+00  1.0000000e+00 -1.1674366e+00 ...  3.6015717e+01\n",
      "  -1.2385798e+02  3.2667263e+01]\n",
      " [-1.5112572e+00  1.0000000e+00 -1.1672182e+00 ... -1.3630607e+03\n",
      "   5.1126883e+02 -3.1970190e+03]\n",
      " [-1.5115547e+00  1.0000000e+00 -1.1668186e+00 ... -8.2347031e+01\n",
      "   3.0750549e+02 -1.1918155e+02]\n",
      " ...\n",
      " [-1.5114822e+00  1.0000000e+00 -1.1669130e+00 ... -2.3748022e+02\n",
      "   1.3908481e+02 -4.6103405e+01]\n",
      " [-1.5110970e+00  1.0000000e+00 -1.1674232e+00 ... -3.4870660e+02\n",
      "  -4.8326355e+01  4.9182629e+01]\n",
      " [-1.5109835e+00  1.0000000e+00 -1.1675968e+00 ...  2.0232420e+01\n",
      "  -1.6728925e+02 -3.1330687e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0761395    1.          -1.5771809 ...  824.80896    545.10974\n",
      "   447.52768  ]\n",
      " [  -1.076355     1.          -1.5770454 ...  314.5136      47.78015\n",
      "   322.26715  ]\n",
      " [  -1.0767956    1.          -1.5767534 ...  189.11774     93.101845\n",
      "   158.33437  ]\n",
      " ...\n",
      " [  -1.0767002    1.          -1.5768194 ...  570.94476   -873.44\n",
      "  -260.68875  ]\n",
      " [  -1.0761681    1.          -1.5771675 ...  -26.618431  -227.0339\n",
      "   -17.489538 ]\n",
      " [  -1.0759706    1.          -1.5773029 ...  268.9927     -91.30609\n",
      "    95.372955 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.3590584e-01  1.0000000e+00 -1.8326778e+00 ... -2.4270461e+03\n",
      "   1.3408671e+03  1.6480338e+03]\n",
      " [-5.3615570e-01  1.0000000e+00 -1.8326206e+00 ... -3.3664978e+02\n",
      "  -1.9022400e+02  2.3788353e+02]\n",
      " [-5.3664017e-01  1.0000000e+00 -1.8324696e+00 ...  1.1557198e+01\n",
      "  -2.6902178e+01  5.7829094e+00]\n",
      " ...\n",
      " [-5.3652191e-01  1.0000000e+00 -1.8324976e+00 ...  5.8328390e+00\n",
      "   1.7623716e+01  1.0139025e+01]\n",
      " [-5.3590775e-01  1.0000000e+00 -1.8326664e+00 ... -7.1297882e+02\n",
      "  -9.8983356e+02  8.4307381e+01]\n",
      " [-5.3571892e-01  1.0000000e+00 -1.8327312e+00 ... -1.2363563e+01\n",
      "  -5.2842407e+01  6.9178734e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.65147400e-02  1.00000000e+00 -1.90868378e+00 ...  2.40919434e+02\n",
      "   3.43409851e+02 -2.37358063e+02]\n",
      " [ 5.62553406e-02  1.00000000e+00 -1.90867615e+00 ... -9.68176956e+01\n",
      "   1.01216934e+02  1.19612541e+02]\n",
      " [ 5.56983948e-02  1.00000000e+00 -1.90870309e+00 ...  3.17200958e+02\n",
      "   5.78775879e+02 -2.58302185e+02]\n",
      " ...\n",
      " [ 5.58300018e-02  1.00000000e+00 -1.90866852e+00 ...  7.90902893e+02\n",
      "   1.46965857e+03 -2.79968140e+03]\n",
      " [ 5.64785004e-02  1.00000000e+00 -1.90867424e+00 ...  1.96526245e+02\n",
      "  -1.59266571e+02 -4.85898094e+01]\n",
      " [ 5.67073822e-02  1.00000000e+00 -1.90865707e+00 ...  2.14383469e+01\n",
      "  -8.98039532e+00  2.08231030e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.64398766    1.           -1.7977467  ...   -5.0930843\n",
      "    19.135294     12.375708  ]\n",
      " [   0.6437435     1.           -1.7977791  ...  230.12749\n",
      "    44.451397    410.7812    ]\n",
      " [   0.6431942     1.           -1.797974   ...  339.54297\n",
      "   -32.439278   -419.26788   ]\n",
      " ...\n",
      " [   0.643322      1.           -1.7978907  ... -261.124\n",
      "  -294.85446     -70.187     ]\n",
      " [   0.64393425    1.           -1.7977428  ...  221.15926\n",
      "  -192.20662    -220.69972   ]\n",
      " [   0.6441612     1.           -1.7976398  ...   13.4830675\n",
      "    84.89419     -23.0518    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1679544    1.          -1.5108776 ...  169.76886    -10.206534\n",
      "    68.70303  ]\n",
      " [   1.1677504    1.          -1.5110083 ... -103.00616    542.92914\n",
      "   165.25917  ]\n",
      " [   1.1672554    1.          -1.5113373 ...    9.1647835  -19.635784\n",
      "    24.696537 ]\n",
      " ...\n",
      " [   1.1673622    1.          -1.5112286 ... -521.9604    -217.19331\n",
      "   -17.9225   ]\n",
      " [   1.1678829    1.          -1.5108776 ... -169.32619    -78.32376\n",
      "   -50.965935 ]\n",
      " [   1.1680984    1.          -1.5107117 ...   70.75196    -88.56283\n",
      "    71.04341  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.57765198e+00  1.00000000e+00 -1.07615471e+00 ...  5.56330719e+01\n",
      "   9.54940796e+00  1.08419189e+01]\n",
      " [ 1.57750416e+00  1.00000000e+00 -1.07633018e+00 ...  3.26356659e+01\n",
      "  -1.06891914e+02 -3.69533813e+02]\n",
      " [ 1.57711792e+00  1.00000000e+00 -1.07678819e+00 ...  8.07344589e+01\n",
      "  -1.05287254e+02 -4.75342484e+01]\n",
      " ...\n",
      " [ 1.57719421e+00  1.00000000e+00 -1.07664967e+00 ...  4.67979126e+02\n",
      "   1.14743225e+03  2.13687485e+02]\n",
      " [ 1.57759476e+00  1.00000000e+00 -1.07615471e+00 ...  2.83044109e+01\n",
      "   1.75834915e+02  7.11963867e+02]\n",
      " [ 1.57774448e+00  1.00000000e+00 -1.07594681e+00 ...  2.17952362e+02\n",
      "   1.07005020e+02  1.85081329e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8330917e+00  1.0000000e+00 -5.3565979e-01 ...  1.4565280e+01\n",
      "   3.2636902e+00  1.1105058e+02]\n",
      " [ 1.8330107e+00  1.0000000e+00 -5.3583908e-01 ... -4.5891758e+03\n",
      "   5.7552163e+03  3.9522686e+03]\n",
      " [ 1.8327751e+00  1.0000000e+00 -5.3638643e-01 ... -9.2364587e+02\n",
      "   1.3489200e+02 -7.1067920e+02]\n",
      " ...\n",
      " [ 1.8327961e+00  1.0000000e+00 -5.3622055e-01 ...  5.8380577e+01\n",
      "  -4.2766840e+02 -6.6276666e+02]\n",
      " [ 1.8330135e+00  1.0000000e+00 -5.3566170e-01 ...  6.9788307e+01\n",
      "  -1.6660455e+03  4.1334686e+02]\n",
      " [ 1.8331079e+00  1.0000000e+00 -5.3540802e-01 ... -4.9182289e+01\n",
      "  -1.0725249e+02  1.6844194e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.6       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.8000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9089251e+00  1.0000000e+00  5.6976318e-02 ... -8.4321365e+00\n",
      "   7.4324562e+01 -1.6992236e+02]\n",
      " [ 1.9089270e+00  1.0000000e+00  5.6798935e-02 ...  1.0806512e+03\n",
      "   7.6773578e+02 -7.7644000e+02]\n",
      " [ 1.9088459e+00  1.0000000e+00  5.6227151e-02 ... -6.6525031e+02\n",
      "   6.6390173e+02  7.3807037e+02]\n",
      " ...\n",
      " [ 1.9088230e+00  1.0000000e+00  5.6404114e-02 ... -6.2823100e+00\n",
      "  -1.4534909e+02  2.0776623e+01]\n",
      " [ 1.9088306e+00  1.0000000e+00  5.6972504e-02 ...  7.5549173e-01\n",
      "  -2.6094098e+02  7.1871513e+01]\n",
      " [ 1.9088678e+00  1.0000000e+00  5.7237625e-02 ...  7.6353043e+01\n",
      "  -3.6107044e+01 -8.5807137e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7976112e+00  1.0000000e+00  6.4400101e-01 ... -1.4137581e+01\n",
      "   1.1826390e+02  1.7032095e+02]\n",
      " [ 1.7976866e+00  1.0000000e+00  6.4384079e-01 ...  1.0720531e+03\n",
      "  -1.1767899e+03  1.9100566e+02]\n",
      " [ 1.7977409e+00  1.0000000e+00  6.4329267e-01 ... -3.1424809e-01\n",
      "  -8.6198769e+00 -1.0442768e+01]\n",
      " ...\n",
      " [ 1.7976742e+00  1.0000000e+00  6.4346790e-01 ... -1.6173318e+02\n",
      "   6.0600487e+01  3.9918961e+01]\n",
      " [ 1.7974911e+00  1.0000000e+00  6.4399910e-01 ...  1.6904410e+02\n",
      "   4.2643964e+02 -1.3774565e+03]\n",
      " [ 1.7974901e+00  1.0000000e+00  6.4425659e-01 ...  2.4545795e+02\n",
      "   3.4408691e+02  4.3816171e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         1.0000001  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70000005 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5104647e+00  1.0000000e+00  1.1682129e+00 ... -5.4821838e+02\n",
      "   6.6594531e+02 -1.1744099e+03]\n",
      " [ 1.5105953e+00  1.0000000e+00  1.1680889e+00 ... -4.2240930e+00\n",
      "   3.8357867e+02  5.1237354e+02]\n",
      " [ 1.5108147e+00  1.0000000e+00  1.1676198e+00 ...  3.0534920e+01\n",
      "  -1.8366493e+02  5.2116013e+01]\n",
      " ...\n",
      " [ 1.5106869e+00  1.0000000e+00  1.1677828e+00 ... -1.2673273e+02\n",
      "  -1.4399904e+02  3.5831442e+02]\n",
      " [ 1.5103550e+00  1.0000000e+00  1.1682129e+00 ...  3.3968625e+02\n",
      "   6.6098526e+01  2.0542816e+02]\n",
      " [ 1.5102835e+00  1.0000000e+00  1.1684361e+00 ...  3.1678522e+02\n",
      "   5.2953075e+01 -4.5216168e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0756369    1.           1.5777588 ... -179.67102    196.04488\n",
      "   319.9379   ]\n",
      " [   1.0758123    1.           1.5776882 ...   35.273994   -42.14318\n",
      "    -3.2047591]\n",
      " [   1.0761509    1.           1.5773468 ... -300.8763    -349.13248\n",
      "   509.2888   ]\n",
      " ...\n",
      " [   1.0759792    1.           1.5774717 ... -127.53179      4.148388\n",
      "  -165.0731   ]\n",
      " [   1.0755253    1.           1.5777588 ... -123.73406   -412.24127\n",
      "   -30.062916 ]\n",
      " [   1.0753956    1.           1.5779247 ...   76.3952      55.755558\n",
      "   -24.019398 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.35134315e-01  1.00000000e+00  1.83302879e+00 ... -1.64120255e+02\n",
      "   6.81425247e+01  1.00958824e-01]\n",
      " [ 5.35337448e-01  1.00000000e+00  1.83300877e+00 ...  9.79272156e+01\n",
      "  -1.43996017e+02  5.86090050e+01]\n",
      " [ 5.35730362e-01  1.00000000e+00  1.83282709e+00 ...  4.98769257e+02\n",
      "   7.62205048e+01 -4.74736786e+02]\n",
      " ...\n",
      " [ 5.35545349e-01  1.00000000e+00  1.83288574e+00 ...  8.99736252e+01\n",
      "   2.58098698e+01 -2.74492760e+01]\n",
      " [ 5.34984589e-01  1.00000000e+00  1.83301926e+00 ... -1.23817917e+02\n",
      "   2.39923843e+02  6.26325302e+01]\n",
      " [ 5.34858704e-01  1.00000000e+00  1.83310509e+00 ... -4.32380463e+02\n",
      "   4.20355811e+03  1.50097412e+04]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.77583313e-02  1.00000000e+00  1.90859032e+00 ... -4.57742405e+00\n",
      "  -6.42495117e+01  1.64874115e+01]\n",
      " [-5.75437546e-02  1.00000000e+00  1.90862751e+00 ...  1.78043182e+02\n",
      "  -3.57510681e+01  2.72216125e+02]\n",
      " [-5.71002960e-02  1.00000000e+00  1.90862811e+00 ...  1.15202942e+01\n",
      "  -1.11107414e+02 -2.69901337e+02]\n",
      " ...\n",
      " [-5.72986603e-02  1.00000000e+00  1.90863037e+00 ...  5.25477066e+01\n",
      "   5.55643425e+01  1.01495613e+02]\n",
      " [-5.78804016e-02  1.00000000e+00  1.90856934e+00 ...  1.30018112e+02\n",
      "   1.28758948e+03  8.87370361e+02]\n",
      " [-5.80444336e-02  1.00000000e+00  1.90859032e+00 ...  1.00080481e+03\n",
      "  -6.99697144e+02  1.66829822e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.64480686    1.            1.7973442  ... -106.46717\n",
      "  -206.96184     -89.932335  ]\n",
      " [  -0.6446028     1.            1.797411   ... -334.3285\n",
      "    -0.76976776 -367.34534   ]\n",
      " [  -0.6441822     1.            1.7975777  ...  -28.759693\n",
      "    89.51284       3.3385296 ]\n",
      " ...\n",
      " [  -0.6443691     1.            1.7975187  ...   66.14462\n",
      "    20.989061    -33.91254   ]\n",
      " [  -0.64491653    1.            1.7973061  ... -515.202\n",
      "  -368.12515     -54.767925  ]\n",
      " [  -0.64507675    1.            1.7972469  ...  124.22909\n",
      "  -299.80222     414.87262   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.16852665e+00  1.00000000e+00  1.50996971e+00 ... -1.93636856e+01\n",
      "  -2.78364964e+01 -1.46093416e+00]\n",
      " [-1.16835785e+00  1.00000000e+00  1.51007366e+00 ...  1.42203711e+03\n",
      "  -1.13971509e+03  2.03942517e+03]\n",
      " [-1.16799927e+00  1.00000000e+00  1.51038945e+00 ... -1.49625416e+01\n",
      "   9.42583618e+01 -6.04700317e+01]\n",
      " ...\n",
      " [-1.16815948e+00  1.00000000e+00  1.51027393e+00 ... -2.32838917e+00\n",
      "   4.99952278e+01 -1.67259323e+02]\n",
      " [-1.16860580e+00  1.00000000e+00  1.50992012e+00 ... -4.05910126e+02\n",
      "   8.23069611e+01 -4.70848274e+01]\n",
      " [-1.16876030e+00  1.00000000e+00  1.50979233e+00 ... -6.48100586e+01\n",
      "  -1.06389854e+02 -2.90453278e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.57792282e+00  1.00000000e+00  1.07475662e+00 ...  4.10653610e+01\n",
      "   2.39134979e+02 -2.23277283e+02]\n",
      " [-1.57779121e+00  1.00000000e+00  1.07493782e+00 ... -2.45740921e+02\n",
      "   1.24869995e+02  1.21818634e+02]\n",
      " [-1.57750511e+00  1.00000000e+00  1.07536519e+00 ...  8.08257080e+02\n",
      "   7.91928894e+02 -1.00482050e+03]\n",
      " ...\n",
      " [-1.57762337e+00  1.00000000e+00  1.07521534e+00 ...  2.89441166e+01\n",
      "  -3.05297699e+01  8.04251862e+01]\n",
      " [-1.57794762e+00  1.00000000e+00  1.07469559e+00 ...  1.65628242e+01\n",
      "   6.78044434e+01  1.44346405e+02]\n",
      " [-1.57809353e+00  1.00000000e+00  1.07454681e+00 ...  1.51115685e+01\n",
      "   7.70727997e+01 -2.32549047e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8329506e+00  1.0000000e+00  5.3444672e-01 ...  2.9646294e+01\n",
      "  -5.6884766e+03  2.3177551e+03]\n",
      " [-1.8328733e+00  1.0000000e+00  5.3463268e-01 ... -4.9968021e+02\n",
      "  -1.5068901e+02  7.2306290e+01]\n",
      " [-1.8327484e+00  1.0000000e+00  5.3514200e-01 ... -3.6791010e+02\n",
      "   5.3338403e+02 -2.5294319e+02]\n",
      " ...\n",
      " [-1.8328018e+00  1.0000000e+00  5.3495693e-01 ...  2.4501667e+01\n",
      "   1.1565067e+02  1.6901773e+01]\n",
      " [-1.8329391e+00  1.0000000e+00  5.3438187e-01 ... -4.8322800e+01\n",
      "   1.7538181e+01  2.5314125e+01]\n",
      " [-1.8330517e+00  1.0000000e+00  5.3420830e-01 ...  4.0496708e+01\n",
      "   2.8560616e+01 -3.1716789e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9085073e+00  1.0000000e+00 -5.8496475e-02 ...  7.3091199e+02\n",
      "  -2.1765684e+03  2.5958928e+03]\n",
      " [-1.9084864e+00  1.0000000e+00 -5.8298111e-02 ...  5.6664941e+02\n",
      "  -1.3136081e+02  7.6225543e+02]\n",
      " [-1.9085159e+00  1.0000000e+00 -5.7767864e-02 ... -1.0484297e+02\n",
      "   1.1602176e+02 -1.0298665e+02]\n",
      " ...\n",
      " [-1.9085045e+00  1.0000000e+00 -5.7959557e-02 ...  9.6795471e+01\n",
      "  -1.3382169e+02  1.2547104e+02]\n",
      " [-1.9084511e+00  1.0000000e+00 -5.8565140e-02 ... -1.0545750e+02\n",
      "  -1.0307637e+01  3.5638437e+00]\n",
      " [-1.9085274e+00  1.0000000e+00 -5.8744431e-02 ... -1.4386134e+02\n",
      "  -1.4796650e+00 -1.3194051e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7970371e+00  1.0000000e+00 -6.4575005e-01 ...  3.2076421e+00\n",
      "  -6.8725946e+02 -8.5982399e+01]\n",
      " [-1.7970753e+00  1.0000000e+00 -6.4552689e-01 ...  1.1210743e+02\n",
      "   2.8957748e+02 -3.0089719e+02]\n",
      " [-1.7972317e+00  1.0000000e+00 -6.4503545e-01 ... -3.3902428e+01\n",
      "  -4.0019516e+02 -3.2041473e+02]\n",
      " ...\n",
      " [-1.7971649e+00  1.0000000e+00 -6.4521599e-01 ... -6.1448059e+02\n",
      "   2.3864886e+02 -6.8557381e+01]\n",
      " [-1.7969208e+00  1.0000000e+00 -6.4581871e-01 ...  2.5646933e+01\n",
      "   4.1436272e+01  1.7639708e+01]\n",
      " [-1.7969685e+00  1.0000000e+00 -6.4598846e-01 ...  1.8996809e+01\n",
      "   1.3845921e+02 -3.9703122e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5097122e+00  1.0000000e+00 -1.1694260e+00 ... -1.1503789e+03\n",
      "  -1.0559950e+03 -1.0247272e+03]\n",
      " [-1.5098057e+00  1.0000000e+00 -1.1692581e+00 ... -3.2009314e+02\n",
      "  -1.6666379e+00  8.2849407e-01]\n",
      " [-1.5101166e+00  1.0000000e+00 -1.1688325e+00 ...  1.1411640e+01\n",
      "   6.3793274e+01  6.4231796e+01]\n",
      " ...\n",
      " [-1.5100002e+00  1.0000000e+00 -1.1689901e+00 ... -4.7725171e+02\n",
      "   5.8000568e+02  2.1640742e+02]\n",
      " [-1.5095882e+00  1.0000000e+00 -1.1694889e+00 ...  1.7732899e+01\n",
      "  -1.9352518e+01  3.6526947e+02]\n",
      " [-1.5095654e+00  1.0000000e+00 -1.1696281e+00 ... -1.4316827e+02\n",
      "   8.1175339e+01  8.4723480e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0743523    1.          -1.5788364 ... -778.3035    -208.19989\n",
      "   154.97896  ]\n",
      " [  -1.0744839    1.          -1.5787611 ...  246.06323    -79.566055\n",
      "   -44.172993 ]\n",
      " [  -1.0749264    1.          -1.5784465 ...  202.63954     45.806477\n",
      "  -345.88113  ]\n",
      " ...\n",
      " [  -1.07477      1.          -1.5785675 ...   52.47857   -115.89094\n",
      "    35.926212 ]\n",
      " [  -1.0742474    1.          -1.5788727 ... -133.36842    167.10837\n",
      "    74.86275  ]\n",
      " [  -1.074151     1.          -1.5789642 ...   53.59992    211.58423\n",
      "   -70.47896  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.5338154     1.           -1.8336105  ... -208.33598\n",
      "  -117.36775     -31.92112   ]\n",
      " [  -0.53397083    1.           -1.8335953  ... -243.20926\n",
      "   -51.503212    -41.17266   ]\n",
      " [  -0.5344734     1.           -1.8334212  ...  106.30288\n",
      "  -218.30142      40.21132   ]\n",
      " ...\n",
      " [  -0.5342941     1.           -1.8334894  ... -194.23146\n",
      "    17.711891    130.6357    ]\n",
      " [  -0.53370476    1.           -1.8336277  ...  168.5452\n",
      "    24.340343     35.381855  ]\n",
      " [  -0.5335808     1.           -1.8336697  ...  -91.73448\n",
      "   -93.86318      27.897884  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.8947563e-02  1.0000000e+00 -1.9088421e+00 ...  1.5270784e+02\n",
      "  -4.8883496e+02 -4.8768488e+02]\n",
      " [ 5.8785439e-02  1.0000000e+00 -1.9088840e+00 ...  3.2712929e+01\n",
      "  -5.0242744e+01  2.5804438e+01]\n",
      " [ 5.8267593e-02  1.0000000e+00 -1.9088720e+00 ... -8.9175224e+01\n",
      "  -7.2538780e+01  1.8587095e+01]\n",
      " ...\n",
      " [ 5.8458328e-02  1.0000000e+00 -1.9088783e+00 ... -7.4303650e+01\n",
      "   7.2869148e+00 -1.4923143e+01]\n",
      " [ 5.9070587e-02  1.0000000e+00 -1.9088268e+00 ...  3.5739646e+00\n",
      "   1.8182716e+01  6.1376147e+00]\n",
      " [ 5.9191704e-02  1.0000000e+00 -1.9088192e+00 ...  9.4892555e+01\n",
      "   3.5362595e+01  3.6142670e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.6460705     1.           -1.7971249  ...   29.361567\n",
      "  -522.4011     -200.66304   ]\n",
      " [   0.645916      1.           -1.7972546  ...  281.1997\n",
      "   108.098976    263.3932    ]\n",
      " [   0.64539146    1.           -1.79741    ... -312.49634\n",
      "  -209.83665      80.84195   ]\n",
      " ...\n",
      " [   0.64557457    1.           -1.7973471  ...  128.97305\n",
      "  -343.1592      141.41566   ]\n",
      " [   0.64616394    1.           -1.7970829  ...  156.24812\n",
      "    62.765335     56.359413  ]\n",
      " [   0.6463022     1.           -1.7970371  ...   46.985214\n",
      "   -46.390324     94.76279   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1697435e+00  1.0000000e+00 -1.5095711e+00 ...  1.5758205e+02\n",
      "  -1.8996246e+02  2.0621332e+02]\n",
      " [ 1.1696215e+00  1.0000000e+00 -1.5097475e+00 ... -5.1345367e+02\n",
      "  -7.5828290e+02 -2.0539585e+03]\n",
      " [ 1.1691284e+00  1.0000000e+00 -1.5100360e+00 ... -1.6295396e+02\n",
      "   4.9064185e+02 -9.6289616e+00]\n",
      " ...\n",
      " [ 1.1692982e+00  1.0000000e+00 -1.5099268e+00 ...  1.4003831e+01\n",
      "   6.8337641e+00 -2.7611356e+00]\n",
      " [ 1.1697979e+00  1.0000000e+00 -1.5095024e+00 ... -2.0267696e+00\n",
      "  -2.0145903e+01  4.1827286e+02]\n",
      " [ 1.1699333e+00  1.0000000e+00 -1.5094109e+00 ...  5.2705345e+01\n",
      "   7.8567520e+01  3.0396906e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.578927     1.          -1.0739822 ...  578.17535    105.682396\n",
      "   -70.47448  ]\n",
      " [   1.578846     1.          -1.0742035 ... -206.24979   -883.7098\n",
      "   223.77437  ]\n",
      " [   1.578434     1.          -1.0746057 ... -178.58511   -347.27725\n",
      "  -119.51982  ]\n",
      " ...\n",
      " [   1.5785675    1.          -1.07446   ...  -37.117477    61.150536\n",
      "  -128.56818  ]\n",
      " [   1.5789413    1.          -1.0738907 ...  -37.98569    -44.916775\n",
      "   -17.859882 ]\n",
      " [   1.5790386    1.          -1.0737686 ...    6.8203797  -14.848861\n",
      "     4.974583 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8335524e+00  1.0000000e+00 -5.3342056e-01 ... -4.3643219e+01\n",
      "   1.8547922e+02  7.4482101e+01]\n",
      " [ 1.8335333e+00  1.0000000e+00 -5.3365135e-01 ...  4.0475757e+02\n",
      "  -1.0371621e+02  2.2821855e+02]\n",
      " [ 1.8332958e+00  1.0000000e+00 -5.3413212e-01 ... -1.1760998e+02\n",
      "  -1.2320731e+02 -3.1934982e+01]\n",
      " ...\n",
      " [ 1.8333626e+00  1.0000000e+00 -5.3394985e-01 ... -2.6914695e+03\n",
      "  -7.5239569e+02 -2.6862278e+03]\n",
      " [ 1.8335419e+00  1.0000000e+00 -5.3331757e-01 ...  1.2843712e+02\n",
      "   5.4818187e+00 -1.4524535e+02]\n",
      " [ 1.8335953e+00  1.0000000e+00 -5.3317070e-01 ... -9.3127899e+01\n",
      "   5.0627991e+01 -1.1256003e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9086514e+00  1.0000000e+00  5.9064865e-02 ... -9.1984039e+01\n",
      "  -4.0580719e+01  5.3438286e+01]\n",
      " [ 1.9086895e+00  1.0000000e+00  5.8850288e-02 ...  1.2282646e+02\n",
      "  -1.4436459e+02 -3.9729633e+02]\n",
      " [ 1.9086246e+00  1.0000000e+00  5.8358103e-02 ...  7.2105961e+00\n",
      "  -2.7417871e+02  9.0795544e+02]\n",
      " ...\n",
      " [ 1.9086208e+00  1.0000000e+00  5.8538437e-02 ...  5.6549377e+02\n",
      "  -1.8503288e+02  2.1412689e+02]\n",
      " [ 1.9086113e+00  1.0000000e+00  5.9175491e-02 ...  5.4973512e+00\n",
      "  -3.6157398e+01 -2.2462072e+01]\n",
      " [ 1.9086151e+00  1.0000000e+00  5.9328079e-02 ... -3.4709229e+01\n",
      "   1.9586734e+02 -3.2574867e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7965860e+00  1.0000000e+00  6.4635468e-01 ... -3.1109354e+01\n",
      "   1.4120456e+02  8.6434555e+01]\n",
      " [ 1.7966928e+00  1.0000000e+00  6.4620781e-01 ... -5.3226900e-01\n",
      "   9.0102364e+01 -1.0810952e+02]\n",
      " [ 1.7968025e+00  1.0000000e+00  6.4574581e-01 ...  1.1503168e+02\n",
      "   1.0297034e+02  1.7423672e+02]\n",
      " ...\n",
      " [ 1.7967472e+00  1.0000000e+00  6.4591312e-01 ...  2.9109909e+02\n",
      "   2.6206229e+02  1.3153874e+02]\n",
      " [ 1.7965260e+00  1.0000000e+00  6.4646339e-01 ... -9.6922607e+01\n",
      "  -1.5587642e+02  1.3202664e+02]\n",
      " [ 1.7964668e+00  1.0000000e+00  6.4661026e-01 ... -1.4235031e+02\n",
      "  -6.6817572e+02  5.7929089e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.508771     1.           1.170105  ... -359.45276   -358.7081\n",
      "   329.31284  ]\n",
      " [   1.5089359    1.           1.1699896 ...  -29.092592   -52.92259\n",
      "   -69.23534  ]\n",
      " [   1.5092297    1.           1.1696002 ...  -66.11857    -35.25846\n",
      "   -53.333942 ]\n",
      " ...\n",
      " [   1.509121     1.           1.1697311 ...  -74.82574    235.2859\n",
      "   471.04633  ]\n",
      " [   1.5086937    1.           1.1702023 ... -456.7282    -812.32654\n",
      "   283.48486  ]\n",
      " [   1.5085878    1.           1.1703243 ...   51.232063   -96.26926\n",
      "    49.072433 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0734930e+00  1.0000000e+00  1.5789986e+00 ... -2.6513126e+01\n",
      "   2.9697424e+01 -1.3181718e+02]\n",
      " [ 1.0736990e+00  1.0000000e+00  1.5789375e+00 ...  2.8166515e+01\n",
      "   2.2221606e+00 -2.0195324e+01]\n",
      " [ 1.0741119e+00  1.0000000e+00  1.5786731e+00 ... -7.6662018e+01\n",
      "   1.2958521e+01  7.7915054e+01]\n",
      " ...\n",
      " [ 1.0739632e+00  1.0000000e+00  1.5787477e+00 ... -8.6819489e+01\n",
      "  -8.9109926e+00 -6.2093730e+00]\n",
      " [ 1.0733871e+00  1.0000000e+00  1.5790768e+00 ... -2.6409698e-01\n",
      "  -3.4056683e+01  7.3856506e+00]\n",
      " [ 1.0732613e+00  1.0000000e+00  1.5791683e+00 ...  7.7144116e+02\n",
      "  -5.1592192e+03  2.2820112e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.3278255e-01  1.0000000e+00  1.8334122e+00 ...  4.8976307e+01\n",
      "   2.0591351e+01 -4.6764664e+01]\n",
      " [ 5.3301239e-01  1.0000000e+00  1.8333979e+00 ... -3.5662323e+01\n",
      "  -3.8783574e+00 -2.0572237e+01]\n",
      " [ 5.3344345e-01  1.0000000e+00  1.8332695e+00 ...  3.6762964e+02\n",
      "   5.4230353e+02 -2.4962218e+02]\n",
      " ...\n",
      " [ 5.3326797e-01  1.0000000e+00  1.8332853e+00 ... -3.4354324e+01\n",
      "  -8.0354538e+01  7.8228844e+01]\n",
      " [ 5.3261375e-01  1.0000000e+00  1.8334522e+00 ...  1.2560963e+01\n",
      "  -6.3575768e+01  8.9462585e+01]\n",
      " [ 5.3251743e-01  1.0000000e+00  1.8334846e+00 ...  1.1851797e+03\n",
      "  -4.9845233e+02  2.7885952e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.02436066e-02  1.00000000e+00  1.90811348e+00 ...  1.61311588e+01\n",
      "  -5.18786469e+01 -6.06320381e+01]\n",
      " [-6.00042343e-02  1.00000000e+00  1.90819359e+00 ... -3.11966797e+02\n",
      "  -1.38088150e+02 -1.03120288e+03]\n",
      " [-5.96160889e-02  1.00000000e+00  1.90820432e+00 ... -8.44084930e+00\n",
      "  -2.64041367e+01  1.43061285e+01]\n",
      " ...\n",
      " [-5.98011017e-02  1.00000000e+00  1.90817928e+00 ... -1.42055023e+02\n",
      "   3.35205475e+02  2.23385574e+02]\n",
      " [-6.04877472e-02  1.00000000e+00  1.90812492e+00 ...  4.53358536e+01\n",
      "  -1.57389221e+02  1.86289513e+00]\n",
      " [-6.05173111e-02  1.00000000e+00  1.90812302e+00 ...  6.06279602e+02\n",
      "   1.37336316e+03 -9.45585266e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.64696217    1.            1.7962399  ...   76.38897\n",
      "   220.17679      78.4952    ]\n",
      " [  -0.64673615    1.            1.7963495  ...  301.5013\n",
      "  -128.73672    -315.46918   ]\n",
      " [  -0.6463947     1.            1.7965237  ...   27.373026\n",
      "   202.18684      53.531025  ]\n",
      " ...\n",
      " [  -0.6465664     1.            1.7964268  ...  497.08755\n",
      "   148.6251     -361.21738   ]\n",
      " [  -0.6472111     1.            1.7962189  ...   25.32123\n",
      "  -330.0982     -223.39299   ]\n",
      " [  -0.6472225     1.            1.796154   ... -101.4511\n",
      "   198.79398      49.594456  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1708841    1.           1.5081329 ...   86.68185   -131.11104\n",
      "    85.2072   ]\n",
      " [  -1.1706829    1.           1.5083294 ...   67.91368    -38.705257\n",
      "   -33.928413 ]\n",
      " [  -1.1704521    1.           1.5086263 ...  -36.830025    60.50072\n",
      "   -13.037908 ]\n",
      " ...\n",
      " [  -1.1705894    1.           1.5084801 ...  -21.307102    80.62004\n",
      "  -222.94269  ]\n",
      " [  -1.1711426    1.           1.50807   ...  138.30203   -119.05536\n",
      "   188.85336  ]\n",
      " [  -1.1710949    1.           1.5079765 ...   15.724775   136.68568\n",
      "   -12.523085 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.57960796e+00  1.00000000e+00  1.07226944e+00 ...  4.02153664e+01\n",
      "  -1.05277002e+03 -4.10648865e+02]\n",
      " [-1.57946301e+00  1.00000000e+00  1.07254028e+00 ...  3.51207047e+01\n",
      "   1.10590887e+00 -7.92802429e+01]\n",
      " [-1.57935143e+00  1.00000000e+00  1.07293141e+00 ... -4.51371498e+01\n",
      "   1.04792885e+02  9.01702042e+01]\n",
      " ...\n",
      " [-1.57944107e+00  1.00000000e+00  1.07274818e+00 ... -6.37056618e+01\n",
      "   9.81089973e+00  9.01313248e+01]\n",
      " [-1.57984352e+00  1.00000000e+00  1.07217407e+00 ...  1.30135956e+02\n",
      "  -7.36306305e+01 -7.28462296e+01]\n",
      " [-1.57975769e+00  1.00000000e+00  1.07205963e+00 ... -1.14937119e+02\n",
      "  -1.61402054e+02  2.12445974e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ -1.8338156    1.           0.53170776 ...  28.774572    11.43913\n",
      "  -11.835313  ]\n",
      " [ -1.8337479    1.           0.5320072  ... -99.43723    116.23313\n",
      "  121.55403   ]\n",
      " [ -1.8337479    1.           0.53245664 ...  11.438265    -9.7792225\n",
      "  -11.208086  ]\n",
      " ...\n",
      " [ -1.8337822    1.           0.5322447  ... -40.07252    324.14697\n",
      "  -17.033192  ]\n",
      " [ -1.8339691    1.           0.5315838  ...  53.693607    10.600119\n",
      "  -84.07915   ]\n",
      " [ -1.8338861    1.           0.5314598  ... -18.100046    -8.950474\n",
      "   26.017982  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9082308e+00  1.0000000e+00 -6.1473846e-02 ...  6.0680382e+01\n",
      "  -2.9255475e+02 -1.1974900e+02]\n",
      " [-1.9082432e+00  1.0000000e+00 -6.1209679e-02 ...  2.0577945e+02\n",
      "   1.8477718e+02  7.4410210e+01]\n",
      " [-1.9084072e+00  1.0000000e+00 -6.0732633e-02 ... -4.2578686e+01\n",
      "  -2.0085289e+01 -1.0654244e+02]\n",
      " ...\n",
      " [-1.9083366e+00  1.0000000e+00 -6.0963631e-02 ... -4.2484814e+02\n",
      "   1.4861389e+03 -7.8465826e+02]\n",
      " [-1.9083176e+00  1.0000000e+00 -6.1613083e-02 ... -6.3305249e+00\n",
      "  -4.8435654e+01 -1.0816721e+02]\n",
      " [-1.9082165e+00  1.0000000e+00 -6.1742783e-02 ...  2.7003372e+02\n",
      "  -7.9330482e+01 -1.3001787e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7957821e+00  1.0000000e+00 -6.4828682e-01 ... -6.1922583e+02\n",
      "   5.6638184e+01 -3.6012073e+02]\n",
      " [-1.7958765e+00  1.0000000e+00 -6.4803982e-01 ...  1.1015788e+02\n",
      "   1.3964880e+02 -1.6760374e+01]\n",
      " [-1.7961617e+00  1.0000000e+00 -6.4759624e-01 ... -2.6268782e+03\n",
      "   5.0540005e+03  3.1167354e+03]\n",
      " ...\n",
      " [-1.7960186e+00  1.0000000e+00 -6.4780712e-01 ...  9.2130255e+02\n",
      "  -2.1705251e+03  3.9274411e+02]\n",
      " [-1.7958183e+00  1.0000000e+00 -6.4841270e-01 ...  6.6603264e+01\n",
      "   1.1225051e+02  4.3785294e+01]\n",
      " [-1.7956743e+00  1.0000000e+00 -6.4854240e-01 ...  8.9822708e+01\n",
      "   9.7879860e+01  1.3261072e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5077915e+00  1.0000000e+00 -1.1715069e+00 ... -3.3938877e+01\n",
      "   4.2835817e+00 -3.9911200e+02]\n",
      " [-1.5079670e+00  1.0000000e+00 -1.1712761e+00 ... -2.9307431e+02\n",
      "   1.9546216e+02 -4.3990158e+02]\n",
      " [-1.5083504e+00  1.0000000e+00 -1.1709039e+00 ... -6.4491816e+03\n",
      "  -2.6206208e+03 -1.0678332e+04]\n",
      " ...\n",
      " [-1.5081558e+00  1.0000000e+00 -1.1710815e+00 ... -6.0619305e+02\n",
      "   1.1396333e+03 -9.0972058e+02]\n",
      " [-1.5077667e+00  1.0000000e+00 -1.1716118e+00 ... -2.8257108e+02\n",
      "   1.8853685e+02  2.6750870e+01]\n",
      " [-1.5076113e+00  1.0000000e+00 -1.1717205e+00 ...  9.6384857e+01\n",
      "  -6.8340401e+01 -4.1523109e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0717649e+00  1.0000000e+00 -1.5801258e+00 ... -1.3990942e+02\n",
      "  -6.8997139e+01 -2.3130786e+02]\n",
      " [-1.0719995e+00  1.0000000e+00 -1.5799685e+00 ... -3.8429132e+02\n",
      "  -4.1659180e+02  5.5912817e+02]\n",
      " [-1.0725307e+00  1.0000000e+00 -1.5796989e+00 ...  1.8557981e+03\n",
      "  -1.8587841e+03  2.3159709e+03]\n",
      " ...\n",
      " [-1.0722847e+00  1.0000000e+00 -1.5798378e+00 ...  1.0384528e+03\n",
      "  -2.9437045e+02 -1.3979221e+02]\n",
      " [-1.0717297e+00  1.0000000e+00 -1.5801983e+00 ... -5.0121625e+02\n",
      "  -5.7100677e+02 -3.4005594e+02]\n",
      " [-1.0715303e+00  1.0000000e+00 -1.5802822e+00 ... -4.7384214e+00\n",
      "   1.4688902e+00 -3.0200059e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.3059673e-01  1.0000000e+00 -1.8340569e+00 ... -1.1172838e+03\n",
      "   1.1430599e+03  1.4095400e+03]\n",
      " [-5.3087234e-01  1.0000000e+00 -1.8339396e+00 ...  9.6840591e+01\n",
      "  -8.9433075e+01 -4.1931568e+01]\n",
      " [-5.3146172e-01  1.0000000e+00 -1.8338224e+00 ... -1.8519740e+03\n",
      "  -3.4928245e+03  8.6680656e+01]\n",
      " ...\n",
      " [-5.3118134e-01  1.0000000e+00 -1.8338909e+00 ... -4.0649872e+01\n",
      "  -2.9464464e+01  5.5945332e+01]\n",
      " [-5.3054047e-01  1.0000000e+00 -1.8340816e+00 ...  9.4913872e+01\n",
      "   2.1127560e+02  9.4807556e+01]\n",
      " [-5.3032780e-01  1.0000000e+00 -1.8341141e+00 ... -1.7020657e+00\n",
      "   5.5239911e+00 -6.4662466e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.2334061e-02  1.0000000e+00 -1.9083900e+00 ... -1.2102655e+03\n",
      "  -5.0025009e+02 -3.8198917e+02]\n",
      " [ 6.2047005e-02  1.0000000e+00 -1.9083652e+00 ...  1.4200058e+02\n",
      "   6.4245033e+01 -1.0003026e+02]\n",
      " [ 6.1420441e-02  1.0000000e+00 -1.9084116e+00 ...  2.4728026e+02\n",
      "  -1.7080710e+03  1.6538229e+03]\n",
      " ...\n",
      " [ 6.1706543e-02  1.0000000e+00 -1.9084091e+00 ... -1.0939995e+01\n",
      "  -8.9413689e+01  1.2936388e+02]\n",
      " [ 6.2368393e-02  1.0000000e+00 -1.9083843e+00 ...  1.1819470e+02\n",
      "  -3.4737328e+01 -4.3461170e+00]\n",
      " [ 6.2606812e-02  1.0000000e+00 -1.9083786e+00 ... -1.1420143e+00\n",
      "   3.5038171e+00  1.5233526e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.4923477e-01  1.0000000e+00 -1.7955761e+00 ...  6.3262642e+01\n",
      "  -2.1570464e+02  8.0848883e+02]\n",
      " [ 6.4896584e-01  1.0000000e+00 -1.7956486e+00 ... -4.2281694e+00\n",
      "  -5.7702713e+01  1.8389244e+01]\n",
      " [ 6.4838219e-01  1.0000000e+00 -1.7958542e+00 ... -1.6119458e+02\n",
      "  -6.8092059e+02 -3.3451184e+02]\n",
      " ...\n",
      " [ 6.4866447e-01  1.0000000e+00 -1.7957869e+00 ...  1.1350114e+02\n",
      "  -1.9209610e+01  1.2675811e+01]\n",
      " [ 6.4929008e-01  1.0000000e+00 -1.7955360e+00 ... -2.9157907e+01\n",
      "   2.2180698e+01 -1.1218861e+02]\n",
      " [ 6.4948750e-01  1.0000000e+00 -1.7954826e+00 ...  5.9825497e+00\n",
      "   6.4166174e+00  1.1376171e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1725569     1.           -1.5070839  ...  -36.6864\n",
      "  -117.36569      47.808434  ]\n",
      " [   1.1723185     1.           -1.5072508  ...  101.67102\n",
      "   -15.625553     71.95927   ]\n",
      " [   1.1718102     1.           -1.5076069  ...   31.906517\n",
      "    91.874115    129.84584   ]\n",
      " ...\n",
      " [   1.1720505     1.           -1.5074797  ...   14.745334\n",
      "   -90.6401      135.74892   ]\n",
      " [   1.1725731     1.           -1.507019   ...   41.874493\n",
      "   -50.805412     86.76326   ]\n",
      " [   1.1727705     1.           -1.506918   ...    1.0579662\n",
      "    -0.71367073   -1.2821747 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.580925     1.          -1.0711708 ...  119.52327    -36.39534\n",
      "   158.05713  ]\n",
      " [   1.5807457    1.          -1.0713949 ...   36.258663    23.272036\n",
      "  -121.115036 ]\n",
      " [   1.5803719    1.          -1.071874  ...  185.8094     180.83951\n",
      "    26.065777 ]\n",
      " ...\n",
      " [   1.5805473    1.          -1.0716867 ...    9.571143    20.019285\n",
      "     9.124241 ]\n",
      " [   1.5808849    1.          -1.071085  ...   66.809875  -129.31828\n",
      "    50.72464  ]\n",
      " [   1.5810804    1.          -1.0709496 ...    1.6210651   -2.7910953\n",
      "    -1.7001519]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8347034e+00  1.0000000e+00 -5.3016090e-01 ...  4.0077664e+02\n",
      "   2.6605667e+02 -1.9344456e+02]\n",
      " [ 1.8345995e+00  1.0000000e+00 -5.3046417e-01 ...  2.9266309e+02\n",
      "   2.9330453e+02  2.7808798e+02]\n",
      " [ 1.8343410e+00  1.0000000e+00 -5.3101087e-01 ...  7.4178309e+00\n",
      "   2.5588875e+01 -1.9479446e+01]\n",
      " ...\n",
      " [ 1.8344498e+00  1.0000000e+00 -5.3078938e-01 ... -9.1010994e+01\n",
      "   6.0543381e+01  7.0861984e+01]\n",
      " [ 1.8346176e+00  1.0000000e+00 -5.3005791e-01 ... -3.7593143e+02\n",
      "  -2.2483646e+02  2.2562257e+02]\n",
      " [ 1.8347807e+00  1.0000000e+00 -5.2990341e-01 ... -2.8591239e-01\n",
      "  -1.7065996e+00 -4.2069817e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90878868e+00  1.00000000e+00  6.28986359e-02 ...  2.37400986e+02\n",
      "   4.05128052e+02  4.27059975e+01]\n",
      " [ 1.90877151e+00  1.00000000e+00  6.25553131e-02 ... -6.39791756e+01\n",
      "  -7.81535912e+00  2.13339405e+01]\n",
      " [ 1.90867424e+00  1.00000000e+00  6.20086715e-02 ... -3.13130894e+01\n",
      "   1.39233446e+01 -3.75378914e+01]\n",
      " ...\n",
      " [ 1.90870667e+00  1.00000000e+00  6.22177124e-02 ... -1.50144958e+01\n",
      "  -1.78293209e+01 -8.59515381e+01]\n",
      " [ 1.90866089e+00  1.00000000e+00  6.30035400e-02 ...  4.32091400e+02\n",
      "   1.14371056e+02  1.46198303e+02]\n",
      " [ 1.90878773e+00  1.00000000e+00  6.31713867e-02 ...  4.83803606e+00\n",
      "   1.14529205e+02 -1.58512848e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.2 0.  0.\n",
      " 0.  0.3]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.79596043e+00  1.00000000e+00  6.49763107e-01 ... -2.74035059e+03\n",
      "   1.08470081e+03  4.18508105e+03]\n",
      " [ 1.79602242e+00  1.00000000e+00  6.49471283e-01 ... -1.55252518e+02\n",
      "  -1.04311691e+02  2.42067627e+02]\n",
      " [ 1.79611397e+00  1.00000000e+00  6.48956716e-01 ... -1.38909592e+02\n",
      "   7.63237457e+01 -2.56247803e+02]\n",
      " ...\n",
      " [ 1.79606819e+00  1.00000000e+00  6.49159431e-01 ... -1.24432045e+02\n",
      "  -1.90459595e+02 -1.55803131e+02]\n",
      " [ 1.79578590e+00  1.00000000e+00  6.49862289e-01 ...  1.34721375e+02\n",
      "  -1.10822731e+02 -7.76754150e+01]\n",
      " [ 1.79587078e+00  1.00000000e+00  6.50028229e-01 ... -6.18473755e+02\n",
      "   3.24283203e+02  3.50135498e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5074234    1.           1.1727562 ... -123.259995  -255.35846\n",
      "  -241.87184  ]\n",
      " [   1.5075655    1.           1.1725311 ... -419.10727    -66.76355\n",
      "     3.9361048]\n",
      " [   1.5078373    1.           1.1721063 ...  186.74986    -74.41851\n",
      "    85.877266 ]\n",
      " ...\n",
      " [   1.5077229    1.           1.1722717 ...   41.213356    18.721235\n",
      "   -36.668407 ]\n",
      " [   1.5072498    1.           1.1728382 ...  -28.484514    11.026156\n",
      "  -165.62119  ]\n",
      " [   1.5072546    1.           1.1729813 ...  110.334595  -210.04489\n",
      "  -117.06978  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.071044     1.           1.58107   ...  -52.47966   -119.381325\n",
      "  -105.479774 ]\n",
      " [   1.0712423    1.           1.5808964 ...   86.14989     21.378996\n",
      "    61.342255 ]\n",
      " [   1.0716       1.           1.5805919 ...   35.301506    32.299927\n",
      "   -49.431824 ]\n",
      " ...\n",
      " [   1.0714607    1.           1.5807171 ...  169.55746     -1.1956735\n",
      "    90.03278  ]\n",
      " [   1.0708427    1.           1.5811272 ... -146.7313     -18.116915\n",
      "  -169.87689  ]\n",
      " [   1.070817     1.           1.5812321 ...  130.4649    -154.91942\n",
      "  -198.04715  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5296507     1.            1.8345318  ...   11.288225\n",
      "    18.735977     25.3456    ]\n",
      " [   0.52988815    1.            1.8344183  ...   85.93769\n",
      "   -94.63387    -102.43347   ]\n",
      " [   0.5303211     1.            1.8342624  ...   42.262344\n",
      "   116.27887     -32.098686  ]\n",
      " ...\n",
      " [   0.53015137    1.            1.8343105  ... -266.56055\n",
      "   149.43758     -10.353522  ]\n",
      " [   0.52944946    1.            1.8345375  ...   29.119541\n",
      "    45.449265      6.761529  ]\n",
      " [   0.5293932     1.            1.8346004  ...    9.181103\n",
      "   -42.993908    -16.480627  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.3199043e-02  1.0000000e+00  1.9084167e+00 ...  1.3064414e+03\n",
      "   7.1862866e+02  1.1078656e+03]\n",
      " [-6.2947273e-02  1.0000000e+00  1.9084120e+00 ...  4.2069757e+02\n",
      "   3.2749060e+02 -2.1325667e+02]\n",
      " [-6.2543869e-02  1.0000000e+00  1.9083978e+00 ... -7.5244553e+01\n",
      "   2.9259453e+01 -2.0836639e+02]\n",
      " ...\n",
      " [-6.2719345e-02  1.0000000e+00  1.9083967e+00 ... -1.1388068e+03\n",
      "  -8.3509259e+02 -6.8883124e+02]\n",
      " [-6.3447952e-02  1.0000000e+00  1.9083691e+00 ...  2.9387607e+02\n",
      "   3.3830759e+02  1.2692692e+02]\n",
      " [-6.3467026e-02  1.0000000e+00  1.9083958e+00 ... -1.0582123e+01\n",
      "  -8.6650864e+01 -1.7431613e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:7, Score:1.29, Best Score:2.33, Average Score:1.47, Best Avg Score:1.72\n",
      "Episode number: 8\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c44a970>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1731701     1.            1.506321   ...   16.602053\n",
      "   201.15794      46.419876  ]\n",
      " [  -1.1729717     1.            1.5064669  ...  351.97638\n",
      "  -109.255516   -163.93768   ]\n",
      " [  -1.1726799     1.            1.5067345  ...   -6.9594297\n",
      "    -0.35534906   -9.881549  ]\n",
      " ...\n",
      " [  -1.172823      1.            1.5066175  ...  108.94735\n",
      "   -43.370117   -337.8278    ]\n",
      " [  -1.17342       1.            1.5061741  ... -125.361916\n",
      "    26.055252    113.776375  ]\n",
      " [  -1.1733742     1.            1.5061474  ...    2.5921159\n",
      "   -33.779064    -94.59701   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5812511     1.            1.0697708  ...  -46.243847\n",
      "    -4.526224     10.095507  ]\n",
      " [  -1.5811157     1.            1.069993   ...    3.1260567\n",
      "   -22.845148    -10.256679  ]\n",
      " [  -1.5809231     1.            1.0703481  ...   -0.42242074\n",
      "     5.435234     -0.9054656 ]\n",
      " ...\n",
      " [  -1.5810299     1.            1.0701895  ... -127.3964\n",
      "   375.07098     214.44815   ]\n",
      " [  -1.5814362     1.            1.0695801  ...  139.37582\n",
      "    39.746433   -234.71768   ]\n",
      " [  -1.5814018     1.            1.0695362  ...    3.9752293\n",
      "    27.123014     18.000868  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.83462524e+00  1.00000000e+00  5.28541565e-01 ... -6.21149719e+02\n",
      "   1.28845779e+02 -1.91461868e+02]\n",
      " [-1.83456516e+00  1.00000000e+00  5.28822899e-01 ...  6.20822182e+01\n",
      "   3.56958199e+01 -1.39085236e+02]\n",
      " [-1.83450317e+00  1.00000000e+00  5.29224753e-01 ...  8.42640591e+00\n",
      "   4.37353935e+01 -9.56813908e+00]\n",
      " ...\n",
      " [-1.83455276e+00  1.00000000e+00  5.29053688e-01 ...  1.49898535e+03\n",
      "  -7.93993530e+02  5.80649658e+02]\n",
      " [-1.83475876e+00  1.00000000e+00  5.28324127e-01 ...  2.75299561e+02\n",
      "   1.46782318e+02  3.33879333e+02]\n",
      " [-1.83470249e+00  1.00000000e+00  5.28270721e-01 ... -2.52281094e+01\n",
      "  -1.17299225e+02 -1.48103275e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9081182e+00  1.0000000e+00 -6.4468384e-02 ...  4.1459988e+02\n",
      "  -1.3151684e+01 -3.6393781e+02]\n",
      " [-1.9081526e+00  1.0000000e+00 -6.4193726e-02 ...  2.2048484e+03\n",
      "  -3.2060223e+02  2.5482448e+02]\n",
      " [-1.9081898e+00  1.0000000e+00 -6.3776478e-02 ...  2.2467619e+02\n",
      "  -2.4806367e+02 -3.6065729e+02]\n",
      " ...\n",
      " [-1.9081936e+00  1.0000000e+00 -6.3952446e-02 ... -8.7034631e+02\n",
      "  -1.2219080e+02  4.4919989e+02]\n",
      " [-1.9081993e+00  1.0000000e+00 -6.4689636e-02 ...  2.6853655e+01\n",
      "   1.5929163e+02  8.6107574e+01]\n",
      " [-1.9081230e+00  1.0000000e+00 -6.4744949e-02 ...  1.5412059e+01\n",
      "  -9.4306002e+00 -4.5519787e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7948332e+00  1.0000000e+00 -6.5125465e-01 ...  7.9930145e+01\n",
      "  -8.2480690e+01  3.1669899e+01]\n",
      " [-1.7949600e+00  1.0000000e+00 -6.5098858e-01 ...  1.8530721e+03\n",
      "  -5.6120477e+02  7.9630280e+02]\n",
      " [-1.7951298e+00  1.0000000e+00 -6.5058386e-01 ... -4.3645008e+01\n",
      "   7.6734138e+01  1.5678622e+02]\n",
      " ...\n",
      " [-1.7950783e+00  1.0000000e+00 -6.5076447e-01 ...  1.8151241e+02\n",
      "   5.4589520e+02  9.9742328e+02]\n",
      " [-1.7948723e+00  1.0000000e+00 -6.5147018e-01 ... -1.7667763e+02\n",
      "   2.2394920e+01  4.5379044e+01]\n",
      " [-1.7947340e+00  1.0000000e+00 -6.5152168e-01 ... -1.0333664e+02\n",
      "  -2.2076352e+02  7.2161110e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.505867     1.          -1.1741276 ...   96.74963   -155.55553\n",
      "  -196.75017  ]\n",
      " [  -1.5060663    1.          -1.1738768 ... -306.92422    393.91846\n",
      "   441.29587  ]\n",
      " [  -1.5063362    1.          -1.1735364 ... -384.77203    324.5988\n",
      "   180.7464   ]\n",
      " ...\n",
      " [  -1.506237     1.          -1.1736879 ... -297.48032    377.18405\n",
      "   401.949    ]\n",
      " [  -1.505825     1.          -1.1742954 ...  162.68173   -121.20912\n",
      "   195.78142  ]\n",
      " [  -1.505681     1.          -1.1743279 ...  293.90698   -509.22217\n",
      "    15.404188 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0692949    1.          -1.5822029 ...  -41.638657   476.07202\n",
      "  -295.4473   ]\n",
      " [  -1.0695505    1.          -1.5819921 ... -471.81128    -29.820168\n",
      "   508.80005  ]\n",
      " [  -1.0699673    1.          -1.5817635 ...   50.35257     61.751316\n",
      "   105.91502  ]\n",
      " ...\n",
      " [  -1.0698242    1.          -1.5818481 ...    1.2265553  -40.060192\n",
      "    -1.0805364]\n",
      " [  -1.06921      1.          -1.5823212 ...   67.99908    -24.46043\n",
      "   104.6496   ]\n",
      " [  -1.0690489    1.          -1.5823383 ... -180.14375   -254.65134\n",
      "    21.763285 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.2796936e-01  1.0000000e+00 -1.8353043e+00 ...  2.8839016e+00\n",
      "  -1.1049397e+02  1.5550912e+02]\n",
      " [-5.2826691e-01  1.0000000e+00 -1.8351860e+00 ... -2.5563585e+02\n",
      "  -1.9565559e+02  7.4853033e+02]\n",
      " [-5.2871895e-01  1.0000000e+00 -1.8350868e+00 ...  1.0292102e+03\n",
      "   3.0353580e+02  3.9233646e+02]\n",
      " ...\n",
      " [-5.2856064e-01  1.0000000e+00 -1.8351231e+00 ...  1.6940466e+02\n",
      "   5.8248386e+00  8.5437447e+01]\n",
      " [-5.2785683e-01  1.0000000e+00 -1.8353615e+00 ... -3.3419533e+01\n",
      "   3.3380203e+01  1.4238688e+01]\n",
      " [-5.2768326e-01  1.0000000e+00 -1.8353634e+00 ... -1.9748041e+02\n",
      "  -2.3932225e+02  3.5714319e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.4752579e-02  1.0000000e+00 -1.9087181e+00 ... -4.6257870e+01\n",
      "   6.6364357e+01 -1.9156956e+01]\n",
      " [ 6.4449310e-02  1.0000000e+00 -1.9086790e+00 ... -8.2897968e+00\n",
      "   4.0694820e+01  2.8070349e+01]\n",
      " [ 6.4004898e-02  1.0000000e+00 -1.9087180e+00 ... -5.3255389e+02\n",
      "   4.7228574e+02  8.2853711e+02]\n",
      " ...\n",
      " [ 6.4170837e-02  1.0000000e+00 -1.9086905e+00 ... -6.3024011e+02\n",
      "   5.5174576e+01  1.4562138e+01]\n",
      " [ 6.4899445e-02  1.0000000e+00 -1.9087009e+00 ...  9.1404510e+01\n",
      "  -3.1887573e+02  3.4288259e+03]\n",
      " [ 6.5052032e-02  1.0000000e+00 -1.9086819e+00 ...  2.0869778e+01\n",
      "  -1.8495771e+01 -1.2396239e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.5168571e-01  1.0000000e+00 -1.7949963e+00 ...  4.8272578e+03\n",
      "  -5.4994277e+03  1.0604138e+03]\n",
      " [ 6.5140724e-01  1.0000000e+00 -1.7950449e+00 ... -1.7318091e+02\n",
      "   1.3117754e+02 -1.3609660e+02]\n",
      " [ 6.5097237e-01  1.0000000e+00 -1.7952249e+00 ...  9.0158701e+00\n",
      "   3.1004009e+01  3.1504669e+01]\n",
      " ...\n",
      " [ 6.5113258e-01  1.0000000e+00 -1.7951374e+00 ... -5.9202728e+02\n",
      "   1.5682733e+02 -5.3994873e+02]\n",
      " [ 6.5181160e-01  1.0000000e+00 -1.7949142e+00 ... -3.3525403e+02\n",
      "   1.0758041e+03 -2.6959260e+03]\n",
      " [ 6.5196991e-01  1.0000000e+00 -1.7948780e+00 ... -2.6041229e+02\n",
      "  -1.6216448e+03  8.3781580e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1745300e+00  1.0000000e+00 -1.5057411e+00 ... -4.3650063e+01\n",
      "   3.8701394e+03 -2.5194704e+02]\n",
      " [ 1.1742926e+00  1.0000000e+00 -1.5059071e+00 ...  3.1918522e+01\n",
      "   6.7382210e+01 -3.6897846e+01]\n",
      " [ 1.1739483e+00  1.0000000e+00 -1.5061971e+00 ... -6.8944595e+01\n",
      "   3.7374168e+01 -1.3006252e+02]\n",
      " ...\n",
      " [ 1.1740856e+00  1.0000000e+00 -1.5060720e+00 ...  1.0418679e+02\n",
      "  -3.6692822e+01 -3.3905606e+02]\n",
      " [ 1.1746693e+00  1.0000000e+00 -1.5056095e+00 ... -2.3901540e+02\n",
      "  -8.2507715e+02 -5.2173743e+02]\n",
      " [ 1.1747694e+00  1.0000000e+00 -1.5055370e+00 ...  5.3377724e+01\n",
      "  -7.0021370e+01 -1.5691829e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.58228683e+00  1.00000000e+00 -1.06933784e+00 ... -1.12541626e+03\n",
      "  -5.67778809e+02 -7.65271606e+02]\n",
      " [ 1.58212185e+00  1.00000000e+00 -1.06951904e+00 ...  3.16802711e+01\n",
      "  -2.09238464e+02 -1.09418434e+02]\n",
      " [ 1.58186340e+00  1.00000000e+00 -1.06989837e+00 ...  4.50210953e+01\n",
      "   8.17414017e+01 -1.98988068e+02]\n",
      " ...\n",
      " [ 1.58196259e+00  1.00000000e+00 -1.06973839e+00 ...  6.64055824e+00\n",
      "  -1.80439163e+02  2.43364319e+02]\n",
      " [ 1.58239365e+00  1.00000000e+00 -1.06916046e+00 ... -2.05825760e+02\n",
      "   8.52831299e+02  4.68634460e+02]\n",
      " [ 1.58245277e+00  1.00000000e+00 -1.06906128e+00 ...  4.66571289e+02\n",
      "  -1.59793213e+02 -1.49357361e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8352785e+00  1.0000000e+00 -5.2806664e-01 ... -1.5532820e+02\n",
      "   4.0361493e+02 -1.5730052e+03]\n",
      " [ 1.8351851e+00  1.0000000e+00 -5.2829361e-01 ... -2.8053732e+02\n",
      "  -1.5506618e+02 -8.0582660e+02]\n",
      " [ 1.8350372e+00  1.0000000e+00 -5.2873486e-01 ...  1.9500476e+02\n",
      "  -3.3297284e+02 -1.1744034e+02]\n",
      " ...\n",
      " [ 1.8351021e+00  1.0000000e+00 -5.2854538e-01 ...  6.0612872e+02\n",
      "   1.1032886e+03 -1.0085782e+03]\n",
      " [ 1.8353310e+00  1.0000000e+00 -5.2786255e-01 ...  2.7314126e+03\n",
      "   4.1270723e+03 -2.3724586e+02]\n",
      " [ 1.8353577e+00  1.0000000e+00 -5.2774429e-01 ...  3.1005020e+02\n",
      "  -3.7683487e+01  6.3378006e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.90855885e+00  1.00000000e+00  6.53553009e-02 ... -4.81654854e+01\n",
      "  -8.23460083e+01 -3.04929230e+02]\n",
      " [ 1.90854549e+00  1.00000000e+00  6.50825500e-02 ... -1.23199573e+03\n",
      "   1.38543237e+03 -4.68566681e+02]\n",
      " [ 1.90858078e+00  1.00000000e+00  6.46435916e-02 ... -1.53272104e+01\n",
      "  -1.06317739e+01 -2.81474380e+01]\n",
      " ...\n",
      " [ 1.90859413e+00  1.00000000e+00  6.48221970e-02 ...  1.85127060e+02\n",
      "  -3.12561188e+01 -6.89449406e+00]\n",
      " [ 1.90856361e+00  1.00000000e+00  6.55651093e-02 ... -1.02322464e+02\n",
      "   1.68391418e+02  1.79060410e+02]\n",
      " [ 1.90855408e+00  1.00000000e+00  6.56852722e-02 ... -4.02451477e+02\n",
      "  -3.71383484e+02  2.05854401e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7948971e+00  1.0000000e+00  6.5206909e-01 ... -1.1976141e+01\n",
      "   1.7393166e+01 -1.0065975e+01]\n",
      " [ 1.7949696e+00  1.0000000e+00  6.5179634e-01 ...  1.1651859e+03\n",
      "   2.9436472e+02  3.5667422e+02]\n",
      " [ 1.7950974e+00  1.0000000e+00  6.5139067e-01 ... -7.4399756e+02\n",
      "   2.3744800e+02 -5.2972382e+02]\n",
      " ...\n",
      " [ 1.7950573e+00  1.0000000e+00  6.5155220e-01 ... -5.2004529e+02\n",
      "   3.1900702e+02  7.3084937e+02]\n",
      " [ 1.7947865e+00  1.0000000e+00  6.5226364e-01 ... -3.9597708e+02\n",
      "   7.7309196e+01 -1.1665662e+02]\n",
      " [ 1.7948074e+00  1.0000000e+00  6.5237617e-01 ...  2.7079636e+01\n",
      "   1.0827733e+02  2.2148140e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5056114e+00  1.0000000e+00  1.1747684e+00 ...  5.3063942e+01\n",
      "   4.5411746e+02 -1.0009838e+03]\n",
      " [ 1.5057335e+00  1.0000000e+00  1.1745300e+00 ...  9.0233688e+01\n",
      "   1.0599468e+02  3.1504373e+02]\n",
      " [ 1.5059566e+00  1.0000000e+00  1.1741798e+00 ...  6.9309827e+02\n",
      "   3.7005005e+01  1.5392192e+02]\n",
      " ...\n",
      " [ 1.5058804e+00  1.0000000e+00  1.1743259e+00 ...  1.2904039e+02\n",
      "   2.7737671e+02 -2.0864069e+02]\n",
      " [ 1.5054169e+00  1.0000000e+00  1.1749268e+00 ... -4.5068256e+01\n",
      "   7.9315765e+01  4.6875534e+01]\n",
      " [ 1.5054216e+00  1.0000000e+00  1.1750202e+00 ...  1.5720610e+02\n",
      "   1.2681223e+02 -4.3059582e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0689211e+00  1.0000000e+00  1.5824928e+00 ... -2.4704445e+01\n",
      "  -4.8949673e+02  1.3017064e+02]\n",
      " [ 1.0690966e+00  1.0000000e+00  1.5823364e+00 ...  3.5345798e+02\n",
      "   1.4538855e+01  8.8531288e+01]\n",
      " [ 1.0693855e+00  1.0000000e+00  1.5820841e+00 ...  3.2733359e+02\n",
      "   7.0276355e+02 -2.1637646e+03]\n",
      " ...\n",
      " [ 1.0692844e+00  1.0000000e+00  1.5821924e+00 ...  5.0360039e+01\n",
      "  -2.7868452e+01 -3.1623126e+02]\n",
      " [ 1.0686684e+00  1.0000000e+00  1.5826168e+00 ...  1.1369571e+02\n",
      "  -3.9560028e+01 -1.4112575e+01]\n",
      " [ 1.0686741e+00  1.0000000e+00  1.5826912e+00 ...  1.0262117e+05\n",
      "   2.8969918e+04  3.7003957e+04]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.27436256e-01  1.00000000e+00  1.83522606e+00 ...  5.01570206e+01\n",
      "  -2.32745667e+02 -3.81467056e+01]\n",
      " [ 5.27649879e-01  1.00000000e+00  1.83514690e+00 ...  1.45322769e+02\n",
      "   2.31945343e+01 -4.27244835e+01]\n",
      " [ 5.27996063e-01  1.00000000e+00  1.83502138e+00 ... -9.56915894e+02\n",
      "   3.80394402e+01  1.10709875e+03]\n",
      " ...\n",
      " [ 5.27853012e-01  1.00000000e+00  1.83507729e+00 ... -1.14006325e+02\n",
      "  -2.14158688e+01  1.35502663e+01]\n",
      " [ 5.27132034e-01  1.00000000e+00  1.83530617e+00 ...  2.93912754e+01\n",
      "   8.45682755e+01 -1.72687778e+01]\n",
      " [ 5.27139664e-01  1.00000000e+00  1.83535767e+00 ... -1.63180310e+03\n",
      "  -1.29244951e+04  1.86325720e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.5746307e-02  1.0000000e+00  1.9083538e+00 ...  2.3809894e+02\n",
      "   1.7068106e+02  4.7669907e+01]\n",
      " [-6.5525055e-02  1.0000000e+00  1.9083757e+00 ... -7.8185074e+01\n",
      "   8.8932045e+01 -1.4101874e+02]\n",
      " [-6.5202713e-02  1.0000000e+00  1.9083678e+00 ... -4.3791479e+02\n",
      "   5.0331653e+02  2.5398190e+02]\n",
      " ...\n",
      " [-6.5349579e-02  1.0000000e+00  1.9083853e+00 ... -7.2315369e+01\n",
      "   8.1945908e+01  1.1245442e+02]\n",
      " [-6.6099167e-02  1.0000000e+00  1.9083557e+00 ...  6.3027630e+01\n",
      "   1.1694096e+01 -1.1169822e+02]\n",
      " [-6.6054344e-02  1.0000000e+00  1.9083843e+00 ... -6.3322369e+02\n",
      "   1.0219206e+03 -1.2162197e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.52297974e-01  1.00000000e+00  1.79455376e+00 ... -2.03879196e+02\n",
      "   1.16278542e+02 -5.96910095e+01]\n",
      " [-6.52088165e-01  1.00000000e+00  1.79470158e+00 ... -1.62906837e+01\n",
      "  -6.57166290e+01 -4.18096275e+01]\n",
      " [-6.51815414e-01  1.00000000e+00  1.79481602e+00 ... -2.41815338e+01\n",
      "   2.50189346e+02  4.44964294e+02]\n",
      " ...\n",
      " [-6.51945114e-01  1.00000000e+00  1.79478455e+00 ...  1.38314468e+02\n",
      "  -2.17081909e+02 -2.09361343e+02]\n",
      " [-6.52624130e-01  1.00000000e+00  1.79448509e+00 ...  1.26160851e+01\n",
      "   1.23153515e+01  3.21174088e+01]\n",
      " [-6.52594566e-01  1.00000000e+00  1.79449844e+00 ... -2.46505029e+03\n",
      "  -6.92429150e+03 -4.09843213e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1748343e+00  1.0000000e+00  1.5052567e+00 ...  6.1336288e+01\n",
      "   1.4023280e+01 -9.1734047e+00]\n",
      " [-1.1746655e+00  1.0000000e+00  1.5054550e+00 ...  7.1331467e+01\n",
      "  -1.5599066e+02 -1.0538336e+02]\n",
      " [-1.1744652e+00  1.0000000e+00  1.5056741e+00 ... -7.5681091e+01\n",
      "  -3.0980465e+01  1.0547725e+01]\n",
      " ...\n",
      " [-1.1745682e+00  1.0000000e+00  1.5056086e+00 ... -5.0538290e+02\n",
      "   7.6485345e+02 -1.4077880e+03]\n",
      " [-1.1751156e+00  1.0000000e+00  1.5051155e+00 ...  5.5139954e+02\n",
      "  -1.7044945e+02 -1.4465488e+02]\n",
      " [-1.1750879e+00  1.0000000e+00  1.5051231e+00 ... -5.2775183e+02\n",
      "   5.4024249e+02 -1.5556459e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5827007    1.           1.0682411 ...  -62.15754    249.40074\n",
      "    20.927292 ]\n",
      " [  -1.5825958    1.           1.0685148 ... -140.73105   -718.0017\n",
      "  -119.93583  ]\n",
      " [  -1.582428     1.           1.0688094 ...  -39.510895    66.94927\n",
      "   -27.277115 ]\n",
      " ...\n",
      " [  -1.5824928    1.           1.0687075 ...  166.83994     84.2236\n",
      "   -21.46798  ]\n",
      " [  -1.5828838    1.           1.0680504 ...   73.24011    201.29895\n",
      "  -320.4721   ]\n",
      " [  -1.5828753    1.           1.0680523 ...  218.51709   -378.9737\n",
      "    62.68806  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8354626     1.            0.5265217  ... -117.069565\n",
      "   -60.02733      -2.973427  ]\n",
      " [  -1.8354397     1.            0.52686405 ...  -96.489914\n",
      "    17.701239   -144.45139   ]\n",
      " [  -1.8353901     1.            0.5271927  ... -192.21303\n",
      "   -51.458923     -1.7335049 ]\n",
      " ...\n",
      " [  -1.8354225     1.            0.52708626 ... -337.76416\n",
      "   142.81645     362.11176   ]\n",
      " [  -1.8355579     1.            0.52630806 ...   56.823864\n",
      "   122.26327      99.0936    ]\n",
      " [  -1.8355589     1.            0.52630806 ... -349.06976\n",
      "   312.80023     106.35257   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9083548e+00  1.0000000e+00 -6.6558838e-02 ... -3.0770029e+02\n",
      "   1.3996744e+02 -6.2701007e+02]\n",
      " [-1.9084244e+00  1.0000000e+00 -6.6203117e-02 ... -1.4218538e+04\n",
      "  -6.9805176e+03  1.3137602e+04]\n",
      " [-1.9084721e+00  1.0000000e+00 -6.5870225e-02 ... -1.1155653e+03\n",
      "  -1.4592551e+01 -1.9959000e+02]\n",
      " ...\n",
      " [-1.9084759e+00  1.0000000e+00 -6.5971375e-02 ... -1.3864693e+02\n",
      "   4.2375420e+01 -7.5840599e+01]\n",
      " [-1.9083824e+00  1.0000000e+00 -6.6791534e-02 ...  1.5995930e+02\n",
      "  -2.2200983e+02  9.8657623e+01]\n",
      " [-1.9083605e+00  1.0000000e+00 -6.6793442e-02 ...  7.1235657e+01\n",
      "  -1.0172298e+02  7.0371361e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7944746e+00  1.0000000e+00 -6.5287971e-01 ... -1.0854096e+00\n",
      "   6.0140815e+00  4.5690010e+01]\n",
      " [-1.7946444e+00  1.0000000e+00 -6.5256023e-01 ... -1.4968079e+04\n",
      "  -1.3152380e+04 -3.6954954e+03]\n",
      " [-1.7947674e+00  1.0000000e+00 -6.5224618e-01 ...  2.1334685e+02\n",
      "  -2.5376524e+02  1.9671144e+02]\n",
      " ...\n",
      " [-1.7947273e+00  1.0000000e+00 -6.5233707e-01 ... -1.6961391e+02\n",
      "  -3.7221985e+01 -1.1330006e+02]\n",
      " [-1.7944279e+00  1.0000000e+00 -6.5310097e-01 ... -2.3120163e+01\n",
      "   1.9304632e+01 -8.2490936e+01]\n",
      " [-1.7943993e+00  1.0000000e+00 -6.5310097e-01 ... -1.3805798e+02\n",
      "  -2.0449034e+02 -1.3113293e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5048122e+00  1.0000000e+00 -1.1755390e+00 ...  6.3092468e+01\n",
      "  -7.5716095e+01 -1.5179642e+02]\n",
      " [-1.5050631e+00  1.0000000e+00 -1.1752806e+00 ... -5.8883975e+03\n",
      "   2.4116119e+04  1.3496764e+03]\n",
      " [-1.5052814e+00  1.0000000e+00 -1.1750189e+00 ... -3.0178616e+02\n",
      "   4.7599677e+02 -1.1126010e+03]\n",
      " ...\n",
      " [-1.5052223e+00  1.0000000e+00 -1.1750984e+00 ...  2.4064713e+01\n",
      "  -3.0923596e+02  1.4880659e+02]\n",
      " [-1.5047359e+00  1.0000000e+00 -1.1757298e+00 ... -6.8975195e+02\n",
      "  -5.7667103e+01 -5.7311565e+01]\n",
      " [-1.5046692e+00  1.0000000e+00 -1.1757298e+00 ... -9.0439807e+02\n",
      "  -6.9235645e+02 -1.9192653e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0678272e+00  1.0000000e+00 -1.5831146e+00 ...  6.4245613e+01\n",
      "   4.3402039e+01  2.8073692e+01]\n",
      " [-1.0681486e+00  1.0000000e+00 -1.5829029e+00 ...  2.8519463e+03\n",
      "  -1.1798719e+03 -2.6168234e+02]\n",
      " [-1.0684452e+00  1.0000000e+00 -1.5827324e+00 ...  3.4574237e+00\n",
      "   1.3659044e+02  4.6469513e+02]\n",
      " ...\n",
      " [-1.0683670e+00  1.0000000e+00 -1.5827742e+00 ... -1.4699989e+02\n",
      "  -1.0751817e+02 -2.8606073e+02]\n",
      " [-1.0676956e+00  1.0000000e+00 -1.5832520e+00 ...  7.4727085e+03\n",
      "  -2.3324397e+03  5.9977012e+03]\n",
      " [-1.0676203e+00  1.0000000e+00 -1.5832500e+00 ... -6.3054333e+02\n",
      "   1.0758455e+03 -3.4444351e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.52630234    1.           -1.8356552  ...   32.588\n",
      "    -6.4516745   -69.28283   ]\n",
      " [  -0.5266695     1.           -1.8355207  ...  419.29797\n",
      "   249.71408     406.9688    ]\n",
      " [  -0.5269985     1.           -1.8354461  ...   54.73653\n",
      "   -39.683475    174.1937    ]\n",
      " ...\n",
      " [  -0.52689743    1.           -1.8354597  ...   20.257524\n",
      "    -1.8578124     9.825394  ]\n",
      " [  -0.52612495    1.           -1.8357143  ... -191.07236\n",
      "   331.78424    -391.94034   ]\n",
      " [  -0.5260649     1.           -1.8357105  ... -137.29303\n",
      "  -350.8257     -313.78345   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.66723251e-02  1.00000000e+00 -1.90836525e+00 ...  1.37329102e+02\n",
      "   1.00568375e+02  1.68344681e+02]\n",
      " [ 6.62937164e-02  1.00000000e+00 -1.90835285e+00 ...  4.60061646e+01\n",
      "  -8.69672089e+01 -2.10398422e+02]\n",
      " [ 6.59275055e-02  1.00000000e+00 -1.90839708e+00 ... -1.04563484e+02\n",
      "  -1.25984173e+01  1.05476135e+02]\n",
      " ...\n",
      " [ 6.60324097e-02  1.00000000e+00 -1.90836620e+00 ...  1.18059436e+03\n",
      "  -2.20357773e+02  2.20839081e+02]\n",
      " [ 6.68373108e-02  1.00000000e+00 -1.90835571e+00 ...  9.51460800e+01\n",
      "   1.97125130e+01  1.30212603e+01]\n",
      " [ 6.69145584e-02  1.00000000e+00 -1.90834427e+00 ... -2.55476013e+02\n",
      "  -1.13802216e+02 -1.27189493e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.5342903e-01  1.0000000e+00 -1.7942810e+00 ... -1.1684431e+01\n",
      "   6.7716148e+01  6.1953190e+01]\n",
      " [ 6.5307140e-01  1.0000000e+00 -1.7943573e+00 ...  2.8152294e+00\n",
      "   9.8075867e+01  7.1491425e+01]\n",
      " [ 6.5270615e-01  1.0000000e+00 -1.7945172e+00 ... -8.8608292e+01\n",
      "   9.9145465e+02 -2.6879448e+01]\n",
      " ...\n",
      " [ 6.5280724e-01  1.0000000e+00 -1.7944355e+00 ... -3.7701771e+01\n",
      "  -7.7683721e+00 -4.0354691e+01]\n",
      " [ 6.5356064e-01  1.0000000e+00 -1.7942009e+00 ... -1.8666000e+01\n",
      "  -3.8579632e+01  9.9744225e+00]\n",
      " [ 6.5365314e-01  1.0000000e+00 -1.7941818e+00 ... -1.8292416e+02\n",
      "  -3.2398062e+02  1.6977509e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1758499e+00  1.0000000e+00 -1.5046234e+00 ...  1.8914070e+02\n",
      "   2.4354538e+02 -7.5389084e+01]\n",
      " [ 1.1755390e+00  1.0000000e+00 -1.5047779e+00 ... -1.8359669e+01\n",
      "  -5.4422424e+02  1.1371116e+03]\n",
      " [ 1.1752338e+00  1.0000000e+00 -1.5050408e+00 ...  5.8808662e+01\n",
      "   1.1164322e+02 -9.6218559e+01]\n",
      " ...\n",
      " [ 1.1753254e+00  1.0000000e+00 -1.5049219e+00 ... -8.9638623e+02\n",
      "   9.1042365e+02  6.1606865e+01]\n",
      " [ 1.1759510e+00  1.0000000e+00 -1.5044708e+00 ...  4.8629522e+00\n",
      "  -4.3708469e+01 -2.2782227e+01]\n",
      " [ 1.1760378e+00  1.0000000e+00 -1.5044441e+00 ...  1.3875647e+02\n",
      "  -3.9951969e+01  3.9855328e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5833626    1.          -1.0674877 ...  -90.05492   -149.79509\n",
      "  -261.17743  ]\n",
      " [   1.5831432    1.          -1.0677423 ... -180.24559    133.55981\n",
      "  -183.86275  ]\n",
      " [   1.5829105    1.          -1.068087  ...  -85.05824    -73.0995\n",
      "    -5.516183 ]\n",
      " ...\n",
      " [   1.5829735    1.          -1.0679426 ... -207.84676    -50.62594\n",
      "   592.4822   ]\n",
      " [   1.5834007    1.          -1.0672874 ... -236.32457   -133.72351\n",
      "   198.3408   ]\n",
      " [   1.5834846    1.          -1.0672474 ...   45.022827    78.68685\n",
      "   -82.36997  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8358059     1.           -0.5258732  ...   88.85209\n",
      "  -174.8068     -172.4107    ]\n",
      " [   1.83568       1.           -0.526186   ... -180.93687\n",
      "   122.630646     81.417435  ]\n",
      " [   1.835577      1.           -0.5265644  ...  -38.80868\n",
      "  -170.4929      -59.298218  ]\n",
      " ...\n",
      " [   1.8356075     1.           -0.52641964 ...   52.639812\n",
      "    11.589042     18.346054  ]\n",
      " [   1.8358116     1.           -0.5256443  ...  200.64583\n",
      "     2.8727016   105.490814  ]\n",
      " [   1.8358355     1.           -0.5255985  ...    0.47423506\n",
      "   -33.285305      1.5324434 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9084749e+00  1.0000000e+00  6.7298889e-02 ...  9.1204880e+01\n",
      "  -2.9136274e+01  3.7244137e+01]\n",
      " [ 1.9084558e+00  1.0000000e+00  6.6987991e-02 ... -3.3903452e+02\n",
      "   1.4446941e+02  9.4944119e-01]\n",
      " [ 1.9084549e+00  1.0000000e+00  6.6593714e-02 ... -1.0981129e+02\n",
      "  -2.2126500e+02 -3.5242317e+01]\n",
      " ...\n",
      " [ 1.9084473e+00  1.0000000e+00  6.6742897e-02 ... -1.8485646e+00\n",
      "   2.1310673e+00 -1.1819086e+01]\n",
      " [ 1.9084091e+00  1.0000000e+00  6.7529678e-02 ... -2.1376498e+02\n",
      "   4.8512058e+01  4.5736168e+01]\n",
      " [ 1.9084206e+00  1.0000000e+00  6.7579269e-02 ... -8.5757469e+01\n",
      "   1.0975946e+02  3.5200244e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.7942753     1.            0.65377426 ...  -23.56654\n",
      "   152.86449     -19.91635   ]\n",
      " [   1.7943487     1.            0.6534977  ... -233.18233\n",
      "    46.404316    160.87645   ]\n",
      " [   1.7944679     1.            0.6531178  ...  -66.74354\n",
      "    92.51744       0.45396686]\n",
      " ...\n",
      " [   1.7944336     1.            0.6532688  ...    0.40976572\n",
      "    -2.6495686    -0.76918054]\n",
      " [   1.794138      1.            0.6539936  ...   -7.872469\n",
      "   -86.84936      59.303     ]\n",
      " [   1.7941265     1.            0.6540394  ... -406.39395\n",
      "     3.0333252   104.962906  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5043888e+00  1.0000000e+00  1.1763000e+00 ... -2.2723415e+02\n",
      "  -5.5671521e+02 -6.2691687e+02]\n",
      " [ 1.5045528e+00  1.0000000e+00  1.1760492e+00 ...  2.6267041e+02\n",
      "   1.2911661e+03 -4.5910089e+02]\n",
      " [ 1.5047607e+00  1.0000000e+00  1.1757411e+00 ... -6.3564846e+01\n",
      "   3.7734512e+01 -1.0078322e+02]\n",
      " ...\n",
      " [ 1.5046959e+00  1.0000000e+00  1.1758480e+00 ...  4.9951534e+00\n",
      "  -3.4742422e+00 -1.5829251e+00]\n",
      " [ 1.5041904e+00  1.0000000e+00  1.1764812e+00 ...  4.8629280e+01\n",
      "   1.2328541e+01 -1.8450934e+02]\n",
      " [ 1.5041847e+00  1.0000000e+00  1.1765213e+00 ... -5.3499325e+01\n",
      "   4.0917087e+00  9.4550781e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.1       0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0672140e+00  1.0000000e+00  1.5836086e+00 ... -2.9681619e+02\n",
      "   1.7047121e+02  1.9574347e+01]\n",
      " [ 1.0674410e+00  1.0000000e+00  1.5834475e+00 ... -1.9231414e+03\n",
      "   1.3341487e+03 -1.5302090e+03]\n",
      " [ 1.0677490e+00  1.0000000e+00  1.5832267e+00 ... -2.2820972e+02\n",
      "  -2.1572200e+02  3.7952151e+02]\n",
      " ...\n",
      " [ 1.0676498e+00  1.0000000e+00  1.5833044e+00 ... -3.8370919e-01\n",
      "  -4.4543705e+00 -3.0339646e-01]\n",
      " [ 1.0669708e+00  1.0000000e+00  1.5837345e+00 ... -8.7003151e+01\n",
      "  -1.1041382e+02 -8.6010109e+01]\n",
      " [ 1.0669470e+00  1.0000000e+00  1.5837688e+00 ...  1.6938399e+02\n",
      "  -1.6839339e+02  2.4263474e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.2552128e-01  1.0000000e+00  1.8358955e+00 ... -4.8433464e+01\n",
      "   1.5368931e+01 -8.4511360e+01]\n",
      " [ 5.2579021e-01  1.0000000e+00  1.8358355e+00 ...  3.1182941e+02\n",
      "   1.3478198e+02 -2.5593096e+03]\n",
      " [ 5.2620125e-01  1.0000000e+00  1.8357171e+00 ... -1.9981781e+02\n",
      "   4.3343835e+02 -1.1706155e+02]\n",
      " ...\n",
      " [ 5.2609444e-01  1.0000000e+00  1.8357639e+00 ...  1.6338803e+00\n",
      "  -2.1814096e+00  3.3413153e+00]\n",
      " [ 5.2529907e-01  1.0000000e+00  1.8359318e+00 ... -4.2304688e+01\n",
      "  -4.8196316e+01  9.7275154e+01]\n",
      " [ 5.2522373e-01  1.0000000e+00  1.8359509e+00 ...  1.8830413e+03\n",
      "  -1.1040968e+03  1.1951443e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7589760e-02  1.0000000e+00  1.9082489e+00 ... -1.4360620e+02\n",
      "  -4.7242172e+02 -1.8806638e+02]\n",
      " [-6.7311287e-02  1.0000000e+00  1.9082985e+00 ...  5.6285455e+02\n",
      "   2.7689481e+02 -7.2506281e+02]\n",
      " [-6.6835403e-02  1.0000000e+00  1.9082795e+00 ... -1.4006326e+02\n",
      "   5.2356830e+02 -6.3897327e+02]\n",
      " ...\n",
      " [-6.6946030e-02  1.0000000e+00  1.9083042e+00 ... -2.8403306e-01\n",
      "  -1.2401404e+00  2.9958231e+00]\n",
      " [-6.7779541e-02  1.0000000e+00  1.9082050e+00 ... -1.2974800e+02\n",
      "   3.4317341e+01  1.8540895e+02]\n",
      " [-6.7890167e-02  1.0000000e+00  1.9082108e+00 ...  6.5211310e+02\n",
      "  -7.9266510e+02  2.9023874e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.654356      1.            1.79385    ... -261.19226\n",
      "  -254.94543       8.766421  ]\n",
      " [  -0.6540947     1.            1.7939901  ...  122.65617\n",
      "    69.71445     264.70175   ]\n",
      " [  -0.65358925    1.            1.7940856  ...   67.2515\n",
      "  -283.3517      -89.97588   ]\n",
      " ...\n",
      " [  -0.65369415    1.            1.7940674  ...   -0.8108925\n",
      "    -0.4333476    -0.6100999 ]\n",
      " [  -0.65447235    1.            1.7937164  ...   87.73064\n",
      "    35.660866      2.6621153 ]\n",
      " [  -0.65464306    1.            1.7937012  ...    1.4330907\n",
      "    17.8652       31.476355  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.1765585    1.           1.5039635 ...  267.4148      97.02104\n",
      "    74.47179  ]\n",
      " [  -1.1763334    1.           1.5041828 ...   61.44995     92.68875\n",
      "   -26.453722 ]\n",
      " [  -1.1759357    1.           1.5044025 ... -551.97656    -10.045732\n",
      "  -194.74406  ]\n",
      " ...\n",
      " [  -1.1760273    1.           1.5043306 ...   -1.0214329    0.9301714\n",
      "    -3.109785 ]\n",
      " [  -1.176691     1.           1.5037708 ...  -13.038567  -300.31882\n",
      "   147.71983  ]\n",
      " [  -1.1767969    1.           1.5037346 ...   19.298265    59.661396\n",
      "    36.167492 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5839777e+00  1.0000000e+00  1.0664387e+00 ... -2.4656350e+03\n",
      "   8.7989905e+02  2.7574233e+03]\n",
      " [-1.5838203e+00  1.0000000e+00  1.0667496e+00 ... -3.0839581e+02\n",
      "   7.1618573e+02 -9.7654657e+02]\n",
      " [-1.5835629e+00  1.0000000e+00  1.0670393e+00 ...  1.6224309e+02\n",
      "  -2.0444812e+02 -3.8033768e+01]\n",
      " ...\n",
      " [-1.5836163e+00  1.0000000e+00  1.0669460e+00 ...  7.1470475e-01\n",
      "   1.0056930e+00 -1.4419770e+00]\n",
      " [-1.5840778e+00  1.0000000e+00  1.0661964e+00 ... -1.1558066e+03\n",
      "  -1.1446351e+02 -1.0758058e+03]\n",
      " [-1.5841599e+00  1.0000000e+00  1.0661392e+00 ... -2.1210974e+02\n",
      "   3.4757697e+02  2.8571115e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.83601570e+00  1.00000000e+00  5.24816513e-01 ... -1.09984680e+03\n",
      "   1.12437524e+03  3.35836395e+02]\n",
      " [-1.83595753e+00  1.00000000e+00  5.25156975e-01 ... -3.48798035e+02\n",
      "  -1.27272484e+02 -4.71033264e+02]\n",
      " [-1.83581924e+00  1.00000000e+00  5.25511265e-01 ...  6.36383606e+02\n",
      "   3.60975067e+02 -5.69814224e+01]\n",
      " ...\n",
      " [-1.83583832e+00  1.00000000e+00  5.25391579e-01 ...  3.59511042e+00\n",
      "  -3.61815405e+00 -2.61868596e-01]\n",
      " [-1.83607101e+00  1.00000000e+00  5.24539948e-01 ...  1.54464722e+02\n",
      "  -3.03447895e+01  1.05897964e+02]\n",
      " [-1.83611393e+00  1.00000000e+00  5.24473190e-01 ... -3.26858673e+02\n",
      "  -1.35612427e+02  2.09929642e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.8000001 0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.1      ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9083490e+00  1.0000000e+00 -6.8410873e-02 ... -2.4148921e+02\n",
      "  -2.6743195e+01  5.5468376e+01]\n",
      " [-1.9083786e+00  1.0000000e+00 -6.8075180e-02 ... -1.7336670e+02\n",
      "  -3.5562973e+02  9.1181923e+01]\n",
      " [-1.9083881e+00  1.0000000e+00 -6.7711473e-02 ...  1.2805090e+03\n",
      "  -3.3354111e+02  1.6533319e+03]\n",
      " ...\n",
      " [-1.9083691e+00  1.0000000e+00 -6.7830086e-02 ... -1.6265157e+00\n",
      "  -8.7257957e-01 -4.4723237e-01]\n",
      " [-1.9083500e+00  1.0000000e+00 -6.8691254e-02 ...  8.0956802e+01\n",
      "  -8.1066307e+01  9.3390976e+01]\n",
      " [-1.9083490e+00  1.0000000e+00 -6.8765640e-02 ... -8.4517273e+01\n",
      "   1.4548388e+00 -2.9075533e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        1.0000001 0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79382896e+00  1.00000000e+00 -6.54716492e-01 ...  1.31417099e+02\n",
      "  -9.56458130e+01  8.43358154e+01]\n",
      " [-1.79395866e+00  1.00000000e+00 -6.54376030e-01 ...  6.88803024e+01\n",
      "  -6.02621031e+00  8.95465374e+00]\n",
      " [-1.79407501e+00  1.00000000e+00 -6.54042304e-01 ... -1.58053780e+00\n",
      "   4.49025940e+02  9.10136963e+02]\n",
      " ...\n",
      " [-1.79401398e+00  1.00000000e+00 -6.54148102e-01 ... -1.61615634e+00\n",
      "  -4.49617386e-01 -2.03924179e-01]\n",
      " [-1.79376030e+00  1.00000000e+00 -6.54981613e-01 ...  7.03742142e+01\n",
      "  -6.40675125e+01  1.10879545e+01]\n",
      " [-1.79372597e+00  1.00000000e+00 -6.55046463e-01 ... -3.85959534e+02\n",
      "  -1.81126556e+02  4.39514313e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.2        0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.         0.         0.1        0.         0.9000001\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.50366783e+00  1.00000000e+00 -1.17714310e+00 ...  1.10778984e+02\n",
      "  -1.46176468e+02  1.36598797e+01]\n",
      " [-1.50388145e+00  1.00000000e+00 -1.17683697e+00 ... -3.84946823e+00\n",
      "   1.80854858e+02 -1.88025513e+02]\n",
      " [-1.50408745e+00  1.00000000e+00 -1.17656481e+00 ...  1.92215881e+02\n",
      "   4.11919060e+01  1.20241272e+02]\n",
      " ...\n",
      " [-1.50399399e+00  1.00000000e+00 -1.17664433e+00 ... -1.24958076e-01\n",
      "   1.07282639e-01 -9.37495232e-02]\n",
      " [-1.50348663e+00  1.00000000e+00 -1.17737770e+00 ... -2.57667198e+01\n",
      "   5.56852493e+01 -4.61599960e+01]\n",
      " [-1.50347900e+00  1.00000000e+00 -1.17742538e+00 ...  5.82234802e+02\n",
      "   1.04309546e+03  4.00394440e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0659285e+00  1.0000000e+00 -1.5842266e+00 ... -1.3322408e+02\n",
      "  -5.8893620e+01 -3.6665323e+00]\n",
      " [-1.0662174e+00  1.0000000e+00 -1.5839462e+00 ... -8.3536003e+01\n",
      "   1.2438796e+02  1.8896610e+01]\n",
      " [-1.0665379e+00  1.0000000e+00 -1.5837747e+00 ... -2.4771092e+02\n",
      "   5.0401059e+02 -1.6449210e+02]\n",
      " ...\n",
      " [-1.0664082e+00  1.0000000e+00 -1.5838175e+00 ...  2.2576706e+00\n",
      "   3.3617947e+00 -3.6168852e-01]\n",
      " [-1.0657253e+00  1.0000000e+00 -1.5843906e+00 ... -3.7206184e+01\n",
      "  -2.7116447e+01 -4.8068184e+01]\n",
      " [-1.0656672e+00  1.0000000e+00 -1.5844097e+00 ...  7.1776527e+01\n",
      "   1.3786952e+02 -4.5022931e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.2402210e-01  1.0000000e+00 -1.8362103e+00 ...  1.1126257e+02\n",
      "  -4.2072162e+02  6.4879333e+02]\n",
      " [-5.2435589e-01  1.0000000e+00 -1.8360014e+00 ... -1.8982527e+02\n",
      "  -6.3848560e+01 -2.9648074e+02]\n",
      " [-5.2473640e-01  1.0000000e+00 -1.8359431e+00 ...  1.1171101e+02\n",
      "  -4.4294319e+01  3.1019001e+01]\n",
      " ...\n",
      " [-5.2459908e-01  1.0000000e+00 -1.8359518e+00 ... -6.3302511e-01\n",
      "   2.1246409e+00  2.1734366e+00]\n",
      " [-5.2378654e-01  1.0000000e+00 -1.8363113e+00 ... -3.3980313e+02\n",
      "   8.5078384e+01  7.2210876e+02]\n",
      " [-5.2372074e-01  1.0000000e+00 -1.8363056e+00 ... -2.6818906e+02\n",
      "   6.3608227e+01  8.3320381e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.8949699e-02  1.0000000e+00 -1.9083328e+00 ... -4.4310757e+01\n",
      "  -1.4018609e+01 -1.0030766e+01]\n",
      " [ 6.8607330e-02  1.0000000e+00 -1.9082327e+00 ... -4.5357437e+01\n",
      "   3.0282310e+01  2.9334435e+01]\n",
      " [ 6.8174362e-02  1.0000000e+00 -1.9082763e+00 ...  5.1657983e+02\n",
      "   3.5782593e+02  6.6169440e+02]\n",
      " ...\n",
      " [ 6.8317413e-02  1.0000000e+00 -1.9082537e+00 ...  4.8130298e-01\n",
      "   3.0737257e+00  1.7610420e+00]\n",
      " [ 6.9169998e-02  1.0000000e+00 -1.9083500e+00 ...  5.8328186e+01\n",
      "   3.4342026e+02  3.0396671e+02]\n",
      " [ 6.9256783e-02  1.0000000e+00 -1.9083309e+00 ...  7.6539734e+01\n",
      "   4.6793438e+01 -1.9558698e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.5541077e-01  1.0000000e+00 -1.7936096e+00 ... -2.7236121e+02\n",
      "  -9.4364204e+00  3.2113289e+01]\n",
      " [ 6.5508080e-01  1.0000000e+00 -1.7935867e+00 ...  3.2508856e+02\n",
      "   9.1137901e+01 -4.5627115e+02]\n",
      " [ 6.5463257e-01  1.0000000e+00 -1.7937731e+00 ...  1.3178282e+02\n",
      "   5.2195034e+01 -3.3364996e+02]\n",
      " ...\n",
      " [ 6.5476418e-01  1.0000000e+00 -1.7937126e+00 ... -9.0143824e-01\n",
      "   1.1145382e+00  4.2764747e-01]\n",
      " [ 6.5558815e-01  1.0000000e+00 -1.7935543e+00 ... -5.3140393e+02\n",
      "  -8.0841724e+02 -1.4136443e+02]\n",
      " [ 6.5569305e-01  1.0000000e+00 -1.7935219e+00 ...  1.3476700e+02\n",
      "  -8.4566437e+01  1.2552834e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1777992e+00  1.0000000e+00 -1.5029182e+00 ...  3.6778973e+02\n",
      "   1.8981767e+02  3.7296455e+01]\n",
      " [ 1.1775198e+00  1.0000000e+00 -1.5029955e+00 ...  6.2674343e+01\n",
      "  -4.1799261e+02 -5.0835501e+02]\n",
      " [ 1.1771183e+00  1.0000000e+00 -1.5032934e+00 ...  1.6527245e+01\n",
      "  -2.5310837e+02 -1.8431291e+02]\n",
      " ...\n",
      " [ 1.1772308e+00  1.0000000e+00 -1.5032034e+00 ... -2.0211842e+00\n",
      "  -2.6955271e-01  2.9279470e-01]\n",
      " [ 1.1779442e+00  1.0000000e+00 -1.5028000e+00 ...  1.9818614e+02\n",
      "  -1.4367943e+02  5.6756611e+01]\n",
      " [ 1.1780281e+00  1.0000000e+00 -1.5027370e+00 ...  1.5896434e+02\n",
      "  -1.2440667e+02 -2.8909866e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5844984e+00  1.0000000e+00 -1.0656300e+00 ... -2.1100426e+02\n",
      "   2.5734832e+03  1.5233341e+01]\n",
      " [ 1.5842915e+00  1.0000000e+00 -1.0657768e+00 ...  2.8745079e+01\n",
      "  -7.3741751e+00 -5.6564093e-01]\n",
      " [ 1.5839920e+00  1.0000000e+00 -1.0661654e+00 ...  1.9036179e+02\n",
      "   8.4493341e+02 -4.0311749e+01]\n",
      " ...\n",
      " [ 1.5840797e+00  1.0000000e+00 -1.0660429e+00 ... -6.2788401e+00\n",
      "   6.0243096e+00 -5.5872655e-01]\n",
      " [ 1.5845985e+00  1.0000000e+00 -1.0654716e+00 ...  1.4501962e+02\n",
      "   7.0023840e+02  2.1207678e+02]\n",
      " [ 1.5846510e+00  1.0000000e+00 -1.0653954e+00 ... -1.0138457e+02\n",
      "   1.2097793e+00  1.0467343e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.83625317e+00  1.00000000e+00 -5.23351669e-01 ... -2.92465477e+01\n",
      "  -1.91567097e+01 -1.83360901e+01]\n",
      " [ 1.83612537e+00  1.00000000e+00 -5.23521423e-01 ... -8.73971558e+01\n",
      "   7.51989685e+02 -6.63481750e+01]\n",
      " [ 1.83596611e+00  1.00000000e+00 -5.23955107e-01 ...  7.98131897e+02\n",
      "  -1.21465324e+02  8.45874100e+01]\n",
      " ...\n",
      " [ 1.83600998e+00  1.00000000e+00 -5.23829460e-01 ...  6.31620884e-01\n",
      "   6.86513901e-01  1.77682221e-01]\n",
      " [ 1.83626938e+00  1.00000000e+00 -5.23160934e-01 ... -1.05907585e+02\n",
      "  -1.34412918e+01 -1.20605148e+02]\n",
      " [ 1.83630848e+00  1.00000000e+00 -5.23069382e-01 ...  1.85960178e+01\n",
      "   6.48448896e+00 -3.33127975e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.9000001 0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.1\n",
      " 0.        0.        0.        0.5       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9081774e+00  1.0000000e+00  6.9429398e-02 ...  6.9672290e+02\n",
      "  -2.6024368e+02 -3.8876761e+02]\n",
      " [ 1.9081402e+00  1.0000000e+00  6.9290161e-02 ... -6.8874191e+01\n",
      "  -6.9526508e+02  1.1054261e+03]\n",
      " [ 1.9081211e+00  1.0000000e+00  6.8823345e-02 ... -1.5058772e+02\n",
      "  -9.6235382e+01  3.7705669e+01]\n",
      " ...\n",
      " [ 1.9081345e+00  1.0000000e+00  6.8972588e-02 ... -8.6878347e-01\n",
      "   8.1468105e-02 -1.2962478e-01]\n",
      " [ 1.9081154e+00  1.0000000e+00  6.9627762e-02 ... -1.1985711e+02\n",
      "  -7.4388077e+01 -7.5172028e+01]\n",
      " [ 1.9081545e+00  1.0000000e+00  6.9721222e-02 ... -1.0557097e+02\n",
      "   1.2122771e+02  3.8045544e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.0000001\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7933025e+00  1.0000000e+00  6.5584755e-01 ...  6.9435097e+01\n",
      "   3.3453262e+01 -1.4319820e+02]\n",
      " [ 1.7933369e+00  1.0000000e+00  6.5573692e-01 ... -2.5051500e+02\n",
      "  -1.8430000e+02  5.8261060e+02]\n",
      " [ 1.7934780e+00  1.0000000e+00  6.5530258e-01 ...  6.8702484e+01\n",
      "  -1.7785794e+02 -5.7323326e+01]\n",
      " ...\n",
      " [ 1.7934513e+00  1.0000000e+00  6.5543747e-01 ... -1.5621471e-01\n",
      "  -4.1739416e-01 -5.2478194e-02]\n",
      " [ 1.7932014e+00  1.0000000e+00  6.5603828e-01 ... -3.9401602e+02\n",
      "  -5.9665131e+01  5.4223315e+02]\n",
      " [ 1.7931900e+00  1.0000000e+00  6.5612793e-01 ... -2.0912395e+00\n",
      "   3.9637527e+01 -1.1600205e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.2       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5028181e+00  1.0000000e+00  1.1780796e+00 ...  1.8946159e+02\n",
      "   1.7636972e+02 -7.6563257e+02]\n",
      " [ 1.5029135e+00  1.0000000e+00  1.1779366e+00 ...  9.6790369e+02\n",
      "   2.8101752e+02  5.8627325e+02]\n",
      " [ 1.5032043e+00  1.0000000e+00  1.1775813e+00 ...  3.8008179e+01\n",
      "   6.4182259e+01  6.9506905e+01]\n",
      " ...\n",
      " [ 1.5031414e+00  1.0000000e+00  1.1776848e+00 ...  2.9837751e-01\n",
      "   7.5664520e-02  7.7132344e-02]\n",
      " [ 1.5026875e+00  1.0000000e+00  1.1782322e+00 ... -1.2867958e+02\n",
      "   7.5324440e+01 -5.4563812e+01]\n",
      " [ 1.5026102e+00  1.0000000e+00  1.1783085e+00 ... -8.0435059e+01\n",
      "  -5.2089695e+01  5.6655750e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.70000005 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.0000001\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0650597e+00  1.0000000e+00  1.5850105e+00 ... -1.4689738e+02\n",
      "  -3.0569872e+01 -9.3918358e+01]\n",
      " [ 1.0651913e+00  1.0000000e+00  1.5849209e+00 ... -2.7390555e+02\n",
      "   5.3662256e+02  1.8963547e+02]\n",
      " [ 1.0655861e+00  1.0000000e+00  1.5846637e+00 ...  1.1896399e+02\n",
      "  -5.6280457e+01  4.1884254e+01]\n",
      " ...\n",
      " [ 1.0654984e+00  1.0000000e+00  1.5847359e+00 ...  7.6275969e-01\n",
      "   3.7042618e-02  1.3065085e-01]\n",
      " [ 1.0649033e+00  1.0000000e+00  1.5851021e+00 ... -8.2062836e+00\n",
      "   4.3775856e+01 -1.4831802e+02]\n",
      " [ 1.0647821e+00  1.0000000e+00  1.5851593e+00 ... -1.9004957e+01\n",
      "   2.0798756e+01  1.3021528e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.2301693e-01  1.0000000e+00  1.8366489e+00 ... -5.5562649e+00\n",
      "   5.4229523e+01 -1.6867851e+01]\n",
      " [ 5.2317810e-01  1.0000000e+00  1.8365831e+00 ...  3.0934766e+02\n",
      "   2.2871216e+02  9.7573029e+01]\n",
      " [ 5.2365494e-01  1.0000000e+00  1.8364650e+00 ... -1.5861777e+01\n",
      "  -7.2282944e+00  3.8463818e+01]\n",
      " ...\n",
      " [ 5.2354622e-01  1.0000000e+00  1.8365049e+00 ...  7.2441535e+00\n",
      "  -6.6258302e+00 -2.2600275e-01]\n",
      " [ 5.2285957e-01  1.0000000e+00  1.8366833e+00 ... -3.5349491e+01\n",
      "   8.1689575e+01  1.4206152e+02]\n",
      " [ 5.2269650e-01  1.0000000e+00  1.8367081e+00 ...  8.2927376e+01\n",
      "  -3.9987450e+01 -5.2122183e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.0214272e-02  1.0000000e+00  1.9084148e+00 ... -2.0598528e+01\n",
      "   4.7064297e+01 -2.9458506e+01]\n",
      " [-7.0044518e-02  1.0000000e+00  1.9084129e+00 ... -1.1699951e+02\n",
      "  -4.9903564e+01  8.1605736e+01]\n",
      " [-6.9559097e-02  1.0000000e+00  1.9084374e+00 ...  3.5668384e+02\n",
      "  -1.1213654e+03 -2.8853314e+02]\n",
      " ...\n",
      " [-6.9671631e-02  1.0000000e+00  1.9084463e+00 ...  6.8205070e+00\n",
      "  -7.0967340e+00 -8.6320609e-01]\n",
      " [-7.0392609e-02  1.0000000e+00  1.9083824e+00 ...  7.0299118e+01\n",
      "   7.6877541e+01 -8.9920479e+01]\n",
      " [-7.0541382e-02  1.0000000e+00  1.9083691e+00 ...  6.1517105e+01\n",
      "   1.4898251e+02  8.8634659e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.5659237e-01  1.0000000e+00  1.7933350e+00 ...  7.4191223e+01\n",
      "  -7.4608673e+01  8.3256935e+01]\n",
      " [-6.5643120e-01  1.0000000e+00  1.7933655e+00 ... -5.5792343e+01\n",
      "  -2.2346130e+02  2.0958241e+01]\n",
      " [-6.5598679e-01  1.0000000e+00  1.7935300e+00 ... -1.4061998e+02\n",
      "  -1.4643874e+02  1.3811115e+02]\n",
      " ...\n",
      " [-6.5609741e-01  1.0000000e+00  1.7934942e+00 ...  1.8083105e+00\n",
      "  -3.1007776e+00 -6.7711091e-01]\n",
      " [-6.5677643e-01  1.0000000e+00  1.7932434e+00 ...  1.5922600e+02\n",
      "   9.4627457e+01 -1.4120367e+03]\n",
      " [-6.5688992e-01  1.0000000e+00  1.7931900e+00 ...  7.5273964e+01\n",
      "  -4.8053032e+01 -7.0171806e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1788006e+00  1.0000000e+00  1.5025597e+00 ... -1.0267023e+01\n",
      "   3.6458794e+01  1.3054541e+02]\n",
      " [-1.1786594e+00  1.0000000e+00  1.5026112e+00 ...  1.3718504e+02\n",
      "  -6.4209351e+01 -9.1863319e+01]\n",
      " [-1.1782742e+00  1.0000000e+00  1.5029072e+00 ...  3.1496096e+01\n",
      "  -2.5491676e+01 -4.9271473e+01]\n",
      " ...\n",
      " [-1.1783733e+00  1.0000000e+00  1.5028324e+00 ... -1.1044054e+00\n",
      "  -1.1426420e+00 -5.0920439e-01]\n",
      " [-1.1789646e+00  1.0000000e+00  1.5024052e+00 ... -1.5649460e+03\n",
      "  -1.4515878e+02 -2.6319529e+03]\n",
      " [-1.1790428e+00  1.0000000e+00  1.5023155e+00 ... -1.0468689e+02\n",
      "  -3.8865295e+01 -1.4635339e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.58524895e+00  1.00000000e+00  1.06502533e+00 ... -1.06557976e+02\n",
      "  -1.64901276e+02  1.71118958e+02]\n",
      " [-1.58514690e+00  1.00000000e+00  1.06510639e+00 ... -1.62644638e+02\n",
      "  -3.99005096e+02 -6.62232910e+02]\n",
      " [-1.58487320e+00  1.00000000e+00  1.06549668e+00 ...  2.04244590e+00\n",
      "   6.81457443e+01  9.77709961e+00]\n",
      " ...\n",
      " [-1.58495712e+00  1.00000000e+00  1.06539917e+00 ... -1.22922182e-01\n",
      "   6.42605305e-01 -8.57105255e-02]\n",
      " [-1.58538246e+00  1.00000000e+00  1.06483841e+00 ... -5.05671692e+02\n",
      "  -6.75734253e+02  1.31991479e+03]\n",
      " [-1.58540916e+00  1.00000000e+00  1.06471825e+00 ... -2.74902405e+02\n",
      "   5.95204430e+01 -5.95546722e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8367853e+00  1.0000000e+00  5.2267647e-01 ...  2.4069109e+01\n",
      "  -8.8129601e+01  1.1232409e+01]\n",
      " [-1.8367329e+00  1.0000000e+00  5.2278042e-01 ...  2.6514255e+02\n",
      "   3.5355029e+02 -1.3130295e+02]\n",
      " [-1.8366127e+00  1.0000000e+00  5.2322221e-01 ... -6.8179008e+01\n",
      "  -1.9367319e+01  1.3493938e+00]\n",
      " ...\n",
      " [-1.8366661e+00  1.0000000e+00  5.2311516e-01 ...  1.2142558e+00\n",
      "   5.0328445e-01 -1.3906568e-02]\n",
      " [-1.8368912e+00  1.0000000e+00  5.2244759e-01 ...  1.8483340e+02\n",
      "   1.6231175e+02  1.5707547e+02]\n",
      " [-1.8368645e+00  1.0000000e+00  5.2229881e-01 ...  9.4356644e+01\n",
      "   2.6489183e+01 -2.1016981e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90816784e+00  1.00000000e+00 -7.04936981e-02 ... -1.83926029e+01\n",
      "   2.80472775e+01 -6.16230011e+00]\n",
      " [-1.90815639e+00  1.00000000e+00 -7.04040527e-02 ... -1.27855850e+02\n",
      "   2.50006485e+01 -1.15930305e+02]\n",
      " [-1.90817642e+00  1.00000000e+00 -6.99488670e-02 ...  1.41051224e+02\n",
      "   2.44165497e+02  1.39447861e+02]\n",
      " ...\n",
      " [-1.90820312e+00  1.00000000e+00 -7.00578690e-02 ...  8.01372528e-02\n",
      "   2.44572163e-01 -1.38269663e-02]\n",
      " [-1.90820694e+00  1.00000000e+00 -7.07283020e-02 ...  1.30384323e+02\n",
      "  -9.79573975e+01  9.20058365e+01]\n",
      " [-1.90816116e+00  1.00000000e+00 -7.08789825e-02 ...  6.40047531e+01\n",
      "   3.45752960e+02 -3.22031342e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7929077e+00  1.0000000e+00 -6.5690041e-01 ... -9.8677277e+01\n",
      "   7.9512733e+01  2.1362056e+02]\n",
      " [-1.7929506e+00  1.0000000e+00 -6.5683174e-01 ...  2.6784203e+01\n",
      "  -1.0496821e+02  8.4959564e+00]\n",
      " [-1.7931061e+00  1.0000000e+00 -6.5640467e-01 ...  2.3150473e+01\n",
      "  -2.7979401e+02  2.6718494e+01]\n",
      " ...\n",
      " [-1.7930946e+00  1.0000000e+00 -6.5651035e-01 ... -3.4974966e+00\n",
      "   7.9691510e+00  6.9059908e-02]\n",
      " [-1.7928905e+00  1.0000000e+00 -6.5712166e-01 ...  5.4655781e+01\n",
      "   9.2820549e+00  1.2225688e+02]\n",
      " [-1.7928066e+00  1.0000000e+00 -6.5726280e-01 ... -3.4922199e+01\n",
      "  -8.4460663e+01 -8.3686066e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.5021181     1.           -1.1788692  ... -130.34087\n",
      "   426.00043    -434.54147   ]\n",
      " [  -1.5021944     1.           -1.1788082  ...   34.804256\n",
      "   -50.359528     -7.5816894 ]\n",
      " [  -1.5024357     1.           -1.178456   ...  -33.70605\n",
      "   -17.226673    -21.84908   ]\n",
      " ...\n",
      " [  -1.5024014     1.           -1.178546   ...    4.004321\n",
      "   -15.285408      0.51629543]\n",
      " [  -1.5020084     1.           -1.17906    ... -149.11717\n",
      "    16.421337    -33.81039   ]\n",
      " [  -1.5019255     1.           -1.179184   ...  348.05887\n",
      "    46.191605    -70.63642   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0642147e+00  1.0000000e+00 -1.5854664e+00 ...  1.2032052e+03\n",
      "   2.6342227e+02  3.4373007e+02]\n",
      " [-1.0643120e+00  1.0000000e+00 -1.5854111e+00 ... -5.3684959e+00\n",
      "   1.4001772e+02  8.1204865e+01]\n",
      " [-1.0646591e+00  1.0000000e+00 -1.5851572e+00 ... -4.2747707e+01\n",
      "   9.0476768e+01  1.1864717e+02]\n",
      " ...\n",
      " [-1.0646000e+00  1.0000000e+00 -1.5852213e+00 ... -2.3247647e+00\n",
      "   1.0507622e+00 -1.1952636e+00]\n",
      " [-1.0640583e+00  1.0000000e+00 -1.5856228e+00 ...  5.1220105e+02\n",
      "   1.2857517e+03 -2.2567395e+02]\n",
      " [-1.0639400e+00  1.0000000e+00 -1.5857105e+00 ... -1.1038300e+02\n",
      "  -1.3045075e+01  3.2556639e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.22294044e-01  1.00000000e+00 -1.83672333e+00 ... -2.16125039e+04\n",
      "   2.05746680e+04  6.28577002e+03]\n",
      " [-5.22412300e-01  1.00000000e+00 -1.83671379e+00 ...  5.37949829e+01\n",
      "  -9.71461563e+01 -2.68907585e+01]\n",
      " [-5.22838593e-01  1.00000000e+00 -1.83658028e+00 ...  1.59587688e+01\n",
      "  -1.06994255e+02 -6.51529388e+01]\n",
      " ...\n",
      " [-5.22768021e-01  1.00000000e+00 -1.83661842e+00 ... -4.42250013e-01\n",
      "   1.25936222e+00 -1.18267059e-01]\n",
      " [-5.22150040e-01  1.00000000e+00 -1.83681488e+00 ... -5.21730286e+02\n",
      "  -2.07611633e+02 -2.05929993e+02]\n",
      " [-5.21965981e-01  1.00000000e+00 -1.83685875e+00 ... -1.88405121e+02\n",
      "   9.26860580e+01 -5.42672882e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.1139336e-02  1.0000000e+00 -1.9082012e+00 ...  1.3059305e+03\n",
      "  -4.9682794e+02 -1.8444874e+03]\n",
      " [ 7.1016312e-02  1.0000000e+00 -1.9082470e+00 ...  8.1569450e+01\n",
      "  -1.1449595e+01  2.6819424e+02]\n",
      " [ 7.0568085e-02  1.0000000e+00 -1.9082450e+00 ...  1.4859665e+01\n",
      "   1.6899753e+02  3.7831088e+02]\n",
      " ...\n",
      " [ 7.0638657e-02  1.0000000e+00 -1.9082470e+00 ...  1.1099637e+00\n",
      "   1.6952739e+00 -5.4617727e-01]\n",
      " [ 7.1281433e-02  1.0000000e+00 -1.9082298e+00 ... -1.3523308e+02\n",
      "  -1.6434644e+02 -1.4154237e+02]\n",
      " [ 7.1476936e-02  1.0000000e+00 -1.9082394e+00 ... -5.9918289e+01\n",
      "   2.8656046e+02 -1.5857254e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.5738201e-01  1.0000000e+00 -1.7928448e+00 ...  1.8491122e+02\n",
      "   2.2621097e+02  1.1279502e+02]\n",
      " [ 6.5726662e-01  1.0000000e+00 -1.7929249e+00 ...  1.9730260e+02\n",
      "   7.4180214e+01 -1.4268940e+01]\n",
      " [ 6.5686798e-01  1.0000000e+00 -1.7930448e+00 ...  3.5613269e+02\n",
      "  -5.8530361e+01 -3.2467297e+01]\n",
      " ...\n",
      " [ 6.5692902e-01  1.0000000e+00 -1.7930269e+00 ... -6.9131202e-01\n",
      "   5.0753489e+00  2.8070951e-01]\n",
      " [ 6.5752220e-01  1.0000000e+00 -1.7928200e+00 ... -1.5655197e+03\n",
      "   2.6357195e+04 -8.3775000e+04]\n",
      " [ 6.5769100e-01  1.0000000e+00 -1.7927818e+00 ...  7.4572815e+02\n",
      "   7.8214050e+02 -1.5707393e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1793070e+00  1.0000000e+00 -1.5019131e+00 ...  7.4166290e+01\n",
      "  -1.6694385e+02  1.8281111e+01]\n",
      " [ 1.1792145e+00  1.0000000e+00 -1.5020485e+00 ... -8.2833473e+01\n",
      "  -1.0013342e+02  1.8741802e+01]\n",
      " [ 1.1788750e+00  1.0000000e+00 -1.5022671e+00 ...  1.6422925e+02\n",
      "  -2.4317496e+02 -1.3953653e+02]\n",
      " ...\n",
      " [ 1.1789265e+00  1.0000000e+00 -1.5022392e+00 ... -7.2346199e-01\n",
      "   2.1980934e+00  1.5695333e-02]\n",
      " [ 1.1794147e+00  1.0000000e+00 -1.5018349e+00 ... -1.1824881e+04\n",
      "  -1.2754150e+04  1.0732419e+04]\n",
      " [ 1.1795635e+00  1.0000000e+00 -1.5017490e+00 ...  1.0650233e+03\n",
      "  -4.6056316e+01  2.3018066e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.58589268e+00  1.00000000e+00 -1.06380844e+00 ...  3.81425568e+02\n",
      "   1.47389636e+03  1.73577728e+02]\n",
      " [ 1.58583164e+00  1.00000000e+00 -1.06394100e+00 ... -1.15265755e+02\n",
      "   8.44993668e+01  1.58150208e+02]\n",
      " [ 1.58558273e+00  1.00000000e+00 -1.06425047e+00 ... -3.47758522e+01\n",
      "  -9.21419373e+01 -3.63290344e+02]\n",
      " ...\n",
      " [ 1.58561516e+00  1.00000000e+00 -1.06420422e+00 ...  1.21367049e+00\n",
      "  -5.29489660e+00  1.79965687e+00]\n",
      " [ 1.58593941e+00  1.00000000e+00 -1.06369781e+00 ...  6.17811084e+03\n",
      "  -2.87901196e+03 -1.84529016e+03]\n",
      " [ 1.58607197e+00  1.00000000e+00 -1.06357193e+00 ...  2.45257861e+03\n",
      "   5.61008484e+02 -9.66593750e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8370113e+00  1.0000000e+00 -5.2171898e-01 ... -4.1763101e+02\n",
      "   4.1792980e+01 -5.6697182e+01]\n",
      " [ 1.8369884e+00  1.0000000e+00 -5.2185822e-01 ...  3.0715579e+02\n",
      "   1.2057881e+02 -1.1727565e+02]\n",
      " [ 1.8368626e+00  1.0000000e+00 -5.2222681e-01 ...  4.6208609e+02\n",
      "   4.2834372e+02  2.0998076e+02]\n",
      " ...\n",
      " [ 1.8368759e+00  1.0000000e+00 -5.2216530e-01 ...  1.2209009e+00\n",
      "   9.5704651e-01 -2.2043371e+00]\n",
      " [ 1.8370247e+00  1.0000000e+00 -5.2157402e-01 ... -3.4881653e+03\n",
      "  -1.2496096e+03 -5.6580792e+02]\n",
      " [ 1.8370953e+00  1.0000000e+00 -5.2142143e-01 ... -5.3620074e+02\n",
      "  -8.8156696e+02  2.3953423e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9081631e+00  1.0000000e+00  7.1474075e-02 ...  1.2197745e+02\n",
      "  -2.4820958e+02  4.7836429e+02]\n",
      " [ 1.9081717e+00  1.0000000e+00  7.1333885e-02 ...  4.1944156e+02\n",
      "  -4.6048193e+02  5.9356335e+02]\n",
      " [ 1.9081440e+00  1.0000000e+00  7.0952073e-02 ...  5.2731930e+01\n",
      "  -1.3000984e+02 -2.0658485e+02]\n",
      " ...\n",
      " [ 1.9081345e+00  1.0000000e+00  7.1016312e-02 ...  1.0924135e+00\n",
      "   1.9414067e+00  1.4465833e-01]\n",
      " [ 1.9081154e+00  1.0000000e+00  7.1626663e-02 ... -1.9001493e+01\n",
      "   2.1458452e+03 -4.1086815e+01]\n",
      " [ 1.9081326e+00  1.0000000e+00  7.1781158e-02 ... -1.6668179e+02\n",
      "  -2.5670276e+02 -2.3482799e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.1 0.  0.  0.5 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.79268265e+00  1.00000000e+00  6.57707214e-01 ...  1.95877304e+02\n",
      "  -8.47522202e+01  7.24654846e+01]\n",
      " [ 1.79273987e+00  1.00000000e+00  6.57536507e-01 ... -2.03123703e+02\n",
      "  -2.46685822e+02 -1.25518265e+02]\n",
      " [ 1.79282188e+00  1.00000000e+00  6.57192826e-01 ... -5.91368141e+01\n",
      "  -1.86330967e+01  5.84341888e+01]\n",
      " ...\n",
      " [ 1.79278183e+00  1.00000000e+00  6.57250404e-01 ...  1.11204553e+00\n",
      "   3.01289463e+00 -5.79257369e-01]\n",
      " [ 1.79260635e+00  1.00000000e+00  6.57854080e-01 ... -9.19772461e+02\n",
      "   6.23810425e+02 -3.32609673e+01]\n",
      " [ 1.79254723e+00  1.00000000e+00  6.57987595e-01 ...  9.73029404e+01\n",
      "   3.81667480e+01  2.93436523e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5016203     1.            1.1796246  ...  -39.713123\n",
      "   199.80872     -71.41353   ]\n",
      " [   1.5017166     1.            1.1794834  ...  248.98174\n",
      "  -180.39813    -210.30978   ]\n",
      " [   1.5019169     1.            1.1791928  ...  153.74126\n",
      "     7.9325805  -101.30374   ]\n",
      " ...\n",
      " [   1.5018616     1.            1.1792488  ...    4.335284\n",
      "     8.383374      0.75630826]\n",
      " [   1.5015144     1.            1.1797504  ... -154.52965\n",
      "   -40.704563   -186.71507   ]\n",
      " [   1.5014057     1.            1.1798534  ...  -19.317532\n",
      "   -13.097676    -27.182705  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0632677e+00  1.0000000e+00  1.5861340e+00 ... -5.5858960e+02\n",
      "   1.0125504e+03  3.6100327e+02]\n",
      " [ 1.0634050e+00  1.0000000e+00  1.5860586e+00 ... -2.4448814e+02\n",
      "   1.6897667e+02 -4.2538960e+01]\n",
      " [ 1.0636864e+00  1.0000000e+00  1.5858581e+00 ... -3.9161087e+01\n",
      "   3.9814850e+01  2.5463802e+01]\n",
      " ...\n",
      " [ 1.0636139e+00  1.0000000e+00  1.5859089e+00 ... -5.1045728e-01\n",
      "  -3.9876623e+00  1.0628726e+00]\n",
      " [ 1.0631275e+00  1.0000000e+00  1.5862103e+00 ...  4.9118943e+00\n",
      "   6.4869255e+01  4.9932926e+01]\n",
      " [ 1.0629873e+00  1.0000000e+00  1.5862942e+00 ...  1.4267934e+02\n",
      "  -9.6528229e+01  1.7444812e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.2102470e-01  1.0000000e+00  1.8368683e+00 ...  5.5898315e+02\n",
      "  -7.8669104e+02 -2.6709131e+02]\n",
      " [ 5.2118778e-01  1.0000000e+00  1.8368521e+00 ... -4.6291615e+01\n",
      "   1.8177957e+02 -1.6573643e+02]\n",
      " [ 5.2149391e-01  1.0000000e+00  1.8367633e+00 ...  2.7314954e+02\n",
      "  -2.8150677e+01  5.4041862e+01]\n",
      " ...\n",
      " [ 5.2141190e-01  1.0000000e+00  1.8367910e+00 ... -5.6795068e+00\n",
      "  -1.5024519e+01 -1.1440651e+00]\n",
      " [ 5.2084351e-01  1.0000000e+00  1.8369179e+00 ...  1.0536860e+02\n",
      "  -3.4748404e+02  1.8707141e+02]\n",
      " [ 5.2070332e-01  1.0000000e+00  1.8369446e+00 ... -4.7768021e+01\n",
      "   1.4364157e+02 -2.3160670e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.2361946e-02  1.0000000e+00  1.9080105e+00 ...  1.1019432e+02\n",
      "   2.1098033e+02  2.6293009e+01]\n",
      " [-7.2191238e-02  1.0000000e+00  1.9080391e+00 ... -3.1947882e+02\n",
      "   1.9158347e+02 -8.2841293e+01]\n",
      " [-7.1863174e-02  1.0000000e+00  1.9080578e+00 ...  4.1563150e+02\n",
      "  -4.4448492e+02 -5.1773053e+02]\n",
      " ...\n",
      " [-7.1945190e-02  1.0000000e+00  1.9080582e+00 ... -2.7866783e+00\n",
      "  -8.5114002e+00  5.2670538e-01]\n",
      " [-7.2538376e-02  1.0000000e+00  1.9080067e+00 ... -7.8631754e+00\n",
      "  -7.0439415e+01 -8.3020683e+01]\n",
      " [-7.2704315e-02  1.0000000e+00  1.9079742e+00 ...  3.3061841e+02\n",
      "  -3.2089182e+02 -6.9060529e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.58626556e-01  1.00000000e+00  1.79221535e+00 ...  1.76833344e+01\n",
      "  -5.79388847e+01  1.00230614e+02]\n",
      " [-6.58471107e-01  1.00000000e+00  1.79226398e+00 ... -2.85735931e+02\n",
      "  -3.73751984e+02  1.74897903e+02]\n",
      " [-6.58197403e-01  1.00000000e+00  1.79240620e+00 ...  3.93230972e+01\n",
      "  -1.03438942e+02 -2.91254913e+02]\n",
      " ...\n",
      " [-6.58275604e-01  1.00000000e+00  1.79236603e+00 ...  1.84231520e-01\n",
      "   6.94345951e+00  1.77737117e-01]\n",
      " [-6.58828735e-01  1.00000000e+00  1.79215431e+00 ... -7.53339920e+01\n",
      "  -3.87259750e+01 -5.49728537e+00]\n",
      " [-6.58949852e-01  1.00000000e+00  1.79207611e+00 ...  3.01031494e+02\n",
      "   1.94286255e+02  1.17339554e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Upperarm\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.17997551e+00  1.00000000e+00  1.50101089e+00 ... -1.56684509e+02\n",
      "  -3.72687454e+01 -8.51856842e+01]\n",
      " [-1.17985725e+00  1.00000000e+00  1.50110722e+00 ...  9.38039703e+01\n",
      "   1.29891830e+02 -1.07794739e+03]\n",
      " [-1.17961693e+00  1.00000000e+00  1.50135112e+00 ...  1.28385666e+02\n",
      "  -1.17773544e+02 -1.05113533e+02]\n",
      " ...\n",
      " [-1.17968178e+00  1.00000000e+00  1.50128841e+00 ...  3.68411243e-02\n",
      "   3.31216788e+00  1.39304101e-02]\n",
      " [-1.18014526e+00  1.00000000e+00  1.50090218e+00 ...  3.04357007e+03\n",
      "   3.90752930e+02 -1.04887427e+03]\n",
      " [-1.18024254e+00  1.00000000e+00  1.50079346e+00 ...  2.24798630e+02\n",
      "   8.80146484e+01  1.60491837e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.58625793e+00  1.00000000e+00  1.06281853e+00 ...  8.85847855e+01\n",
      "   3.94006310e+01  1.03094345e+02]\n",
      " [-1.58617210e+00  1.00000000e+00  1.06291580e+00 ...  3.65072266e+02\n",
      "   2.63902435e+02 -3.20745819e+02]\n",
      " [-1.58600807e+00  1.00000000e+00  1.06323314e+00 ...  2.26835678e+02\n",
      "   1.65589676e+01 -7.54238205e+01]\n",
      " ...\n",
      " [-1.58605003e+00  1.00000000e+00  1.06316090e+00 ... -1.91885746e+00\n",
      "   5.70137691e+00  1.65185022e+00]\n",
      " [-1.58637428e+00  1.00000000e+00  1.06267548e+00 ... -1.03686353e+03\n",
      "  -7.49396912e+02 -4.15219696e+02]\n",
      " [-1.58642864e+00  1.00000000e+00  1.06251907e+00 ... -1.88766823e+01\n",
      "   5.24895668e+01  5.73369637e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8371964e+00  1.0000000e+00  5.2056122e-01 ... -6.8858435e+02\n",
      "   2.3418343e+02 -1.0181041e+02]\n",
      " [-1.8371449e+00  1.0000000e+00  5.2062702e-01 ... -1.1167838e+01\n",
      "   3.2125488e+01 -1.8571564e+01]\n",
      " [-1.8370266e+00  1.0000000e+00  5.2099955e-01 ...  2.1064075e+02\n",
      "   3.4058185e+02 -3.6153433e+02]\n",
      " ...\n",
      " [-1.8370361e+00  1.0000000e+00  5.2089882e-01 ... -2.2557018e+00\n",
      "   5.2811108e+00  6.3640976e-01]\n",
      " [-1.8372097e+00  1.0000000e+00  5.2037811e-01 ... -3.3913612e+01\n",
      "  -1.8997936e+01  2.6637579e+01]\n",
      " [-1.8372650e+00  1.0000000e+00  5.2019882e-01 ... -8.9360107e+01\n",
      "   1.1564208e+01  4.4980652e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9082012e+00  1.0000000e+00 -7.2925568e-02 ... -1.1648963e+02\n",
      "   1.6785175e+02  1.1200683e+01]\n",
      " [-1.9081841e+00  1.0000000e+00 -7.2886467e-02 ... -2.9594523e+04\n",
      "  -8.3561123e+03 -1.6749752e+04]\n",
      " [-1.9081707e+00  1.0000000e+00 -7.2487138e-02 ... -1.3887194e+00\n",
      "  -2.7733543e+01 -1.3376076e+02]\n",
      " ...\n",
      " [-1.9081593e+00  1.0000000e+00 -7.2604179e-02 ... -2.5810585e+00\n",
      "   5.1558247e+00  1.1354669e+00]\n",
      " [-1.9081726e+00  1.0000000e+00 -7.3112488e-02 ... -8.0902069e+01\n",
      "  -5.0808258e+01 -2.1507889e+01]\n",
      " [-1.9081545e+00  1.0000000e+00 -7.3297501e-02 ... -2.7004681e+02\n",
      "  -4.6830295e+02 -9.7253448e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79207230e+00  1.00000000e+00 -6.59078598e-01 ... -5.45476685e+01\n",
      "  -4.22093582e+00  5.98460732e+01]\n",
      " [-1.79208374e+00  1.00000000e+00 -6.59062386e-01 ... -4.94918994e+03\n",
      "   2.49943750e+04  1.13590381e+04]\n",
      " [-1.79219246e+00  1.00000000e+00 -6.58672333e-01 ... -7.29293776e+00\n",
      "   4.43618896e+02 -3.17091888e+02]\n",
      " ...\n",
      " [-1.79214859e+00  1.00000000e+00 -6.58794403e-01 ... -3.17224407e+00\n",
      "   2.98317456e+00  1.04368246e+00]\n",
      " [-1.79198647e+00  1.00000000e+00 -6.59254074e-01 ... -3.60817780e+02\n",
      "   2.82249012e+01 -4.04756683e+02]\n",
      " [-1.79192352e+00  1.00000000e+00 -6.59425735e-01 ... -9.26921310e+01\n",
      "  -1.01319756e+02  2.82318573e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5007362e+00  1.0000000e+00 -1.1806793e+00 ...  1.6994877e+00\n",
      "  -4.1004524e+00 -6.9951134e+01]\n",
      " [-1.5007753e+00  1.0000000e+00 -1.1806564e+00 ... -1.5912593e+04\n",
      "   3.4961590e+04  8.0445283e+03]\n",
      " [-1.5009575e+00  1.0000000e+00 -1.1803291e+00 ...  1.4507762e+03\n",
      "  -1.0310280e+03  2.9965244e+02]\n",
      " ...\n",
      " [-1.5008907e+00  1.0000000e+00 -1.1804323e+00 ...  5.2232575e+00\n",
      "  -6.5264802e+00  9.5123798e-02]\n",
      " [-1.5005894e+00  1.0000000e+00 -1.1808147e+00 ...  2.2238773e+02\n",
      "   2.3291606e+02 -2.1377173e+02]\n",
      " [-1.5004930e+00  1.0000000e+00 -1.1809597e+00 ...  2.2589350e+01\n",
      "   3.6630502e+00 -3.1232407e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.1 0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0621948     1.           -1.5868835  ...  155.16794\n",
      "  -155.25533      36.761463  ]\n",
      " [  -1.0622511     1.           -1.5868778  ...  -38.253304\n",
      "  -228.04909     100.511475  ]\n",
      " [  -1.0625        1.           -1.5866358  ...  -70.12538\n",
      "    70.23513     165.1561    ]\n",
      " ...\n",
      " [  -1.0623989     1.           -1.5867147  ...    2.6164212\n",
      "    -1.4047093     0.33970407]\n",
      " [  -1.0619793     1.           -1.5869598  ...  153.62889\n",
      "   -31.813347   -311.02274   ]\n",
      " [  -1.0618763     1.           -1.5870609  ...    7.5213904\n",
      "    11.24531      74.79836   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.1966667e-01  1.0000000e+00 -1.8375549e+00 ...  1.8465789e+03\n",
      "   1.5098566e+03  3.4979075e+03]\n",
      " [-5.1973438e-01  1.0000000e+00 -1.8375750e+00 ...  4.5738301e+03\n",
      "  -2.3328647e+03 -4.5091995e+02]\n",
      " [-5.2001762e-01  1.0000000e+00 -1.8374394e+00 ...  1.8993776e+02\n",
      "  -3.6976662e+01 -4.4718156e+00]\n",
      " ...\n",
      " [-5.1990509e-01  1.0000000e+00 -1.8374987e+00 ...  1.1407764e+01\n",
      "  -8.9460125e+00  4.7411239e-01]\n",
      " [-5.1942062e-01  1.0000000e+00 -1.8375912e+00 ... -6.7877368e+02\n",
      "   4.4588245e+01 -2.1410756e+02]\n",
      " [-5.1930237e-01  1.0000000e+00 -1.8376465e+00 ...  1.5076052e+01\n",
      "   2.1685535e+01  1.7371147e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.3326111e-02  1.0000000e+00 -1.9082489e+00 ...  8.2394751e+02\n",
      "  -8.7388904e+02  1.6328723e+03]\n",
      " [ 7.3253632e-02  1.0000000e+00 -1.9082813e+00 ...  1.7254359e+03\n",
      "   2.1132681e+03 -4.4286631e+03]\n",
      " [ 7.2956085e-02  1.0000000e+00 -1.9082695e+00 ...  5.9897180e+02\n",
      "   1.6074541e+02  2.3366385e+02]\n",
      " ...\n",
      " [ 7.3074341e-02  1.0000000e+00 -1.9082861e+00 ... -7.1063197e-01\n",
      "  -3.4115460e+00 -1.3263897e+00]\n",
      " [ 7.3583603e-02  1.0000000e+00 -1.9082127e+00 ...  1.7451488e+02\n",
      "  -8.8888426e+00 -2.2999585e+02]\n",
      " [ 7.3698997e-02  1.0000000e+00 -1.9082222e+00 ...  1.0131540e+02\n",
      "   6.1120544e+01  5.8254936e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.59430504e-01  1.00000000e+00 -1.79219627e+00 ...  8.42213318e+02\n",
      "  -8.43861328e+02 -6.12530640e+02]\n",
      " [ 6.59361839e-01  1.00000000e+00 -1.79225349e+00 ... -1.14368416e+02\n",
      "  -3.99773865e+01 -3.39999924e+01]\n",
      " [ 6.59076691e-01  1.00000000e+00 -1.79234910e+00 ...  8.11814499e+01\n",
      "  -1.74573441e+02 -2.80007568e+02]\n",
      " ...\n",
      " [ 6.59189224e-01  1.00000000e+00 -1.79232502e+00 ...  2.03984761e+00\n",
      "  -3.24506021e+00  9.55705166e-01]\n",
      " [ 6.59662247e-01  1.00000000e+00 -1.79210281e+00 ...  1.71107769e+01\n",
      "   2.92871284e+01  8.47061157e+00]\n",
      " [ 6.59787178e-01  1.00000000e+00 -1.79208565e+00 ...  1.15370926e+02\n",
      "   4.27299561e+02 -8.13197021e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1813402e+00  1.0000000e+00 -1.5004253e+00 ...  1.5862314e+02\n",
      "   5.4236670e+02  4.6910034e+01]\n",
      " [ 1.1812868e+00  1.0000000e+00 -1.5005484e+00 ... -9.6162178e+01\n",
      "   3.0791364e+00  4.2012939e+01]\n",
      " [ 1.1810856e+00  1.0000000e+00 -1.5007226e+00 ...  4.9084576e+01\n",
      "   5.6363445e+01  8.2536682e+01]\n",
      " ...\n",
      " [ 1.1811752e+00  1.0000000e+00 -1.5006809e+00 ...  2.1073635e+00\n",
      "  -1.9439812e+00 -5.3689337e-01]\n",
      " [ 1.1815662e+00  1.0000000e+00 -1.5002728e+00 ... -2.3969696e+02\n",
      "   4.9665698e+02  6.1275000e+02]\n",
      " [ 1.1816378e+00  1.0000000e+00 -1.5002136e+00 ... -1.1842281e+01\n",
      "   2.3275629e+02  4.7029816e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5870638e+00  1.0000000e+00 -1.0617714e+00 ... -1.6843731e+01\n",
      "  -4.5974789e+01  7.7257927e+01]\n",
      " [ 1.5870247e+00  1.0000000e+00 -1.0619068e+00 ...  1.7368291e+02\n",
      "  -9.3094959e+00  1.3133192e+01]\n",
      " [ 1.5869179e+00  1.0000000e+00 -1.0621440e+00 ...  7.1635663e+02\n",
      "   7.6437109e+02  1.2149425e+03]\n",
      " ...\n",
      " [ 1.5869789e+00  1.0000000e+00 -1.0620804e+00 ...  1.3785660e+00\n",
      "  -1.3475636e+01  6.1463273e-01]\n",
      " [ 1.5872517e+00  1.0000000e+00 -1.0615673e+00 ... -2.0258189e+02\n",
      "   4.5125214e+02  2.6725122e+02]\n",
      " [ 1.5872793e+00  1.0000000e+00 -1.0614910e+00 ...  1.0273139e+02\n",
      "  -1.5779187e+02 -9.0469254e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.83758450e+00  1.00000000e+00 -5.19227982e-01 ...  2.40584488e+01\n",
      "   6.90256424e+01  7.99263000e+01]\n",
      " [ 1.83757019e+00  1.00000000e+00 -5.19387245e-01 ...  3.72579498e+01\n",
      "   6.86893738e+02 -1.00347626e+03]\n",
      " [ 1.83752441e+00  1.00000000e+00 -5.19662201e-01 ...  2.04221634e+02\n",
      "  -3.25797791e+02 -4.04458733e+01]\n",
      " ...\n",
      " [ 1.83753967e+00  1.00000000e+00 -5.19581795e-01 ...  1.52730942e-03\n",
      "  -2.57940197e+00 -8.27980936e-02]\n",
      " [ 1.83767319e+00  1.00000000e+00 -5.18987656e-01 ...  9.00921783e+01\n",
      "  -3.92584000e+01 -1.87124100e+02]\n",
      " [ 1.83767891e+00  1.00000000e+00 -5.18890381e-01 ... -4.39269686e+00\n",
      "   2.53331814e+01 -7.78793573e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9081497e+00  1.0000000e+00  7.3839188e-02 ...  3.3782433e+01\n",
      "   6.6584824e+01  2.3665586e+01]\n",
      " [ 1.9081602e+00  1.0000000e+00  7.3699951e-02 ... -3.2749484e+02\n",
      "   1.8711727e+01  2.4826208e+02]\n",
      " [ 1.9081974e+00  1.0000000e+00  7.3399484e-02 ... -1.4258849e+03\n",
      "   1.6476170e+02 -1.6641235e+02]\n",
      " ...\n",
      " [ 1.9081898e+00  1.0000000e+00  7.3498726e-02 ... -5.6204736e-01\n",
      "  -3.2779455e-01 -5.2820981e-02]\n",
      " [ 1.9081516e+00  1.0000000e+00  7.4079514e-02 ...  4.3056599e+01\n",
      "   6.5100378e+02 -7.0473625e+01]\n",
      " [ 1.9081373e+00  1.0000000e+00  7.4182510e-02 ...  1.7476891e+02\n",
      "   1.1991374e+02 -7.3177284e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.79180622e+00  1.00000000e+00  6.60249710e-01 ...  8.25516357e+01\n",
      "   6.76534271e+00  4.44300995e+01]\n",
      " [ 1.79184532e+00  1.00000000e+00  6.60133362e-01 ... -1.22581696e+02\n",
      "   3.10303955e+02 -8.74100876e+01]\n",
      " [ 1.79196739e+00  1.00000000e+00  6.59860849e-01 ... -4.32167282e+01\n",
      "   2.59755058e+01  6.83138885e+01]\n",
      " ...\n",
      " [ 1.79193306e+00  1.00000000e+00  6.59945488e-01 ...  1.56704187e-02\n",
      "   9.91802216e-02 -6.34866953e-03]\n",
      " [ 1.79173470e+00  1.00000000e+00  6.60478592e-01 ...  5.40727905e+02\n",
      "  -6.51744446e+02 -9.75616074e+01]\n",
      " [ 1.79167843e+00  1.00000000e+00  6.60577774e-01 ... -9.92748260e+01\n",
      "  -4.18917809e+01 -5.27610283e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.9000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4999933e+00  1.0000000e+00  1.1816235e+00 ...  2.0290749e+02\n",
      "   4.2049738e+02  4.1567307e+01]\n",
      " [ 1.5000515e+00  1.0000000e+00  1.1815205e+00 ... -2.3177151e+03\n",
      "  -2.5570804e+02 -1.0670525e+03]\n",
      " [ 1.5002365e+00  1.0000000e+00  1.1812891e+00 ...  1.8574539e+02\n",
      "  -3.3620822e+02 -3.3574677e+02]\n",
      " ...\n",
      " [ 1.5001717e+00  1.0000000e+00  1.1813641e+00 ...  3.0247980e-01\n",
      "   3.9786544e+00 -1.9531408e-01]\n",
      " [ 1.4998093e+00  1.0000000e+00  1.1818180e+00 ... -1.7166650e+02\n",
      "  -7.4268074e+01 -1.2989578e+02]\n",
      " [ 1.4997625e+00  1.0000000e+00  1.1819077e+00 ...  1.1472116e+02\n",
      "   2.9112261e+01  1.0113761e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0614691e+00  1.0000000e+00  1.5871220e+00 ...  1.8255232e+01\n",
      "   6.4651154e+01  3.5451706e+01]\n",
      " [ 1.0615520e+00  1.0000000e+00  1.5870600e+00 ...  4.4048813e+01\n",
      "   1.2957675e+03 -3.4703412e+02]\n",
      " [ 1.0617867e+00  1.0000000e+00  1.5868891e+00 ... -5.9069321e+01\n",
      "  -2.4955710e+02  4.1968167e+02]\n",
      " ...\n",
      " [ 1.0616989e+00  1.0000000e+00  1.5869474e+00 ... -6.4297843e-01\n",
      "   1.2547625e+01  6.9012594e-01]\n",
      " [ 1.0612030e+00  1.0000000e+00  1.5872498e+00 ... -2.9644618e+00\n",
      "   1.3282603e+02  8.8041679e+01]\n",
      " [ 1.0611506e+00  1.0000000e+00  1.5873127e+00 ... -8.4599247e+00\n",
      "  -1.1550526e+02  5.1077183e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.18603325e-01  1.00000000e+00  1.83761215e+00 ... -1.30730072e+02\n",
      "  -1.51725140e+01 -4.85210495e+01]\n",
      " [ 5.18701553e-01  1.00000000e+00  1.83757496e+00 ... -1.09260273e+01\n",
      "   2.56857178e+02  1.04225914e+02]\n",
      " [ 5.18970490e-01  1.00000000e+00  1.83750033e+00 ...  2.02216843e+02\n",
      "  -1.70910660e+02 -3.73971634e+01]\n",
      " ...\n",
      " [ 5.18875122e-01  1.00000000e+00  1.83752728e+00 ... -1.63303173e+00\n",
      "   1.52637043e+01  4.58702445e-02]\n",
      " [ 5.18299103e-01  1.00000000e+00  1.83767700e+00 ... -2.57128410e+01\n",
      "  -1.13559923e+01  4.77338638e+01]\n",
      " [ 5.18241882e-01  1.00000000e+00  1.83770752e+00 ... -1.39051428e+01\n",
      "  -1.15868950e+01 -2.01143284e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.4674606e-02  1.0000000e+00  1.9077892e+00 ...  4.0393469e+02\n",
      "   1.6310464e+02 -4.3577579e+01]\n",
      " [-7.4573517e-02  1.0000000e+00  1.9078102e+00 ...  3.3903046e+01\n",
      "   3.7579617e+02 -3.2073184e+02]\n",
      " [-7.4260712e-02  1.0000000e+00  1.9078239e+00 ... -4.9080502e+01\n",
      "  -7.2420273e+00  7.5391418e+01]\n",
      " ...\n",
      " [-7.4361801e-02  1.0000000e+00  1.9078293e+00 ... -2.9213411e-01\n",
      "   7.2757449e+00  9.3424395e-02]\n",
      " [-7.4964523e-02  1.0000000e+00  1.9077740e+00 ...  2.6331979e+02\n",
      "  -8.0459579e+01  5.1289051e+01]\n",
      " [-7.5050354e-02  1.0000000e+00  1.9077740e+00 ...  1.7444214e+02\n",
      "   1.6208839e+02  2.3654190e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:8, Score:1.39, Best Score:2.33, Average Score:1.46, Best Avg Score:1.72\n",
      "Episode number: 9\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c3e8700>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1820526e+00  1.0000000e+00  1.4994698e+00 ...  2.2960588e+03\n",
      "  -1.3659487e+03  3.0599519e+03]\n",
      " [-1.1819735e+00  1.0000000e+00  1.4995356e+00 ...  8.4495071e+01\n",
      "   3.0511505e+02 -5.5541406e+02]\n",
      " [-1.1817436e+00  1.0000000e+00  1.4997282e+00 ...  1.0976578e+03\n",
      "  -1.3568317e+03 -4.2958462e+03]\n",
      " ...\n",
      " [-1.1818218e+00  1.0000000e+00  1.4996824e+00 ...  6.8074484e+00\n",
      "  -2.3234940e+00 -4.4468908e+00]\n",
      " [-1.1823235e+00  1.0000000e+00  1.4992905e+00 ... -4.9538698e+00\n",
      "   1.3596400e+03  1.3969667e+03]\n",
      " [-1.1823397e+00  1.0000000e+00  1.4992390e+00 ...  6.4955856e+01\n",
      "  -2.9106146e+02  2.0891171e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5878716e+00  1.0000000e+00  1.0604954e+00 ...  1.1893480e+02\n",
      "   1.3349769e+03  5.8206958e+02]\n",
      " [-1.5878124e+00  1.0000000e+00  1.0606022e+00 ... -5.2304764e+00\n",
      "  -1.5114117e+01  1.8468325e+01]\n",
      " [-1.5876598e+00  1.0000000e+00  1.0608594e+00 ... -3.7831142e+01\n",
      "  -4.8639633e+02  2.1723726e+03]\n",
      " ...\n",
      " [-1.5877247e+00  1.0000000e+00  1.0607948e+00 ... -5.3643422e+00\n",
      "   4.7628105e-01 -2.6057873e+00]\n",
      " [-1.5880795e+00  1.0000000e+00  1.0602608e+00 ... -1.2841178e+03\n",
      "   1.7509164e+02  2.9503186e+02]\n",
      " [-1.5880756e+00  1.0000000e+00  1.0601807e+00 ... -1.3695647e+00\n",
      "   9.9402153e+01 -2.4118408e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.5\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.83796787e+00  1.00000000e+00  5.17854691e-01 ...  2.61004425e+02\n",
      "  -8.67392090e+02 -1.40426208e+03]\n",
      " [-1.83793831e+00  1.00000000e+00  5.18012047e-01 ... -6.58668823e+01\n",
      "   4.21966400e+01 -4.52583456e+00]\n",
      " [-1.83789635e+00  1.00000000e+00  5.18304586e-01 ...  6.32254333e+01\n",
      "  -1.72382324e+02 -1.14943581e+01]\n",
      " ...\n",
      " [-1.83793831e+00  1.00000000e+00  5.18232346e-01 ... -1.03741474e-01\n",
      "  -7.79206038e-01 -8.71189177e-01]\n",
      " [-1.83809280e+00  1.00000000e+00  5.17591476e-01 ...  9.77733421e+00\n",
      "   4.34766273e+01 -3.81339684e+01]\n",
      " [-1.83807564e+00  1.00000000e+00  5.17501831e-01 ...  2.00765686e+01\n",
      "   2.15503120e+01 -2.74226456e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9080153e+00  1.0000000e+00 -7.5479507e-02 ...  2.7842249e+03\n",
      "  -1.2939816e+03 -1.9160914e+03]\n",
      " [-1.9080324e+00  1.0000000e+00 -7.5326920e-02 ...  4.7099683e+02\n",
      "  -8.7902673e+02  2.6752344e+02]\n",
      " [-1.9081078e+00  1.0000000e+00 -7.5018600e-02 ...  1.6272049e+01\n",
      "   1.8055496e+02 -3.1818115e+01]\n",
      " ...\n",
      " [-1.9081306e+00  1.0000000e+00 -7.5099945e-02 ... -3.5207257e+00\n",
      "  -1.5837398e+00  8.4831834e-01]\n",
      " [-1.9081001e+00  1.0000000e+00 -7.5746536e-02 ...  6.2003448e+01\n",
      "   5.0633354e+01  5.2819656e+01]\n",
      " [-1.9080076e+00  1.0000000e+00 -7.5843811e-02 ...  4.1071205e+00\n",
      "   1.1069756e+01 -5.2356060e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7913313e+00  1.0000000e+00 -6.6157913e-01 ...  1.0453672e+03\n",
      "   1.8722647e+02  4.5579903e+01]\n",
      " [-1.7913876e+00  1.0000000e+00 -6.6143990e-01 ...  3.8240781e+02\n",
      "   3.7905585e+02 -7.8463489e+02]\n",
      " [-1.7915440e+00  1.0000000e+00 -6.6114169e-01 ... -2.7133246e+02\n",
      "  -2.4944723e+01 -1.3678746e+03]\n",
      " ...\n",
      " [-1.7915382e+00  1.0000000e+00 -6.6123009e-01 ... -4.0285563e-01\n",
      "  -2.1976128e+00 -5.2317345e-01]\n",
      " [-1.7912979e+00  1.0000000e+00 -6.6182899e-01 ...  3.4051865e+01\n",
      "  -2.1049574e+01 -8.8682318e+00]\n",
      " [-1.7911968e+00  1.0000000e+00 -6.6192055e-01 ... -2.3882159e+02\n",
      "  -1.5126180e+02 -2.5173756e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4991465e+00  1.0000000e+00 -1.1829090e+00 ...  3.3109244e+02\n",
      "  -1.2592930e+02  9.4305292e+02]\n",
      " [-1.4992294e+00  1.0000000e+00 -1.1828156e+00 ...  7.1329926e+01\n",
      "   2.4001312e+01 -6.7018219e+01]\n",
      " [-1.4994831e+00  1.0000000e+00 -1.1825625e+00 ...  1.3127195e+01\n",
      "   3.8120132e+01 -7.3117874e+01]\n",
      " ...\n",
      " [-1.4994583e+00  1.0000000e+00 -1.1826372e+00 ... -2.9310541e+00\n",
      "  -9.4739044e-01  4.6240538e-02]\n",
      " [-1.4990463e+00  1.0000000e+00 -1.1831264e+00 ... -7.8265579e+01\n",
      "  -5.7193077e+01 -2.4633224e+02]\n",
      " [-1.4989166e+00  1.0000000e+00 -1.1832085e+00 ...  7.5619896e+01\n",
      "  -8.3226051e+01 -1.5459283e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0602789e+00  1.0000000e+00 -1.5883007e+00 ...  4.5367621e+02\n",
      "   3.4058475e+01  6.1297430e+02]\n",
      " [-1.0603819e+00  1.0000000e+00 -1.5882416e+00 ...  9.5790637e-01\n",
      "  -7.9628329e+00 -4.1235371e+01]\n",
      " [-1.0607147e+00  1.0000000e+00 -1.5880666e+00 ... -1.2435077e+02\n",
      "   7.7113091e+01 -4.7601772e+01]\n",
      " ...\n",
      " [-1.0606670e+00  1.0000000e+00 -1.5881090e+00 ...  5.7218778e-01\n",
      "   2.9417491e-01 -8.9511871e-03]\n",
      " [-1.0601368e+00  1.0000000e+00 -1.5884571e+00 ... -5.6364288e+01\n",
      "   6.4230988e+01 -1.6142214e+01]\n",
      " [-1.0599632e+00  1.0000000e+00 -1.5885258e+00 ... -3.3837440e+01\n",
      "   1.4601231e+01  9.9011612e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.\n",
      " 0.1 0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.1756096e-01  1.0000000e+00 -1.8381748e+00 ... -2.9620901e+01\n",
      "  -7.2562164e+01  4.2932350e+01]\n",
      " [-5.1768303e-01  1.0000000e+00 -1.8382120e+00 ... -9.4934502e+01\n",
      "   1.7193811e+02 -4.2044647e+01]\n",
      " [-5.1805115e-01  1.0000000e+00 -1.8381175e+00 ...  7.1243980e+01\n",
      "  -5.7879189e+01  5.4585442e+01]\n",
      " ...\n",
      " [-5.1799011e-01  1.0000000e+00 -1.8381414e+00 ...  1.0112537e+00\n",
      "   3.1313396e-01  5.2157432e-02]\n",
      " [-5.1736641e-01  1.0000000e+00 -1.8382587e+00 ... -5.0721255e+02\n",
      "   4.7971265e+02  4.3429337e+02]\n",
      " [-5.1718807e-01  1.0000000e+00 -1.8382988e+00 ...  5.4389600e+02\n",
      "   9.7419080e+02 -1.5760536e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.6017380e-02  1.0000000e+00 -1.9081631e+00 ... -1.3143852e+02\n",
      "  -8.0921379e+01 -1.6090329e+02]\n",
      " [ 7.5893402e-02  1.0000000e+00 -1.9082003e+00 ... -2.8907397e+02\n",
      "   7.0556061e+01 -3.9270953e+02]\n",
      " [ 7.5471878e-02  1.0000000e+00 -1.9082209e+00 ... -1.4667517e+02\n",
      "   2.4603932e+02 -9.3351555e+01]\n",
      " ...\n",
      " [ 7.5536728e-02  1.0000000e+00 -1.9082241e+00 ...  4.2581892e-01\n",
      "   3.5722572e-01  8.7117553e-03]\n",
      " [ 7.6190948e-02  1.0000000e+00 -1.9081764e+00 ... -1.4433055e+02\n",
      "   7.4701575e+02  8.1792102e+02]\n",
      " [ 7.6397896e-02  1.0000000e+00 -1.9081764e+00 ...  1.4191211e+02\n",
      "  -1.2875568e+01  8.1036523e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.61724091e-01  1.00000000e+00 -1.79141998e+00 ... -2.27845508e+03\n",
      "   6.32765991e+02 -1.02438461e+02]\n",
      " [ 6.61605835e-01  1.00000000e+00 -1.79142380e+00 ... -9.54448013e+01\n",
      "   4.80722160e+01 -4.93867722e+01]\n",
      " [ 6.61224365e-01  1.00000000e+00 -1.79156864e+00 ...  9.55581726e+02\n",
      "  -1.02251306e+03 -8.69412109e+02]\n",
      " ...\n",
      " [ 6.61287308e-01  1.00000000e+00 -1.79154205e+00 ...  1.38857841e-01\n",
      "   1.62595287e-01  1.14847720e-02]\n",
      " [ 6.61899567e-01  1.00000000e+00 -1.79134560e+00 ...  7.39961929e+01\n",
      "   2.43877274e+02  1.16303978e+02]\n",
      " [ 6.62075043e-01  1.00000000e+00 -1.79130554e+00 ...  9.99061584e+00\n",
      "   2.00979263e+02 -2.09591095e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1830053e+00  1.0000000e+00 -1.4991570e+00 ... -1.7313524e+02\n",
      "   7.7045143e+01  2.4072656e+02]\n",
      " [ 1.1829052e+00  1.0000000e+00 -1.4991932e+00 ... -1.9935519e+00\n",
      "   1.2197897e+01  2.3583231e+01]\n",
      " [ 1.1826000e+00  1.0000000e+00 -1.4994359e+00 ... -9.9109729e+02\n",
      "   2.1217104e+03 -8.5620624e+02]\n",
      " ...\n",
      " [ 1.1826591e+00  1.0000000e+00 -1.4993916e+00 ... -1.3125455e-01\n",
      "  -8.4578395e-02  1.1815190e-02]\n",
      " [ 1.1831722e+00  1.0000000e+00 -1.4990139e+00 ... -3.1236549e+01\n",
      "  -2.6135952e+01 -2.5617176e+01]\n",
      " [ 1.1832933e+00  1.0000000e+00 -1.4989452e+00 ...  8.6918500e+02\n",
      "   2.1175657e+03 -1.2231429e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5883818e+00  1.0000000e+00 -1.0601540e+00 ... -9.5130844e+01\n",
      "   1.1722878e+02 -2.9328320e+02]\n",
      " [ 1.5883112e+00  1.0000000e+00 -1.0602369e+00 ...  1.3939968e+03\n",
      "  -1.1716652e+03 -2.1422466e+03]\n",
      " [ 1.5881271e+00  1.0000000e+00 -1.0605503e+00 ...  2.8789215e+02\n",
      "   7.3661237e+02  4.4593713e+02]\n",
      " ...\n",
      " [ 1.5881691e+00  1.0000000e+00 -1.0604792e+00 ... -2.3742187e-01\n",
      "  -1.5218240e-01  1.2190819e-02]\n",
      " [ 1.5885239e+00  1.0000000e+00 -1.0599537e+00 ...  7.1545982e+00\n",
      "  -8.0582397e+01  6.7441452e+01]\n",
      " [ 1.5885916e+00  1.0000000e+00 -1.0598564e+00 ...  7.2708328e+01\n",
      "  -2.5368343e+02 -9.9800346e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.5 0.  0.  0.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8383427e+00  1.0000000e+00 -5.1705742e-01 ...  1.7072033e+01\n",
      "   5.3664433e+01  3.2817913e+01]\n",
      " [ 1.8382950e+00  1.0000000e+00 -5.1713562e-01 ... -2.0871713e+02\n",
      "  -1.5404292e+02  4.9352337e+01]\n",
      " [ 1.8382301e+00  1.0000000e+00 -5.1751155e-01 ...  2.7835844e+01\n",
      "   1.5571864e+01  1.3543142e+01]\n",
      " ...\n",
      " [ 1.8382397e+00  1.0000000e+00 -5.1742458e-01 ... -1.8409681e-01\n",
      "  -6.1504930e-02  9.1839172e-03]\n",
      " [ 1.8384018e+00  1.0000000e+00 -5.1682472e-01 ...  2.4371539e+02\n",
      "   1.2736926e+03  3.0742902e+02]\n",
      " [ 1.8384514e+00  1.0000000e+00 -5.1671600e-01 ...  2.5239558e+03\n",
      "  -2.6664104e+03 -7.5244281e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9081392e+00  1.0000000e+00  7.5975418e-02 ... -5.0717903e+01\n",
      "  -2.2735623e+02 -2.0722546e+01]\n",
      " [ 1.9081354e+00  1.0000000e+00  7.5942993e-02 ... -6.7054459e+01\n",
      "  -3.9074405e+02 -7.8272659e+01]\n",
      " [ 1.9081993e+00  1.0000000e+00  7.5536557e-02 ... -6.4138390e+01\n",
      "   1.8450771e+02 -1.8687247e+02]\n",
      " ...\n",
      " [ 1.9081917e+00  1.0000000e+00  7.5645447e-02 ...  5.4018319e-01\n",
      "   1.6475767e-03  1.4725912e-02]\n",
      " [ 1.9081516e+00  1.0000000e+00  7.6208115e-02 ... -2.8714294e+02\n",
      "   1.6065837e+02  7.4635002e+01]\n",
      " [ 1.9081478e+00  1.0000000e+00  7.6320648e-02 ... -1.0373670e+02\n",
      "  -3.0509564e+02  1.4651402e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7910891e+00  1.0000000e+00  6.6234589e-01 ... -5.4756374e+01\n",
      "   9.3459679e+01  1.8920053e+02]\n",
      " [ 1.7911215e+00  1.0000000e+00  6.6226387e-01 ...  4.4971622e+01\n",
      "   3.4991947e+01  4.1412590e+01]\n",
      " [ 1.7912884e+00  1.0000000e+00  6.6189688e-01 ... -4.1732010e+01\n",
      "  -4.8171448e+01 -8.8330872e+01]\n",
      " ...\n",
      " [ 1.7912560e+00  1.0000000e+00  6.6198349e-01 ...  2.2978282e+00\n",
      "   4.6947232e-01  1.3954663e-01]\n",
      " [ 1.7910309e+00  1.0000000e+00  6.6255760e-01 ... -3.2064590e+01\n",
      "  -6.1743579e+02  6.8782672e+02]\n",
      " [ 1.7909746e+00  1.0000000e+00  6.6266823e-01 ... -2.4353220e+03\n",
      "  -1.1862880e+03 -2.3122949e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4985676e+00  1.0000000e+00  1.1833153e+00 ...  7.1701294e+01\n",
      "   2.1965367e+02  1.3734058e+02]\n",
      " [ 1.4986200e+00  1.0000000e+00  1.1832180e+00 ...  1.4669691e+01\n",
      "   4.3595383e+02  1.1574359e+03]\n",
      " [ 1.4988976e+00  1.0000000e+00  1.1829064e+00 ...  8.2579979e+01\n",
      "  -2.1562454e+01 -2.7773029e+01]\n",
      " ...\n",
      " [ 1.4988327e+00  1.0000000e+00  1.1829767e+00 ...  1.9345149e+00\n",
      "   1.3036686e+00  7.6717925e-01]\n",
      " [ 1.4984150e+00  1.0000000e+00  1.1834888e+00 ... -7.3869568e+01\n",
      "  -1.3334375e+02 -6.9339569e+01]\n",
      " [ 1.4983416e+00  1.0000000e+00  1.1835842e+00 ...  6.2998928e+01\n",
      "   1.6313148e+02 -1.2573108e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0595684     1.            1.5884533  ...  -17.311157\n",
      "  -410.03253    -485.4615    ]\n",
      " [   1.0596399     1.            1.588398   ... -319.15173\n",
      "  -211.246       -29.870262  ]\n",
      " [   1.0600185     1.            1.5881816  ...  374.49362\n",
      "   176.26259    -346.38315   ]\n",
      " ...\n",
      " [   1.0599289     1.            1.5882263  ...    2.0855408\n",
      "     1.3285966     0.73170763]\n",
      " [   1.059391      1.            1.588583   ...   60.766624\n",
      "   -42.417202     46.357788  ]\n",
      " [   1.0592623     1.            1.5886555  ...   77.12073\n",
      "   275.4276     -110.14063   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.5168028     1.            1.838171   ...  124.64221\n",
      "   -16.686556    104.18368   ]\n",
      " [   0.51688385    1.            1.8381243  ...  119.05293\n",
      "    25.225199    -12.408786  ]\n",
      " [   0.5173378     1.            1.8380264  ...   38.03196\n",
      "  -123.80883     -98.68966   ]\n",
      " ...\n",
      " [   0.51722527    1.            1.8380308  ...    3.2450962\n",
      "     1.6424286     1.124524  ]\n",
      " [   0.516592      1.            1.8382416  ...  -42.05981\n",
      "     2.2714398   -38.292812  ]\n",
      " [   0.5164509     1.            1.8382607  ...   13.353101\n",
      "   503.38913     261.36523   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.69309998e-02  1.00000000e+00  1.90789413e+00 ... -1.18748405e+02\n",
      "   1.89659805e+01  1.48589890e+02]\n",
      " [-7.68451691e-02  1.00000000e+00  1.90790367e+00 ... -2.69920947e+03\n",
      "   6.46617859e+02  7.52240658e+00]\n",
      " [-7.64102936e-02  1.00000000e+00  1.90793526e+00 ...  8.78296566e+00\n",
      "  -2.94723625e+01  7.79600334e+00]\n",
      " ...\n",
      " [-7.65247345e-02  1.00000000e+00  1.90789700e+00 ...  6.47086525e+00\n",
      "   8.59465027e+00  3.24635148e-01]\n",
      " [-7.71808624e-02  1.00000000e+00  1.90788651e+00 ...  7.70977783e+01\n",
      "   9.41698265e+00  2.32562561e+01]\n",
      " [-7.72972107e-02  1.00000000e+00  1.90785599e+00 ...  3.40321312e+01\n",
      "  -1.09168144e+02 -9.61390991e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.6275787e-01  1.0000000e+00  1.7907848e+00 ...  6.7790155e+02\n",
      "   1.2641540e+00  9.0339554e+01]\n",
      " [-6.6267681e-01  1.0000000e+00  1.7908163e+00 ...  4.7433887e+02\n",
      "   1.6353888e+02 -4.9160770e+02]\n",
      " [-6.6227341e-01  1.0000000e+00  1.7909728e+00 ... -6.4431717e+01\n",
      "  -1.0914902e+02  3.0863715e+02]\n",
      " ...\n",
      " [-6.6237831e-01  1.0000000e+00  1.7909155e+00 ...  1.3557184e+00\n",
      "   6.4389038e+00  1.5331326e+00]\n",
      " [-6.6300201e-01  1.0000000e+00  1.7907200e+00 ...  1.9012164e+02\n",
      "   3.4143719e+01 -1.5661801e+02]\n",
      " [-6.6311169e-01  1.0000000e+00  1.7906418e+00 ...  1.3654137e+02\n",
      "   6.6949135e+01 -2.5029335e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1838369e+00  1.0000000e+00  1.4982662e+00 ...  1.8261443e+02\n",
      "  -1.4839127e+02 -3.7390714e+02]\n",
      " [-1.1837759e+00  1.0000000e+00  1.4982996e+00 ...  4.6857254e+01\n",
      "  -2.0923309e+02  3.4069279e+01]\n",
      " [-1.1834526e+00  1.0000000e+00  1.4985914e+00 ... -4.7250443e+00\n",
      "   4.1784299e+02  3.4090802e+02]\n",
      " ...\n",
      " [-1.1835365e+00  1.0000000e+00  1.4984999e+00 ...  3.0444720e+00\n",
      "   1.1217443e+01  4.4131771e-01]\n",
      " [-1.1840668e+00  1.0000000e+00  1.4981365e+00 ...  4.8186997e+03\n",
      "  -1.5583844e+04  1.3377876e+03]\n",
      " [-1.1841450e+00  1.0000000e+00  1.4980278e+00 ...  6.8786911e+01\n",
      "   1.1222906e+00  1.4613736e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5890207e+00  1.0000000e+00  1.0590305e+00 ... -5.6789362e+02\n",
      "   2.5233943e+02 -1.3034712e+02]\n",
      " [-1.5889788e+00  1.0000000e+00  1.0590563e+00 ...  8.1423817e+00\n",
      "  -5.6161022e-01  1.7801003e+00]\n",
      " [-1.5887413e+00  1.0000000e+00  1.0594326e+00 ... -5.5174426e+02\n",
      "   1.9665411e+02 -9.9173615e+01]\n",
      " ...\n",
      " [-1.5888023e+00  1.0000000e+00  1.0593195e+00 ... -6.1503017e-01\n",
      "   7.9588860e-01  4.2037091e-01]\n",
      " [-1.5891876e+00  1.0000000e+00  1.0588436e+00 ... -7.0687583e+03\n",
      "  -2.0160734e+03 -2.1253718e+03]\n",
      " [-1.5892401e+00  1.0000000e+00  1.0586891e+00 ...  7.8903966e+00\n",
      "   8.3384926e+01 -1.2197095e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8385105e+00  1.0000000e+00  5.1627159e-01 ...  1.6684203e+02\n",
      "  -8.3786781e+01 -3.3894333e+02]\n",
      " [-1.8384905e+00  1.0000000e+00  5.1631641e-01 ... -8.2788658e-01\n",
      "   1.9650964e+02  2.7151285e+02]\n",
      " [-1.8383713e+00  1.0000000e+00  5.1674831e-01 ... -1.0704328e+02\n",
      "  -3.6588097e+01 -2.8787491e+01]\n",
      " ...\n",
      " [-1.8383923e+00  1.0000000e+00  5.1662540e-01 ... -1.0458430e+00\n",
      "   1.6350428e+00  4.1215047e-01]\n",
      " [-1.8386135e+00  1.0000000e+00  5.1604652e-01 ...  5.3295312e+02\n",
      "   3.1608733e+03 -2.0533242e+03]\n",
      " [-1.8386059e+00  1.0000000e+00  5.1587677e-01 ...  2.7750767e+01\n",
      "   1.1189678e+01 -7.4676361e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9080887e+00  1.0000000e+00 -7.7272415e-02 ... -8.6374977e+01\n",
      "  -4.2113924e+00  3.6113342e+01]\n",
      " [-1.9080944e+00  1.0000000e+00 -7.7230453e-02 ... -2.2370361e+02\n",
      "   3.8071178e+01 -3.9664935e+02]\n",
      " [-1.9080849e+00  1.0000000e+00 -7.6787651e-02 ... -1.9163885e+02\n",
      "  -2.0486279e+02  5.1904126e+02]\n",
      " ...\n",
      " [-1.9080677e+00  1.0000000e+00 -7.6910973e-02 ... -1.9056031e-01\n",
      "   9.8301291e-01  2.1205534e-01]\n",
      " [-1.9081135e+00  1.0000000e+00 -7.7507019e-02 ...  9.9593076e+03\n",
      "  -4.2954681e+02  1.6423475e+03]\n",
      " [-1.9080601e+00  1.0000000e+00 -7.7686310e-02 ...  1.8273857e+01\n",
      "   4.9176626e+00 -2.8428291e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.79074478e+00  1.00000000e+00 -6.63415909e-01 ... -4.20255394e+01\n",
      "   1.18913902e+02  6.61237030e+01]\n",
      " [-1.79078102e+00  1.00000000e+00 -6.63372040e-01 ...  6.85512695e+02\n",
      "  -2.58985168e+02  1.22500893e+02]\n",
      " [-1.79090691e+00  1.00000000e+00 -6.62973940e-01 ...  2.99513519e+02\n",
      "   1.06022705e+02  5.00253418e+02]\n",
      " ...\n",
      " [-1.79084396e+00  1.00000000e+00 -6.63065910e-01 ...  3.40131187e+00\n",
      "  -1.22587452e+01  1.85602641e+00]\n",
      " [-1.79068756e+00  1.00000000e+00 -6.63637161e-01 ... -2.25482864e+02\n",
      "  -8.97678162e+02  4.76798737e+02]\n",
      " [-1.79059792e+00  1.00000000e+00 -6.63810730e-01 ... -2.31295557e+03\n",
      "   5.65788232e+03 -2.80615967e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4979420e+00  1.0000000e+00 -1.1840954e+00 ... -1.4664073e+02\n",
      "   1.6710843e+02  5.5189003e+01]\n",
      " [-1.4980049e+00  1.0000000e+00 -1.1840887e+00 ... -5.5670853e+00\n",
      "  -9.0778984e+01  4.0905312e+01]\n",
      " [-1.4982357e+00  1.0000000e+00 -1.1837518e+00 ...  1.1653279e+02\n",
      "  -7.0930310e+02  3.2486365e+02]\n",
      " ...\n",
      " [-1.4981518e+00  1.0000000e+00 -1.1838322e+00 ...  2.6823487e+00\n",
      "  -1.7795889e+01  9.2652166e-01]\n",
      " [-1.4978352e+00  1.0000000e+00 -1.1842670e+00 ...  9.5050812e+02\n",
      "  -3.6785583e+01  9.2268820e+00]\n",
      " [-1.4976788e+00  1.0000000e+00 -1.1844177e+00 ... -1.5745315e+03\n",
      "   5.3688765e+03  4.6746348e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0584097e+00  1.0000000e+00 -1.5892544e+00 ... -4.4695322e+03\n",
      "   1.3961119e+03 -6.4776538e+02]\n",
      " [-1.0584841e+00  1.0000000e+00 -1.5892658e+00 ... -7.3245056e+01\n",
      "   1.0334834e+02 -7.8704033e+01]\n",
      " [-1.0588207e+00  1.0000000e+00 -1.5890334e+00 ... -5.8665143e+02\n",
      "  -3.5277882e+01  1.6904034e+02]\n",
      " ...\n",
      " [-1.0587139e+00  1.0000000e+00 -1.5890770e+00 ...  1.1517091e+00\n",
      "  -5.2684288e+00  2.6626766e-01]\n",
      " [-1.0582523e+00  1.0000000e+00 -1.5893917e+00 ... -5.0026691e+01\n",
      "   4.3116391e+02 -5.4779565e+02]\n",
      " [-1.0580606e+00  1.0000000e+00 -1.5894756e+00 ... -6.1518608e+03\n",
      "  -4.5425396e+03  3.0107837e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.15371323e-01  1.00000000e+00 -1.83862877e+00 ... -2.76513195e+01\n",
      "  -5.75368408e+02  1.35756485e+02]\n",
      " [-5.15455246e-01  1.00000000e+00 -1.83862209e+00 ... -3.59709501e-01\n",
      "  -1.58282585e+01 -6.54724884e+01]\n",
      " [-5.15796661e-01  1.00000000e+00 -1.83852005e+00 ... -2.00187714e+02\n",
      "  -1.80500656e+02  7.36900940e+01]\n",
      " ...\n",
      " [-5.15670776e-01  1.00000000e+00 -1.83851337e+00 ... -9.21884537e-01\n",
      "   1.09935503e+01 -1.01213321e-01]\n",
      " [-5.15130997e-01  1.00000000e+00 -1.83870125e+00 ...  2.91929016e+02\n",
      "   2.29892548e+02  6.94569855e+01]\n",
      " [-5.14951706e-01  1.00000000e+00 -1.83872795e+00 ...  7.67529419e+02\n",
      "   1.29515247e+03 -4.52384229e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.78856277e-02  1.00000000e+00 -1.90789223e+00 ... -1.16964066e+02\n",
      "  -1.72288345e+02 -5.62434631e+02]\n",
      " [ 7.77978897e-02  1.00000000e+00 -1.90792179e+00 ... -3.69953583e+02\n",
      "  -9.12536621e+00 -2.00583817e+02]\n",
      " [ 7.74345398e-02  1.00000000e+00 -1.90795445e+00 ...  1.44990570e+02\n",
      "  -2.24138016e+02  7.94606705e+01]\n",
      " ...\n",
      " [ 7.75718689e-02  1.00000000e+00 -1.90790749e+00 ... -4.88573194e-01\n",
      "   4.33421087e+00  1.64606953e+00]\n",
      " [ 7.81250000e-02  1.00000000e+00 -1.90788841e+00 ...  1.17278000e+02\n",
      "   7.13095474e+01  8.15152359e+01]\n",
      " [ 7.83214569e-02  1.00000000e+00 -1.90786362e+00 ...  1.25731055e+03\n",
      "  -2.81439240e+02 -5.04231354e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.6395569e-01  1.0000000e+00 -1.7902355e+00 ... -1.4651762e+02\n",
      "   5.2325806e+01  1.4770952e+02]\n",
      " [ 6.6387558e-01  1.0000000e+00 -1.7902679e+00 ...  4.1338497e+01\n",
      "   2.0005684e+01 -1.3644829e+02]\n",
      " [ 6.6353798e-01  1.0000000e+00 -1.7904309e+00 ... -8.1632128e+00\n",
      "   2.1392683e+02 -7.2329514e+01]\n",
      " ...\n",
      " [ 6.6366959e-01  1.0000000e+00 -1.7903442e+00 ...  7.4334621e-01\n",
      "  -6.4605107e+00  8.6073011e-01]\n",
      " [ 6.6418076e-01  1.0000000e+00 -1.7901459e+00 ...  3.8838856e+01\n",
      "   1.5361990e+01 -3.5456577e+01]\n",
      " [ 6.6436195e-01  1.0000000e+00 -1.7900810e+00 ...  1.6084866e+02\n",
      "   7.5071735e+02 -3.0360217e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1846170e+00  1.0000000e+00 -1.4975491e+00 ... -1.7717128e+02\n",
      "   1.4461467e+02 -7.2989891e+01]\n",
      " [ 1.1845484e+00  1.0000000e+00 -1.4976025e+00 ...  3.8532886e+02\n",
      "   3.5079581e+02  1.3263755e+03]\n",
      " [ 1.1842842e+00  1.0000000e+00 -1.4978806e+00 ...  4.9478004e+01\n",
      "   3.3501797e+01 -5.8068970e+01]\n",
      " ...\n",
      " [ 1.1843948e+00  1.0000000e+00 -1.4977589e+00 ...  2.7871978e-01\n",
      "  -6.1256857e+00 -3.7011558e-01]\n",
      " [ 1.1848087e+00  1.0000000e+00 -1.4974022e+00 ...  2.6682755e+02\n",
      "  -1.4173808e+01 -6.5219139e+01]\n",
      " [ 1.1849623e+00  1.0000000e+00 -1.4972820e+00 ...  3.5889636e+02\n",
      "   3.0864556e+00 -3.0518451e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.58945656e+00  1.00000000e+00 -1.05818748e+00 ... -9.44135254e+02\n",
      "  -1.14622095e+03  1.61553638e+03]\n",
      " [ 1.58939552e+00  1.00000000e+00 -1.05824566e+00 ...  1.02867775e+02\n",
      "   1.60096268e+02  2.54026230e+02]\n",
      " [ 1.58927727e+00  1.00000000e+00 -1.05860090e+00 ... -2.61934937e+02\n",
      "   1.37531631e+02 -1.28754776e+02]\n",
      " ...\n",
      " [ 1.58936882e+00  1.00000000e+00 -1.05845833e+00 ...  1.84572017e+00\n",
      "  -3.15936971e+00 -1.17019188e+00]\n",
      " [ 1.58963776e+00  1.00000000e+00 -1.05798912e+00 ... -7.28304932e+02\n",
      "   1.60632278e+02  2.93457764e+02]\n",
      " [ 1.58971119e+00  1.00000000e+00 -1.05780792e+00 ...  4.42356110e+02\n",
      "  -2.71572754e+02 -2.09297882e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8387852e+00  1.0000000e+00 -5.1501465e-01 ... -2.9520288e+02\n",
      "  -2.3907179e+02 -2.7908624e+02]\n",
      " [ 1.8387489e+00  1.0000000e+00 -5.1509190e-01 ... -3.7430718e+03\n",
      "   6.5529502e+03  8.9205151e+02]\n",
      " [ 1.8387337e+00  1.0000000e+00 -5.1549655e-01 ... -1.5087475e+01\n",
      "  -1.7528324e+01  3.4315773e+01]\n",
      " ...\n",
      " [ 1.8387852e+00  1.0000000e+00 -5.1533699e-01 ...  1.1307831e+00\n",
      "  -1.4416323e+00 -1.7662923e+00]\n",
      " [ 1.8388786e+00  1.0000000e+00 -5.1478004e-01 ... -3.7182126e+02\n",
      "   5.0457013e+02 -2.2141711e+02]\n",
      " [ 1.8389149e+00  1.0000000e+00 -5.1457024e-01 ... -6.1974060e+01\n",
      "  -8.7110634e+00  4.2381306e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9079885e+00  1.0000000e+00  7.8468323e-02 ...  7.0321057e+02\n",
      "  -5.0238264e-01  3.1818515e+02]\n",
      " [ 1.9079762e+00  1.0000000e+00  7.8341484e-02 ...  9.2141449e+02\n",
      "  -3.7827127e+02  1.6039575e+02]\n",
      " [ 1.9080486e+00  1.0000000e+00  7.7933446e-02 ...  1.8403881e+01\n",
      "   1.3029650e+01 -2.0841236e+01]\n",
      " ...\n",
      " [ 1.9080505e+00  1.0000000e+00  7.8089714e-02 ...  1.3713468e+00\n",
      "   1.7241149e+00 -1.7148387e+00]\n",
      " [ 1.9079800e+00  1.0000000e+00  7.8710556e-02 ...  6.2408363e+01\n",
      "   1.2932114e+01  1.3670886e+02]\n",
      " [ 1.9079638e+00  1.0000000e+00  7.8926086e-02 ...  3.8223877e+02\n",
      "   2.6612039e+02 -1.7264280e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7903118e+00  1.0000000e+00  6.6452789e-01 ...  1.4278233e+02\n",
      "   7.9511406e+01 -3.6673477e+01]\n",
      " [ 1.7903185e+00  1.0000000e+00  6.6439915e-01 ...  6.7325275e+02\n",
      "   2.6038030e+03 -7.0133527e+02]\n",
      " [ 1.7905140e+00  1.0000000e+00  6.6401428e-01 ... -1.3110474e+02\n",
      "   3.2461853e+01  8.2336090e+01]\n",
      " ...\n",
      " [ 1.7904835e+00  1.0000000e+00  6.6416073e-01 ...  1.1854846e+00\n",
      "   3.8436580e+00 -1.7930965e+00]\n",
      " [ 1.7902184e+00  1.0000000e+00  6.6475296e-01 ...  1.9090719e+03\n",
      "   8.5772263e+01  7.2002679e+02]\n",
      " [ 1.7901468e+00  1.0000000e+00  6.6496086e-01 ...  5.9023434e+01\n",
      "  -1.8612261e+01 -7.1740349e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4974537e+00  1.0000000e+00  1.1851921e+00 ...  1.1339394e+01\n",
      "   2.1242334e+01 -1.9083536e+01]\n",
      " [ 1.4974880e+00  1.0000000e+00  1.1850634e+00 ...  9.4925714e+02\n",
      "   1.7158247e+03 -1.0706741e+03]\n",
      " [ 1.4977589e+00  1.0000000e+00  1.1847475e+00 ...  1.2446346e+00\n",
      "   9.8166199e+01 -3.2805234e+02]\n",
      " ...\n",
      " [ 1.4976845e+00  1.0000000e+00  1.1848621e+00 ... -1.3672903e+00\n",
      "  -5.1234138e-01 -2.5403905e+00]\n",
      " [ 1.4972458e+00  1.0000000e+00  1.1853733e+00 ...  1.3313563e+00\n",
      "   4.0192097e+01 -3.8599289e+01]\n",
      " [ 1.4971514e+00  1.0000000e+00  1.1855564e+00 ... -1.1997188e+02\n",
      "   5.0998233e+02 -2.1123462e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.1       0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0578375e+00  1.0000000e+00  1.5898628e+00 ... -7.3383667e+02\n",
      "  -9.9820038e+01  5.0302948e+02]\n",
      " [ 1.0578976e+00  1.0000000e+00  1.5897455e+00 ...  7.2269287e+01\n",
      "   2.0564009e+02 -2.8130707e+01]\n",
      " [ 1.0582600e+00  1.0000000e+00  1.5895271e+00 ...  1.1213113e+03\n",
      "  -2.0368559e+02  1.2046814e+03]\n",
      " ...\n",
      " [ 1.0581455e+00  1.0000000e+00  1.5896120e+00 ... -2.7645195e+00\n",
      "  -6.6902035e-01 -2.3170412e+00]\n",
      " [ 1.0575848e+00  1.0000000e+00  1.5899963e+00 ... -1.7038208e+02\n",
      "  -5.9145172e+01  7.9678436e+01]\n",
      " [ 1.0574303e+00  1.0000000e+00  1.5901337e+00 ... -4.0224323e+00\n",
      "  -3.7854917e+00  1.0573287e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.14599800e-01  1.00000000e+00  1.83900261e+00 ...  1.10621185e+02\n",
      "  -8.13241730e+01  5.23663940e+02]\n",
      " [ 5.14679909e-01  1.00000000e+00  1.83893108e+00 ...  7.89061737e+00\n",
      "   1.03765350e+02  6.87814789e+01]\n",
      " [ 5.15033722e-01  1.00000000e+00  1.83881938e+00 ... -5.59790527e+02\n",
      "   5.65689240e+01 -2.13211472e+02]\n",
      " ...\n",
      " [ 5.14896393e-01  1.00000000e+00  1.83887386e+00 ...  2.09829807e+00\n",
      "  -6.54916823e-01 -2.01324582e+00]\n",
      " [ 5.14274597e-01  1.00000000e+00  1.83906746e+00 ... -2.58778717e+02\n",
      "  -1.41411469e+02  9.38818207e+01]\n",
      " [ 5.14135361e-01  1.00000000e+00  1.83915710e+00 ... -4.85611677e-01\n",
      "   2.44967224e+02  1.91041412e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-7.9172134e-02  1.0000000e+00  1.9079132e+00 ...  1.5221396e+02\n",
      "   2.9390021e+02  1.2202975e+02]\n",
      " [-7.9090118e-02  1.0000000e+00  1.9078817e+00 ...  8.8051831e+02\n",
      "   4.8653088e+02 -1.3109564e+02]\n",
      " [-7.8762054e-02  1.0000000e+00  1.9078823e+00 ...  4.4047096e+01\n",
      "  -3.7496220e+01  3.5448879e+01]\n",
      " ...\n",
      " [-7.8908920e-02  1.0000000e+00  1.9078989e+00 ...  9.2117286e-01\n",
      "  -1.5720298e+00 -2.6093760e+00]\n",
      " [-7.9553604e-02  1.0000000e+00  1.9078846e+00 ...  2.3911064e+04\n",
      "  -4.0682297e+00 -2.6620569e+03]\n",
      " [-7.9649925e-02  1.0000000e+00  1.9079151e+00 ... -4.9185211e+01\n",
      "   1.5804060e+01  2.4256584e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.6489601e-01  1.0000000e+00  1.7900791e+00 ... -8.1770897e-02\n",
      "  -1.3573978e+02  1.2076478e+03]\n",
      " [-6.6482067e-01  1.0000000e+00  1.7900667e+00 ... -2.2701563e+01\n",
      "   4.3785484e+01  2.7489147e+01]\n",
      " [-6.6449547e-01  1.0000000e+00  1.7901815e+00 ...  1.2158612e+02\n",
      "   1.9932364e+02  5.9516945e+00]\n",
      " ...\n",
      " [-6.6463089e-01  1.0000000e+00  1.7901592e+00 ...  1.7612407e+00\n",
      "   4.9444313e+00 -2.1763127e+00]\n",
      " [-6.6523552e-01  1.0000000e+00  1.7899837e+00 ...  5.4539563e+02\n",
      "  -1.0571051e+03  2.6348904e+03]\n",
      " [-6.6534805e-01  1.0000000e+00  1.7899685e+00 ... -7.4638954e+01\n",
      "   6.2536865e+01  4.3491398e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.18539619e+00  1.00000000e+00  1.49711418e+00 ... -1.45345459e+02\n",
      "  -2.70426910e+02  8.79793762e+02]\n",
      " [-1.18533516e+00  1.00000000e+00  1.49713707e+00 ... -1.02832443e+02\n",
      "   6.48683777e+02  7.97215332e+02]\n",
      " [-1.18506432e+00  1.00000000e+00  1.49734533e+00 ...  8.42140274e+01\n",
      "  -3.78713684e+01 -6.94460220e+01]\n",
      " ...\n",
      " [-1.18517685e+00  1.00000000e+00  1.49730015e+00 ... -2.92456841e+00\n",
      "  -8.73229980e-01 -1.46357036e+00]\n",
      " [-1.18567276e+00  1.00000000e+00  1.49691772e+00 ...  1.35823975e+02\n",
      "  -8.23199585e+02  8.59831360e+02]\n",
      " [-1.18577290e+00  1.00000000e+00  1.49685287e+00 ... -1.37735306e+02\n",
      "  -1.27355194e+02 -4.98453751e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.590045     1.           1.0574493 ...  235.20255   -268.08243\n",
      "   110.71024  ]\n",
      " [  -1.5900106    1.           1.0575275 ... -307.6138     219.07553\n",
      "  -100.28082  ]\n",
      " [  -1.589819     1.           1.0577984 ...  196.82965    187.05318\n",
      "   136.97672  ]\n",
      " ...\n",
      " [  -1.5899105    1.           1.0577335 ...   -1.0393729   -0.7468995\n",
      "    -0.7312901]\n",
      " [  -1.5902691    1.           1.0571861 ...  218.04837   -430.56573\n",
      "   -41.803944 ]\n",
      " [  -1.590311     1.           1.0570965 ...   -3.7758489   17.446274\n",
      "    55.72589  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8390961e+00  1.0000000e+00  5.1401329e-01 ... -6.4586167e+00\n",
      "  -5.9017596e+00 -1.7494493e+01]\n",
      " [-1.8390875e+00  1.0000000e+00  5.1410294e-01 ... -4.1342255e+01\n",
      "   2.8451792e+01  2.2359358e+02]\n",
      " [-1.8390007e+00  1.0000000e+00  5.1441544e-01 ...  2.3682291e+02\n",
      "   4.4972144e+02  6.6129387e+01]\n",
      " ...\n",
      " [-1.8390503e+00  1.0000000e+00  5.1434135e-01 ...  6.8598032e-02\n",
      "  -6.9519407e-01 -1.8586503e+00]\n",
      " [-1.8392239e+00  1.0000000e+00  5.1371384e-01 ... -4.6238620e+02\n",
      "   4.3193035e+01 -2.8430179e+02]\n",
      " [-1.8392391e+00  1.0000000e+00  5.1360512e-01 ... -6.7899605e+01\n",
      "  -2.2219866e+01  5.9871441e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.5       0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9079294e+00  1.0000000e+00 -7.9549789e-02 ...  1.5123046e+01\n",
      "   4.9711411e+01  1.0060067e+01]\n",
      " [-1.9079494e+00  1.0000000e+00 -7.9479218e-02 ... -5.5564323e+01\n",
      "  -4.0838829e+01 -2.2005627e+01]\n",
      " [-1.9079514e+00  1.0000000e+00 -7.9137497e-02 ... -7.1991371e+01\n",
      "   2.7147641e+02 -7.3627663e+01]\n",
      " ...\n",
      " [-1.9079666e+00  1.0000000e+00 -7.9227448e-02 ...  3.8557966e+00\n",
      "  -3.2218680e+00 -9.1440457e-01]\n",
      " [-1.9079609e+00  1.0000000e+00 -7.9860687e-02 ...  1.0104398e+02\n",
      "  -3.0596915e+02 -1.3261993e+01]\n",
      " [-1.9079027e+00  1.0000000e+00 -7.9973221e-02 ...  3.1053967e+01\n",
      "  -7.6518631e+01  1.5905920e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.7900534     1.           -0.6650677  ...   77.20893\n",
      "   125.366585     10.699224  ]\n",
      " [  -1.7900887     1.           -0.66501427 ...  335.21683\n",
      "   358.92822    -288.82364   ]\n",
      " [  -1.790226      1.           -0.66468614 ...  -78.89888\n",
      "   160.29088    -103.05611   ]\n",
      " ...\n",
      " [  -1.7902164     1.           -0.6647701  ...    1.6807821\n",
      "    -3.161152     -3.3669758 ]\n",
      " [  -1.7899837     1.           -0.6653538  ...   87.41256\n",
      "    -4.2334714    56.5454    ]\n",
      " [  -1.7898817     1.           -0.6654587  ... -107.15725\n",
      "   -27.406261     -2.6578577 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.9000001 0.        0.        0.        0.        0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.496727      1.           -1.1859283  ... -115.34061\n",
      "  -170.91245      51.766655  ]\n",
      " [  -1.4967823     1.           -1.185893   ...   -6.005113\n",
      "    69.059135     14.393888  ]\n",
      " [  -1.4970322     1.           -1.1856245  ...  -75.912994\n",
      "   -11.9084635   -62.44206   ]\n",
      " ...\n",
      " [  -1.4969902     1.           -1.1856842  ...    0.27035886\n",
      "    -4.653665     -1.1248097 ]\n",
      " [  -1.4965687     1.           -1.1861877  ... -123.2424\n",
      "    -7.924392    -11.708313  ]\n",
      " [  -1.4964314     1.           -1.1862679  ...   46.10359\n",
      "    79.64017      20.062637  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.0570736     1.           -1.5903435  ...  235.5992\n",
      "   -22.188093     74.380974  ]\n",
      " [  -1.0571423     1.           -1.5903282  ...  -64.752846\n",
      "   -12.153732    -75.9369    ]\n",
      " [  -1.0574665     1.           -1.5901394  ...  -91.57498\n",
      "   -40.19233     -45.9591    ]\n",
      " ...\n",
      " [  -1.0573997     1.           -1.5901766  ...   -0.46152306\n",
      "    -4.86528      -1.2777903 ]\n",
      " [  -1.0568447     1.           -1.5905418  ...   44.307858\n",
      "    21.608765     24.352865  ]\n",
      " [  -1.0566645     1.           -1.5905781  ...   66.03888\n",
      "   152.5108     -127.820656  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -0.51368904    1.           -1.8392391  ...  -70.35728\n",
      "   -91.53215    -120.35235   ]\n",
      " [  -0.5137682     1.           -1.8392134  ...  -29.236774\n",
      "   -10.102745   -133.9359    ]\n",
      " [  -0.5141258     1.           -1.8391404  ...   46.051674\n",
      "    53.39127    -226.01654   ]\n",
      " ...\n",
      " [  -0.51405334    1.           -1.8391485  ...    2.1382163\n",
      "     0.7530738     0.32483602]\n",
      " [  -0.5134163     1.           -1.8393612  ...  -42.018543\n",
      "   -84.62343     -52.322704  ]\n",
      " [  -0.51322556    1.           -1.8393536  ...  -77.278656\n",
      "   -58.810352     29.13563   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 7.9830170e-02  1.0000000e+00 -1.9079437e+00 ... -1.0656030e+01\n",
      "  -1.8689386e+02 -4.4352548e+02]\n",
      " [ 7.9746246e-02  1.0000000e+00 -1.9079208e+00 ... -5.1386005e+01\n",
      "  -4.6982736e+02  2.3410481e+02]\n",
      " [ 7.9347610e-02  1.0000000e+00 -1.9079748e+00 ...  1.3474486e+02\n",
      "   5.1614769e+01 -3.1069049e+02]\n",
      " ...\n",
      " [ 7.9421997e-02  1.0000000e+00 -1.9079342e+00 ...  1.9888654e+00\n",
      "   2.7478355e-01 -1.2548530e+00]\n",
      " [ 8.0083847e-02  1.0000000e+00 -1.9079800e+00 ...  4.0880615e+01\n",
      "   3.5184204e+01  5.6929817e+01]\n",
      " [ 8.0309868e-02  1.0000000e+00 -1.9079189e+00 ...  1.6416199e+01\n",
      "   8.0936729e+01  3.3040588e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.6583252e-01  1.0000000e+00 -1.7897377e+00 ... -2.1900028e+01\n",
      "   2.7758911e+02 -1.7983041e+03]\n",
      " [ 6.6575146e-01  1.0000000e+00 -1.7897463e+00 ... -4.6402527e+01\n",
      "   4.3405225e+02 -1.6740286e+02]\n",
      " [ 6.6539574e-01  1.0000000e+00 -1.7899158e+00 ...  3.6268383e+01\n",
      "   3.7943245e+01 -9.2710220e+01]\n",
      " ...\n",
      " [ 6.6546822e-01  1.0000000e+00 -1.7898474e+00 ...  1.5942454e+00\n",
      "   9.8328227e-01 -1.2854267e+00]\n",
      " [ 6.6608429e-01  1.0000000e+00 -1.7896976e+00 ...  4.2625149e+01\n",
      "  -5.9553707e+01  1.6121349e+01]\n",
      " [ 6.6628647e-01  1.0000000e+00 -1.7895966e+00 ...  9.7210670e+01\n",
      "  -1.0808752e+02  4.7651287e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1861992    1.          -1.4964962 ...  295.17783    472.26175\n",
      "   103.647835 ]\n",
      " [   1.1861258    1.          -1.4965181 ...  260.54654   -537.4235\n",
      "   169.13399  ]\n",
      " [   1.1858139    1.          -1.4967844 ... -276.55673    456.14966\n",
      "  -220.99786  ]\n",
      " ...\n",
      " [   1.1858749    1.          -1.4966927 ...    3.3653684    1.5325058\n",
      "    -1.3171597]\n",
      " [   1.1864071    1.          -1.4963741 ... -835.07153     66.484955\n",
      "  -266.07434  ]\n",
      " [   1.1865768    1.          -1.4962349 ...   -5.3905725  109.73021\n",
      "  -345.12408  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.59041500e+00  1.00000000e+00 -1.05657387e+00 ... -6.02964630e+01\n",
      "  -5.25469894e+01 -1.49139771e+02]\n",
      " [ 1.59035587e+00  1.00000000e+00 -1.05659485e+00 ...  6.85876541e+01\n",
      "   2.04896652e+02 -3.05994453e+01]\n",
      " [ 1.59016418e+00  1.00000000e+00 -1.05695522e+00 ... -1.68997864e+02\n",
      "   2.03327209e+02 -3.50890106e+02]\n",
      " ...\n",
      " [ 1.59020424e+00  1.00000000e+00 -1.05683708e+00 ... -1.21549106e+00\n",
      "   2.88238317e-01 -1.03564644e+00]\n",
      " [ 1.59059143e+00  1.00000000e+00 -1.05638695e+00 ... -1.99545784e+01\n",
      "  -1.68264484e+00  6.34885559e+01]\n",
      " [ 1.59068108e+00  1.00000000e+00 -1.05620193e+00 ...  2.20834389e+01\n",
      "  -1.89430351e+01  1.09641285e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8392019e+00  1.0000000e+00 -5.1298714e-01 ...  4.3517178e+02\n",
      "  -5.6513934e+02 -2.2922757e+02]\n",
      " [ 1.8391657e+00  1.0000000e+00 -5.1302338e-01 ... -1.4112944e+02\n",
      "  -2.5747592e+02  4.7463727e+02]\n",
      " [ 1.8390999e+00  1.0000000e+00 -5.1344121e-01 ...  8.7063403e+02\n",
      "  -1.2670576e+02 -7.9878796e+02]\n",
      " ...\n",
      " [ 1.8391056e+00  1.0000000e+00 -5.1331615e-01 ... -3.6136727e+00\n",
      "   6.6952658e-01 -8.3917177e-01]\n",
      " [ 1.8393097e+00  1.0000000e+00 -5.1277733e-01 ... -5.7241657e+01\n",
      "   4.4642414e+01  7.6652031e+01]\n",
      " [ 1.8393459e+00  1.0000000e+00 -5.1255989e-01 ...  1.6119160e+02\n",
      "  -3.3801544e+01 -1.9126494e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.         0.3        0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9076090e+00  1.0000000e+00  8.0480576e-02 ...  3.1270954e+01\n",
      "  -6.2358395e+01  7.3043915e+01]\n",
      " [ 1.9075928e+00  1.0000000e+00  8.0422401e-02 ...  5.1326788e+02\n",
      "   2.9287753e+02  5.5127942e+02]\n",
      " [ 1.9076443e+00  1.0000000e+00  7.9987831e-02 ... -1.8559347e+03\n",
      "   1.9061854e+03  9.7340448e+02]\n",
      " ...\n",
      " [ 1.9076099e+00  1.0000000e+00  8.0122948e-02 ...  7.3388773e-01\n",
      "  -6.0789579e-01 -2.7110295e+00]\n",
      " [ 1.9076176e+00  1.0000000e+00  8.0698013e-02 ...  9.5937096e+01\n",
      "  -9.4897278e+01  1.4483202e+02]\n",
      " [ 1.9076023e+00  1.0000000e+00  8.0924988e-02 ... -4.8842101e+00\n",
      "   2.3619053e+01 -4.2677364e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.78936386e+00  1.00000000e+00  6.66278839e-01 ... -3.42984886e+01\n",
      "  -2.46038788e+02 -1.00229956e+03]\n",
      " [ 1.78936958e+00  1.00000000e+00  6.66198730e-01 ...  1.66924225e+02\n",
      "   3.55135536e+01 -1.49487915e+02]\n",
      " [ 1.78952217e+00  1.00000000e+00  6.65797293e-01 ...  4.82585754e+01\n",
      "  -3.47846870e+01  9.24481583e+00]\n",
      " ...\n",
      " [ 1.78946686e+00  1.00000000e+00  6.65920258e-01 ...  6.42670512e-01\n",
      "  -9.92788017e-01 -2.41361189e+00]\n",
      " [ 1.78926659e+00  1.00000000e+00  6.66477203e-01 ...  3.36133308e+01\n",
      "   7.56729698e+00  1.03000122e+02]\n",
      " [ 1.78921413e+00  1.00000000e+00  6.66687012e-01 ... -3.16451550e+01\n",
      "   4.03039307e+02  1.37156708e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4958754e+00  1.0000000e+00  1.1867485e+00 ...  6.0293343e+01\n",
      "  -2.1896105e+01 -1.2600227e+02]\n",
      " [ 1.4959068e+00  1.0000000e+00  1.1866865e+00 ... -1.5217711e+02\n",
      "   2.9540659e+02 -3.5283286e+02]\n",
      " [ 1.4961910e+00  1.0000000e+00  1.1863501e+00 ... -1.0410986e+02\n",
      "   3.2818439e+02 -5.0678174e+02]\n",
      " ...\n",
      " [ 1.4961128e+00  1.0000000e+00  1.1864433e+00 ... -1.0899467e+00\n",
      "  -2.9212713e-02 -1.5433161e+00]\n",
      " [ 1.4957581e+00  1.0000000e+00  1.1868973e+00 ... -4.2919374e+00\n",
      "  -4.0599184e+00 -1.4050344e+01]\n",
      " [ 1.4955959e+00  1.0000000e+00  1.1870899e+00 ...  3.2658691e+02\n",
      "   1.0370822e+02  1.8658813e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        1.0000001 0.        0.        0.\n",
      " 0.        0.9000001 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0557327e+00  1.0000000e+00  1.5911331e+00 ... -1.2770313e+02\n",
      "  -2.0611131e+02 -1.5301276e+02]\n",
      " [ 1.0557775e+00  1.0000000e+00  1.5910807e+00 ... -1.0150451e+03\n",
      "   1.6280630e+02 -7.3183632e+01]\n",
      " [ 1.0561676e+00  1.0000000e+00  1.5908511e+00 ... -4.1943485e+01\n",
      "   2.9874905e+02  1.3127541e+02]\n",
      " ...\n",
      " [ 1.0560570e+00  1.0000000e+00  1.5909185e+00 ... -1.4302158e+00\n",
      "  -1.2930353e+00 -1.5728396e+00]\n",
      " [ 1.0555782e+00  1.0000000e+00  1.5912209e+00 ... -7.7490288e+01\n",
      "   8.5605812e+00  3.4233109e+01]\n",
      " [ 1.0553608e+00  1.0000000e+00  1.5913715e+00 ...  1.6594242e+01\n",
      "  -1.7377329e+01  6.7585106e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.12335777e-01  1.00000000e+00  1.83958054e+00 ... -6.55393372e+02\n",
      "   1.86663361e+01  9.96167374e+01]\n",
      " [ 5.12391090e-01  1.00000000e+00  1.83952332e+00 ... -6.15228638e+02\n",
      "   1.07434265e+03  1.00863304e+02]\n",
      " [ 5.12823105e-01  1.00000000e+00  1.83942831e+00 ... -7.90559921e+01\n",
      "  -4.68393593e+01 -1.67066299e+02]\n",
      " ...\n",
      " [ 5.12689590e-01  1.00000000e+00  1.83945751e+00 ...  1.30179596e+01\n",
      "   3.44564128e+00 -8.58214974e-01]\n",
      " [ 5.12126923e-01  1.00000000e+00  1.83958817e+00 ...  1.53646378e+02\n",
      "  -1.33891205e+02  3.62002930e+02]\n",
      " [ 5.11901855e-01  1.00000000e+00  1.83966827e+00 ... -1.44308441e+02\n",
      "   3.16509644e+02 -2.20079132e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-8.13312531e-02  1.00000000e+00  1.90787506e+00 ... -3.69847031e+01\n",
      "  -1.61519089e+01  2.35310974e+01]\n",
      " [-8.12740326e-02  1.00000000e+00  1.90787888e+00 ...  2.33475098e+02\n",
      "  -5.33936844e+01  2.21289764e+02]\n",
      " [-8.08277130e-02  1.00000000e+00  1.90790498e+00 ... -7.14533844e+01\n",
      "  -2.14829636e+01 -5.00347939e+01]\n",
      " ...\n",
      " [-8.09612274e-02  1.00000000e+00  1.90791035e+00 ...  7.85499811e-01\n",
      "  -2.16482902e+00 -9.90977883e-01]\n",
      " [-8.15544128e-02  1.00000000e+00  1.90782738e+00 ... -7.65330261e+02\n",
      "  -1.17453165e+01  2.42104965e+02]\n",
      " [-8.17794800e-02  1.00000000e+00  1.90785027e+00 ...  5.27319450e+01\n",
      "  -4.59919319e+01 -4.95968895e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.66941643e-01  1.00000000e+00  1.78940773e+00 ... -1.97773895e+02\n",
      "   2.33021088e+02  5.25994019e+02]\n",
      " [-6.66891098e-01  1.00000000e+00  1.78941917e+00 ...  2.92930176e+02\n",
      "   1.02197655e+02 -2.82308655e+02]\n",
      " [-6.66460037e-01  1.00000000e+00  1.78957927e+00 ... -3.07386055e+01\n",
      "  -4.11780609e+02 -1.68139648e+02]\n",
      " ...\n",
      " [-6.66587830e-01  1.00000000e+00  1.78956127e+00 ...  2.76696229e+00\n",
      "  -1.92014623e+00 -5.40239334e-01]\n",
      " [-6.67142868e-01  1.00000000e+00  1.78928566e+00 ... -4.83249603e+02\n",
      "  -8.05731934e+02  5.59964722e+02]\n",
      " [-6.67372704e-01  1.00000000e+00  1.78924179e+00 ...  8.69061230e+03\n",
      "   7.05235840e+02  4.37652246e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1871586e+00  1.0000000e+00  1.4957905e+00 ... -2.4128395e+02\n",
      "  -2.1426505e+02  4.7176379e+02]\n",
      " [-1.1871166e+00  1.0000000e+00  1.4958467e+00 ... -1.1003871e+02\n",
      "   2.7478094e+02 -3.9852838e+02]\n",
      " [-1.1867886e+00  1.0000000e+00  1.4961237e+00 ...  3.1586148e+02\n",
      "  -1.9900045e+01  4.0006741e+01]\n",
      " ...\n",
      " [-1.1869011e+00  1.0000000e+00  1.4960737e+00 ...  1.5663629e+00\n",
      "  -1.8832297e+00 -1.2664546e+00]\n",
      " [-1.1873741e+00  1.0000000e+00  1.4956188e+00 ... -7.6059776e+01\n",
      "   2.1150693e+02 -1.6596107e+02]\n",
      " [-1.1875324e+00  1.0000000e+00  1.4955349e+00 ...  1.2609268e+03\n",
      "   1.5038510e+03 -7.1471130e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5915442e+00  1.0000000e+00  1.0553627e+00 ...  8.3329437e+01\n",
      "   7.6007645e+01 -2.5189259e+02]\n",
      " [-1.5915155e+00  1.0000000e+00  1.0554438e+00 ... -7.1296974e+01\n",
      "  -2.6506293e+02  1.0875792e+02]\n",
      " [-1.5912895e+00  1.0000000e+00  1.0558214e+00 ...  7.2763109e-01\n",
      "   1.1880638e+01  1.0645547e+01]\n",
      " ...\n",
      " [-1.5913734e+00  1.0000000e+00  1.0557356e+00 ...  1.3674508e+00\n",
      "  -1.4403284e+00 -1.1856356e+00]\n",
      " [-1.5917130e+00  1.0000000e+00  1.0551338e+00 ...  9.1554504e+01\n",
      "   4.9397972e+01  6.8311264e+01]\n",
      " [-1.5918102e+00  1.0000000e+00  1.0550022e+00 ...  2.7101135e+03\n",
      "  -2.9796967e+02 -1.9170178e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.8397799     1.            0.5119209  ... -258.3238\n",
      "    82.31761      69.468636  ]\n",
      " [  -1.8397627     1.            0.5120249  ...  171.14688\n",
      "  -300.12695     181.95119   ]\n",
      " [  -1.839653      1.            0.51244533 ...   84.04483\n",
      "  -143.03752    -122.2559    ]\n",
      " ...\n",
      " [  -1.8397102     1.            0.5123434  ...    1.7389096\n",
      "    -2.3377917    -1.0286319 ]\n",
      " [  -1.8398666     1.            0.51166534 ...  -85.70499\n",
      "   165.03514      48.830532  ]\n",
      " [  -1.8399143     1.            0.51151085 ...   18.763098\n",
      "    41.1906      210.44075   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90779018e+00  1.00000000e+00 -8.16745758e-02 ... -2.95665833e+02\n",
      "   9.47353134e+01 -1.03258801e+03]\n",
      " [-1.90779400e+00  1.00000000e+00 -8.15811157e-02 ...  9.33223724e+00\n",
      "  -9.87335443e-01 -9.30144501e+00]\n",
      " [-1.90782166e+00  1.00000000e+00 -8.11340138e-02 ... -1.03739975e+02\n",
      "  -1.05285469e+02 -1.34870773e+02]\n",
      " ...\n",
      " [-1.90782547e+00  1.00000000e+00 -8.12530518e-02 ... -5.31217217e-01\n",
      "  -8.52243233e+00 -1.07219219e+00]\n",
      " [-1.90777016e+00  1.00000000e+00 -8.19339752e-02 ...  3.75234604e+01\n",
      "  -1.80078461e+02 -2.42754602e+00]\n",
      " [-1.90779018e+00  1.00000000e+00 -8.20941925e-02 ... -5.54441681e+01\n",
      "  -2.90055634e+02 -6.63291473e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.7891092     1.           -0.66747665 ... -208.67442\n",
      "   -21.491154   -425.60825   ]\n",
      " [  -1.7891331     1.           -0.66738796 ...  104.22402\n",
      "   -18.8845      267.1567    ]\n",
      " [  -1.7892838     1.           -0.6669831  ...   57.50897\n",
      "   -15.960966   -112.66899   ]\n",
      " ...\n",
      " [  -1.7892494     1.           -0.66708946 ...    1.6530299\n",
      "     0.89854985   -1.8696089 ]\n",
      " [  -1.7890053     1.           -0.667717   ...  -26.808252\n",
      "    84.94933     161.12672   ]\n",
      " [  -1.7889862     1.           -0.66786575 ...   41.397495\n",
      "   -73.693474    -47.999084  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.4       0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.495264     1.          -1.1877213 ... -376.847     -312.09827\n",
      "    33.997585 ]\n",
      " [  -1.4953127    1.          -1.1876459 ...  139.81334    -96.9017\n",
      "    36.97749  ]\n",
      " [  -1.4955654    1.          -1.1873002 ...  190.59373    248.3874\n",
      "   -59.87603  ]\n",
      " ...\n",
      " [  -1.4954929    1.          -1.1873875 ...    2.5246441    2.5430975\n",
      "    -1.911917 ]\n",
      " [  -1.4950924    1.          -1.1879082 ...    0.9491404  -12.731955\n",
      "    19.356651 ]\n",
      " [  -1.4950256    1.          -1.1880322 ...  -94.80027      8.282246\n",
      "    34.24152  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.4\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0549021e+00  1.0000000e+00 -1.5917664e+00 ...  3.6595325e+02\n",
      "   4.2593851e+02  8.1694879e+02]\n",
      " [-1.0549707e+00  1.0000000e+00 -1.5917196e+00 ... -1.6017506e+02\n",
      "   1.7272008e+02 -1.3493268e+02]\n",
      " [-1.0553093e+00  1.0000000e+00 -1.5914770e+00 ... -5.8905078e+02\n",
      "   4.6404700e+02  6.1243042e+02]\n",
      " ...\n",
      " [-1.0552025e+00  1.0000000e+00 -1.5915356e+00 ...  5.4857707e-01\n",
      "   1.6569463e+00 -2.0940919e+00]\n",
      " [-1.0546494e+00  1.0000000e+00 -1.5918903e+00 ...  3.3479259e+01\n",
      "   5.6498318e+01  2.5765015e+01]\n",
      " [-1.0545893e+00  1.0000000e+00 -1.5919838e+00 ...  1.8675121e+02\n",
      "   3.0239971e+01 -1.6751021e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.11433601e-01  1.00000000e+00 -1.83979416e+00 ...  8.08059875e+02\n",
      "   5.35735474e+02  2.07689850e+02]\n",
      " [-5.11515617e-01  1.00000000e+00 -1.83979511e+00 ...  1.42777237e+02\n",
      "  -5.39765587e+01 -5.88106804e+01]\n",
      " [-5.11873245e-01  1.00000000e+00 -1.83967400e+00 ...  3.67494354e+02\n",
      "  -2.90556427e+02 -5.71955757e+01]\n",
      " ...\n",
      " [-5.11747360e-01  1.00000000e+00 -1.83969402e+00 ... -1.36590552e+00\n",
      "  -6.78904474e-01 -2.02089334e+00]\n",
      " [-5.11108398e-01  1.00000000e+00 -1.83985138e+00 ...  1.34089615e+02\n",
      "   1.01286674e+02  4.43409958e+01]\n",
      " [-5.11071205e-01  1.00000000e+00 -1.83989716e+00 ... -1.08933342e+02\n",
      "   2.17222023e+01  1.06913109e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.2092285e-02  1.0000000e+00 -1.9077606e+00 ... -1.3253360e+02\n",
      "   2.6177637e+01  2.5713647e+02]\n",
      " [ 8.2006454e-02  1.0000000e+00 -1.9078150e+00 ...  1.8991652e+02\n",
      "   3.3488467e+02  6.3044174e+01]\n",
      " [ 8.1636429e-02  1.0000000e+00 -1.9078079e+00 ...  1.4324500e+02\n",
      "   2.9624947e+01 -8.8692642e+01]\n",
      " ...\n",
      " [ 8.1766129e-02  1.0000000e+00 -1.9078007e+00 ... -9.4901973e-01\n",
      "  -2.1684891e-01 -2.0093529e+00]\n",
      " [ 8.2426071e-02  1.0000000e+00 -1.9077435e+00 ... -3.0181904e+01\n",
      "  -9.0956297e+00  5.3786846e+01]\n",
      " [ 8.2464218e-02  1.0000000e+00 -1.9077511e+00 ... -2.4615362e+01\n",
      "   4.8023037e+01  5.7549263e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.6796684e-01  1.0000000e+00 -1.7887058e+00 ...  5.8805597e+02\n",
      "  -3.4921532e+01 -1.7727187e+02]\n",
      " [ 6.6788769e-01  1.0000000e+00 -1.7888031e+00 ... -5.6217026e+01\n",
      "   5.2200745e+01  3.2698196e+01]\n",
      " [ 6.6755676e-01  1.0000000e+00 -1.7889072e+00 ... -7.2869705e+01\n",
      "  -1.6821237e+00 -4.6306175e+01]\n",
      " ...\n",
      " [ 6.6768646e-01  1.0000000e+00 -1.7888670e+00 ...  8.8389242e-01\n",
      "  -9.8599635e-02 -2.1023149e+00]\n",
      " [ 6.6830635e-01  1.0000000e+00 -1.7886105e+00 ... -1.3319998e+01\n",
      "  -2.6605740e+02  1.6358241e+02]\n",
      " [ 6.6831875e-01  1.0000000e+00 -1.7885876e+00 ...  1.0647495e+02\n",
      "   3.4462619e+02  3.7207756e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1880112e+00  1.0000000e+00 -1.4947987e+00 ...  1.1626126e+00\n",
      "  -4.1209728e+01 -2.0384824e+00]\n",
      " [ 1.1879473e+00  1.0000000e+00 -1.4949207e+00 ...  3.4273392e+02\n",
      "   2.4059963e+03 -4.2326553e+03]\n",
      " [ 1.1877041e+00  1.0000000e+00 -1.4951291e+00 ...  1.7444055e+02\n",
      "  -6.8265100e+02 -4.4205164e+02]\n",
      " ...\n",
      " [ 1.1878223e+00  1.0000000e+00 -1.4950609e+00 ...  9.4868988e-02\n",
      "   1.5432805e-01 -1.9970059e+00]\n",
      " [ 1.1883373e+00  1.0000000e+00 -1.4946289e+00 ... -1.6307143e+02\n",
      "  -2.2487610e+02  1.2434442e+02]\n",
      " [ 1.1883039e+00  1.0000000e+00 -1.4945850e+00 ...  4.7776248e+02\n",
      "  -5.2414051e+01  7.7251884e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5916977    1.          -1.0544128 ...   51.76038   -231.9857\n",
      "   -44.839294 ]\n",
      " [   1.5916529    1.          -1.054574  ...   39.393295   162.2069\n",
      "  -182.0473   ]\n",
      " [   1.5915165    1.          -1.0548605 ... -177.35947    100.13063\n",
      "  -203.16037  ]\n",
      " ...\n",
      " [   1.5916042    1.          -1.0547676 ...    0.9550678    2.6803753\n",
      "    -1.9926941]\n",
      " [   1.59198      1.          -1.0541801 ...  -41.201607  -101.11735\n",
      "   -55.009712 ]\n",
      " [   1.5919037    1.          -1.0541229 ...   63.27869     19.69841\n",
      "  -183.58252  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8397694     1.           -0.5108433  ...  150.82332\n",
      "  -105.607124   -124.04147   ]\n",
      " [   1.8397493     1.           -0.51100636 ...  131.19388\n",
      "    95.22421    -109.7389    ]\n",
      " [   1.8397388     1.           -0.5113426  ... -224.64523\n",
      "   138.93736     -97.42587   ]\n",
      " ...\n",
      " [   1.839777      1.           -0.51123524 ...    5.466007\n",
      "     4.2686076    -1.7190727 ]\n",
      " [   1.8399506     1.           -0.51057816 ...   98.62588\n",
      "   -97.82868    -114.25752   ]\n",
      " [   1.8398647     1.           -0.5105095  ... -161.78957\n",
      "    58.435867     24.334888  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9076061e+00  1.0000000e+00  8.3177567e-02 ... -7.2844983e+02\n",
      "   4.0490082e+02  3.9556039e+02]\n",
      " [ 1.9076147e+00  1.0000000e+00  8.3004951e-02 ...  1.4531334e+01\n",
      "  -1.6264670e+02  1.3519759e+02]\n",
      " [ 1.9077053e+00  1.0000000e+00  8.2651615e-02 ...  1.5464018e+02\n",
      "  -1.9987537e+02  1.2976323e+02]\n",
      " ...\n",
      " [ 1.9077168e+00  1.0000000e+00  8.2767487e-02 ...  2.9408398e+00\n",
      "  -1.7302853e+00 -2.1492393e+00]\n",
      " [ 1.9076939e+00  1.0000000e+00  8.3456039e-02 ... -3.5306392e+00\n",
      "  -5.9494778e+01  4.7686562e+01]\n",
      " [ 1.9075670e+00  1.0000000e+00  8.3526611e-02 ... -8.4233917e+01\n",
      "  -1.3209566e+02  8.5209793e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7886496e+00  1.0000000e+00  6.6855240e-01 ...  3.1832556e+01\n",
      "   2.7561954e+02 -3.5781250e+02]\n",
      " [ 1.7886934e+00  1.0000000e+00  6.6840172e-01 ...  1.3730540e+02\n",
      "   1.7084155e+02  1.5963908e+02]\n",
      " [ 1.7889099e+00  1.0000000e+00  6.6806483e-01 ... -7.1350476e+02\n",
      "  -4.2610120e+02 -1.2501871e+02]\n",
      " ...\n",
      " [ 1.7888851e+00  1.0000000e+00  6.6817856e-01 ...  3.5430579e+00\n",
      "  -2.0132389e+00 -2.3565502e+00]\n",
      " [ 1.7886391e+00  1.0000000e+00  6.6881561e-01 ... -1.2837602e+02\n",
      "  -4.4565731e+01  5.3751095e+01]\n",
      " [ 1.7884912e+00  1.0000000e+00  6.6888618e-01 ...  8.5478172e+01\n",
      "   4.2568915e+02  3.1375708e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4946108e+00  1.0000000e+00  1.1885567e+00 ...  4.2823040e+01\n",
      "   1.0701390e+01 -8.4445679e+01]\n",
      " [ 1.4946899e+00  1.0000000e+00  1.1884375e+00 ...  5.9570423e+01\n",
      "   2.6318863e+01 -8.6404762e+01]\n",
      " [ 1.4949780e+00  1.0000000e+00  1.1881574e+00 ...  7.3558510e+01\n",
      "   3.5421497e+02  4.6186184e+01]\n",
      " ...\n",
      " [ 1.4949322e+00  1.0000000e+00  1.1882629e+00 ...  5.3525700e+01\n",
      "  -1.6168228e+01  6.2480232e+01]\n",
      " [ 1.4945374e+00  1.0000000e+00  1.1887798e+00 ... -1.4480475e+03\n",
      "   1.0171494e+03  1.2586323e+03]\n",
      " [ 1.4943371e+00  1.0000000e+00  1.1888409e+00 ...  3.3243076e+01\n",
      "  -2.2147337e+02 -7.7342735e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0537128    1.           1.5923595 ...  -75.07753     36.012527\n",
      "  -114.85697  ]\n",
      " [   1.0538206    1.           1.5922842 ...  145.51552      6.761182\n",
      "   142.73013  ]\n",
      " [   1.0542431    1.           1.5920812 ...   -2.7061925  187.71187\n",
      "   -93.115395 ]\n",
      " ...\n",
      " [   1.0541801    1.           1.5921812 ...    5.5847487   -3.4028084\n",
      "    10.715082 ]\n",
      " [   1.0536366    1.           1.5925274 ... -118.005974   132.42455\n",
      "    14.335554 ]\n",
      " [   1.0533571    1.           1.5925694 ... -376.8288     -28.856802\n",
      "   435.5686   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.10241508e-01  1.00000000e+00  1.84003067e+00 ... -1.12035103e+02\n",
      "  -1.06455116e+02 -1.05995735e+02]\n",
      " [ 5.10366440e-01  1.00000000e+00  1.83998299e+00 ... -1.12532568e+03\n",
      "  -1.33010498e+02 -5.56812561e+02]\n",
      " [ 5.10862350e-01  1.00000000e+00  1.83990073e+00 ... -3.39152679e+01\n",
      "   1.95086873e+00  1.00583855e+02]\n",
      " ...\n",
      " [ 5.10786057e-01  1.00000000e+00  1.83996105e+00 ...  1.22244959e+01\n",
      "  -8.18188858e+00  8.01479721e+00]\n",
      " [ 5.10147095e-01  1.00000000e+00  1.84014130e+00 ... -2.72091888e+02\n",
      "  -4.49399353e+02 -4.34324341e+01]\n",
      " [ 5.09825706e-01  1.00000000e+00  1.84014511e+00 ...  4.88974152e+02\n",
      "  -9.43560608e+02 -1.45065887e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-8.3483696e-02  1.0000000e+00  1.9076881e+00 ... -2.1835022e+02\n",
      "  -3.1292343e+01 -1.0590281e+02]\n",
      " [-8.3355904e-02  1.0000000e+00  1.9076805e+00 ...  1.3230613e+01\n",
      "   1.1186673e+02  9.1523216e+01]\n",
      " [-8.2839966e-02  1.0000000e+00  1.9077246e+00 ...  1.5738867e+01\n",
      "   1.0350925e+02 -1.3301768e+02]\n",
      " ...\n",
      " [-8.2920074e-02  1.0000000e+00  1.9077415e+00 ... -3.9086956e-01\n",
      "  -2.6932616e+00  1.2779797e+01]\n",
      " [-8.3583832e-02  1.0000000e+00  1.9077301e+00 ... -5.1530530e+02\n",
      "   1.5408846e+02  1.0547388e+03]\n",
      " [-8.3909035e-02  1.0000000e+00  1.9076862e+00 ... -3.5062013e+00\n",
      "  -5.9371960e+01 -9.1910454e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.6919231e-01  1.0000000e+00  1.7884789e+00 ... -4.4457203e+01\n",
      "  -3.1638638e+01 -5.3349884e+02]\n",
      " [-6.6907597e-01  1.0000000e+00  1.7885303e+00 ...  5.7139397e+01\n",
      "   2.2519951e+02 -2.1590497e+02]\n",
      " [-6.6857719e-01  1.0000000e+00  1.7886987e+00 ... -6.5598633e+02\n",
      "   6.0683429e+02  3.7755066e+02]\n",
      " ...\n",
      " [-6.6865158e-01  1.0000000e+00  1.7886829e+00 ...  4.9078946e+00\n",
      "  -1.9056535e+00 -1.4540702e-01]\n",
      " [-6.6927528e-01  1.0000000e+00  1.7884598e+00 ...  1.4364426e+02\n",
      "   1.4770927e+02 -3.7729242e+02]\n",
      " [-6.6959190e-01  1.0000000e+00  1.7883530e+00 ...  3.7586125e+03\n",
      "   2.2843396e+04 -4.8068877e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1892519e+00  1.0000000e+00  1.4941158e+00 ... -5.4474048e+02\n",
      "   1.0483198e+03  4.1214203e+02]\n",
      " [-1.1891527e+00  1.0000000e+00  1.4942064e+00 ...  9.4398071e+01\n",
      "   5.9746212e+01 -9.2844933e+01]\n",
      " [-1.1887550e+00  1.0000000e+00  1.4944838e+00 ... -5.5278973e+01\n",
      "  -1.3568988e+01 -3.6513714e+01]\n",
      " ...\n",
      " [-1.1888123e+00  1.0000000e+00  1.4944496e+00 ...  1.8029803e-01\n",
      "   3.1599364e+00  1.0825445e+01]\n",
      " [-1.1893234e+00  1.0000000e+00  1.4940414e+00 ... -1.4210635e+02\n",
      "   1.7168059e+02  5.2052250e+01]\n",
      " [-1.1895866e+00  1.0000000e+00  1.4938831e+00 ... -1.1213301e+03\n",
      "   5.0114341e+03  5.3897178e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5928221e+00  1.0000000e+00  1.0533886e+00 ...  1.5141371e+01\n",
      "   1.9415665e+02 -1.6850345e+02]\n",
      " [-1.5927496e+00  1.0000000e+00  1.0534878e+00 ... -5.9225296e+01\n",
      "  -7.4909554e+00 -1.0420313e+02]\n",
      " [-1.5924282e+00  1.0000000e+00  1.0538632e+00 ... -8.7009554e+00\n",
      "   3.0164051e+00 -4.6570196e+00]\n",
      " ...\n",
      " [-1.5924587e+00  1.0000000e+00  1.0538034e+00 ...  5.3077488e+00\n",
      "  -1.1602108e+00  2.6048343e+00]\n",
      " [-1.5928192e+00  1.0000000e+00  1.0532761e+00 ...  3.2436813e+01\n",
      "   1.6596342e+02 -6.4725250e+01]\n",
      " [-1.5930624e+00  1.0000000e+00  1.0530663e+00 ... -1.3901981e+03\n",
      "  -2.6127988e+03  3.1724243e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8404541e+00  1.0000000e+00  5.0949097e-01 ...  8.7862419e+01\n",
      "   1.3184320e+02 -2.4081512e+01]\n",
      " [-1.8404226e+00  1.0000000e+00  5.0961399e-01 ... -7.5333023e-01\n",
      "  -1.5359192e+02  1.1376615e+02]\n",
      " [-1.8402405e+00  1.0000000e+00  5.1005924e-01 ...  3.1505985e+00\n",
      "   1.9292667e+02  5.3398918e+01]\n",
      " ...\n",
      " [-1.8402443e+00  1.0000000e+00  5.0998688e-01 ...  3.3195515e+00\n",
      "   3.6067447e-01  6.3090980e-01]\n",
      " [-1.8403664e+00  1.0000000e+00  5.0935745e-01 ... -8.9458618e+01\n",
      "   2.0272835e+02  1.0452608e+02]\n",
      " [-1.8405800e+00  1.0000000e+00  5.0912285e-01 ...  4.9464258e+02\n",
      "  -1.5947252e+02  3.8069798e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9078436e+00  1.0000000e+00 -8.3927155e-02 ...  5.6088554e+01\n",
      "  -1.7966141e+01 -1.6412975e+02]\n",
      " [-1.9078436e+00  1.0000000e+00 -8.3798409e-02 ... -1.2326452e+01\n",
      "  -4.5260700e+01  5.7445507e+01]\n",
      " [-1.9078445e+00  1.0000000e+00 -8.3326772e-02 ... -1.3903112e+01\n",
      "  -1.8198957e+01 -3.6990826e+01]\n",
      " ...\n",
      " [-1.9078274e+00  1.0000000e+00 -8.3413124e-02 ... -3.8209376e-01\n",
      "   3.0489194e+00  2.0919344e+00]\n",
      " [-1.9077644e+00  1.0000000e+00 -8.4066391e-02 ... -1.3972156e+02\n",
      "   1.0444473e+02 -9.1942345e+01]\n",
      " [-1.9078569e+00  1.0000000e+00 -8.4312439e-02 ... -2.5717859e+02\n",
      "  -2.3858182e+02  3.1701097e-01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.6       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7884283e+00  1.0000000e+00 -6.6968536e-01 ... -1.4362891e+02\n",
      "   7.8022547e+00 -7.8310242e+01]\n",
      " [-1.7884769e+00  1.0000000e+00 -6.6954136e-01 ... -1.2635700e+02\n",
      "   1.7406348e+02 -1.2931765e+01]\n",
      " [-1.7886181e+00  1.0000000e+00 -6.6910321e-01 ...  1.7857889e+01\n",
      "  -1.8845818e+02  1.6750975e+02]\n",
      " ...\n",
      " [-1.7885857e+00  1.0000000e+00 -6.6918278e-01 ... -7.2262192e-01\n",
      "   7.1759534e-01  5.6295052e+00]\n",
      " [-1.7883110e+00  1.0000000e+00 -6.6981316e-01 ...  4.2000034e+01\n",
      "   1.1476250e+02 -5.4068623e+00]\n",
      " [-1.7883167e+00  1.0000000e+00 -6.7004204e-01 ...  1.7995172e+02\n",
      "   8.2000238e+02 -3.4538550e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4939575e+00  1.0000000e+00 -1.1896114e+00 ...  6.3125206e+01\n",
      "   5.0955776e+01  1.2123615e+02]\n",
      " [-1.4940500e+00  1.0000000e+00 -1.1894321e+00 ... -6.4908241e+01\n",
      "  -3.2410767e+01 -3.2925217e+00]\n",
      " [-1.4943237e+00  1.0000000e+00 -1.1890770e+00 ...  7.6117943e+01\n",
      "  -1.8175871e+02 -1.7764374e+02]\n",
      " ...\n",
      " [-1.4942703e+00  1.0000000e+00 -1.1891403e+00 ... -2.8161693e-01\n",
      "   4.6385250e+00  6.7258120e-01]\n",
      " [-1.4937935e+00  1.0000000e+00 -1.1897240e+00 ... -2.1635432e+02\n",
      "   1.3124791e+02 -1.1860465e+02]\n",
      " [-1.4937344e+00  1.0000000e+00 -1.1899052e+00 ...  3.8056186e+02\n",
      "  -1.8366510e+02  4.3713992e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.05314732e+00  1.00000000e+00 -1.59309578e+00 ...  3.87569427e+01\n",
      "   1.94080658e+02 -2.97021885e+01]\n",
      " [-1.05328178e+00  1.00000000e+00 -1.59293365e+00 ...  1.52265915e+02\n",
      "  -1.15803482e+02 -9.79399261e+01]\n",
      " [-1.05364227e+00  1.00000000e+00 -1.59270108e+00 ...  3.88021094e+03\n",
      "  -3.46764496e+02  1.00257635e+03]\n",
      " ...\n",
      " [-1.05357933e+00  1.00000000e+00 -1.59272575e+00 ...  2.51207024e-01\n",
      "   3.41380000e+00 -6.32079422e-01]\n",
      " [-1.05294609e+00  1.00000000e+00 -1.59316444e+00 ... -4.31857109e+01\n",
      "  -8.28680344e+01  2.21824524e+02]\n",
      " [-1.05283642e+00  1.00000000e+00 -1.59328842e+00 ... -1.29589752e+02\n",
      "   1.32043381e+02  8.74991684e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.1        0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.09213448e-01  1.00000000e+00 -1.84057236e+00 ... -2.34939926e+02\n",
      "   2.68250885e+01  3.80764282e+02]\n",
      " [-5.09367943e-01  1.00000000e+00 -1.84045601e+00 ... -3.62363342e+02\n",
      "   1.71459396e+02 -4.08588501e+02]\n",
      " [-5.09805679e-01  1.00000000e+00 -1.84036636e+00 ... -9.25696411e+01\n",
      "   2.40575180e+01 -4.00366882e+02]\n",
      " ...\n",
      " [-5.09737015e-01  1.00000000e+00 -1.84035778e+00 ...  1.52089798e+00\n",
      "  -3.89399767e+00  2.10288391e-01]\n",
      " [-5.08998871e-01  1.00000000e+00 -1.84058762e+00 ...  2.82116333e+02\n",
      "  -1.90750519e+02  1.20323235e+02]\n",
      " [-5.08853912e-01  1.00000000e+00 -1.84064293e+00 ...  5.66848633e+02\n",
      "   5.69499634e+02  1.36224121e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.4491730e-02  1.0000000e+00 -1.9078140e+00 ...  7.6903172e+00\n",
      "   1.2969771e+01 -2.3145163e+01]\n",
      " [ 8.4334373e-02  1.0000000e+00 -1.9077330e+00 ... -1.4190324e+02\n",
      "   1.1479525e+02  6.6900487e+00]\n",
      " [ 8.3873749e-02  1.0000000e+00 -1.9078001e+00 ...  9.4029462e+02\n",
      "  -9.2816063e+01  4.6291202e+02]\n",
      " ...\n",
      " [ 8.3944321e-02  1.0000000e+00 -1.9077682e+00 ...  8.5486059e+00\n",
      "  -6.8693080e+00  1.1761509e-01]\n",
      " [ 8.4709167e-02  1.0000000e+00 -1.9077415e+00 ... -4.5413361e+02\n",
      "  -7.1043768e+02 -6.1325385e+02]\n",
      " [ 8.4867477e-02  1.0000000e+00 -1.9077206e+00 ... -6.9084155e+02\n",
      "   1.5307704e+03 -6.3516736e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.6972351e-01  1.0000000e+00 -1.7883072e+00 ... -2.7659815e+01\n",
      "  -6.2977417e+01 -6.5918343e+01]\n",
      " [ 6.6957569e-01  1.0000000e+00 -1.7882967e+00 ... -1.8484125e+01\n",
      "  -5.6908875e+00  7.2553925e+01]\n",
      " [ 6.6915131e-01  1.0000000e+00 -1.7885153e+00 ... -1.4493961e+02\n",
      "  -3.2136868e+02 -1.0816448e+02]\n",
      " ...\n",
      " [ 6.6921425e-01  1.0000000e+00 -1.7884512e+00 ...  5.8296294e+00\n",
      "  -3.5465574e+00 -6.5796685e-01]\n",
      " [ 6.6993523e-01  1.0000000e+00 -1.7881889e+00 ...  7.0221977e+01\n",
      "   8.6622070e+02  2.2709502e+02]\n",
      " [ 6.7008114e-01  1.0000000e+00 -1.7881203e+00 ...  1.0137029e+02\n",
      "  -2.7655844e+02 -1.2884040e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.1899347    1.          -1.4934978 ...   -5.023627   -19.526173\n",
      "  -137.7049   ]\n",
      " [   1.1898165    1.          -1.4935493 ...  -74.66678   -133.25383\n",
      "    42.30154  ]\n",
      " [   1.189436     1.          -1.4939023 ...  -33.39063    234.3473\n",
      "  -102.4711   ]\n",
      " ...\n",
      " [   1.1894779    1.          -1.4938126 ...    2.0038478   -3.9213037\n",
      "    -2.4500947]\n",
      " [   1.1900959    1.          -1.4933357 ...   74.47566    -14.354619\n",
      "    57.943348 ]\n",
      " [   1.190239     1.          -1.4932346 ...  224.06886   -224.2588\n",
      "   212.4373   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.5930548     1.           -1.0529594  ...  111.41176\n",
      "   -53.710793    134.48395   ]\n",
      " [   1.592969      1.           -1.0530014  ...  188.49287\n",
      "   -95.870316     44.473377  ]\n",
      " [   1.5927048     1.           -1.0534662  ...   76.43586\n",
      "   -68.570015   -102.9767    ]\n",
      " ...\n",
      " [   1.5927391     1.           -1.0533533  ...    1.5374084\n",
      "    -2.1871042    -0.33945966]\n",
      " [   1.5931759     1.           -1.052763   ...  -23.199156\n",
      "    17.973925     28.692009  ]\n",
      " [   1.5932627     1.           -1.0526314  ... -113.12949\n",
      "    81.1515      159.29547   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.5\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8405457e+00  1.0000000e+00 -5.0868225e-01 ... -9.2160309e+01\n",
      "   8.1446457e+01 -9.3311371e+01]\n",
      " [ 1.8405008e+00  1.0000000e+00 -5.0873566e-01 ...  5.3295795e+01\n",
      "  -1.1914748e+01  7.4715505e+00]\n",
      " [ 1.8403587e+00  1.0000000e+00 -5.0927156e-01 ...  1.2796777e+01\n",
      "   1.2496561e+02 -8.4019936e+01]\n",
      " ...\n",
      " [ 1.8403721e+00  1.0000000e+00 -5.0915051e-01 ... -2.7213156e+00\n",
      "  -5.0650723e-02 -1.9842274e+00]\n",
      " [ 1.8405838e+00  1.0000000e+00 -5.0846100e-01 ...  5.4179058e+01\n",
      "  -1.0766330e+02  6.3413010e+01]\n",
      " [ 1.8406391e+00  1.0000000e+00 -5.0830650e-01 ...  2.4815271e+01\n",
      "  -2.7121680e+02 -2.0363156e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.3       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9076490e+00  1.0000000e+00  8.4926605e-02 ... -4.9194107e+00\n",
      "   1.2627656e+01  2.4065182e+00]\n",
      " [ 1.9076462e+00  1.0000000e+00  8.4868431e-02 ...  1.5967615e+03\n",
      "  -3.5115918e+03 -4.4960352e+02]\n",
      " [ 1.9076576e+00  1.0000000e+00  8.4312923e-02 ...  3.7509239e+01\n",
      "   1.3745552e+02  2.6742151e+01]\n",
      " ...\n",
      " [ 1.9076519e+00  1.0000000e+00  8.4442139e-02 ...  8.3028838e-02\n",
      "  -3.6710434e+00  3.2056868e-01]\n",
      " [ 1.9076080e+00  1.0000000e+00  8.5153580e-02 ... -1.0392757e+02\n",
      "   7.4428784e+02  2.1848445e+02]\n",
      " [ 1.9076223e+00  1.0000000e+00  8.5313797e-02 ... -4.6398211e+00\n",
      "  -1.9283173e+01  6.4402542e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7879143e+00  1.0000000e+00  6.7059898e-01 ...  4.9951527e+01\n",
      "   2.2854198e+01  8.5067234e+00]\n",
      " [ 1.7879524e+00  1.0000000e+00  6.7053032e-01 ...  4.9930234e+02\n",
      "  -9.4579407e+01 -4.2308084e+02]\n",
      " [ 1.7880898e+00  1.0000000e+00  6.7001426e-01 ... -4.1499866e+02\n",
      "  -1.2490432e+01 -7.0777649e+01]\n",
      " ...\n",
      " [ 1.7880497e+00  1.0000000e+00  6.7013741e-01 ...  1.1011471e+00\n",
      "   1.6148034e+00 -9.0818989e-01]\n",
      " [ 1.7877903e+00  1.0000000e+00  6.7081070e-01 ... -3.2096432e+02\n",
      "  -4.4349140e+01  7.9415840e+01]\n",
      " [ 1.7877769e+00  1.0000000e+00  6.7095947e-01 ...  1.8929215e+03\n",
      "  -4.8090128e+02 -6.0969818e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4933882e+00  1.0000000e+00  1.1900692e+00 ...  1.9518385e+01\n",
      "  -1.2089079e+01 -2.3315073e+01]\n",
      " [ 1.4934549e+00  1.0000000e+00  1.1900063e+00 ... -1.8213220e+02\n",
      "   2.4057007e-01  8.4851105e+01]\n",
      " [ 1.4937477e+00  1.0000000e+00  1.1895680e+00 ... -6.6250763e+01\n",
      "   2.4874368e+02  1.6713141e+02]\n",
      " ...\n",
      " [ 1.4936790e+00  1.0000000e+00  1.1896830e+00 ...  4.8339186e+00\n",
      "   1.5687859e+01 -1.2663038e+00]\n",
      " [ 1.4931984e+00  1.0000000e+00  1.1902447e+00 ... -1.2847194e+03\n",
      "  -9.4026239e+02 -1.9568755e+02]\n",
      " [ 1.4931431e+00  1.0000000e+00  1.1903744e+00 ... -3.5480368e+02\n",
      "  -2.5208561e+01  1.9562961e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.1       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0522442e+00  1.0000000e+00  1.5935326e+00 ... -3.1840229e+01\n",
      "  -4.9995785e+01  3.8394806e+01]\n",
      " [ 1.0523338e+00  1.0000000e+00  1.5934601e+00 ... -2.2803522e+02\n",
      "   8.7339478e+01 -1.0234698e+02]\n",
      " [ 1.0527802e+00  1.0000000e+00  1.5931482e+00 ... -3.0707085e+03\n",
      "   1.5790750e+02 -8.0545392e+02]\n",
      " ...\n",
      " [ 1.0526867e+00  1.0000000e+00  1.5932188e+00 ...  2.9596252e+00\n",
      "   1.0471799e+01 -2.9011866e-01]\n",
      " [ 1.0520020e+00  1.0000000e+00  1.5936584e+00 ... -4.0362195e+02\n",
      "  -7.1963354e+02 -4.7605936e+02]\n",
      " [ 1.0519276e+00  1.0000000e+00  1.5937405e+00 ... -1.2383835e+02\n",
      "   1.1726831e+02 -3.5172995e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0811577e-01  1.0000000e+00  1.8405151e+00 ...  1.1072561e+02\n",
      "  -5.1351543e+01 -1.6292308e+02]\n",
      " [ 5.0821781e-01  1.0000000e+00  1.8404417e+00 ...  3.3942226e+01\n",
      "  -2.3728649e+01  5.0390987e+01]\n",
      " [ 5.0868607e-01  1.0000000e+00  1.8402698e+00 ... -5.8045447e+02\n",
      "   3.4103302e+01  3.2837726e+02]\n",
      " ...\n",
      " [ 5.0857353e-01  1.0000000e+00  1.8403053e+00 ... -1.4839255e+00\n",
      "  -4.6338921e+00  2.0491584e-01]\n",
      " [ 5.0779724e-01  1.0000000e+00  1.8405762e+00 ... -5.4377399e+02\n",
      "   2.3643263e+02 -6.0964197e+02]\n",
      " [ 5.0775909e-01  1.0000000e+00  1.8406219e+00 ...  9.7280853e+01\n",
      "   2.2973944e+01  9.9644615e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-8.54597092e-02  1.00000000e+00  1.90747643e+00 ... -8.87085571e+01\n",
      "   2.19289856e+01  5.00538788e+01]\n",
      " [-8.53548050e-02  1.00000000e+00  1.90746880e+00 ... -7.18576202e+01\n",
      "   1.65975609e+01  1.64323273e+01]\n",
      " [-8.48617554e-02  1.00000000e+00  1.90743935e+00 ...  5.62314301e+01\n",
      "   6.00412109e+02  2.74246429e+02]\n",
      " ...\n",
      " [-8.49800110e-02  1.00000000e+00  1.90743351e+00 ... -3.61567354e+00\n",
      "  -6.94790649e+00 -1.54430032e+00]\n",
      " [-8.57753754e-02  1.00000000e+00  1.90745354e+00 ...  1.71771332e+02\n",
      "  -7.43787766e+01  1.19041435e+02]\n",
      " [-8.58278275e-02  1.00000000e+00  1.90746689e+00 ... -4.32809021e+02\n",
      "   2.52903809e+02  2.86527435e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:9, Score:1.94, Best Score:2.33, Average Score:1.51, Best Avg Score:1.72\n",
      "Episode number: 10\n",
      "Printing decisionSteps: \n",
      "<mlagents_envs.base_env.DecisionSteps object at 0x7fa57c3e8430>\n",
      "<class 'mlagents_envs.base_env.DecisionSteps'>\n",
      "Number of agents: 20\n",
      "scores_agents type:  <class 'numpy.ndarray'>\n",
      "scores_agents shape:  (20,)\n",
      "Progressing at step 0\n",
      "Forearm\n",
      "Step 0, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1907425e+00  1.0000000e+00  1.4925365e+00 ... -7.6133949e+01\n",
      "   9.0731422e+01 -2.7145740e+02]\n",
      " [-1.1906624e+00  1.0000000e+00  1.4926043e+00 ... -5.6277069e+01\n",
      "   2.2460289e+01  2.8991816e+01]\n",
      " [-1.1902370e+00  1.0000000e+00  1.4928581e+00 ... -2.7478813e+01\n",
      "   3.4912869e+02  6.0774944e+01]\n",
      " ...\n",
      " [-1.1903286e+00  1.0000000e+00  1.4927769e+00 ... -2.4719448e+00\n",
      "  -5.4044132e+00  7.1718395e-01]\n",
      " [-1.1909904e+00  1.0000000e+00  1.4923592e+00 ... -1.9327707e+02\n",
      "  -1.7728687e+01 -7.7778998e+02]\n",
      " [-1.1910362e+00  1.0000000e+00  1.4923058e+00 ...  3.2872177e+01\n",
      "   5.8351353e+01  3.9102108e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 1, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5937347e+00  1.0000000e+00  1.0513096e+00 ...  1.5279517e+02\n",
      "   2.1071260e+02 -2.6498254e+02]\n",
      " [-1.5936813e+00  1.0000000e+00  1.0514078e+00 ...  2.6122572e+01\n",
      "   9.0694849e+02 -3.5005292e+02]\n",
      " [-1.5933762e+00  1.0000000e+00  1.0517725e+00 ...  2.9500146e+02\n",
      "   7.3810699e+01  1.2687890e+02]\n",
      " ...\n",
      " [-1.5934372e+00  1.0000000e+00  1.0516615e+00 ... -2.8255806e+00\n",
      "  -3.7611136e+00  2.3172787e-01]\n",
      " [-1.5939159e+00  1.0000000e+00  1.0510845e+00 ... -3.0746744e+02\n",
      "   9.6357062e+02  1.4695717e+02]\n",
      " [-1.5939369e+00  1.0000000e+00  1.0510082e+00 ...  1.5944308e+02\n",
      "   1.3098125e+02 -2.0356396e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 2, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8407907e+00  1.0000000e+00  5.0709152e-01 ...  1.5068426e+00\n",
      "  -5.6260265e+01  1.5554160e+02]\n",
      " [-1.8407698e+00  1.0000000e+00  5.0721264e-01 ... -8.6817590e+02\n",
      "   6.4297937e+02 -1.1780649e+03]\n",
      " [-1.8405590e+00  1.0000000e+00  5.0764942e-01 ... -4.5444798e+01\n",
      "  -2.1490189e+02  1.4836475e+02]\n",
      " ...\n",
      " [-1.8405952e+00  1.0000000e+00  5.0751114e-01 ...  4.9537945e-01\n",
      "   7.2913313e-01 -9.3306959e-02]\n",
      " [-1.8408470e+00  1.0000000e+00  5.0683594e-01 ... -1.6887653e+02\n",
      "  -1.3774159e+02 -2.5019318e+02]\n",
      " [-1.8408785e+00  1.0000000e+00  5.0674438e-01 ...  6.9803375e+01\n",
      "  -1.3222598e+02 -1.6541292e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 3, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9075403e+00  1.0000000e+00 -8.6515427e-02 ...  1.3096031e+02\n",
      "   3.2661789e+01  1.7938726e+01]\n",
      " [-1.9075546e+00  1.0000000e+00 -8.6399078e-02 ...  9.7045975e+01\n",
      "  -2.5473982e+02 -4.8784767e+01]\n",
      " [-1.9074993e+00  1.0000000e+00 -8.5940994e-02 ...  2.2047441e+02\n",
      "  -2.5337402e+02  2.0609474e+02]\n",
      " ...\n",
      " [-1.9074936e+00  1.0000000e+00 -8.6085320e-02 ... -1.1555102e+00\n",
      "  -6.6884995e-02  6.6566980e-01]\n",
      " [-1.9074841e+00  1.0000000e+00 -8.6774826e-02 ... -1.5238226e+03\n",
      "   1.4902693e+03  1.1803120e+03]\n",
      " [-1.9075241e+00  1.0000000e+00 -8.6868286e-02 ... -3.9594238e+01\n",
      "   7.2702637e+01 -5.8318820e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 4, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7874126e+00  1.0000000e+00 -6.7194748e-01 ...  1.8334074e+02\n",
      "   1.2596486e+02 -2.5163263e+02]\n",
      " [-1.7874556e+00  1.0000000e+00 -6.7182732e-01 ...  1.3202209e+02\n",
      "   2.5228728e+02  1.6080353e+02]\n",
      " [-1.7875233e+00  1.0000000e+00 -6.7138714e-01 ... -4.1849613e-03\n",
      "  -1.4715652e+02  9.8392960e+01]\n",
      " ...\n",
      " [-1.7874851e+00  1.0000000e+00 -6.7153549e-01 ... -2.4120450e-01\n",
      "   7.7587891e-01  2.2799945e-01]\n",
      " [-1.7872581e+00  1.0000000e+00 -6.7218971e-01 ...  5.7671686e+02\n",
      "  -1.4100570e+03 -4.8219037e+02]\n",
      " [-1.7872801e+00  1.0000000e+00 -6.7227554e-01 ...  1.3180608e+02\n",
      "  -1.3324557e+02  2.8683290e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 5, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4923754e+00  1.0000000e+00 -1.1914501e+00 ...  3.2949306e+01\n",
      "   1.8011748e+02  2.1162561e+01]\n",
      " [-1.4924431e+00  1.0000000e+00 -1.1913805e+00 ... -1.5860220e+02\n",
      "   1.5348302e+02 -2.3926392e+02]\n",
      " [-1.4926586e+00  1.0000000e+00 -1.1909987e+00 ... -1.9607172e+02\n",
      "   6.5219498e+01 -2.7185865e+01]\n",
      " ...\n",
      " [-1.4925632e+00  1.0000000e+00 -1.1911364e+00 ...  1.3770545e-01\n",
      "   5.4636288e-01 -2.3862267e-01]\n",
      " [-1.4921417e+00  1.0000000e+00 -1.1916561e+00 ... -4.9608881e+02\n",
      "   5.6196222e+02  4.8345337e+02]\n",
      " [-1.4921446e+00  1.0000000e+00 -1.1917305e+00 ... -1.7318855e+02\n",
      "   9.3940239e+00 -1.4896693e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 6, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0508528e+00  1.0000000e+00 -1.5946045e+00 ...  4.1952094e+02\n",
      "   3.9876385e+01  1.7071255e+02]\n",
      " [-1.0509396e+00  1.0000000e+00 -1.5945635e+00 ...  7.2380096e+01\n",
      "   4.6949253e+00 -7.9973125e-01]\n",
      " [-1.0512543e+00  1.0000000e+00 -1.5942928e+00 ...  1.7549002e+02\n",
      "   8.7987602e+01 -3.3223257e+02]\n",
      " ...\n",
      " [-1.0511379e+00  1.0000000e+00 -1.5944004e+00 ... -3.3386827e-02\n",
      "  -2.7892780e-01 -2.3606753e-01]\n",
      " [-1.0505733e+00  1.0000000e+00 -1.5947552e+00 ... -9.5377655e+01\n",
      "  -3.7709238e+02 -2.4134209e+02]\n",
      " [-1.0505428e+00  1.0000000e+00 -1.5948086e+00 ...  7.2605194e+02\n",
      "  -3.9984174e+02 -2.4719772e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 7, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0691032e-01  1.0000000e+00 -1.8412762e+00 ... -6.0947449e+01\n",
      "  -3.6301922e+01 -3.8621044e+01]\n",
      " [-5.0701237e-01  1.0000000e+00 -1.8412457e+00 ... -7.5480598e+01\n",
      "  -8.0618675e+01 -2.5700497e+02]\n",
      " [-5.0737572e-01  1.0000000e+00 -1.8411161e+00 ...  4.7690571e+01\n",
      "   8.0654251e+01 -4.4113548e+01]\n",
      " ...\n",
      " [-5.0724030e-01  1.0000000e+00 -1.8411798e+00 ... -2.1940923e-01\n",
      "  -8.5046434e-01 -3.3647132e-01]\n",
      " [-5.0658798e-01  1.0000000e+00 -1.8413486e+00 ... -4.2015797e+01\n",
      "  -1.2399523e+02 -8.9374901e+01]\n",
      " [-5.0653839e-01  1.0000000e+00 -1.8413734e+00 ... -1.5914442e+02\n",
      "  -3.8908899e+00 -5.1731552e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 8, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.7065697e-02  1.0000000e+00 -1.9078197e+00 ... -2.6719229e+02\n",
      "   5.5049987e+00  1.2936888e+02]\n",
      " [ 8.6957932e-02  1.0000000e+00 -1.9078188e+00 ... -3.5836948e+02\n",
      "   8.8737970e+02  2.4098186e+02]\n",
      " [ 8.6587906e-02  1.0000000e+00 -1.9078162e+00 ...  1.9105701e+01\n",
      "   2.1128532e+01  4.2923504e+01]\n",
      " ...\n",
      " [ 8.6729050e-02  1.0000000e+00 -1.9078474e+00 ... -1.2068393e+00\n",
      "  -3.8861327e+00  8.9246631e-02]\n",
      " [ 8.7400436e-02  1.0000000e+00 -1.9077988e+00 ... -7.9001411e+01\n",
      "  -5.6333557e+02 -2.8760785e+02]\n",
      " [ 8.7455750e-02  1.0000000e+00 -1.9078026e+00 ... -7.0997528e+01\n",
      "  -1.0649133e+02 -1.1931607e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 9, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7245960e-01  1.0000000e+00 -1.7872658e+00 ...  1.3854068e+02\n",
      "  -8.7599297e+01  1.2709575e+02]\n",
      " [ 6.7235279e-01  1.0000000e+00 -1.7873011e+00 ...  2.3850571e+02\n",
      "  -1.4533578e+03  3.6395154e+02]\n",
      " [ 6.7206001e-01  1.0000000e+00 -1.7874274e+00 ...  7.5227499e+00\n",
      "  -1.4602299e+01  2.5587467e+01]\n",
      " ...\n",
      " [ 6.7219162e-01  1.0000000e+00 -1.7874050e+00 ... -1.8524714e+00\n",
      "  -4.5794320e+00  1.1459975e+00]\n",
      " [ 6.7282104e-01  1.0000000e+00 -1.7871666e+00 ... -4.2707526e+02\n",
      "  -1.1736693e+03 -3.4101901e+02]\n",
      " [ 6.7283058e-01  1.0000000e+00 -1.7871475e+00 ...  1.8647678e+02\n",
      "  -8.2449776e+01 -3.7806408e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 10, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1918678e+00  1.0000000e+00 -1.4919682e+00 ... -1.6000696e+02\n",
      "  -1.3883037e+02 -3.1979654e+02]\n",
      " [ 1.1917753e+00  1.0000000e+00 -1.4920187e+00 ... -1.1276670e+01\n",
      "  -4.9800726e+02 -2.1005533e+02]\n",
      " [ 1.1915989e+00  1.0000000e+00 -1.4922603e+00 ...  7.0042048e+00\n",
      "  -9.2750267e+01 -7.7964935e+01]\n",
      " ...\n",
      " [ 1.1917057e+00  1.0000000e+00 -1.4922066e+00 ... -5.7070398e-01\n",
      "  -2.6797228e+00 -3.9742827e-02]\n",
      " [ 1.1921902e+00  1.0000000e+00 -1.4917812e+00 ... -3.9496189e+01\n",
      "   9.9533287e+01  4.5400063e+01]\n",
      " [ 1.1921864e+00  1.0000000e+00 -1.4917450e+00 ... -9.8705931e+00\n",
      "  -9.7354950e+01  3.0976717e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 11, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5945759e+00  1.0000000e+00 -1.0506248e+00 ...  6.5261780e+01\n",
      "  -1.3192842e+02 -1.5631989e+02]\n",
      " [ 1.5945082e+00  1.0000000e+00 -1.0506926e+00 ... -1.8326508e+01\n",
      "   2.1729164e+01  7.8191597e+01]\n",
      " [ 1.5944481e+00  1.0000000e+00 -1.0510195e+00 ...  5.5618786e+01\n",
      "  -6.7014290e+01 -2.3317463e+01]\n",
      " ...\n",
      " [ 1.5945206e+00  1.0000000e+00 -1.0509367e+00 ... -8.5960555e-01\n",
      "  -2.3053808e+00 -1.3122025e-01]\n",
      " [ 1.5948582e+00  1.0000000e+00 -1.0503559e+00 ...  1.7339966e+02\n",
      "  -6.3808978e-01  1.4527333e+01]\n",
      " [ 1.5948076e+00  1.0000000e+00 -1.0503082e+00 ...  5.2668919e+01\n",
      "  -5.5771225e+01 -8.2624138e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 12, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.8411446     1.           -0.50663376 ...  136.3634\n",
      "     8.156323     56.073963  ]\n",
      " [   1.8410997     1.           -0.50670147 ...   59.84326\n",
      "   101.81723     -74.17871   ]\n",
      " [   1.8411503     1.           -0.5070682  ...  181.88052\n",
      "    15.121754   -143.14848   ]\n",
      " ...\n",
      " [   1.8411789     1.           -0.50696754 ...    1.8308126\n",
      "     2.276723      1.2560315 ]\n",
      " [   1.8413448     1.           -0.5063076  ...    2.2809117\n",
      "   -10.254828    -15.063213  ]\n",
      " [   1.8412533     1.           -0.5062542  ...  118.40328\n",
      "    47.79052     241.23837   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 13, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9075890e+00  1.0000000e+00  8.7526321e-02 ... -6.4133934e+01\n",
      "  -1.5657663e+02 -4.0344654e+01]\n",
      " [ 1.9075766e+00  1.0000000e+00  8.7413788e-02 ...  2.2184145e+02\n",
      "  -9.9591347e+01 -1.7449078e+02]\n",
      " [ 1.9077168e+00  1.0000000e+00  8.7049454e-02 ... -2.4554619e+02\n",
      "   3.1740088e+01  1.3770182e+01]\n",
      " ...\n",
      " [ 1.9077110e+00  1.0000000e+00  8.7139130e-02 ... -9.4706953e-01\n",
      "   1.8782082e+00  3.6096758e-01]\n",
      " [ 1.9076920e+00  1.0000000e+00  8.7858200e-02 ... -3.2304196e+02\n",
      "   1.5483284e+02  1.0596703e+03]\n",
      " [ 1.9075727e+00  1.0000000e+00  8.7913513e-02 ...  3.5466793e+01\n",
      "   7.4090839e+02  5.0351948e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 14, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7872210e+00  1.0000000e+00  6.7264366e-01 ... -3.0737747e+02\n",
      "   4.1910656e+01 -2.1908240e+02]\n",
      " [ 1.7872353e+00  1.0000000e+00  6.7252922e-01 ... -1.6868196e+02\n",
      "   4.9904990e+00  2.8236057e+01]\n",
      " [ 1.7875309e+00  1.0000000e+00  6.7219836e-01 ...  1.1808964e+03\n",
      "   4.5237268e+02  3.7260385e+02]\n",
      " ...\n",
      " [ 1.7874794e+00  1.0000000e+00  6.7227173e-01 ...  2.6480913e-02\n",
      "   7.3076601e+00  5.6646913e-01]\n",
      " [ 1.7872219e+00  1.0000000e+00  6.7295456e-01 ...  5.0938257e+02\n",
      "   1.3637408e+02  5.4671893e+02]\n",
      " [ 1.7870922e+00  1.0000000e+00  6.7300797e-01 ...  3.8339285e+02\n",
      "   1.0863924e+02  1.1432053e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 15, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.49169350e+00  1.00000000e+00  1.19223213e+00 ...  1.42902161e+03\n",
      "   3.47903046e+02  9.33329407e+02]\n",
      " [ 1.49173832e+00  1.00000000e+00  1.19209480e+00 ... -4.63544922e+01\n",
      "  -8.69793777e+01  4.32673645e+01]\n",
      " [ 1.49215698e+00  1.00000000e+00  1.19183838e+00 ...  2.91930634e+02\n",
      "  -8.96368042e+02 -9.17829132e+01]\n",
      " ...\n",
      " [ 1.49207306e+00  1.00000000e+00  1.19187450e+00 ...  1.08394539e+00\n",
      "  -4.14770508e+00 -6.36853933e-01]\n",
      " [ 1.49162483e+00  1.00000000e+00  1.19246674e+00 ...  9.41538010e+01\n",
      "  -3.19527130e+01 -1.25636665e+02]\n",
      " [ 1.49145508e+00  1.00000000e+00  1.19251633e+00 ... -4.39945129e+02\n",
      "   3.25419312e+02 -1.52697006e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 16, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.0502777e+00  1.0000000e+00  1.5948372e+00 ... -2.1584790e+03\n",
      "   1.5791353e+03  1.5439845e+03]\n",
      " [ 1.0503492e+00  1.0000000e+00  1.5947342e+00 ... -1.4914175e+01\n",
      "   6.4404572e+01  3.2945085e+00]\n",
      " [ 1.0508366e+00  1.0000000e+00  1.5945724e+00 ... -1.2494696e+01\n",
      "   7.2504471e+01 -1.0735836e+02]\n",
      " ...\n",
      " [ 1.0507298e+00  1.0000000e+00  1.5945902e+00 ...  3.0439657e-01\n",
      "  -7.8540206e+00 -5.9780955e-02]\n",
      " [ 1.0501137e+00  1.0000000e+00  1.5950069e+00 ...  4.0905285e+01\n",
      "  -2.1188017e+01  2.9685501e+01]\n",
      " [ 1.0499659e+00  1.0000000e+00  1.5950413e+00 ...  1.3141471e+03\n",
      "  -7.3275238e+02  7.3666052e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 17, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0609589e-01  1.0000000e+00  1.8413277e+00 ... -1.7441713e+01\n",
      "   3.6405445e+01 -7.2997726e+01]\n",
      " [ 5.0617695e-01  1.0000000e+00  1.8412418e+00 ...  1.1649566e+01\n",
      "   1.2439331e+01 -1.2677251e+01]\n",
      " [ 5.0671005e-01  1.0000000e+00  1.8411978e+00 ...  3.0138498e+02\n",
      "   4.6071906e+02  7.0235817e+01]\n",
      " ...\n",
      " [ 5.0658607e-01  1.0000000e+00  1.8411741e+00 ...  7.5999761e-01\n",
      "  -1.1066304e+01 -8.2594788e-01]\n",
      " [ 5.0588608e-01  1.0000000e+00  1.8414135e+00 ... -6.9515717e+01\n",
      "  -4.4095670e+02 -6.0829723e+01]\n",
      " [ 5.0573826e-01  1.0000000e+00  1.8414268e+00 ... -8.4918048e+02\n",
      "  -1.7534598e+03  6.6846637e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 18, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-8.7958336e-02  1.0000000e+00  1.9075222e+00 ... -8.9930286e+00\n",
      "   2.9081108e+01 -5.8182610e+01]\n",
      " [-8.7873459e-02  1.0000000e+00  1.9074793e+00 ... -2.0171371e+02\n",
      "   1.7680017e+02  1.0834140e+02]\n",
      " [-8.7274551e-02  1.0000000e+00  1.9075706e+00 ...  3.2404572e+02\n",
      "   3.0851093e+02 -1.7288539e+02]\n",
      " ...\n",
      " [-8.7398529e-02  1.0000000e+00  1.9075098e+00 ... -1.7490882e-01\n",
      "  -2.0856476e-01 -4.4894844e-02]\n",
      " [-8.8123322e-02  1.0000000e+00  1.9075127e+00 ... -1.5760619e+02\n",
      "   2.3558696e+02  2.6246475e+02]\n",
      " [-8.8330269e-02  1.0000000e+00  1.9074917e+00 ...  5.2570703e+03\n",
      "  -3.9418005e+03  3.6375483e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 19, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7304516e-01  1.0000000e+00  1.7869911e+00 ... -4.2243477e+01\n",
      "   1.9932209e+01  4.3964436e+01]\n",
      " [-6.7296982e-01  1.0000000e+00  1.7869873e+00 ... -7.8189288e+02\n",
      "  -9.9587079e+02  6.7165906e+02]\n",
      " [-6.7239952e-01  1.0000000e+00  1.7872343e+00 ... -3.3314255e+01\n",
      "   5.5747402e+01 -2.3925533e+01]\n",
      " ...\n",
      " [-6.7251015e-01  1.0000000e+00  1.7871513e+00 ...  6.9539213e-01\n",
      "  -1.1296773e+00 -2.8559849e-01]\n",
      " [-6.7317581e-01  1.0000000e+00  1.7869034e+00 ...  4.6509369e+01\n",
      "   2.3029123e+01 -9.2308342e+01]\n",
      " [-6.7339897e-01  1.0000000e+00  1.7868443e+00 ...  1.2358010e+03\n",
      "  -4.9794067e+02  6.8159540e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 20, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.19230938e+00  1.00000000e+00  1.49153900e+00 ... -1.41541809e+02\n",
      "   2.01508041e+02 -1.19277885e+02]\n",
      " [-1.19225407e+00  1.00000000e+00  1.49154186e+00 ... -2.94613678e+02\n",
      "   5.83668579e+02 -9.87389221e+02]\n",
      " [-1.19174767e+00  1.00000000e+00  1.49193299e+00 ... -2.27719589e+02\n",
      "  -1.27101181e+02 -2.34975754e+02]\n",
      " ...\n",
      " [-1.19184113e+00  1.00000000e+00  1.49183369e+00 ...  5.40842116e-02\n",
      "  -3.85633945e-01 -6.09726906e-02]\n",
      " [-1.19240952e+00  1.00000000e+00  1.49139023e+00 ...  4.57746658e+01\n",
      "   2.25332077e+02  5.27231140e+01]\n",
      " [-1.19260979e+00  1.00000000e+00  1.49130249e+00 ... -1.30080566e+02\n",
      "   2.71415768e+01  1.85786800e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 21, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.59514236e+00  1.00000000e+00  1.04969978e+00 ... -1.62063560e+01\n",
      "   1.69425613e+02  1.68608932e+02]\n",
      " [-1.59511185e+00  1.00000000e+00  1.04975700e+00 ...  9.56366119e+01\n",
      "  -8.74790421e+01 -6.34597397e+01]\n",
      " [-1.59470940e+00  1.00000000e+00  1.05024326e+00 ...  2.17938557e+01\n",
      "  -2.97206955e+01 -2.16855450e+01]\n",
      " ...\n",
      " [-1.59475899e+00  1.00000000e+00  1.05012131e+00 ... -3.64548117e-01\n",
      "  -1.33934021e-01 -1.84898973e-02]\n",
      " [-1.59517670e+00  1.00000000e+00  1.04950142e+00 ... -2.50699577e+01\n",
      "   1.14338776e+02 -1.32118210e+02]\n",
      " [-1.59534836e+00  1.00000000e+00  1.04939270e+00 ...  6.32586441e+01\n",
      "  -1.25462830e+02  8.99154510e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Step 22, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8415174e+00  1.0000000e+00  5.0524139e-01 ... -9.3520088e+01\n",
      "   6.8146530e+01 -4.3203236e+01]\n",
      " [-1.8415041e+00  1.0000000e+00  5.0532055e-01 ... -3.5940891e+01\n",
      "  -1.2855666e+02  1.1123665e+02]\n",
      " [-1.8412476e+00  1.0000000e+00  5.0588292e-01 ...  1.8251429e+00\n",
      "  -5.4258695e+00  1.5546514e+01]\n",
      " ...\n",
      " [-1.8412743e+00  1.0000000e+00  5.0574207e-01 ...  1.1417909e-01\n",
      "   5.7488146e+00 -1.0222529e+00]\n",
      " [-1.8415012e+00  1.0000000e+00  5.0501823e-01 ...  7.5869385e+01\n",
      "  -5.7630676e+01 -1.5609471e+02]\n",
      " [-1.8416214e+00  1.0000000e+00  5.0488853e-01 ... -2.3774149e+02\n",
      "   2.9392017e+02 -1.0782230e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.3\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 23, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.90752697e+00  1.00000000e+00 -8.82492065e-02 ...  3.74884949e+01\n",
      "  -2.67905182e+02 -2.62209595e+02]\n",
      " [-1.90753555e+00  1.00000000e+00 -8.81595612e-02 ... -4.42289209e+00\n",
      "   3.76203117e+01 -9.81283569e+00]\n",
      " [-1.90748024e+00  1.00000000e+00 -8.75780508e-02 ...  1.34242678e+00\n",
      "   1.36050171e+02 -1.01845856e+02]\n",
      " ...\n",
      " [-1.90746117e+00  1.00000000e+00 -8.77199173e-02 ... -2.98255539e+00\n",
      "  -1.19228773e+01 -4.45106983e-01]\n",
      " [-1.90745926e+00  1.00000000e+00 -8.84819031e-02 ...  2.42387791e+01\n",
      "  -3.04864979e+01  4.55967178e+01]\n",
      " [-1.90752506e+00  1.00000000e+00 -8.86230469e-02 ...  8.56244659e+00\n",
      "   1.87234116e+01  4.58613110e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.1       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.6\n",
      " 0.        1.0000001 0.        0.6       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 24, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.78677654e+00  1.00000000e+00 -6.73784256e-01 ... -5.62949467e+00\n",
      "  -1.91807957e+01  1.06968737e+00]\n",
      " [-1.78680611e+00  1.00000000e+00 -6.73668861e-01 ...  3.92647491e+02\n",
      "  -8.66928528e+02 -1.05417676e+03]\n",
      " [-1.78693008e+00  1.00000000e+00 -6.73134744e-01 ... -1.46535683e+00\n",
      "   6.60393953e-01  4.85051632e+00]\n",
      " ...\n",
      " [-1.78687096e+00  1.00000000e+00 -6.73247337e-01 ... -1.12960482e+00\n",
      "  -6.72211647e+00  6.01382554e-01]\n",
      " [-1.78666306e+00  1.00000000e+00 -6.74001694e-01 ... -1.07110306e+02\n",
      "   1.08179817e+02 -1.97130833e+01]\n",
      " [-1.78666115e+00  1.00000000e+00 -6.74135208e-01 ...  2.07643234e+02\n",
      "  -7.29093475e+01 -2.81028564e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.1       0.        0.\n",
      " 0.2       0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 25, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4909801e+00  1.0000000e+00 -1.1931534e+00 ... -6.5528241e+02\n",
      "  -8.0342657e+02 -1.1550237e+02]\n",
      " [-1.4910355e+00  1.0000000e+00 -1.1930265e+00 ... -7.4174019e+01\n",
      "   2.1625700e+01 -8.7333885e+01]\n",
      " [-1.4912891e+00  1.0000000e+00 -1.1925787e+00 ... -3.4354019e+00\n",
      "  -1.5373416e+00  3.3801293e+00]\n",
      " ...\n",
      " [-1.4911766e+00  1.0000000e+00 -1.1926727e+00 ... -1.5234823e+00\n",
      "  -4.4234471e+00  3.7141466e-01]\n",
      " [-1.4907913e+00  1.0000000e+00 -1.1933403e+00 ...  7.3744040e+00\n",
      "   2.0754552e+00 -2.0226448e+01]\n",
      " [-1.4907846e+00  1.0000000e+00 -1.1934471e+00 ... -7.5868805e+01\n",
      "   2.1258059e+01 -2.1758610e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.2       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 26, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.0494337e+00  1.0000000e+00 -1.5954189e+00 ...  4.5674521e+02\n",
      "  -4.6143448e+02 -3.7920044e+02]\n",
      " [-1.0495148e+00  1.0000000e+00 -1.5953293e+00 ... -1.4072129e+03\n",
      "  -2.0358224e+03  9.0287140e+02]\n",
      " [-1.0498676e+00  1.0000000e+00 -1.5950174e+00 ...  1.5613279e+00\n",
      "   1.4612591e+00 -3.9931023e-01]\n",
      " ...\n",
      " [-1.0497322e+00  1.0000000e+00 -1.5950966e+00 ...  5.9393948e-01\n",
      "   1.4967813e+00 -1.3641658e-01]\n",
      " [-1.0492210e+00  1.0000000e+00 -1.5955448e+00 ... -2.0932230e+02\n",
      "  -4.2705284e+01  9.6785736e-01]\n",
      " [-1.0491590e+00  1.0000000e+00 -1.5956230e+00 ...  2.8424183e+01\n",
      "   4.6695244e+01 -1.2552665e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 1.0000001 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 27, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.05098343e-01  1.00000000e+00 -1.84161186e+00 ... -1.81786560e+02\n",
      "  -4.29326973e+01  4.50263748e+01]\n",
      " [-5.05193710e-01  1.00000000e+00 -1.84156990e+00 ... -2.35690491e+02\n",
      "  -8.10129929e+01  1.10671606e+01]\n",
      " [-5.05640030e-01  1.00000000e+00 -1.84140718e+00 ... -1.61252308e+00\n",
      "   2.36297131e-01  6.70254707e-01]\n",
      " ...\n",
      " [-5.05491257e-01  1.00000000e+00 -1.84144783e+00 ... -8.99224043e-01\n",
      "   1.13176346e+00  7.15507269e-01]\n",
      " [-5.04896164e-01  1.00000000e+00 -1.84166718e+00 ...  7.30761719e+01\n",
      "   1.20781746e+02 -3.00601616e+01]\n",
      " [-5.04777908e-01  1.00000000e+00 -1.84170151e+00 ...  1.09682076e+02\n",
      "   2.26428482e+02 -3.70616486e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 28, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 8.8800430e-02  1.0000000e+00 -1.9075127e+00 ... -3.7677071e+01\n",
      "   7.5690851e+02 -1.7553336e+02]\n",
      " [ 8.8699341e-02  1.0000000e+00 -1.9075089e+00 ... -5.0776477e+00\n",
      "  -3.6001511e+01  2.2635311e+01]\n",
      " [ 8.8245392e-02  1.0000000e+00 -1.9075090e+00 ... -2.3600054e-01\n",
      "   4.8001719e-01  1.3041902e-01]\n",
      " ...\n",
      " [ 8.8396072e-02  1.0000000e+00 -1.9074974e+00 ... -3.8442242e-01\n",
      "   2.0129051e+00  3.4916437e-01]\n",
      " [ 8.9010239e-02  1.0000000e+00 -1.9074841e+00 ...  4.8651663e+02\n",
      "  -1.5019322e+02  9.0908307e+02]\n",
      " [ 8.9130402e-02  1.0000000e+00 -1.9074802e+00 ... -6.7547920e+01\n",
      "   3.6144379e+01  7.8080597e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 29, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7414093e-01  1.0000000e+00 -1.7866096e+00 ... -1.1013305e+01\n",
      "  -6.2833210e+01 -1.1225924e+01]\n",
      " [ 6.7404175e-01  1.0000000e+00 -1.7866669e+00 ...  5.6334541e+01\n",
      "  -6.5441141e+00 -1.5288052e+02]\n",
      " [ 6.7365265e-01  1.0000000e+00 -1.7868142e+00 ...  4.6716404e-01\n",
      "  -1.3283248e+00 -1.0207093e-01]\n",
      " ...\n",
      " [ 6.7379379e-01  1.0000000e+00 -1.7867641e+00 ... -3.3871174e-02\n",
      "   3.5700457e+00 -2.5665116e-01]\n",
      " [ 6.7434883e-01  1.0000000e+00 -1.7865200e+00 ... -9.9306854e+01\n",
      "  -2.8777702e+01 -5.7820946e+01]\n",
      " [ 6.7444992e-01  1.0000000e+00 -1.7864799e+00 ... -8.9982399e+01\n",
      "  -5.0290367e+01  5.7899750e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 30, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.19341278e+00  1.00000000e+00 -1.49042511e+00 ...  7.52351303e+01\n",
      "   1.02157736e+00 -1.15539856e+01]\n",
      " [ 1.19332695e+00  1.00000000e+00 -1.49051476e+00 ... -2.15451851e+01\n",
      "  -6.18484545e+00 -1.07935295e+01]\n",
      " [ 1.19300079e+00  1.00000000e+00 -1.49079394e+00 ... -1.42915249e+00\n",
      "  -2.01074839e+00  2.32511759e-01]\n",
      " ...\n",
      " [ 1.19312859e+00  1.00000000e+00 -1.49069595e+00 ... -6.58789635e-01\n",
      "   9.03196335e+00 -1.84483600e+00]\n",
      " [ 1.19358444e+00  1.00000000e+00 -1.49026680e+00 ... -2.24772596e+00\n",
      "   7.74272585e+00  1.44118185e+01]\n",
      " [ 1.19367504e+00  1.00000000e+00 -1.49019814e+00 ...  1.30601654e+02\n",
      "  -5.32928284e+02 -2.17537003e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 31, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5955210e+00  1.0000000e+00 -1.0489273e+00 ...  2.5562626e+01\n",
      "   1.6380597e+02 -6.1956127e+01]\n",
      " [ 1.5954580e+00  1.0000000e+00 -1.0490036e+00 ...  1.4347377e+02\n",
      "  -6.6340530e+01 -1.6124525e+02]\n",
      " [ 1.5952148e+00  1.0000000e+00 -1.0493906e+00 ...  2.6168017e+00\n",
      "   1.3892519e+00 -4.7484201e-01]\n",
      " ...\n",
      " [ 1.5953121e+00  1.0000000e+00 -1.0492554e+00 ... -8.2394612e-01\n",
      "  -3.9189749e+00 -1.4628649e-02]\n",
      " [ 1.5956097e+00  1.0000000e+00 -1.0487175e+00 ... -5.0591445e+00\n",
      "   1.8638809e+02  4.8427992e+00]\n",
      " [ 1.5956964e+00  1.0000000e+00 -1.0486317e+00 ...  9.5372305e+00\n",
      "   3.1706738e+01  3.3733459e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0.2]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Step 32, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84169006e+00  1.00000000e+00 -5.04312515e-01 ... -1.66407852e+02\n",
      "   3.18666744e+01  1.27628708e+01]\n",
      " [ 1.84165668e+00  1.00000000e+00 -5.04425049e-01 ...  1.00827185e+03\n",
      "   3.98546753e+01  5.50882996e+02]\n",
      " [ 1.84155846e+00  1.00000000e+00 -5.04857957e-01 ...  3.52404404e+00\n",
      "   2.98178434e-01 -2.37645769e+00]\n",
      " ...\n",
      " [ 1.84160233e+00  1.00000000e+00 -5.04715919e-01 ...  1.06056333e-01\n",
      "  -6.89185429e+00 -6.49295211e-01]\n",
      " [ 1.84171104e+00  1.00000000e+00 -5.04053116e-01 ...  2.30301529e+02\n",
      "   3.81026268e+01  2.46143097e+02]\n",
      " [ 1.84175491e+00  1.00000000e+00 -5.03953934e-01 ...  4.12465096e+00\n",
      "   7.08374710e+01  1.62024832e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 33, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9074345e+00  1.0000000e+00  8.9405060e-02 ...  2.7333737e+02\n",
      "   2.3527884e+02 -4.9852180e+01]\n",
      " [ 1.9074345e+00  1.0000000e+00  8.9283943e-02 ... -3.3547302e+02\n",
      "   2.1979556e+02 -3.8651941e+02]\n",
      " [ 1.9074879e+00  1.0000000e+00  8.8845260e-02 ...  1.0946070e+01\n",
      "  -4.8075861e-01  7.4777055e-01]\n",
      " ...\n",
      " [ 1.9074802e+00  1.0000000e+00  8.8981628e-02 ...  2.4557257e+00\n",
      "   5.1907215e+00  8.7701023e-02]\n",
      " [ 1.9073715e+00  1.0000000e+00  8.9672089e-02 ...  1.5178628e+01\n",
      "   1.2220175e+02 -6.2544617e+01]\n",
      " [ 1.9073849e+00  1.0000000e+00  8.9776993e-02 ... -7.2229553e+01\n",
      "   6.4504211e+01 -4.6599834e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.2\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 34, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7863808e+00  1.0000000e+00  6.7476082e-01 ... -1.1937224e+03\n",
      "   2.7636941e+03 -6.0162622e+02]\n",
      " [ 1.7864056e+00  1.0000000e+00  6.7465401e-01 ...  1.4539876e+02\n",
      "   3.7049579e+02  2.8614114e+02]\n",
      " [ 1.7866344e+00  1.0000000e+00  6.7423344e-01 ... -3.4476185e-01\n",
      "  -2.1050022e+00 -1.3017451e+00]\n",
      " ...\n",
      " [ 1.7865753e+00  1.0000000e+00  6.7437553e-01 ...  2.9679599e+00\n",
      "   1.0734004e+01  7.9014856e-01]\n",
      " [ 1.7862682e+00  1.0000000e+00  6.7501640e-01 ...  7.3268706e+03\n",
      "   6.2660225e+03 -2.0210226e+03]\n",
      " [ 1.7862206e+00  1.0000000e+00  6.7511559e-01 ...  2.2725064e+02\n",
      "  -1.8998938e+02  2.1323019e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 35, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4905062e+00  1.0000000e+00  1.1937428e+00 ... -4.7179245e+02\n",
      "   3.9609866e+02  9.3134689e+01]\n",
      " [ 1.4905615e+00  1.0000000e+00  1.1936445e+00 ...  3.2926395e+01\n",
      "  -1.2665784e+02 -4.7389999e+01]\n",
      " [ 1.4909229e+00  1.0000000e+00  1.1933028e+00 ...  7.9512215e-01\n",
      "  -2.0072711e+00 -5.6133032e-02]\n",
      " ...\n",
      " [ 1.4908295e+00  1.0000000e+00  1.1934099e+00 ... -1.5401459e+00\n",
      "  -2.3113990e+00  1.7309998e-01]\n",
      " [ 1.4903259e+00  1.0000000e+00  1.1939793e+00 ... -4.7442642e+01\n",
      "  -4.3680038e+01  2.8962872e+01]\n",
      " [ 1.4902554e+00  1.0000000e+00  1.1940517e+00 ... -3.2194133e+00\n",
      "  -1.0399481e+02 -8.3208801e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.5       0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.6\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 36, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.04827499e+00  1.00000000e+00  1.59619141e+00 ... -6.91315308e+01\n",
      "  -1.27198925e+01 -1.18218603e+01]\n",
      " [ 1.04834557e+00  1.00000000e+00  1.59613419e+00 ... -4.28638672e+03\n",
      "   5.48887744e+03  4.62689990e+03]\n",
      " [ 1.04882812e+00  1.00000000e+00  1.59589207e+00 ... -6.48204923e-01\n",
      "  -5.09032488e-01 -8.32812190e-01]\n",
      " ...\n",
      " [ 1.04869843e+00  1.00000000e+00  1.59596539e+00 ... -1.19755769e+00\n",
      "  -2.17959642e+00  1.07766390e-02]\n",
      " [ 1.04805374e+00  1.00000000e+00  1.59638023e+00 ...  1.97546783e+02\n",
      "   3.99185600e+01  6.64090500e+01]\n",
      " [ 1.04794502e+00  1.00000000e+00  1.59642982e+00 ... -3.18971634e+01\n",
      "   1.67643976e+00  9.76123581e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 37, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.0395012e-01  1.0000000e+00  1.8419857e+00 ...  3.0328394e+01\n",
      "  -1.3607774e+01 -1.4080937e+02]\n",
      " [ 5.0403500e-01  1.0000000e+00  1.8419418e+00 ...  8.7569916e+02\n",
      "  -1.5378623e+03  2.6490247e+02]\n",
      " [ 5.0461388e-01  1.0000000e+00  1.8418361e+00 ...  1.9488859e-01\n",
      "   1.0905730e+00  4.2028236e-01]\n",
      " ...\n",
      " [ 5.0446129e-01  1.0000000e+00  1.8418636e+00 ... -1.8736153e+00\n",
      "  -2.7390203e+00 -6.7841035e-01]\n",
      " [ 5.0372505e-01  1.0000000e+00  1.8420963e+00 ...  1.5184721e+02\n",
      "   1.0179222e+02 -1.0559919e+02]\n",
      " [ 5.0357437e-01  1.0000000e+00  1.8421268e+00 ... -1.2972023e+02\n",
      "  -3.4556226e+02 -2.3987469e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.2 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 38, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.0024948e-02  1.0000000e+00  1.9075565e+00 ... -1.4691906e+01\n",
      "  -5.4826632e+00  8.3656380e+01]\n",
      " [-8.9935303e-02  1.0000000e+00  1.9075556e+00 ... -2.7583607e+01\n",
      "  -4.7805141e+01  9.8043480e+00]\n",
      " [-8.9319229e-02  1.0000000e+00  1.9075968e+00 ... -2.5571012e-01\n",
      "   1.6557643e+00  1.5640855e-01]\n",
      " ...\n",
      " [-8.9477539e-02  1.0000000e+00  1.9075727e+00 ... -6.3095341e+00\n",
      "  -9.9421768e+00  1.3673298e+00]\n",
      " [-9.0230942e-02  1.0000000e+00  1.9075737e+00 ... -2.2185446e+02\n",
      "   3.1432886e+02 -1.3553928e+02]\n",
      " [-9.0411186e-02  1.0000000e+00  1.9075718e+00 ...  2.0835567e+02\n",
      "  -5.8309303e+01  6.2828598e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 39, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7530346e-01  1.0000000e+00  1.7862625e+00 ...  1.7520168e+02\n",
      "   3.2051596e+02  8.3252179e+02]\n",
      " [-6.7522335e-01  1.0000000e+00  1.7863092e+00 ... -8.9191231e+01\n",
      "  -2.6851617e+02  1.0787555e+02]\n",
      " [-6.7465019e-01  1.0000000e+00  1.7864919e+00 ...  1.6986074e+00\n",
      "  -4.4190276e-01 -1.6243613e-01]\n",
      " ...\n",
      " [-6.7479515e-01  1.0000000e+00  1.7864408e+00 ...  1.3950107e+00\n",
      "   1.8433461e+00  2.9700402e-02]\n",
      " [-6.7552185e-01  1.0000000e+00  1.7862110e+00 ...  1.9032101e+02\n",
      "  -5.6201556e+02 -2.3803851e+02]\n",
      " [-6.7567635e-01  1.0000000e+00  1.7861652e+00 ...  1.2541344e+02\n",
      "  -3.8722432e+00  2.8941683e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 40, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1943035e+00  1.0000000e+00  1.4901257e+00 ... -2.2761419e+01\n",
      "  -1.9833908e+01  4.5957512e+01]\n",
      " [-1.1942368e+00  1.0000000e+00  1.4901619e+00 ...  1.7908092e+02\n",
      "   1.5502361e+02 -9.8715332e+01]\n",
      " [-1.1937771e+00  1.0000000e+00  1.4905046e+00 ...  6.3479576e+00\n",
      "  -2.4530401e+00 -4.8632205e-01]\n",
      " ...\n",
      " [-1.1938934e+00  1.0000000e+00  1.4904060e+00 ...  4.3100538e+00\n",
      "   8.2207155e+00  4.4861421e-02]\n",
      " [-1.1945095e+00  1.0000000e+00  1.4899883e+00 ... -2.6197604e+02\n",
      "  -1.5453735e+02  3.1295364e+01]\n",
      " [-1.1946211e+00  1.0000000e+00  1.4899139e+00 ...  7.2948990e+01\n",
      "   1.4160104e+02 -3.7949116e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 41, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5962658e+00  1.0000000e+00  1.0481663e+00 ... -1.8982500e+02\n",
      "   2.1906369e+02 -2.9243731e-01]\n",
      " [-1.5962143e+00  1.0000000e+00  1.0482264e+00 ... -1.4695930e+02\n",
      "  -2.2521782e+00  6.8014778e+01]\n",
      " [-1.5958824e+00  1.0000000e+00  1.0486909e+00 ...  8.8186693e-01\n",
      "  -5.8564460e-01 -8.2655251e-02]\n",
      " ...\n",
      " [-1.5959682e+00  1.0000000e+00  1.0485563e+00 ...  1.1602411e+00\n",
      "   2.9149864e+00  2.2903369e-01]\n",
      " [-1.5964146e+00  1.0000000e+00  1.0479813e+00 ... -5.6903961e+02\n",
      "  -5.8657294e+02  7.8783386e+02]\n",
      " [-1.5965023e+00  1.0000000e+00  1.0478802e+00 ... -9.2619492e+01\n",
      "   2.1594785e+02 -3.1790869e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.4        0.         0.1        0.         0.70000005\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Step 42, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8420744e+00  1.0000000e+00  5.0341225e-01 ... -8.0651398e+00\n",
      "  -4.0518980e+00 -2.3162756e+00]\n",
      " [-1.8420467e+00  1.0000000e+00  5.0345421e-01 ...  7.0231445e+01\n",
      "  -2.3539798e+02  4.6441589e+01]\n",
      " [-1.8418884e+00  1.0000000e+00  5.0399351e-01 ...  3.5309792e-02\n",
      "  -2.8680897e-01  4.2891130e-03]\n",
      " ...\n",
      " [-1.8419361e+00  1.0000000e+00  5.0384808e-01 ...  9.7939980e-01\n",
      "   5.4838614e+00  1.8591099e+00]\n",
      " [-1.8421631e+00  1.0000000e+00  5.0317764e-01 ... -1.7392128e+02\n",
      "   3.0243875e+02  9.8493945e+02]\n",
      " [-1.8422070e+00  1.0000000e+00  5.0305557e-01 ...  5.3124548e+02\n",
      "  -1.2540118e+02 -3.3056613e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.2       1.0000001\n",
      " 0.        0.        0.        0.6       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 43, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9074974e+00  1.0000000e+00 -9.0406418e-02 ...  1.7576600e+03\n",
      "  -5.0535615e+03 -8.4851074e+03]\n",
      " [-1.9075050e+00  1.0000000e+00 -9.0369225e-02 ...  2.9413954e+01\n",
      "   4.0679634e+01  1.3535799e+01]\n",
      " [-1.9074612e+00  1.0000000e+00 -8.9820392e-02 ... -3.6430836e-02\n",
      "   3.1464958e-01 -4.7211451e-03]\n",
      " ...\n",
      " [-1.9074783e+00  1.0000000e+00 -8.9963913e-02 ...  9.4061404e-02\n",
      "   2.5667429e+00  2.4031043e-01]\n",
      " [-1.9074821e+00  1.0000000e+00 -9.0654373e-02 ... -2.8486029e+02\n",
      "  -8.4563950e+01  8.7944622e+00]\n",
      " [-1.9074974e+00  1.0000000e+00 -9.0780258e-02 ... -3.5457007e+02\n",
      "  -9.6344720e+02  1.6646547e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 1.0000001\n",
      " 0.2       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 44, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7861309e+00  1.0000000e+00 -6.7547607e-01 ... -3.1108716e+02\n",
      "   9.8656403e+01 -5.9453846e+01]\n",
      " [-1.7861586e+00  1.0000000e+00 -6.7545509e-01 ... -1.6680319e+01\n",
      "  -1.7092480e+01 -5.1691109e+01]\n",
      " [-1.7863007e+00  1.0000000e+00 -6.7492747e-01 ... -7.6058388e-02\n",
      "   2.7846193e-01 -1.0490298e-02]\n",
      " ...\n",
      " [-1.7862682e+00  1.0000000e+00 -6.7507267e-01 ... -1.1906533e+00\n",
      "   9.6442099e+00  1.7060575e+00]\n",
      " [-1.7860947e+00  1.0000000e+00 -6.7570496e-01 ...  2.0596599e+02\n",
      "  -1.5090771e+02  7.9754633e+02]\n",
      " [-1.7860146e+00  1.0000000e+00 -6.7581749e-01 ... -1.3252623e+02\n",
      "  -3.3609827e+02  1.1934545e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.1       0.\n",
      " 0.        0.        0.        0.        0.        0.        1.0000001\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 45, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4897776e+00  1.0000000e+00 -1.1945992e+00 ...  6.2983051e+02\n",
      "   7.6762042e+02  9.9358854e+00]\n",
      " [-1.4898186e+00  1.0000000e+00 -1.1945782e+00 ... -7.2671799e+01\n",
      "  -2.8706087e+01  3.4238464e+01]\n",
      " [-1.4901237e+00  1.0000000e+00 -1.1941545e+00 ... -3.1317234e-02\n",
      "   2.9398680e-02  5.1659346e-04]\n",
      " ...\n",
      " [-1.4900398e+00  1.0000000e+00 -1.1942587e+00 ... -1.1288669e+00\n",
      "   2.8912299e+00  4.6316600e-01]\n",
      " [-1.4896984e+00  1.0000000e+00 -1.1947861e+00 ... -8.5242010e+02\n",
      "  -2.0288261e+02 -3.7414719e+02]\n",
      " [-1.4895544e+00  1.0000000e+00 -1.1948757e+00 ...  2.1099979e+02\n",
      "  -3.5400189e+02  3.4799823e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.1       0.        0.4\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 46, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.04770374e+00  1.00000000e+00 -1.59643364e+00 ... -1.19654785e+02\n",
      "  -4.96674713e+02  1.59688766e+02]\n",
      " [-1.04775238e+00  1.00000000e+00 -1.59648418e+00 ...  4.34895477e+01\n",
      "   2.49733780e+02  1.04228004e+02]\n",
      " [-1.04819679e+00  1.00000000e+00 -1.59617674e+00 ...  2.34439802e+00\n",
      "  -1.35050666e+00 -1.08442046e-01]\n",
      " ...\n",
      " [-1.04808044e+00  1.00000000e+00 -1.59625244e+00 ... -2.75005198e+00\n",
      "   5.41934252e+00  1.20251536e+00]\n",
      " [-1.04759789e+00  1.00000000e+00 -1.59657288e+00 ... -4.29003174e+02\n",
      "   3.75762726e+02  2.66934174e+02]\n",
      " [-1.04740715e+00  1.00000000e+00 -1.59664345e+00 ... -3.46630280e+02\n",
      "  -1.11481621e+02  9.97983826e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Step 47, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0301075e-01  1.0000000e+00 -1.8420467e+00 ...  3.0149878e+01\n",
      "   2.0105263e+01 -4.4707217e+02]\n",
      " [-5.0306892e-01  1.0000000e+00 -1.8421278e+00 ...  1.6860455e+01\n",
      "   5.8420429e+00  1.1513349e+02]\n",
      " [-5.0356865e-01  1.0000000e+00 -1.8419638e+00 ...  7.1067753e+00\n",
      "  -3.5141234e+00 -1.4905628e+00]\n",
      " ...\n",
      " [-5.0343513e-01  1.0000000e+00 -1.8420124e+00 ... -1.6455575e+00\n",
      "   2.4075601e+00  2.3062691e-01]\n",
      " [-5.0287247e-01  1.0000000e+00 -1.8421192e+00 ... -1.3832475e+02\n",
      "  -3.4763727e+02  5.4338831e+02]\n",
      " [-5.0266171e-01  1.0000000e+00 -1.8421516e+00 ... -1.0119475e+01\n",
      "   3.6412902e+00  4.7076748e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Step 48, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.0999603e-02  1.0000000e+00 -1.9073219e+00 ... -1.5606295e+01\n",
      "  -2.8152890e+02  1.5125380e+01]\n",
      " [ 9.0939522e-02  1.0000000e+00 -1.9073963e+00 ... -3.0462914e+01\n",
      "   2.0533461e+01 -4.7737660e+00]\n",
      " [ 9.0457916e-02  1.0000000e+00 -1.9074020e+00 ...  1.8916099e+00\n",
      "  -2.2416289e+00 -6.1993366e-01]\n",
      " ...\n",
      " [ 9.0591431e-02  1.0000000e+00 -1.9074125e+00 ... -6.0779910e+00\n",
      "   7.5036860e+00  1.6806662e+00]\n",
      " [ 9.1180801e-02  1.0000000e+00 -1.9073391e+00 ...  3.2556284e+02\n",
      "  -2.6146988e+01 -2.3522882e+02]\n",
      " [ 9.1358185e-02  1.0000000e+00 -1.9073238e+00 ...  5.4506995e+02\n",
      "  -1.8294035e+02 -3.4222984e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 49, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7575836e-01  1.0000000e+00 -1.7859058e+00 ... -6.0746460e+01\n",
      "  -6.3955170e+01 -1.6441438e+02]\n",
      " [ 6.7570496e-01  1.0000000e+00 -1.7860069e+00 ...  3.1277935e+03\n",
      "  -9.7612091e+01 -6.0468989e+03]\n",
      " [ 6.7526054e-01  1.0000000e+00 -1.7861601e+00 ...  5.6943560e+00\n",
      "  -7.0285554e+00 -2.6480780e+00]\n",
      " ...\n",
      " [ 6.7538643e-01  1.0000000e+00 -1.7861252e+00 ...  2.2864938e+00\n",
      "  -2.0892313e+00  8.1342793e-01]\n",
      " [ 6.7594910e-01  1.0000000e+00 -1.7858543e+00 ...  2.1089943e+02\n",
      "   2.6581921e+02 -5.5421722e+01]\n",
      " [ 6.7609406e-01  1.0000000e+00 -1.7857838e+00 ... -2.0553564e+02\n",
      "   1.4928722e+02 -7.4882713e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 50, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.19489670e+00  1.00000000e+00 -1.48943901e+00 ...  3.34275757e+02\n",
      "  -2.71889435e+02  2.30863388e+02]\n",
      " [ 1.19484711e+00  1.00000000e+00 -1.48953056e+00 ... -7.56899805e+03\n",
      "  -4.29336914e+03  2.50325171e+03]\n",
      " [ 1.19448280e+00  1.00000000e+00 -1.48984206e+00 ...  5.05671024e-01\n",
      "  -4.40281677e+00 -1.85738230e+00]\n",
      " ...\n",
      " [ 1.19459534e+00  1.00000000e+00 -1.48977089e+00 ...  8.80078793e-01\n",
      "  -1.71006501e+00 -3.62967700e-01]\n",
      " [ 1.19503975e+00  1.00000000e+00 -1.48933220e+00 ...  5.07744827e+01\n",
      "   1.20642334e+02 -3.75980721e+01]\n",
      " [ 1.19517231e+00  1.00000000e+00 -1.48919868e+00 ... -3.77627983e+01\n",
      "   5.07048859e+02 -7.70128113e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 51, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.59665489e+00  1.00000000e+00 -1.04740906e+00 ...  6.90983582e+01\n",
      "  -8.34465561e+01  1.40816240e+01]\n",
      " [ 1.59662247e+00  1.00000000e+00 -1.04746628e+00 ... -1.16851001e+03\n",
      "   9.66320129e+02 -2.81275293e+03]\n",
      " [ 1.59638596e+00  1.00000000e+00 -1.04788959e+00 ...  1.14840865e+00\n",
      "  -3.51936913e+00 -1.50624180e+00]\n",
      " ...\n",
      " [ 1.59645844e+00  1.00000000e+00 -1.04778004e+00 ... -1.25753641e+00\n",
      "  -4.84895706e-03 -4.56997871e-01]\n",
      " [ 1.59676361e+00  1.00000000e+00 -1.04725075e+00 ... -1.23984375e+02\n",
      "  -7.23970413e+01  2.03092896e+02]\n",
      " [ 1.59684086e+00  1.00000000e+00 -1.04706955e+00 ... -1.87849197e+02\n",
      "  -2.79192352e+02 -2.03405899e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 52, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8421984e+00  1.0000000e+00 -5.0273895e-01 ...  3.3272472e+01\n",
      "  -1.9632195e+01  9.7486563e+00]\n",
      " [ 1.8421793e+00  1.0000000e+00 -5.0283146e-01 ...  1.7837154e+02\n",
      "   4.2303961e+02 -4.2664532e+01]\n",
      " [ 1.8420811e+00  1.0000000e+00 -5.0330663e-01 ...  6.5097213e-01\n",
      "   3.0470276e-01 -4.8843718e-01]\n",
      " ...\n",
      " [ 1.8421307e+00  1.0000000e+00 -5.0318336e-01 ... -3.4977722e-01\n",
      "   1.4902184e+00  1.6862416e-01]\n",
      " [ 1.8422260e+00  1.0000000e+00 -5.0254440e-01 ...  1.5206892e+02\n",
      "   8.7206573e+01 -4.0924808e+02]\n",
      " [ 1.8422928e+00  1.0000000e+00 -5.0234604e-01 ...  1.2706956e+02\n",
      "   3.1548788e+02 -1.1785728e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.0000001  0.         0.\n",
      " 0.70000005 0.         1.0000001  0.         0.         0.2\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 53, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9074144e+00  1.0000000e+00  9.1215134e-02 ...  2.2196373e+02\n",
      "   5.9225445e+01  1.4155652e+02]\n",
      " [ 1.9074202e+00  1.0000000e+00  9.1128349e-02 ... -4.1384778e+00\n",
      "   6.4721115e+01  2.5641434e+02]\n",
      " [ 1.9074783e+00  1.0000000e+00  9.0637572e-02 ...  4.5913267e-01\n",
      "  -2.9924917e-01 -7.2085571e-01]\n",
      " ...\n",
      " [ 1.9074936e+00  1.0000000e+00  9.0764999e-02 ... -5.7779741e-01\n",
      "   8.9598083e-01  6.8512976e-02]\n",
      " [ 1.9073868e+00  1.0000000e+00  9.1417313e-02 ...  2.7697568e+02\n",
      "  -8.6138735e+00  7.2412384e+02]\n",
      " [ 1.9073792e+00  1.0000000e+00  9.1630936e-02 ...  2.2020065e+02\n",
      "   5.7846771e+02  4.0829636e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        1.0000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 54, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.78549576e+00  1.00000000e+00  6.76656723e-01 ...  1.55907755e+01\n",
      "   3.71221008e+01  2.80781982e+02]\n",
      " [ 1.78552341e+00  1.00000000e+00  6.76568985e-01 ...  4.03262978e+01\n",
      "  -4.81223068e+01 -7.91211243e+01]\n",
      " [ 1.78574181e+00  1.00000000e+00  6.76101387e-01 ... -3.17371249e-01\n",
      "  -5.52833080e-01  3.66139412e-03]\n",
      " ...\n",
      " [ 1.78570366e+00  1.00000000e+00  6.76233292e-01 ... -2.53912926e-01\n",
      "   6.88138008e-02  2.79701352e-02]\n",
      " [ 1.78540993e+00  1.00000000e+00  6.76845551e-01 ...  1.09223892e+02\n",
      "  -7.96724777e+01  5.09552288e+00]\n",
      " [ 1.78535366e+00  1.00000000e+00  6.77034378e-01 ...  2.14257172e+02\n",
      "   1.81436798e+02  2.90828633e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.8000001 0.\n",
      " 1.0000001 0.        0.        0.8000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 55, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4888115e+00  1.0000000e+00  1.1952667e+00 ...  5.3828175e+01\n",
      "  -4.0751238e+00 -3.4257702e+01]\n",
      " [ 1.4888668e+00  1.0000000e+00  1.1952095e+00 ...  1.7899747e+02\n",
      "   5.8416790e+01  1.9690784e+01]\n",
      " [ 1.4892216e+00  1.0000000e+00  1.1948223e+00 ... -5.8688509e-01\n",
      "  -5.1879072e-01  7.2989631e-01]\n",
      " ...\n",
      " [ 1.4891376e+00  1.0000000e+00  1.1949396e+00 ...  1.6785641e+00\n",
      "  -1.4057964e+00 -7.5417399e-02]\n",
      " [ 1.4886589e+00  1.0000000e+00  1.1954212e+00 ...  3.5312634e+02\n",
      "  -2.4010550e+01 -6.3797115e+01]\n",
      " [ 1.4885674e+00  1.0000000e+00  1.1955853e+00 ...  3.4940277e+01\n",
      "  -8.2740868e+01  2.1235969e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.0000001 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 56, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0467482     1.            1.5967999  ...  -94.19782\n",
      "   354.9997      -44.769985  ]\n",
      " [   1.0468235     1.            1.5967836  ...  139.00104\n",
      "   222.98044    -132.97821   ]\n",
      " [   1.0472775     1.            1.5965042  ...   -1.2139714\n",
      "    -8.826351     -0.65267587]\n",
      " ...\n",
      " [   1.0471687     1.            1.5966015  ...    5.632373\n",
      "    -4.6080103    -1.4712147 ]\n",
      " [   1.0465584     1.            1.59692    ...  267.62183\n",
      "  -430.97977    -298.8118    ]\n",
      " [   1.0464048     1.            1.5970249  ...    2.6477225\n",
      "     4.143936     40.274952  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70000005 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.1       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 57, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 5.01391411e-01  1.00000000e+00  1.84217262e+00 ... -2.01114441e+02\n",
      "  -1.74301056e+02 -3.57105347e+02]\n",
      " [ 5.01479149e-01  1.00000000e+00  1.84217453e+00 ...  9.24394226e+01\n",
      "   4.36908150e+01 -2.10014820e+01]\n",
      " [ 5.02014160e-01  1.00000000e+00  1.84204388e+00 ...  7.85575211e-01\n",
      "  -1.43064060e+01  6.34145737e-03]\n",
      " ...\n",
      " [ 5.01882553e-01  1.00000000e+00  1.84211636e+00 ...  1.20132542e+00\n",
      "  -1.53237247e+00 -2.20491201e-01]\n",
      " [ 5.01173019e-01  1.00000000e+00  1.84223557e+00 ... -1.06559494e+02\n",
      "   5.64329712e+02 -5.04947968e+01]\n",
      " [ 5.00997543e-01  1.00000000e+00  1.84229088e+00 ... -6.71419678e+01\n",
      "   1.34055527e+02  2.89185791e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 58, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.2247009e-02  1.0000000e+00  1.9070511e+00 ... -2.0690149e+02\n",
      "  -3.2298218e+02  2.9405481e+02]\n",
      " [-9.2158318e-02  1.0000000e+00  1.9071045e+00 ...  1.5146393e+03\n",
      "   6.1089893e+02 -5.8059784e+02]\n",
      " [-9.1629028e-02  1.0000000e+00  1.9071198e+00 ... -3.6890694e-01\n",
      "   9.1162634e-01 -4.8022225e-02]\n",
      " ...\n",
      " [-9.1766357e-02  1.0000000e+00  1.9071541e+00 ...  3.2851458e+00\n",
      "  -6.0754375e+00 -2.0224860e+00]\n",
      " [-9.2489243e-02  1.0000000e+00  1.9070549e+00 ...  5.1079105e+01\n",
      "   5.2037811e+01 -1.1896980e+02]\n",
      " [-9.2656136e-02  1.0000000e+00  1.9070606e+00 ... -7.1964737e+01\n",
      "   2.3777199e+01 -6.1349621e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 59, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7728996e-01  1.0000000e+00  1.7851849e+00 ...  1.7435651e+02\n",
      "   1.2173318e+02  8.4599922e+01]\n",
      " [-6.7720795e-01  1.0000000e+00  1.7852612e+00 ...  6.0199409e+01\n",
      "  -1.4603732e+02 -1.1435968e+02]\n",
      " [-6.7668533e-01  1.0000000e+00  1.7854424e+00 ... -3.2458612e-01\n",
      "  -2.3575068e-01  9.4530225e-02]\n",
      " ...\n",
      " [-6.7681313e-01  1.0000000e+00  1.7854280e+00 ...  1.2768211e+00\n",
      "  -6.2336316e+00 -1.0535133e+00]\n",
      " [-6.7750740e-01  1.0000000e+00  1.7851429e+00 ...  1.1794876e+02\n",
      "  -9.9474010e+00 -6.5157310e+01]\n",
      " [-6.7767525e-01  1.0000000e+00  1.7850933e+00 ... -1.8670763e+02\n",
      "  -5.8071485e+00 -3.8420300e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 60, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1962347e+00  1.0000000e+00  1.4883041e+00 ... -3.0671500e+02\n",
      "   3.4674933e+02  6.5718796e+01]\n",
      " [-1.1961708e+00  1.0000000e+00  1.4883842e+00 ... -9.3793617e+01\n",
      "   2.1345917e+01 -3.0104976e+00]\n",
      " [-1.1957340e+00  1.0000000e+00  1.4887294e+00 ... -1.3299060e-01\n",
      "  -1.8517447e-01  7.8738153e-02]\n",
      " ...\n",
      " [-1.1958466e+00  1.0000000e+00  1.4886665e+00 ... -1.4188899e+00\n",
      "   2.1541023e-01 -5.6411439e-01]\n",
      " [-1.1964054e+00  1.0000000e+00  1.4882069e+00 ...  6.5605438e+01\n",
      "  -1.2049756e+02  1.8052811e+02]\n",
      " [-1.1965628e+00  1.0000000e+00  1.4881039e+00 ...  1.3507140e+01\n",
      "   7.6271652e+01  8.0415970e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 61, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.59745789e+00  1.00000000e+00  1.04583740e+00 ...  1.48784653e+02\n",
      "   4.68725800e+02  1.19997322e+02]\n",
      " [-1.59742260e+00  1.00000000e+00  1.04598999e+00 ...  3.05971222e+01\n",
      "  -3.85073967e+01  2.16189224e+02]\n",
      " [-1.59713936e+00  1.00000000e+00  1.04644012e+00 ... -1.42705047e+00\n",
      "   7.94764423e+00 -5.62651038e-01]\n",
      " ...\n",
      " [-1.59721947e+00  1.00000000e+00  1.04636383e+00 ...  6.12344444e-01\n",
      "  -1.41061306e-01  2.21048594e-01]\n",
      " [-1.59759331e+00  1.00000000e+00  1.04570198e+00 ... -5.45992310e+02\n",
      "   5.71746094e+02 -2.27344971e+02]\n",
      " [-1.59768963e+00  1.00000000e+00  1.04555893e+00 ... -5.45093498e+01\n",
      "  -3.91308327e+01 -1.08829765e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.8000001 0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 62, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8425760e+00  1.0000000e+00  5.0097656e-01 ... -5.5280697e+01\n",
      "   2.5918042e+02  1.6107224e+02]\n",
      " [-1.8425817e+00  1.0000000e+00  5.0114059e-01 ... -7.7516827e+02\n",
      "   1.7122668e+03 -2.7386682e+03]\n",
      " [-1.8424492e+00  1.0000000e+00  5.0165695e-01 ...  1.1097858e+00\n",
      "   1.0512881e+00 -6.4180040e-01]\n",
      " ...\n",
      " [-1.8424740e+00  1.0000000e+00  5.0157833e-01 ...  7.5358880e-01\n",
      "   2.8717375e-01 -7.2841555e-02]\n",
      " [-1.8426247e+00  1.0000000e+00  5.0081635e-01 ... -6.7499237e+01\n",
      "   3.7932961e+01  9.8411278e+01]\n",
      " [-1.8427095e+00  1.0000000e+00  5.0066185e-01 ... -6.5161560e+01\n",
      "  -4.7314892e+00  3.5745396e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.1       0.        0.        0.1       0.        0.\n",
      " 0.        0.        0.        0.        0.        0.9000001 0.\n",
      " 0.        0.        0.        0.4       0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 63, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9073162e+00  1.0000000e+00 -9.3448639e-02 ... -1.6055038e+03\n",
      "   4.9972754e+02  2.5048515e+02]\n",
      " [-1.9073477e+00  1.0000000e+00 -9.3276024e-02 ... -6.7190010e+01\n",
      "   6.2955894e+01 -4.1399906e+01]\n",
      " [-1.9074230e+00  1.0000000e+00 -9.2735022e-02 ...  1.0212328e+00\n",
      "  -5.4653826e+00 -1.7259967e-01]\n",
      " ...\n",
      " [-1.9074059e+00  1.0000000e+00 -9.2828751e-02 ... -4.6919397e-01\n",
      "   6.9393764e+00  2.9087520e-01]\n",
      " [-1.9073105e+00  1.0000000e+00 -9.3620300e-02 ...  1.4218826e+02\n",
      "  -4.2324059e+01 -2.7940500e+01]\n",
      " [-1.9073324e+00  1.0000000e+00 -9.3772888e-02 ... -3.5099041e+01\n",
      "   2.9736757e+01  3.8220325e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 64, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.7851706e+00  1.0000000e+00 -6.7837524e-01 ... -1.6921362e+02\n",
      "  -2.2049491e+00 -9.4944016e+01]\n",
      " [-1.7852402e+00  1.0000000e+00 -6.7823887e-01 ...  2.7426390e+01\n",
      "  -1.0396163e+02  1.4592004e+03]\n",
      " [-1.7854576e+00  1.0000000e+00 -6.7771304e-01 ... -8.0035329e-02\n",
      "  -5.7173991e+00 -1.3358502e+00]\n",
      " ...\n",
      " [-1.7854023e+00  1.0000000e+00 -6.7781734e-01 ... -1.6297901e+00\n",
      "   1.7744498e+00  1.7320113e-01]\n",
      " [-1.7850723e+00  1.0000000e+00 -6.7853737e-01 ... -1.9319962e+01\n",
      "   3.2251602e+01  1.1042100e+02]\n",
      " [-1.7850771e+00  1.0000000e+00 -6.7867661e-01 ...  1.2746942e+02\n",
      "  -1.7546837e+02  8.6455032e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 65, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.4881716e+00  1.0000000e+00 -1.1969032e+00 ...  7.8059776e+01\n",
      "  -1.3335802e+01 -1.1160149e+01]\n",
      " [-1.4882727e+00  1.0000000e+00 -1.1967802e+00 ...  3.0393442e+02\n",
      "   3.1708533e+02  3.6468326e+02]\n",
      " [-1.4886150e+00  1.0000000e+00 -1.1963429e+00 ...  5.6391299e-01\n",
      "  -5.5184526e+00 -1.2610166e+00]\n",
      " ...\n",
      " [-1.4885292e+00  1.0000000e+00 -1.1964350e+00 ...  2.4027613e-01\n",
      "  -6.7464714e+00 -2.4539274e-01]\n",
      " [-1.4880276e+00  1.0000000e+00 -1.1970387e+00 ...  2.9162619e+01\n",
      "  -1.4701988e+01 -5.3691730e+01]\n",
      " [-1.4879808e+00  1.0000000e+00 -1.1971550e+00 ...  2.0059464e+01\n",
      "  -4.2777588e+01 -8.8050556e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        1.0000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 66, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.04564       1.           -1.5981674  ...   59.530975\n",
      "  -147.97926     109.57997   ]\n",
      " [  -1.0457697     1.           -1.5980349  ...  196.00998\n",
      "  -327.33176     160.43602   ]\n",
      " [  -1.0462074     1.           -1.5977453  ...   -0.5638024\n",
      "    -1.3049235    -0.84164727]\n",
      " ...\n",
      " [  -1.0461006     1.           -1.5977955  ...    0.6658013\n",
      "    -9.406156      1.0107644 ]\n",
      " [  -1.0454102     1.           -1.5982685  ... -172.94638\n",
      "   175.33331     -76.258194  ]\n",
      " [  -1.0453691     1.           -1.5983524  ...  120.5693\n",
      "  -210.56572     111.95407   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 67, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-5.0083828e-01  1.0000000e+00 -1.8430462e+00 ...  2.0291687e+02\n",
      "  -8.2766914e+01 -2.6368994e+02]\n",
      " [-5.0097752e-01  1.0000000e+00 -1.8429356e+00 ... -2.3534393e+02\n",
      "   1.4553830e+01  1.9446259e+02]\n",
      " [-5.0147629e-01  1.0000000e+00 -1.8427947e+00 ...  4.3243033e-01\n",
      "   2.0703030e+00  6.1146915e-02]\n",
      " ...\n",
      " [-5.0135040e-01  1.0000000e+00 -1.8428249e+00 ...  3.7979881e+01\n",
      "  -1.1742872e+02 -5.1703342e+01]\n",
      " [-5.0056839e-01  1.0000000e+00 -1.8430958e+00 ... -7.0258514e+01\n",
      "   1.7329643e+01  1.2262447e+01]\n",
      " [-5.0052547e-01  1.0000000e+00 -1.8431473e+00 ...  2.2397124e+02\n",
      "   8.6401489e+01  4.8181774e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.1]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 68, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.33437347e-02  1.00000000e+00 -1.90756035e+00 ...  6.23351097e+01\n",
      "   7.35535431e+01  4.35833778e+01]\n",
      " [ 9.32006836e-02  1.00000000e+00 -1.90750027e+00 ... -8.66359787e+01\n",
      "  -1.07963524e+02  1.35702915e+01]\n",
      " [ 9.26952362e-02  1.00000000e+00 -1.90751290e+00 ...  1.63707972e+00\n",
      "   6.31655931e+00 -6.54557347e-01]\n",
      " ...\n",
      " [ 9.28230286e-02  1.00000000e+00 -1.90750694e+00 ... -8.28181076e+01\n",
      "   5.35881577e+01  1.81342453e+02]\n",
      " [ 9.36374664e-02  1.00000000e+00 -1.90754128e+00 ... -6.63204527e+00\n",
      "  -1.99454823e+01 -1.02696352e+01]\n",
      " [ 9.36670303e-02  1.00000000e+00 -1.90755653e+00 ...  1.72309372e+02\n",
      "  -2.12599503e+02  1.48804903e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 69, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 6.7849541e-01  1.0000000e+00 -1.7852268e+00 ...  1.1823523e+03\n",
      "   1.9724350e+01 -2.9030975e+02]\n",
      " [ 6.7836189e-01  1.0000000e+00 -1.7851744e+00 ...  5.1118862e+01\n",
      "  -8.1190590e+01  1.1526445e+01]\n",
      " [ 6.7789459e-01  1.0000000e+00 -1.7853640e+00 ... -2.3317065e+00\n",
      "  -3.6567292e+00  2.9858255e-01]\n",
      " ...\n",
      " [ 6.7801666e-01  1.0000000e+00 -1.7853155e+00 ...  8.9985161e+01\n",
      "   7.8151718e+01 -2.4341440e+01]\n",
      " [ 6.7875099e-01  1.0000000e+00 -1.7851276e+00 ...  1.2835629e+01\n",
      "  -2.1443859e+01 -2.5894590e+01]\n",
      " [ 6.7879581e-01  1.0000000e+00 -1.7851009e+00 ...  7.5797096e+01\n",
      "   1.3732112e+02  1.1088425e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 70, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.19658947e+00  1.00000000e+00 -1.48842430e+00 ...  2.49725250e+02\n",
      "  -6.57316895e+02  3.41310394e+02]\n",
      " [ 1.19647312e+00  1.00000000e+00 -1.48839474e+00 ...  2.85468018e+02\n",
      "  -1.15123474e+02  2.59118164e+02]\n",
      " [ 1.19611931e+00  1.00000000e+00 -1.48873043e+00 ... -2.76058745e+00\n",
      "  -5.95082521e+00  2.40836501e-01]\n",
      " ...\n",
      " [ 1.19622040e+00  1.00000000e+00 -1.48864651e+00 ... -2.06051326e+00\n",
      "   1.74714184e+01  4.45459862e+01]\n",
      " [ 1.19682884e+00  1.00000000e+00 -1.48828125e+00 ...  5.97700729e+01\n",
      "  -2.56124146e+02  3.65743774e+02]\n",
      " [ 1.19684315e+00  1.00000000e+00 -1.48821068e+00 ...  5.32268944e+01\n",
      "   5.63033867e+01  1.71758938e+00]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Step 71, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.5979509e+00  1.0000000e+00 -1.0452595e+00 ... -2.6926151e+02\n",
      "   5.5055676e+02  1.2326718e+03]\n",
      " [ 1.5978594e+00  1.0000000e+00 -1.0452414e+00 ...  1.8015276e+01\n",
      "   2.2331823e+01 -7.4415138e+01]\n",
      " [ 1.5976753e+00  1.0000000e+00 -1.0456963e+00 ...  3.3328617e+00\n",
      "   2.7291176e+00  3.4406686e-01]\n",
      " ...\n",
      " [ 1.5977325e+00  1.0000000e+00 -1.0455828e+00 ... -1.4904479e+01\n",
      "  -1.3866331e+02 -2.1020142e+02]\n",
      " [ 1.5981331e+00  1.0000000e+00 -1.0450821e+00 ...  5.5766815e+01\n",
      "  -4.0571575e+01 -6.0710724e+01]\n",
      " [ 1.5981388e+00  1.0000000e+00 -1.0449924e+00 ... -9.0443530e+02\n",
      "  -3.9727756e+02  2.0546348e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5        0.         0.70000005 0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Step 72, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.8426218e+00  1.0000000e+00 -5.0008965e-01 ... -2.2623976e+01\n",
      "   1.6956482e+02 -7.7297394e+01]\n",
      " [ 1.8425636e+00  1.0000000e+00 -5.0008965e-01 ...  7.2418292e+02\n",
      "  -9.9231042e+02 -2.5796503e+02]\n",
      " [ 1.8425274e+00  1.0000000e+00 -5.0062400e-01 ...  3.3953371e+00\n",
      "   6.1456509e+00  1.4068723e-02]\n",
      " ...\n",
      " [ 1.8425369e+00  1.0000000e+00 -5.0047207e-01 ... -4.4985788e+02\n",
      "   7.0532066e+01  8.3005341e+02]\n",
      " [ 1.8427086e+00  1.0000000e+00 -4.9987602e-01 ...  9.2533165e+01\n",
      "  -3.7332644e+00 -5.5845612e+01]\n",
      " [ 1.8427114e+00  1.0000000e+00 -4.9977303e-01 ... -3.5513330e+02\n",
      "  -1.0527234e+03  9.3086273e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 1.0000001 0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 73, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9069557e+00  1.0000000e+00  9.4074249e-02 ...  3.6343365e+02\n",
      "  -2.7499197e+01  2.9705139e+02]\n",
      " [ 1.9069328e+00  1.0000000e+00  9.4066620e-02 ... -2.4236055e+01\n",
      "   4.3290943e+01  7.8584000e+01]\n",
      " [ 1.9070339e+00  1.0000000e+00  9.3514048e-02 ...  5.4241462e+00\n",
      "   8.2680826e+00  9.5955014e-01]\n",
      " ...\n",
      " [ 1.9070034e+00  1.0000000e+00  9.3668938e-02 ... -1.2607960e+03\n",
      "   5.7050543e+02 -1.2763937e+02]\n",
      " [ 1.9069843e+00  1.0000000e+00  9.4289780e-02 ... -6.5568951e+02\n",
      "   7.5406873e+02 -1.0908604e+03]\n",
      " [ 1.9069328e+00  1.0000000e+00  9.4394684e-02 ... -1.3093106e+02\n",
      "  -6.0969090e+01  3.5709314e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.5       0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 74, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.7845440e+00  1.0000000e+00  6.7886162e-01 ...  2.8313837e+02\n",
      "  -2.7348345e+01  1.5695584e+01]\n",
      " [ 1.7845440e+00  1.0000000e+00  6.7885399e-01 ...  1.0192232e+02\n",
      "   1.7938922e+02 -1.6261276e+02]\n",
      " [ 1.7848225e+00  1.0000000e+00  6.7833710e-01 ...  1.2767999e+00\n",
      "   2.5304217e+00  3.9664507e-01]\n",
      " ...\n",
      " [ 1.7847481e+00  1.0000000e+00  6.7848492e-01 ... -2.9328067e+02\n",
      "   2.9244119e+03 -3.7615274e+02]\n",
      " [ 1.7845116e+00  1.0000000e+00  6.7906380e-01 ... -2.2480255e+02\n",
      "   2.1542940e+02 -1.4147287e+02]\n",
      " [ 1.7844229e+00  1.0000000e+00  6.7916107e-01 ...  1.6488596e+02\n",
      "  -1.2346610e+02  3.8765923e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        1.0000001 0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 75, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.4874344e+00  1.0000000e+00  1.1971264e+00 ...  5.5785625e+01\n",
      "   8.0332573e+01 -2.7772760e+02]\n",
      " [ 1.4874496e+00  1.0000000e+00  1.1971502e+00 ... -6.8086868e+01\n",
      "  -5.0817101e+01 -6.9021576e+01]\n",
      " [ 1.4878998e+00  1.0000000e+00  1.1967020e+00 ...  6.5927505e-03\n",
      "   3.6553202e+00  9.3290114e-01]\n",
      " ...\n",
      " [ 1.4877815e+00  1.0000000e+00  1.1968346e+00 ... -1.0763109e+03\n",
      "  -7.9780170e+02  4.4343536e+02]\n",
      " [ 1.4873829e+00  1.0000000e+00  1.1973019e+00 ... -1.6435219e+02\n",
      "  -1.4848367e+02 -1.5485887e+02]\n",
      " [ 1.4872360e+00  1.0000000e+00  1.1973782e+00 ... -1.8072479e+02\n",
      "  -2.7013422e+02  2.7328667e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 76, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.04446125e+00  1.00000000e+00  1.59816360e+00 ... -2.00492615e+02\n",
      "  -2.24608673e+02 -7.31915955e+02]\n",
      " [ 1.04448414e+00  1.00000000e+00  1.59814930e+00 ...  5.08887367e+01\n",
      "   1.44757874e+02  1.72262726e+02]\n",
      " [ 1.04501152e+00  1.00000000e+00  1.59785175e+00 ...  9.25547242e-01\n",
      "   6.25259352e+00 -9.73749161e-03]\n",
      " ...\n",
      " [ 1.04486275e+00  1.00000000e+00  1.59794140e+00 ... -5.73447205e+02\n",
      "  -6.64504578e+02  1.33727600e+02]\n",
      " [ 1.04432297e+00  1.00000000e+00  1.59829903e+00 ... -2.46896400e+01\n",
      "   9.00777512e+01 -2.74444336e+02]\n",
      " [ 1.04419327e+00  1.00000000e+00  1.59835052e+00 ... -1.15191704e+02\n",
      "  -1.00058685e+03  1.10066504e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.\n",
      " 0.  0. ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 77, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.99215126e-01  1.00000000e+00  1.84291649e+00 ... -6.54396057e-01\n",
      "  -7.61759949e+01 -1.17572260e+01]\n",
      " [ 4.99242783e-01  1.00000000e+00  1.84288692e+00 ...  1.04174385e+02\n",
      "  -3.34806824e+01  5.90477610e+00]\n",
      " [ 4.99860764e-01  1.00000000e+00  1.84274328e+00 ...  2.54426807e-01\n",
      "   6.86116791e+00 -3.79021943e-01]\n",
      " ...\n",
      " [ 4.99687195e-01  1.00000000e+00  1.84278393e+00 ...  1.79801355e+03\n",
      "  -3.52831787e+02  1.56868457e+03]\n",
      " [ 4.99067307e-01  1.00000000e+00  1.84297180e+00 ... -2.64933491e+01\n",
      "   1.41102552e+01  3.27345657e+01]\n",
      " [ 4.98906136e-01  1.00000000e+00  1.84298897e+00 ... -9.03229446e+01\n",
      "  -1.55825607e+02  2.76615204e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8000001 0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Step 78, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.4996452e-02  1.0000000e+00  1.9070587e+00 ... -6.1701748e+01\n",
      "  -8.8123398e+00  1.5377238e+01]\n",
      " [-9.4964027e-02  1.0000000e+00  1.9070463e+00 ...  8.8057165e+00\n",
      "   7.6801872e+00  8.8640032e+00]\n",
      " [-9.4329834e-02  1.0000000e+00  1.9070832e+00 ...  1.0867839e+00\n",
      "   2.5763302e+00 -9.1957383e-02]\n",
      " ...\n",
      " [-9.4507217e-02  1.0000000e+00  1.9070768e+00 ...  9.3027973e+00\n",
      "   9.3367300e+00 -2.6226664e+01]\n",
      " [-9.5148087e-02  1.0000000e+00  1.9070473e+00 ...  7.6925598e+02\n",
      "  -2.3492664e+03  4.1509827e+02]\n",
      " [-9.5312119e-02  1.0000000e+00  1.9070263e+00 ... -7.0409991e+02\n",
      "  -1.7447797e+02  1.9831494e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 79, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-6.7990971e-01  1.0000000e+00  1.7843475e+00 ...  3.2947969e-01\n",
      "   6.6496704e+01 -1.2986005e+02]\n",
      " [-6.7988014e-01  1.0000000e+00  1.7843676e+00 ... -8.8445656e+01\n",
      "  -5.8985040e+02  6.0648909e+00]\n",
      " [-6.7924690e-01  1.0000000e+00  1.7845674e+00 ...  8.1905502e-01\n",
      "  -8.3797369e+00  4.4179904e-01]\n",
      " ...\n",
      " [-6.7942238e-01  1.0000000e+00  1.7845383e+00 ...  3.4242867e+01\n",
      "   2.3257797e+01 -2.1212791e+01]\n",
      " [-6.8002510e-01  1.0000000e+00  1.7842865e+00 ...  6.2417310e+02\n",
      "   3.8953609e+00 -3.6773642e+02]\n",
      " [-6.8020344e-01  1.0000000e+00  1.7842159e+00 ...  1.0002745e+02\n",
      "   9.1359428e+01  5.4048663e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 80, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.1981888e+00  1.0000000e+00  1.4869061e+00 ... -1.1285302e+02\n",
      "  -9.2942383e+01  1.8670392e+00]\n",
      " [-1.1981678e+00  1.0000000e+00  1.4869404e+00 ...  2.4210582e+00\n",
      "  -6.0068893e+01 -7.4376764e+00]\n",
      " [-1.1976643e+00  1.0000000e+00  1.4873043e+00 ... -3.6635218e+00\n",
      "   1.2104790e+01 -8.8162494e-01]\n",
      " ...\n",
      " [-1.1978188e+00  1.0000000e+00  1.4872293e+00 ... -9.7756584e+01\n",
      "  -1.2340889e+01  5.3678371e+01]\n",
      " [-1.1983166e+00  1.0000000e+00  1.4867878e+00 ... -4.9364639e+02\n",
      "  -1.3033824e+03 -3.6299511e+01]\n",
      " [-1.1984282e+00  1.0000000e+00  1.4866886e+00 ... -1.7584854e+02\n",
      "  -5.7613155e+01  2.4306969e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 81, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.5990334e+00  1.0000000e+00  1.0439911e+00 ... -1.0199753e+03\n",
      "   4.3464996e+02  2.3703240e+03]\n",
      " [-1.5990210e+00  1.0000000e+00  1.0440493e+00 ...  6.7309564e+02\n",
      "   1.9768007e+02 -7.5409387e+02]\n",
      " [-1.5986671e+00  1.0000000e+00  1.0445255e+00 ... -3.3525469e+00\n",
      "   1.4942984e+01 -1.3791115e+00]\n",
      " ...\n",
      " [-1.5987816e+00  1.0000000e+00  1.0444155e+00 ...  3.6887020e+01\n",
      "   4.1056099e+01 -1.3491608e+02]\n",
      " [-1.5991344e+00  1.0000000e+00  1.0438347e+00 ... -3.8301041e+01\n",
      "  -1.2988481e+03  5.3274304e+02]\n",
      " [-1.5991993e+00  1.0000000e+00  1.0437126e+00 ... -2.1277274e+02\n",
      "  -2.7781561e+02 -7.9853798e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 82, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.8434277e+00  1.0000000e+00  4.9893951e-01 ...  7.2932632e+01\n",
      "  -1.2966012e+01 -1.1232595e+02]\n",
      " [-1.8434238e+00  1.0000000e+00  4.9895382e-01 ... -1.0100915e+01\n",
      "   7.6995720e+01  4.5664307e+01]\n",
      " [-1.8432198e+00  1.0000000e+00  4.9952546e-01 ...  1.5985845e+00\n",
      "  -4.6759934e+00 -3.0326191e-01]\n",
      " ...\n",
      " [-1.8432884e+00  1.0000000e+00  4.9938774e-01 ... -1.7637098e+03\n",
      "   1.1932528e+03  3.3109552e+02]\n",
      " [-1.8434753e+00  1.0000000e+00  4.9875069e-01 ... -6.5148047e+02\n",
      "   1.7255049e+02 -5.4457361e+02]\n",
      " [-1.8435030e+00  1.0000000e+00  4.9861526e-01 ...  6.9140190e+01\n",
      "   1.7411499e+01  7.3220276e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Step 83, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.9071522e+00  1.0000000e+00 -9.5314026e-02 ...  4.0824406e+01\n",
      "   2.0132040e+01 -2.8569775e+01]\n",
      " [-1.9071646e+00  1.0000000e+00 -9.5290184e-02 ... -2.3175276e+02\n",
      "  -3.2738998e+01 -1.9909046e+01]\n",
      " [-1.9071598e+00  1.0000000e+00 -9.4702847e-02 ... -1.8902941e+00\n",
      "   6.8616052e+00  5.5791730e-01]\n",
      " ...\n",
      " [-1.9071865e+00  1.0000000e+00 -9.4841003e-02 ... -4.4403952e+02\n",
      "   1.2125073e+03  1.9521785e+03]\n",
      " [-1.9071941e+00  1.0000000e+00 -9.5506668e-02 ...  2.6050644e+02\n",
      "  -9.3992577e+00  5.5401585e+01]\n",
      " [-1.9071312e+00  1.0000000e+00 -9.5645905e-02 ...  2.2989532e+02\n",
      "  -1.0250217e+02  1.7688406e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 84, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.784214      1.           -0.68008804 ...   96.95473\n",
      "   -76.261        15.862349  ]\n",
      " [  -1.7842388     1.           -0.68005085 ...  151.13228\n",
      "  -208.61562      98.814     ]\n",
      " [  -1.784399      1.           -0.679514   ...   -0.7633753\n",
      "     2.6564798     0.21086928]\n",
      " ...\n",
      " [  -1.78438       1.           -0.6796303  ... -151.10268\n",
      "   -35.09826    -115.92846   ]\n",
      " [  -1.7841797     1.           -0.68026924 ...   24.867798\n",
      "    23.920774     29.255241  ]\n",
      " [  -1.7840948     1.           -0.6803951  ...  122.93454\n",
      "   110.662025     56.365234  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 85, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[  -1.4865751    1.          -1.1984005 ...  -19.43639      9.009843\n",
      "     4.1883254]\n",
      " [  -1.4866114    1.          -1.1984053 ...   51.911583   160.22276\n",
      "  -126.361336 ]\n",
      " [  -1.4869804    1.          -1.1979612 ...   -3.4623532    5.73305\n",
      "     1.2489247]\n",
      " ...\n",
      " [  -1.4869118    1.          -1.1980476 ...  -88.83465   -400.41687\n",
      "   121.23017  ]\n",
      " [  -1.486496     1.          -1.1985607 ...  -30.256       53.2132\n",
      "     3.8214629]\n",
      " [  -1.4863777    1.          -1.1986465 ...   34.805195    -7.866674\n",
      "   -16.69037  ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 86, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-1.04338837e+00  1.00000000e+00 -1.59921837e+00 ...  3.22810791e+02\n",
      "   5.19406013e+01  1.12813126e+02]\n",
      " [-1.04344273e+00  1.00000000e+00 -1.59923744e+00 ...  1.58073624e+02\n",
      "   6.83184082e+02  7.68740112e+02]\n",
      " [-1.04395103e+00  1.00000000e+00 -1.59893990e+00 ... -3.87061119e-01\n",
      "   1.63908958e+00  1.40278816e-01]\n",
      " ...\n",
      " [-1.04385567e+00  1.00000000e+00 -1.59898853e+00 ...  3.05389984e+02\n",
      "   2.42848480e+02 -2.49134560e+01]\n",
      " [-1.04330444e+00  1.00000000e+00 -1.59934044e+00 ...  3.24820361e+03\n",
      "   8.31772156e+02 -2.17223709e+02]\n",
      " [-1.04313183e+00  1.00000000e+00 -1.59939384e+00 ...  7.03129349e+01\n",
      "   1.87772617e+01  1.22267609e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 87, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-4.98273849e-01  1.00000000e+00 -1.84333611e+00 ...  9.50406418e+01\n",
      "  -4.18286514e+00 -5.56889191e+01]\n",
      " [-4.98329163e-01  1.00000000e+00 -1.84341049e+00 ... -6.29824295e+01\n",
      "  -7.75115585e+01 -1.09635918e+02]\n",
      " [-4.98954773e-01  1.00000000e+00 -1.84327722e+00 ...  1.62784910e+00\n",
      "  -1.68360376e+00 -2.21510649e-01]\n",
      " ...\n",
      " [-4.98836517e-01  1.00000000e+00 -1.84329224e+00 ... -7.76626892e+01\n",
      "   1.33821518e+02 -1.05374622e+00]\n",
      " [-4.98180389e-01  1.00000000e+00 -1.84340858e+00 ...  6.20076721e+02\n",
      "   1.75057877e+02 -4.96421448e+02]\n",
      " [-4.97981071e-01  1.00000000e+00 -1.84341812e+00 ... -1.27942726e+02\n",
      "   7.64070435e+01 -5.23037758e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 88, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 9.6009254e-02  1.0000000e+00 -1.9071426e+00 ... -2.0193815e+01\n",
      "  -7.6690071e+01 -4.3127281e+01]\n",
      " [ 9.5953941e-02  1.0000000e+00 -1.9071922e+00 ...  1.9465050e+01\n",
      "   4.3342117e+01 -5.3486507e+01]\n",
      " [ 9.5376968e-02  1.0000000e+00 -1.9072629e+00 ...  2.1331067e+00\n",
      "  -1.7272778e+00  6.6260886e-01]\n",
      " ...\n",
      " [ 9.5499039e-02  1.0000000e+00 -1.9072237e+00 ...  5.6036678e+01\n",
      "  -4.0301456e+01  7.5956154e+01]\n",
      " [ 9.6176147e-02  1.0000000e+00 -1.9071598e+00 ...  6.7683173e+02\n",
      "   2.1606927e+02  5.8436072e+02]\n",
      " [ 9.6319199e-02  1.0000000e+00 -1.9071121e+00 ...  5.2266289e+03\n",
      "   1.3003584e+03 -4.0715981e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 89, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   0.6809015     1.           -1.7840729  ...   -6.0274396\n",
      "    74.896416    -76.51295   ]\n",
      " [   0.68084717    1.           -1.7841339  ...   37.777523\n",
      "   -42.992893    -17.424713  ]\n",
      " [   0.68032646    1.           -1.7843741  ...    1.3228302\n",
      "    -0.7451143     0.6455252 ]\n",
      " ...\n",
      " [   0.680439      1.           -1.7842922  ... -171.53076\n",
      "  -183.47345    -134.34212   ]\n",
      " [   0.68107605    1.           -1.7840385  ...  165.91856\n",
      "   547.0969       26.926828  ]\n",
      " [   0.68119335    1.           -1.7839279  ...  -95.10095\n",
      "   -33.157074     40.3776    ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Step 90, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.1986771e+00  1.0000000e+00 -1.4865761e+00 ... -1.9414328e+02\n",
      "  -3.1294910e+02 -2.3280464e+02]\n",
      " [ 1.1986256e+00  1.0000000e+00 -1.4866095e+00 ... -2.0701495e+02\n",
      "  -5.3484581e+01  1.3975372e+02]\n",
      " [ 1.1982021e+00  1.0000000e+00 -1.4870143e+00 ...  6.6578388e-01\n",
      "  -4.3370986e-01 -1.1276913e-01]\n",
      " ...\n",
      " [ 1.1982937e+00  1.0000000e+00 -1.4868851e+00 ... -3.2589610e+00\n",
      "   1.1395116e+02  9.5946442e+01]\n",
      " [ 1.1988297e+00  1.0000000e+00 -1.4864922e+00 ... -2.4697356e+02\n",
      "   9.9887749e+01  2.2932707e+02]\n",
      " [ 1.1989231e+00  1.0000000e+00 -1.4863453e+00 ... -1.4827213e+03\n",
      "  -5.9067297e+02 -1.0675507e+03]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 91, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.59961033e+00  1.00000000e+00 -1.04308128e+00 ... -4.38640228e+02\n",
      "  -2.91883907e+01  6.92516098e+01]\n",
      " [ 1.59956551e+00  1.00000000e+00 -1.04315186e+00 ...  7.02293604e+03\n",
      "   2.24938721e+03 -1.34017598e+04]\n",
      " [ 1.59929848e+00  1.00000000e+00 -1.04366255e+00 ...  2.54426956e-01\n",
      "  -4.85792637e-01  1.57452822e-02]\n",
      " ...\n",
      " [ 1.59935760e+00  1.00000000e+00 -1.04349041e+00 ... -1.69502609e+02\n",
      "  -5.50183289e+02 -3.36572815e+02]\n",
      " [ 1.59973335e+00  1.00000000e+00 -1.04295731e+00 ...  1.56274231e+02\n",
      "  -1.07836334e+02  2.97365448e+02]\n",
      " [ 1.59977913e+00  1.00000000e+00 -1.04277992e+00 ... -3.10625671e+02\n",
      "  -7.00365753e+01  7.09275208e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.         0.         0.         0.         0.         0.1\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70000005 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Step 92, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.84343910e+00  1.00000000e+00 -4.97669220e-01 ...  1.65022720e+02\n",
      "  -9.18863449e+01  8.68464966e+01]\n",
      " [ 1.84341049e+00  1.00000000e+00 -4.97724533e-01 ... -1.83845374e+03\n",
      "   1.77519543e+03  2.63308813e+03]\n",
      " [ 1.84331512e+00  1.00000000e+00 -4.98315930e-01 ...  8.35280418e-02\n",
      "  -1.98882580e-01  1.29370689e-02]\n",
      " ...\n",
      " [ 1.84333420e+00  1.00000000e+00 -4.98115540e-01 ... -4.44124817e+02\n",
      "   2.21238281e+03 -1.85125623e+03]\n",
      " [ 1.84353256e+00  1.00000000e+00 -4.97514725e-01 ... -1.33432755e+02\n",
      "   1.02123764e+02  4.56442657e+02]\n",
      " [ 1.84352303e+00  1.00000000e+00 -4.97303009e-01 ... -6.46479492e+02\n",
      "   8.43015930e+02  1.31365265e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.8000001\n",
      " 0.        0.        0.        0.        0.        0.       ]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Step 93, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 1.9068947e+00  1.0000000e+00  9.6654892e-02 ...  1.1319076e+02\n",
      "   4.0633324e+01 -6.0432678e+01]\n",
      " [ 1.9068880e+00  1.0000000e+00  9.6643448e-02 ... -2.3164825e+02\n",
      "  -4.1032657e+02  1.8457071e+02]\n",
      " [ 1.9069595e+00  1.0000000e+00  9.6024148e-02 ...  9.7055960e-01\n",
      "  -5.2233362e-01 -4.1013271e-02]\n",
      " ...\n",
      " [ 1.9069290e+00  1.0000000e+00  9.6240044e-02 ... -5.8080157e+02\n",
      "   5.0657309e+02 -1.5762250e+03]\n",
      " [ 1.9069157e+00  1.0000000e+00  9.6809387e-02 ... -6.8992267e+02\n",
      "   9.6446643e+02 -1.1765427e+03]\n",
      " [ 1.9068689e+00  1.0000000e+00  9.7030640e-02 ... -2.4507748e+02\n",
      "   3.0057980e+02  1.7403069e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Step 94, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.7837305     1.            0.68125916 ...  109.1755\n",
      "  -392.26056    -332.18835   ]\n",
      " [   1.7837362     1.            0.6812315  ...   10.593796\n",
      "   168.0635      168.4901    ]\n",
      " [   1.7839851     1.            0.6806538  ...    5.0006866\n",
      "    -3.4568253    -1.0346528 ]\n",
      " ...\n",
      " [   1.783905      1.            0.68086433 ...  140.53833\n",
      "    77.94209    -347.20407   ]\n",
      " [   1.7836952     1.            0.68139076 ...  -70.61799\n",
      "   -59.084057      9.371927  ]\n",
      " [   1.7835932     1.            0.68159676 ...   39.1106\n",
      "   -28.072367    -57.26829   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 95, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.4857798     1.            1.1992283  ...   67.30159\n",
      "  -172.87932    -306.65167   ]\n",
      " [   1.485795      1.            1.1992216  ... -218.88252\n",
      "  -175.6713       69.94128   ]\n",
      " [   1.4861946     1.            1.1987274  ...    4.377374\n",
      "    -3.2489774    -0.38503405]\n",
      " ...\n",
      " [   1.486063      1.            1.1989117  ...   -4.342758\n",
      "   345.13724     -21.421537  ]\n",
      " [   1.4856796     1.            1.1993427  ...  155.11183\n",
      "   -29.973843     47.282257  ]\n",
      " [   1.4855356     1.            1.1995258  ...   56.224117\n",
      "   -57.33838      42.68746   ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Hand Exit\n",
      "Forearm\n",
      "Step 96, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[   1.0425425    1.           1.5997295 ...   47.9301      71.55247\n",
      "   -43.56666  ]\n",
      " [   1.0425653    1.           1.5997162 ...   16.996065    -4.052074\n",
      "    81.44106  ]\n",
      " [   1.0430927    1.           1.5993626 ...    3.248904    -4.86775\n",
      "    -2.359386 ]\n",
      " ...\n",
      " [   1.0429211    1.           1.5994864 ...   72.58326    261.15543\n",
      "  -128.8608   ]\n",
      " [   1.0423965    1.           1.5998058 ...  -10.043483     2.4713733\n",
      "   -35.26143  ]\n",
      " [   1.0422249    1.           1.5999336 ...  -41.627613   -37.277634\n",
      "   -24.386955 ]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 97, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[ 4.9684143e-01  1.0000000e+00  1.8434772e+00 ...  1.8514778e+03\n",
      "   1.4897989e+01  2.2995063e+03]\n",
      " [ 4.9686718e-01  1.0000000e+00  1.8434496e+00 ...  4.0050925e+02\n",
      "   1.9649450e+01 -5.0526768e+01]\n",
      " [ 4.9747849e-01  1.0000000e+00  1.8432745e+00 ...  1.9624834e+00\n",
      "  -4.7875333e+00 -7.5490868e-01]\n",
      " ...\n",
      " [ 4.9728775e-01  1.0000000e+00  1.8433475e+00 ...  2.5361922e+02\n",
      "   5.8742943e+01 -2.8660516e+02]\n",
      " [ 4.9666214e-01  1.0000000e+00  1.8434982e+00 ...  7.1180702e+01\n",
      "  -8.9635611e+00 -4.6043057e+00]\n",
      " [ 4.9648285e-01  1.0000000e+00  1.8435707e+00 ...  4.1218132e+01\n",
      "  -2.6570044e+02  6.2291725e+01]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Step 98, Decision steps: 20, Terminal steps: 0\n",
      "Next state vector:  [[-9.7586632e-02  1.0000000e+00  1.9068317e+00 ... -1.0153255e+01\n",
      "   1.0922070e+01 -1.2904924e+01]\n",
      " [-9.7557068e-02  1.0000000e+00  1.9068346e+00 ...  9.2717670e+02\n",
      "   1.6599783e+02 -5.2927838e+02]\n",
      " [-9.6918106e-02  1.0000000e+00  1.9068545e+00 ... -1.0463185e+00\n",
      "   1.9644732e+00 -2.4565387e+00]\n",
      " ...\n",
      " [-9.7122192e-02  1.0000000e+00  1.9068565e+00 ... -7.3228783e+01\n",
      "   2.1988214e+01 -1.1944787e+02]\n",
      " [-9.7761154e-02  1.0000000e+00  1.9068165e+00 ...  2.4542805e+02\n",
      "  -4.2593732e+02  2.5065578e+02]\n",
      " [-9.7953796e-02  1.0000000e+00  1.9068279e+00 ...  3.4708899e+02\n",
      "  -5.6665460e+02 -4.4000992e+02]]\n",
      "rewards type:  <class 'numpy.ndarray'>\n",
      "rewards shape:  (20,)\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Episode finished:  [False False False False False False False False False False False False\n",
      " False False False False False False False False]\n",
      "Step 99, Decision steps: 0, Terminal steps: 20\n",
      "All agents are in terminal states at step 99. Ending episode early.\n",
      "Episode:10, Score:2.98, Best Score:2.98, Average Score:1.66, Best Avg Score:1.72\n",
      "\n",
      "Total training time = 3.3 min\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent \n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from workspace_utils import active_session\n",
    "import os\n",
    "\n",
    "# env_name = \"/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux/robotics_reaching_exe_linux.x86_64\" # Path to robotics reaching exe\n",
    "env_name = \"/root/DDPG/DDPG_MMC/robotics_reaching_executable_linux_no_log/robotics_reaching_exe_linux.x86_64\" # Path to robotics reaching exe wihtout script debugging\n",
    "\n",
    "# Ensure the executable has the necessary permissions \n",
    "os.chmod(env_name, 0o755)\n",
    "\n",
    "try: \n",
    "    # Launch unity environment\n",
    "    env = UnityEnvironment(file_name=env_name,seed=1, side_channels=[], worker_id=1)\n",
    "\n",
    "    # Start the environment \n",
    "    env.reset()\n",
    "\n",
    "    # Get behaviour names \n",
    "    behaviour_names = env.behavior_specs.keys()\n",
    "\n",
    "    # Check that behaviour names have been retrieved from the environment\n",
    "    if not behaviour_names:\n",
    "        print(\"No behaviours found. Ensure that the unity environment has agents with behaviours\")\n",
    "    else:\n",
    "        behaviour_name = list(env.behavior_specs.keys())[0]\n",
    "        print(f\"Behaviour name: {behaviour_name}\")\n",
    "\n",
    "        # Get what actions the environment expects and the required shape\n",
    "        behaviour_spec = env.behavior_specs[behaviour_name]\n",
    "        print(f\"Behaviour specifications: {behaviour_spec.action_spec}\")\n",
    "\n",
    "        # Get the number of agents \n",
    "        decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "        num_agents = len(decisionSteps) + len(terminalSteps)\n",
    "        print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing environment: {e}\")\n",
    "    behavior_name = None\n",
    "\n",
    "\n",
    "agent = Agent(state_size=52, action_size=4, random_seed=2) # Altered from origional to fit new environment\n",
    "\n",
    "def ddpg(n_episodes=200, max_t=1000):\n",
    "    \n",
    "    print(\"Enter ddpg...\\n\")\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    actions = []\n",
    "    best_score = 0\n",
    "    best_average_score = 0\n",
    "    # try:\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        print(f\"Episode number: {i_episode}\")\n",
    "        avg_score = 0\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset()\n",
    "\n",
    "        #get the decision and terminal steps\n",
    "        decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "        print(\"Printing decisionSteps: \" )\n",
    "        print(decisionSteps)\n",
    "        print(type(decisionSteps))\n",
    "\n",
    "        # get number of agents\n",
    "        num_agents = len(decisionSteps)\n",
    "        print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "        # get number of continuous actions\n",
    "        num_continuous_actions = env.behavior_specs[behaviour_name].action_spec.continuous_size\n",
    "\n",
    "        # create 2D numpy array of continuous actions \n",
    "        continuous_actions = np.random.rand(num_agents, num_continuous_actions).astype(np.float32)\n",
    "\n",
    "        # create actiontuple\n",
    "        action_tuple = ActionTuple(continuous=continuous_actions)\n",
    "\n",
    "        # get the states vector\n",
    "        stateVector = decisionSteps.obs[0]\n",
    "\n",
    "        #init score agents\n",
    "        scores_agents = np.zeros(num_agents)\n",
    "        print(\"scores_agents type: \", type(scores_agents))\n",
    "        print(\"scores_agents shape: \", scores_agents.shape)\n",
    "\n",
    "        score = 0\n",
    "        agent.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "\n",
    "            try:\n",
    "                # Checkpoint to ensure it's not getting stuck\n",
    "                if t % 100 == 0: \n",
    "                    print(f\"Progressing at step {t}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "            # set the actions for the behaviour and step the environment\n",
    "            env.set_actions(behavior_name=behaviour_name, action=action_tuple)\n",
    "\n",
    "            # Step the environment to get the next states \n",
    "            env.step()\n",
    "\n",
    "            # get the next states\n",
    "            decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "            print(f\"Step {t}, Decision steps: {len(decisionSteps)}, Terminal steps: {len(terminalSteps)}\")\n",
    "\n",
    "            # Check if all agents are in terminal state\n",
    "            if len(decisionSteps)==0 and len(terminalSteps)>0:\n",
    "                print(f\"All agents are in terminal states at step {t}. Ending episode early.\")\n",
    "                break\n",
    "\n",
    "            # extract the next states vector from the decision steps \n",
    "            next_state_vector = decisionSteps.obs[0]\n",
    "            print(\"Next state vector: \", next_state_vector)\n",
    "\n",
    "            # get the rewards\n",
    "            rewards = decisionSteps.reward\n",
    "            print(\"rewards type: \", type(rewards))\n",
    "            print(\"rewards shape: \", rewards.shape)\n",
    "            print(\"rewards: \", rewards)\n",
    "\n",
    "            episode_finished = np.array([len(terminalSteps) > 0] * num_agents) # episode_fiished values must be passed into agent.step function as an array\n",
    "            print(\"Episode finished: \", episode_finished)\n",
    "\n",
    "            # see if episode has finished\n",
    "            if next_state_vector is not None: \n",
    "                agent.step(stateVector, actions, rewards, next_state_vector, episode_finished)\n",
    "                stateVector = next_state_vector\n",
    "            #Check if scores-agents and rewards are compatible for addition\n",
    "            scores_agents = np.add(scores_agents, rewards)\n",
    "            # scores_agents += rewards\n",
    "            if np.any(episode_finished):\n",
    "                break\n",
    "\n",
    "        # mean score of 20 agents in this episode\n",
    "        score = np.mean(scores_agents)\n",
    "        scores_deque.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        scores.append(score)\n",
    "\n",
    "        #refresh the best agent score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "        #refresh the best average score    \n",
    "        if avg_score > best_average_score:\n",
    "            best_average_score = avg_score\n",
    "        \n",
    "        #print current episode\n",
    "        print(\"Episode:{}, Score:{:.2f}, Best Score:{:.2f}, Average Score:{:.2f}, Best Avg Score:{:.2f}\".format(\n",
    "            i_episode, score, best_score, avg_score, best_average_score))\n",
    "        if (avg_score >= 32):\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_solved.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_solved.pth')\n",
    "            break\n",
    "    # finally:\n",
    "        # Save the solved paths \n",
    "    torch.save(agent.actor_local.state_dict(), 'actor_solved.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), 'critic_solved.pth')\n",
    "        # env.close() # Ensure env.close() is alwyas called    \n",
    "    return scores\n",
    "\n",
    "start = time.time()\n",
    "with active_session():\n",
    "    scores = ddpg()\n",
    "end = time.time()\n",
    "print('\\nTotal training time = {:.1f} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot of the training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing using the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 52\n",
      "action size: 4\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 52)\n",
      "Upperarm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Hand Exit\n",
      "Upperarm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "HandAgent\n",
      "Hand Collided with CUBE\n",
      "Forearm\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "ElbowBend\n",
      "Forearm\n",
      "Hand Exit\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Forearm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(20, 52)\n",
      "Forearm\n",
      "Upperarm\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "Upperarm\n",
      "Hand Exit\n",
      "Forearm\n",
      "(20, 52)\n",
      "Hand Exit\n",
      "(20, 52)\n",
      "(0, 52)\n",
      "Total score (averaged over agents) this episode: 1.9800001792609692\n"
     ]
    }
   ],
   "source": [
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "env.reset() # reset the unity environment\n",
    "behaviour_name = list(env.behavior_specs.keys())[0] # get behviour name\n",
    "decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name) #get the decision and terminal steps\n",
    "stateVector = decisionSteps.obs[0] # get the current state for each agent\n",
    "agent = Agent(state_size=52, action_size=4, random_seed=2)\n",
    "num_agents = len(decisionSteps) + len(terminalSteps)# get the number of agents\n",
    "scores = np.zeros(num_agents) # initialize the score for each agent \n",
    "agent.actor_local.load_state_dict(torch.load('actor_solved.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('critic_solved.pth'))\n",
    "agent.reset()\n",
    "\n",
    "# Get behavior specs\n",
    "behavior_specs = env.behavior_specs[behaviour_name]\n",
    "action_size = behavior_specs.action_spec.continuous_size\n",
    "\n",
    "print(f\"state size: {state_size}\")\n",
    "print(f\"action size: {action_size}\")\n",
    "\n",
    "while True:\n",
    "    num_continuous_actions = env.behavior_specs[behaviour_name].action_spec.continuous_size # get number of continuous actions\n",
    "    continuous_actions = np.random.rand(num_agents, num_continuous_actions).astype(np.float32) # select actions \n",
    "    action_tuple = ActionTuple(continuous=continuous_actions) # create action tuple for continuous actions \n",
    "    env.set_actions(behavior_name=behaviour_name, action=action_tuple) #send the actions to the environemnt\n",
    "    env.step() # step the environment\n",
    "\n",
    "    decisionSteps, terminalSteps = env.get_steps(behavior_name=behaviour_name)\n",
    "    next_state_vector = decisionSteps.obs[0] # get next state vector\n",
    "\n",
    "    rewards = decisionSteps.reward # get rewards \n",
    "\n",
    "    episode_finished = np.array([len(terminalSteps) > 0] * num_agents) # episode_fiished values must be passed into agent.step function as an array\n",
    "    if rewards.shape[0] == num_agents: \n",
    "        scores = np.add(scores, rewards) # update the score\n",
    "    if next_state_vector is not None: # roll over states to next timestep \n",
    "        print(next_state_vector.shape)\n",
    "        agent.step(stateVector, actions, rewards, next_state_vector, episode_finished)\n",
    "        stateVector = next_state_vector\n",
    "    if np.any(episode_finished): # Exit loop if episode finshed \n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forearm\n",
      "Hand Exit\n",
      "Forearm\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 1422249 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 128 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 134.6 KB\n",
      "      Overflow Count 4\n",
      "    [ALLOC_TEMP_Background Job.Worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 6\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 11\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 199 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.Worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.Worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 27\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [8.0 MB-16.0 MB]: 1422250 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 12.8 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 1422250 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 1422249 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 65.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 1422250 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 64.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 1422250 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [256.0 KB-0.5 MB]: 1422249 frames, [1.0 MB-2.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 1.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.2 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 1422250 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 96 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [0-1.0 KB]: 1422250 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 0\n",
      "      Peak Allocated memory 0 B\n",
      "      Peak Large allocation bytes 0 B\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DDPG Docker)",
   "language": "python",
   "name": "ddpg_docker_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
